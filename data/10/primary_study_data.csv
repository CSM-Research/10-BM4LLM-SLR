Secondary study DOI,Primary study Scopus EID,Primary study DOI,Title,Abstract,Keywords,Publication venue,Publication date,Publication type,Authors,Ground truth,Reason
10.1016/j.jss.2022.111515,2-s2.0-84855465840,10.1109/ASE.2011.6100046,"DC2: A framework for scalable, scope-bounded software verification","Software model checking and static analysis have matured over the last decade, enabling their use in automated software verification. However, lack of scalability makes these tools hard to apply. Furthermore, approximations in the models of program and environment lead to a profusion of false alarms. This paper proposes DC2, a verification framework using scope-bounding to bridge these gaps. DC2 splits the analysis problem into manageable parts, relying on a combination of three automated techniques: (a) techniques to infer useful specifications for functions in the form of pre- and post-conditions; (b) stub inference techniques that infer abstractions to replace function calls beyond the verification scope; and (c) automatic refinement of pre- and post-conditions from false alarms identified by a user. DC2 enables iterative reasoning over the calling environment, to help in finding non-trivial bugs and fewer false alarms. We present an experimental evaluation that demonstrates the effectiveness of DC2 on several open-source and industrial software projects. © 2011 IEEE.",,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Ivančić, Franjo;Balakrishnan, Gogul;Gupta, Aarti;Sankaranarayanan, Sriram;Maeda, Naoto;Tokuoka, Hiroki;Imoto, Takashi;Miyazaki, Yoshiaki",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855417563,10.1109/ASE.2011.6100058,Iterative mining of resource-releasing specifications,"Software systems commonly use resources such as network connections or external file handles. Once finish using the resources, the software systems must release these resources by explicitly calling specific resource-releasing API methods. Failing to release resources properly could result in resource leaks or even outright system failures. Existing verification techniques could analyze software systems to detect defects related to failing to release resources. However, these techniques require resource-releasing specifications for specifying which API method acquires/releases certain resources, and such specifications are not well documented in practice, due to the large amount of manual effort required to document them. To address this issue, we propose an iterative mining approach, called RRFinder, to automatically mining resource-releasing specifications for API libraries in the form of (resource-acquiring, resource-releasing) API method pairs. RRFinder first identifies resource-releasing API methods, for which RRFinder then identifies the corresponding resource-acquiring API methods. To identify resource-releasing API methods, RRFinder performs an iterative process including three steps: model-based prediction, call-graph-based propagation, and class-hierarchy-based propagation. From heterogeneous information (e.g., source code, natural language), the model-based prediction employs a classification model to predict the likelihood that an API method is a resource-releasing method. The call-graph-based and class-hierarchy-based propagation propagates the likelihood information across methods. We evaluated RRFinder on eight open source libraries, and the results show that RRFinder achieved an average recall of 94.0% with precision of 86.6% in mining resource-releasing specifications, and the mined specifications are useful in detecting resource leak defects. © 2011 IEEE.",resource leak detection | resource-releasing specification | specification mining,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Wu, Qian;Liang, Guangtai;Wang, Qianxiang;Xie, Tao;Mei, Hong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855424476,10.1109/ASE.2011.6100061,Towards more accurate retrieval of duplicate bug reports,"In a bug tracking system, different testers or users may submit multiple reports on the same bugs, referred to as duplicates, which may cost extra maintenance efforts in triaging and fixing bugs. In order to identify such duplicates accurately, in this paper we propose a retrieval function (REP) to measure the similarity between two bug reports. It fully utilizes the information available in a bug report including not only the similarity of textual content in summary and description fields, but also similarity of non-textual fields such as product, component, version, etc. For more accurate measurement of textual similarity, we extend BM25F - an effective similarity formula in information retrieval community, specially for duplicate report retrieval. Lastly we use a two-round stochastic gradient descent to automatically optimize REP for specific bug repositories in a supervised learning manner. We have validated our technique on three large software bug repositories from Mozilla, Eclipse and OpenOffice. The experiments show 10-27% relative improvement in recall rate@k and 17-23% relative improvement in mean average precision over our previous model. We also applied our technique to a very large dataset consisting of 209,058 reports from Eclipse, resulting in a recall rate@k of 37-71% and mean average precision of 47%. © 2011 IEEE.",,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Sun, Chengnian;Lo, David;Khoo, Siau Cheng;Jiang, Jing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855461930,10.1109/ASE.2011.6100069,Finding relevant answers in software forums,"Online software forums provide a huge amount of valuable content. Developers and users often ask questions and receive answers from such forums. The availability of a vast amount of thread discussions in forums provides ample opportunities for knowledge acquisition and summarization. For a given search query, current search engines use traditional information retrieval approach to extract webpages containing relevant keywords. However, in software forums, often there are many threads containing similar keywords where each thread could contain a lot of posts as many as 1,000 or more. Manually finding relevant answers from these long threads is a painstaking task to the users. Finding relevant answers is particularly hard in software forums as: complexities of software systems cause a huge variety of issues often expressed in similar technical jargons, and software forum users are often expert internet users who often posts answers in multiple venues creating many duplicate posts, often without satisfying answers, in the world wide web. To address this problem, this paper provides a semantic search engine framework to process software threads and recover relevant answers according to user queries. Different from standard information retrieval engine, our framework infer semantic tags of posts in the software forum threads and utilize these tags to recover relevant answer posts. In our case study, we analyze 6,068 posts from three software forums. In terms of accuracy of our inferred tags, we could achieve on average an overall precision, recall and F-measure of 67%, 71%, and 69% respectively. To empirically study the benefit of our overall framework, we also conduct a user-assisted study which shows that as compared to a standard information retrieval approach, our proposed framework could increase mean average precision from 17% to 71% in retrieving relevant answers to various queries and achieve a Normalized Discounted Cumulative Gain (nDCG) @1 score of 91.2% and nDCG@2 score of 71.6%. © 2011 IEEE.",,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Gottipati, Swapna;Lo, David;Jiang, Jing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855421322,10.1109/ASE.2011.6100086,AutoODC: Automated generation of Orthogonal Defect Classifications,"Orthogonal Defect Classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human intensive and requires experts' knowledge of both ODC and system domains. This paper presents AutoODC, an approach and tool for automating ODC classification by casting it as a supervised text classification problem. Rather than merely apply the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts' ODC experience and domain knowledge into the learning process via proposing a novel Relevance Annotation Framework. We evaluated AutoODC on an industrial defect report from the social network domain. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves an overall accuracy of 80.2% when using manual classifications as a basis of comparison. © 2011 IEEE.",natural language processing | Orthogonal Defect Classification (ODC) | text classification,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Huang, Li Guo;Ng, Vincent;Persing, Isaac;Geng, Ruili;Bai, Xu;Tian, Jeff",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855422163,10.1109/ASE.2011.6100090,Using model checking to analyze static properties of declarative models,"We show how static properties of declarative models can be efficiently analyzed in a symbolic model checker; in particular, we use Cadence SMV to analyze Alloy models by translating Alloy to SMV. The computational paths of the SMV models represent interpretations of the Alloy models. The produced SMV model satisfies its LTL specifications if and only if the original Alloy model is inconsistent with respect to its finite scopes; counterexamples produced by the model checker are valid instances of the Alloy model. Our experiments show that the translation of many frequently used constructs of Alloy to SMV results in optimized models such that their analysis in SMV is much faster than in the Alloy Analyzer. Model checking is faster than SAT solving for static problems when an interpretation can be eliminated by early decisions in the model checking search. © 2011 IEEE.",,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Vakili, Amirhossein;Day, Nancy A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855452278,10.1109/ASE.2011.6100091,Finding the merits and drawbacks of software resources from comments,"In order to reuse software resources efficiently, developers need necessary quality guarantee on software resources. However, our investigation proved that most software resources on the Internet did not provide enough quality descriptions. In this paper, we propose an approach to help developers judge a software resource's quality based on comments. In our approach, the software resources' comments on the Internet are automatically collected, the sentiment polarity (positive or negative) of a comment is identified and the quality aspects which the comment talks about are extracted. As a result, the merits and drawbacks of software resources are drew out which could help developers judge a software resource's quality in the process of software resource selection and reuse. To evaluate our approach, we applied our method to a group of open source software and the results showed that our method achieved satisfying precision in merits and drawbacks finding. © 2011 IEEE.",Sentiment Polarity | Software Quality | Software Resource | Software Reuse | User Comment,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Liu, Changsheng;Zou, Yanzhen;Cai, Sibo;Xie, Bing;Mei, Hong",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84855446808,10.1109/ASE.2011.6100098,Analyzing temporal API usage patterns,"Software reuse through Application Programming Interfaces (APIs) is an integral part of software development. As developers write client programs, their understanding and usage of APIs change over time. Can we learn from long-term changes in how developers work with APIs in the lifetime of a client program? We propose Temporal API Usage Mining to detect significant changes in API usage. We describe a framework to extract detailed models representing addition and removal of calls to API methods over the change history of a client program. We apply machine learning technique to these models to semi-automatically infer temporal API usage patterns, i.e., coherent addition of API calls at different phases in the life-cycle of the client program. © 2011 IEEE.",API Usability | API Usage | Mining Software Repositories | Software Reuse | Usage Pattern,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Uddin, Gias;Dagenais, Barthélémy;Robillard, Martin P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855471774,10.1109/ASE.2011.6100100,A case for alloy annotations for efficient incremental analysis via domain specific solvers,"Alloy is a declarative modelling language based on first-order logic with sets and relations. Alloy formulas are checked for satisfiability by the fully automatic Alloy Analyzer. The analyzer, given an Alloy formula and a scope, i.e. a bound on the universe of discourse, searches for an instance i.e. a valuation to the sets and relations in the formula, such that it evaluates to true. The analyzer translates the Alloy problem to a propositional formula for which it searches a satisfying assignment via an off-the-shelf propositional satisfiability (SAT) solver. The SAT solver performs an exhaustive search and increasing the scope leads to the combinatorial explosion problem. We envision annotations, a meta-data facility used in imperative languages, as a means of augmenting Alloy models to enable more efficient analysis by specifying the priority, i.e. order of solving, of a given constraint and the slover to be used. This additional information would enable using the solutions to a particular constraint as partial solutions to the next in case constraint priority is specified and using a specific solver for reasoning about a given constraint in case a constraint solver is specified. © 2011 IEEE.",,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Ganov, Svetoslav;Khurshid, Sarfraz;Perry, Dewayne E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855458675,10.1109/ASE.2011.6100105,Run-time systems failure prediction via proactive monitoring,"In run-time evolving systems, components may evolve while the system is being operated. Unsafe run-time changes may compromise the correct execution of the entire system. Traditional design-time verification techniques difficultly cope with run-time changes, and run-time monitoring may detect disfunctions only too late, when the failure arises. The desire would be to define advanced monitors with the ability to predict and prevent the potential errors happening in the future. In this direction, this paper proposes CASSANDRA, a new approach that by combining design-time and run-time analysis techniques, can ""look ahead"" in the near execution future, and predict potential failures. During run-time we on-the-fly construct a model of the future k-step global state space according to design-time specifications and the current execution state. Consequently, we can run-time check whether possible failures might happen in the future. © 2011 IEEE.",Component-based Software Engineering | Failure Prediction | Proactive run-time Monitoring | Software Evolution,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Zhang, Pengcheng;Muccini, Henry;Polini, Andrea;Li, Xuandong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855425046,10.1109/ASE.2011.6100133,APIExample: An effective web search based usage example recommendation system for java APIs,"Programmers often learn how to use an API by studying its usage examples. There are many usage examples scattered in web pages on the Internet. However, it often takes programmers much effort to find out the desired examples from a large number of web pages by web search. This paper proposes a tool named APIExample that can extract usage examples for java APIs from web pages on the Internet and recommend them to programmers. Given a java API, the tool collects its related web pages from the Internet, extracts java code snippets and their surrounding descriptive texts embedded in the pages, then assembles them into usage examples for programmers. Furthermore, in order to help programmers capture more kinds of usages of the target API by browsing fewer examples, our tool clusters and ranks the listed examples based on the target API's usage. Besides, as a practical tool, APIExample provides multiple aspects of frequently-used information about using the target API in a concise user interface with friendly user experience. Two kinds of user-interaction style, a web search portal and an Eclipse plug-in, are now both publicly available. © 2011 IEEE.",API | recommendation | usage example | web search,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Wang, Lijie;Fang, Lu;Wang, Leye;Li, Ge;Xie, Bing;Yang, Fuqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84855468752,10.1109/ASE.2011.6100151,Automatic assessment of software documentation quality,"In this paper I describe the concept of my dissertation, which aims at adapting the Continuous Quality Monitoring Method (CQMM) for the automated quality assessment of software documentation using a developed document quality analysis framework and software document quality rules which represent best practices for software documentation. © 2011 IEEE.",continuous quality monitoring | quality model | software documentation | tool-based approach,"2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings",2011-12-01,Conference Paper,"Dautovic, Andreas",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79953107914,10.1145/1950365.1950377,Specifying and checking semantic atomicity for multithreaded programs,"In practice, it is quite difficult to write correct multithreaded programs due to the potential for unintended and nondeterministic interference between parallel threads. A fundamental correctness property for such programs is atomicity-a block of code in a program is atomic if, for any parallel execution of the program, there is an execution with the same overall program behavior in which the block is executed serially. We propose semantic atomicity, a generalization of atomicity with respect to a programmer-defined notion of equivalent behavior. We propose an assertion framework in which a programmer can use bridge predicates to specify noninterference properties at the level of abstraction of their application. Further, we propose a novel algorithm for systematically testing atomicity specifications on parallel executions with a bounded number of interruptions- i.e. atomic blocks whose execution is interleaved with that of other threads. We further propose a set of sound heuristics and optional user annotations that increase the efficiency of checking atomicity specifications in the common case where the specifications hold. We have implemented our assertion framework for specifying and checking semantic atomicity for parallel Java programs, and we have written semantic atomicity specifications for a number of benchmarks. We found that using bridge predicates allowed us to specify the natural and intended atomic behavior of a wider range of programs than did previous approaches. Further, in checking our specifications, we found several previously unknown bugs, including in the widely-used java.util.concurrent library. Copyright © 2011 ACM.",Algorithms | Reliability | Verification,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2011-03-31,Conference Paper,"Burnim, Jacob;Necula, George;Sen, Koushik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953069556,10.1145/1950365.1950396,S2E: A platform for in-vivo multi-path analysis of software systems,"This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each. S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S 2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/precision trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack-user programs, libraries, kernel, drivers, etc.-instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software. Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and one can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API. Copyright © 2011 ACM.",Performance | Reliability | Security | Verification,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2011-03-31,Conference Paper,"Chipounov, Vitaly;Kuznetsov, Volodymyr;Candea, George",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953070187,10.1145/1950365.1950398,Ensuring operating system kernel integrity with OSck,"Kernel rootkits that modify operating system state to avoid detection are a dangerous threat to system security. This paper presents OSck, a system that discovers kernel rootkits by detecting malicious modifications to operating system data. OSck integrates and extends existing techniques for detecting rootkits, and verifies safety properties for large portions of the kernel heap with minimal overhead. We deduce type information for verification by analyzing unmodified kernel source code and in-memory kernel data structures. High-performance integrity checks that execute concurrently with a running operating system create data races, and we demonstrate a deterministic solution for ensuring kernel memory is in a consistent state. We introduce two new classes of kernel rootkits that are undetectable by current systems, motivating the need for the OSck API that allows kernel developers to conveniently specify arbitrary integrity properties. Copyright © 2011 ACM.",Rootkit detection,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2011-03-31,Conference Paper,"Hofmann, Owen S.;Dunn, Alan M.;Kim, Sangman;Roy, Indrajit;Witchel, Emmett",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953089159,10.1145/1950365.1950404,Synthesizing concurrent schedulers for irregular algorithms,"Scheduling is the assignment of tasks or activities to processors for execution, and it is an important concern in parallel programming. Most prior work on scheduling has focused either on static scheduling of applications in which the dependence graph is known at compile-time or on dynamic scheduling of independent loop iterations such as in OpenMP. In irregular algorithms, dependences between activities are complex functions of runtime values so these algorithms are not amenable to compile-time analysis nor do they consist of independent activities. Moreover, the amount of work can vary dramatically with the scheduling policy. To handle these complexities, implementations of irregular algorithms employ carefully handcrafted, algorithm-specific schedulers but these schedulers are themselves parallel programs, complicating the parallel programming problem further. In this paper, we present a flexible and efficient approach for specifying and synthesizing scheduling policies for irregular algorithms. We develop a simple compositional specification language and show how it can concisely encode scheduling policies in the literature. Then, we show how to synthesize efficient parallel schedulers from these specifications. We evaluate our approach for five irregular algorithms on three multicore architectures and show that (1) the performance of some algorithms can improve by orders of magnitude with the right scheduling policy, and (2) for the same policy, the overheads of our synthesized schedulers are comparable to those of fixed-function schedulers. Copyright © 2011 ACM.",Amorphous data-parallelism | Irregular programs | Multicore processors | Optimistic parallelization | Program synthesis | Scheduling,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2011-03-31,Conference Paper,"Nguyen, Donald;Pingali, Keshav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960369467,10.1007/978-3-642-22110-1_21,Symbolic algorithms for qualitative analysis of Markov decision processes with Büchi objectives,"We consider Markov decision processes (MDPs) with ω-regular specifications given as parity objectives. We consider the problem of computing the set of almost-sure winning states from where the objective can be ensured with probability 1. The algorithms for the computation of the almost-sure winning set for parity objectives iteratively use the solutions for the almost-sure winning set for Büchi objectives (a special case of parity objectives). Our contributions are as follows: First, we present the first subquadratic symbolic algorithm to compute the almost-sure winning set for MDPs with Büchi objectives; our algorithm takes O(n·√m) symbolic steps as compared to the previous known algorithm that takes O(n2) symbolic steps, where n is the number of states and m is the number of edges of the MDP. In practice MDPs often have constant out-degree, and then our symbolic algorithm takes O(n·√n) symbolic steps, as compared to the previous known O(n2) symbolic steps algorithm. Second, we present a new algorithm, namely win-lose algorithm, with the following two properties: (a) the algorithm iteratively computes subsets of the almost-sure winning set and its complement, as compared to all previous algorithms that discover the almost-sure winning set upon termination; and (b) requires O(n·√K) symbolic steps, where K is the maximal number of edges of strongly connected components (scc's) of the MDP. The win-lose algorithm requires symbolic computation of scc's. Third, we improve the algorithm for symbolic scc computation; the previous known algorithm takes linear symbolic steps, and our new algorithm improves the constants associated with the linear number of steps. In the worst case the previous known algorithm takes 5·n symbolic steps, whereas our new algorithm takes 4·n symbolic steps. © 2011 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-07-20,Conference Paper,"Chatterjee, Krishnendu;Henzinger, Monika;Joglekar, Manas;Shah, Nisarg",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960380351,10.1007/978-3-642-22110-1_43,Formalization and automated verification of RESTful behavior,"REST is a software architectural style used for the design of highly scalable web applications. Interest in REST has grown rapidly over the past decade, spurred by the growth of open web APIs. On the other hand, there is also considerable confusion surrounding REST: many examples of supposedly RESTful APIs violate key REST constraints. We show that the constraints of REST and of RESTful HTTP can be precisely formulated within temporal logic. This leads to methods for model checking and run-time verification of RESTful behavior. We formulate several relevant verification questions and analyze their complexity. © 2011 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-07-20,Conference Paper,"Klein, Uri;Namjoshi, Kedar S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960374829,10.1007/978-3-642-22110-1_44,Linear completeness thresholds for bounded model checking,"Bounded model checking is a symbolic bug-finding method that examines paths of bounded length for violations of a given LTL formula. Its rapid adoption in industry owes much to advances in SAT technology over the past 10-15 years. More recently, there have been increasing efforts to apply SAT-based methods to unbounded model checking. One such approach is based on computing a completeness threshold: a bound k such that, if no counterexample of length k or less to a given LTL formula is found, then the formula in fact holds over all infinite paths in the model. The key challenge lies in determining sufficiently small completeness thresholds. In this paper, we show that if the Büchi automaton associated with an LTL formula is cliquey, i.e., can be decomposed into clique-shaped strongly connected components, then the associated completeness threshold is linear in the recurrence diameter of the Kripke model under consideration. We moreover establish that all unary temporal logic formulas give rise to cliquey automata, and observe that this group includes a vast range of specifications used in practice, considerably strengthening earlier results, which report manageable thresholds only for elementary formulas of the form Fp and Gq . © 2011 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-07-20,Conference Paper,"Kroening, Daniel;Ouaknine, Joël;Strichman, Ofer;Wahl, Thomas;Worrell, James",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960344307,10.1007/978-3-642-22110-1_45,Interpolation-based software verification with Wolverine,"Wolverine is a software verification tool using Craig interpolation to compute invariants of ANSI-C and C++ programs. The tool is an implementation of the lazy abstraction approach, generating a reachability tree by unwinding the transition relation of the input program and annotating its nodes with interpolants representing safe states. Wolverine features a built-in interpolating decision procedure for equality logic with uninterpreted functions which provides limited support for bit-vector operations. In addition, it provides an API enabling the integration of other interpolating decision procedures, making it a valuable source of benchmarks and allowing it to take advantage of the continuous performance improvements of SMT solvers. We evaluate the performance of Wolverine by comparing it to the predicate abstraction-based verifier SatAbs on a number of verification conditions of Linux device drivers. © 2011 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-07-20,Conference Paper,"Kroening, Daniel;Weissenbacher, Georg",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960378440,10.1007/978-3-642-22110-1_46,Synthesizing biological theories,"Studies of biological systems are often facilitated by diagram ""models"" that summarize the current understanding of underlying mechanisms. The increasing complexity of our understanding of biology necessitates computational models that can extend these representations to include their dynamic behavior. We present here a new tool we call Synthesizing Biological Theories which enables biologists and modelers to construct high-level theories and models of biological systems, capturing biological hypotheses, inferred mechanisms, and experimental results within the same framework. Among the key features of the tool are convenient ways to represent several competing theories and the interactive nature of building and running the models using an intuitive, rigorous scenario-based visual language. The definition of the modeling language is geared towards enabling formal verification and analysis. © 2011 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-07-20,Conference Paper,"Kugler, Hillel;Plock, Cory;Roberts, Andy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960369704,10.1007/978-3-642-22110-1_54,Analyzing unsynthesizable specifications for high-level robot behavior using LTLMoP,"Recent work in robotics has applied formal verification tools to automatically generate correct-by-construction controllers for autonomous robots. However, when it is not possible to create such a controller, these approaches do not provide the user with feedback on the source of failure, making the experience of debugging a specification somewhat ad hoc and unstructured, and a source of frustration for the user. This paper describes an extension to the LTLMoP toolkit for robot mission planning that encloses the control-generation process in a layer of automated reasoning to identify the cause of failure, and targets the user's attention to flawed portions of the specification. © 2011 Springer-Verlag.",GR(1) | LTL | robot control | synthesis | unrealizability | unsatisfiability,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-07-20,Conference Paper,"Raman, Vasumathi;Kress-Gazit, Hadas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80155122734,10.1145/1978802.1978811,Coverage problems in sensor networks: A survey,"Sensor networks, which consist of sensor nodes each capable of sensing environment and transmitting data, have lots of applications in battlefield surveillance, environmental monitoring, industrial diagnostics, etc. Coverage which is one of the most important performance metrics for sensor networks reflects how well a sensor field is monitored. Individual sensor coverage models are dependent on the sensing functions of different types of sensors, while network-wide sensing coverage is a collective performance measure for geographically distributed sensor nodes. This article surveys research progress made to address various coverage problems in sensor networks. We first provide discussions on sensor coverage models and design issues. The coverage problems in sensor networks can be classified into three categories according to the subject to be covered. We state the basic coverage problems in each category, and review representative solution approaches in the literature. We also provide comments and discussions on some extensions and variants of these basic coverage problems. © 2011 ACM.",Coverage | Network coverage control | Network protocol design | Sensor coverage model | Sensor networks,ACM Computing Surveys,2011-10-01,Article,"Wang, Bang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82955242465,10.1049/ic.2011.0014,An empirical evaluation of requirement engineering techniques for collaborative systems,"A collaborative system is a distributed software which allows several users to work together and carry out collaboration, communication and coordination tasks. To perform these tasks, the users have to be aware of other user's actions, usually by means of a set of awareness techniques. When we are defining a collaborative system, the awareness techniques can be considered as non-functional requirements bounded to some quality factors, such as usability. However, serious flaws can be found during the specification of these systems if we use the usual Requirement Engineering techniques available, because their expressiveness limitations when dealing with non-functional requirements. In this paper an empirical evaluation is introduced to determine if these techniques are really appropriate to model groupware requirements and which is the best approach to specify this kind of systems. With this aim, a collaborative text editor is used to evaluate whether the current techniques for Requirement Engineering are appropriated or not, exploiting the relation between awareness capabilities and standard quality factors.",,IET Seminar Digest,2011-12-12,Conference Paper,"Teruel, M. A.;Navarro, E.;López-Jaquero, V.;Montero, F.;González, P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80052475889,10.1007/s10664-010-9152-6,Using grounded theory to study the experience of software development,"Grounded Theory is a research method that generates theory from data and is useful for understanding how people resolve problems that are of concern to them. Although the method looks deceptively simple in concept, implementing Grounded Theory research can often be confusing in practice. Furthermore, despite many papers in the social science disciplines and nursing describing the use of Grounded Theory, there are very few examples and relevant guides for the software engineering researcher. This paper describes our experience using classical (i.e., Glaserian) Grounded Theory in a software engineering context and attempts to interpret the canons of classical Grounded Theory in a manner that is relevant to software engineers.We providemodel to help the software engineering researchers interpret the often fuzzy definitions found in Grounded Theory texts and share our experience and lessons learned during our research. We summarize these lessons learned in a set of fifteen guidelines. © Springer Science+Business Media, LLC 2011.",Empirical software engineering research | Grounded theory | Qualitative research | Theory generation,Empirical Software Engineering,2011-08-01,Article,"Adolph, S.;Hall, W.;Kruchten, P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953175945,10.1007/s10664-010-9150-8,A field study of API learning obstacles,"Large APIs can be hard to learn, and this can lead to decreased programmer productivity. But what makes APIs hard to learn? We conducted a mixed approach, multi-phased study of the obstacles faced by Microsoft developers learning a wide variety of new APIs. The study involved a combination of surveys and in-person interviews, and collected the opinions and experiences of over 440 professional developers. We found that some of the most severe obstacles faced by developers learning new APIs pertained to the documentation and other learning resources. We report on the obstacles developers face when learning new APIs, with a special focus on obstacles related to API documentation. Our qualitative analysis elicited five important factors to consider when designing API documentation: documentation of intent; code examples; matching APIs with scenarios; penetrability of the API; and format and presentation. We analyzed how these factors can be interpreted to prioritize API documentation development efforts © 2010 Springer Science+Business Media, LLC.",Application programming interfaces | Documentation | Programming | Software libraries,Empirical Software Engineering,2011-12-01,Article,"Robillard, Martin P.;Deline, Robert",Include,
10.1016/j.jss.2022.111515,2-s2.0-84858730781,10.1109/esem.2011.22,How good is your comment? A study of comments in java programs,"Comments are very useful to developers during maintenance tasks and are useful as well to help structuring a code at development time. They convey useful information about the system functionalities as well as the state of mind of a developer. Comments in code have been the focus of several studies, but none of them was targeted at analyzing commenting habits precisely. In this paper, we present an empirical study which analyzes existing comments in different open source Java projects. We study comments from both a quantitative and a qualitative point of view. We propose a taxonomy of comments that we used for conducting our analysis. © 2011 IEEE.",Comment content | Comment distribution | Comment frequency | Comment relevance,International Symposium on Empirical Software Engineering and Measurement,2011-01-01,Conference Paper,"Haouari, Dorsaf;Sahraoui, Houari;Langlais, Philippe",Include,
10.1016/j.jss.2022.111515,2-s2.0-84858725378,10.1109/esem.2011.25,Experimental analysis of textual and graphical representations for software architecture design,"Software architecture design documentation should communicate design decisions effectively. However, little is known about the way recipients respond to the different types of media used in documentation. We therefore conducted a controlled experiment to study whether visual or textual artifacts are more effective in communicating architecture software design decisions to software developers. Our participant group consisted of 47 participants from both industry and academia. Our results show that neither diagrams nor textual descriptions proved to be significantly more efficient in terms of communicating software architecture design decisions. Remarkably, participants who predominantly used text, scored significantly better; overall and with respect to topology related questions. Furthermore, surprisingly, diagrams were not able to alleviate the difficulties participants with a native language other than English had in extracting information from the documentation. In combination, these findings at the very least question the role of diagrams in software architecture documentation. © 2011 IEEE.",Controlled experiment | Global software development | Software architecture design | Textual notation | Visual notation,International Symposium on Empirical Software Engineering and Measurement,2011-01-01,Conference Paper,"Heijstek, Werner;Kühne, Thomas;Chaudron, Michel R.V.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84858735899,10.1109/esem.2011.49,Scaling scrum in a large distributed project,"This paper presents a currently ongoing single case study on adopting and scaling Scrum in a large software development project distributed across four sites. The data was gathered by 19 semi-structured interviews of project personnel, including managers, architects, developers and testers. At the time of the interviews the project had grown in size during the past 2,5 years from two collocated Scrum teams to 20 teams located in four countries and employing over 170 persons. In this paper we first describe our research approach and goals. Then we briefly summarize the preliminary results of this ongoing study: we explain how Scrum practices were scaled, as well as discuss the successes and challenges experienced when adopting the agile practices and scaling them, while at the same time growing the project size at a fast pace. Even though this project has been very successful from the business point of view, it has experienced a lot of problems in applying Scrum, especially related to scaling the agile practices. Thus, it seems that adapting Scrum practices to work well in a large distributed setting is challenging. © 2011 IEEE.",Agile software development | Global software development | Scaling scrum,International Symposium on Empirical Software Engineering and Measurement,2011-01-01,Conference Paper,"Paasivaara, Maria;Lassenius, Casper",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80053183055,10.1145/2025113.2025125,Modeling the HTML DOM and browser API in static analysis of JavaScript Web Applications,"Developers of JavaScript web applications have little tool support for catching errors early in development. In comparison, an abundance of tools exist for statically typed languages, including sophisticated integrated development environments and specialized static analyses. Transferring such technologies to the domain of JavaScript web applications is challenging. In this paper, we discuss the challenges, which include the dynamic aspects of JavaScript and the complex interactions between JavaScript, HTML, and the browser. From this, we present the first static analysis that is capable of reasoning about the flow of control and data in modern JavaScript applications that interact with the HTML DOM and browser API. One application of such a static analysis is to detect type-related and dataflow-related programming errors. We report on experiments with a range of modern web applications, including Chrome Experiments and IE Test Drive applications, to measure the precision and performance of the technique. The experiments indicate that the analysis is able to show absence of errors related to missing object properties and to identify dead and unreachable code. By measuring the precision of the types inferred for object properties, the analysis is precise enough to show that most expressions have unique types. By also producing precise call graphs, the analysis additionally shows that most invocations in the programs are monomorphic. We furthermore study the usefulness of the analysis to detect spelling errors in the code. Despite the encouraging results, not all problems are solved and some of the experiments indicate a potential for improvement, which allows us to identify central remaining challenges and outline directions for future work. © 2011 ACM.",,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Jensen, Simon Holm;Madsen, Magnus;Møller, Anders",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053195771,10.1145/2025113.2025129,Effective communication of software development knowledge through community portals,"Knowledge management plays an important role in many software organizations. Knowledge can be captured and distributed using a variety of media, including traditional help files and manuals, videos, technical articles, wikis, and blogs. In recent years, web-based community portals have emerged as an important mechanism for combining various communication channels. However, there is little advice on how they can be effectively deployed in a software project. In this paper, we present a first study of a community portal used by a closed source software project. Using grounded theory, we develop a model that characterizes documentation artifacts along several dimensions, such as content type, intended audience, feedback options, and review mechanisms. Our findings lead to actionable advice for industry by articulating the benefits and possible shortcomings of the various communication channels in a knowledge-sharing portal. We conclude by suggesting future research on the increasing adoption of community portals in software engineering projects. © 2011 ACM.",Community portal | Documentation | Knowledge,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Treude, Christoph;Storey, Margaret Anne",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053206751,10.1145/2025113.2025140,ADDiff: Semantic differencing for activity diagrams,"Activity diagrams (ADs) have recently become widely used in the modeling of workflows, business processes, and web-services, where they serve various purposes, from documentation, requirement definitions, and test case specifications, to simulation and code generation. As models, programs, and systems evolve over time, understanding changes and their impact is an important challenge, which has attracted much research efforts in recent years. In this paper we present addiff, a semantic differencing operator for ADs. Unlike most existing approaches to model comparison, which compare the concrete or the abstract syntax of two given diagrams and output a list of syntactical changes or edit operations, addiff considers the semantics of the diagrams at hand and outputs a set of diff witnesses, each of which is an execution trace that is possible in the first AD and is not possible in the second. We motivate the use of addiff, formally define it, and show two algorithms to compute it, a concrete forward-search algorithm and a symbolic fixpoint algorithm, implemented using BDDs and integrated into the Eclipse IDE. Empirical results and examples demonstrate the feasibility and unique contribution of addiff to the state-of-the-art in version comparison and evolution analysis. © 2011 ACM.",Activity diagrams | Differencing | Software evolution,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Maoz, Shahar;Ringert, Jan Oliver;Rumpe, Bernhard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053181755,10.1145/2025113.2025160,Boosting the performance of flow-sensitive points-to analysis using value flow,"Points-to analysis is a fundamental static analysis technique which computes the set of memory objects that a pointer may point to. Many different applications, such as security-related program analyses, bug checking, and analyses of multi-threaded programs, require precise points-to information to be effective. Recent work has focused on improving the precision of points-to analysis through flow-sensitivity and great progress has been made. However, even with all recent progress, flow-sensitive points-to analysis can still be much slower than a flow-insensitive analysis. In this paper, we propose a novel method that simplifies flow-sensitive points-to analysis to a general graph reachability problem in a value flow graph. The value flow graph summarizes dependencies between pointer variables, including those memory dependencies via pointer dereferences. The points-to set for each pointer variable can then be computed as the set of memory objects that can reach it in the graph. We develop an algorithm to build the value flow graph efficiently by examining the pointed-to-by set of a memory object, i.e., the set of pointers that point to an object. The pointed-to-by information of memory objects is very useful for applications such as escape analysis, and information flow analysis. Our approach is intuitive, easy to implement and very efficient. The implementation is around 2000 lines of code and it is more efficient than existing flow-sensitive points-to analyses. The runtime is comparable with the state-of-the-art flow-insensitive points-to analysis. © 2011 ACM.",,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Li, Lian;Cifuentes, Cristina;Keynes, Nathan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053177824,10.1145/2025113.2025166,Reputation-based self-management of software process artifact quality in consortium research projects,"This paper proposes a PhD research that deals with improving internal documentation in software projects. Software developers often do not like to create documentation because it has few value to the individual himself. Yet the purpose of internal documentation is to help others understand the software and the problem it addresses. Documentation increases development speed, reduces software maintenance costs, helps to keep development on track, and mitigates the negative effects of distance in distributed settings. This research aims to increase the individuals'motivation to write documentation by means of reputation. The CollabReview prototype is a web-based reputation system that analyzes artifacts of internal documentation to derive personal reputation scores. Developers making many good contributions will achieve higher reputation scores. These scores can then be employed to softly influence developer behavior, e.g. by incentivizing them to contribute to documentation with social rewards. No strict rule enforcement is necessary. © 2011 ACM.",Collaboration | Documentation | Reputation | Software quality,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Prause, Christian R.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80053204237,10.1145/2025113.2025167,An architecture-centric approach for goal-driven requirements elicitation,"Software system development typically starts from a requirement specification followed by stepwise refinement of available requirements while transferring them into the system architecture. However, the granularity and the amount of requirements to be elicited for a successful architectural design are not well understood. This paper proposes a process concept to support system development with the help of an architecture-centric approach for goal-driven requirements elicitation. The process focuses on multiple quality dimensions, such as performance, reliability and scalability, and at the same time shall reduce costs and risks through early decision evaluation. The main contribution of this paper is a novel process where not only requirements can drive architectural design, but also architectural design can selectively drive requirement elicitation with the help of hypotheses connected to the selected architectural solutions. The paper concludes with a discussion on its possible empirical validation. © 2011 ACM.",Architectural decisions | Architectural modelling | Architectural solutions | Design patterns | Development process | Requirements elicitation | Software architecture,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Durdik, Zoya",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80053210640,10.1145/2025113.2025193,PSPWizard: Machine-assisted definition of temporal logical properties with specification patterns,"Model checking provides a powerful means to assert and verify desired system properties. But, for the verification process to become feasible, a correct formulation of these properties in a temporal logic is necessary - a potential barrier to application in practice. Research on property specification has supplied us with rich pattern catalogs that capture commonly occurring system properties in different temporal logics. Furthermore, these property specification pattern catalogs usually offer both a structured English grammar to facilitate the pattern selection and an associated template solutions to express the properties formally. Yet, the actual use of property specification patterns remains cumbersome, due to limited tool support. For this reason, we have developed the Property Specification Pattern Wizard (PSPWizard), a framework that defines an interface for the currently accepted property specification pattern libraries. PSPWizard consists of two main building blocks: a mapping generator that weaves a given pattern library with a target logic and a GUI front-end to the structured English grammar tailored to those patterns that are supported in the target logic. © 2011 ACM.",Specification patterns | Structured english grammar | Temporal logic,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-01-01,Conference Paper,"Lumpe, Markus;Meedeniya, Indika;Grunske, Lars",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80053190674,10.1145/2025113.2025195,Using social media to study the diversity of example usage among professional developers,"Socio-professional websites such as LinkedIn use various mechanisms such as network of colleagues, groups and discussions to assist their users in maintaining their professional network and keeping up with relevant discussions. Software professionals post hundreds of thousands of comments each week in these group discussions regarding technological and methodological aspects of their work. Analyzing these comments enables us, the software community at large, to better understand the state of the practice of many aspects of software development. In this paper we describe a case study in which we use such LinkedIn group discussion to learn about developers'perception of using code examples. We show how the discussion participants' online profile could be used for research purposes and to provide the data with additional context. Our preliminary findings suggest that although many developers advocate the usage of code examples in their work, many of them use examples in a limited way. We also show that some developers oppose using examples altogether. The implications of this work are twofold; we pose research questions and suggest future work both in order to leverage further the tools offered by social media websites, as well as to explore further aspects involved in example usage. © 2011 ACM.",Example usage | LinkedIn | Social media | Virtual focus group,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Barzilay, Ohad;Hazzan, Orit;Yehudai, Amiram",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80053203354,10.1145/2025113.2025208,Workshop on assurances for self-adaptive systems (ASAS 2011),"Assurances for Self-Adaptive Systems (ASAS) is a workshop that will bring together researchers to discuss software engineering aspects of self-adaptive systems, including methods, architectures, languages, algorithms, techniques, and tools that can be used to support assurances in self-adaptive system development. ASAS is intended as a complement to the efforts started a while ago at the successful FSE workshop series on Self-Healing Systems (WOSS), or the Software Engineering for Adaptive and Self-Managing Systems (SEAMS) symposium. However, in contrast with those events, ASAS is focused on the collection, storage, and analysis of evidence for the provision of assurances that a self-adaptive software system is able to behave functionally and non-functionally according to its specification. © 2011 ACM.",Adaptive software | Assurances | Run-time verification | Stochastic analysis | Testing,SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software Engineering,2011-09-30,Conference Paper,"Cámara, Javier;De Lemos, Rogério;Ghezzi, Carlo;Lopes, Antónia",Exclude,Not a full paper
10.1016/j.jss.2022.111515,,10.1145/2034773.2034775,,,,,,,,Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80054087154,10.1145/2034773.2034785,Pushdown flow analysis of first-class control,"Pushdown models are better than control-flow graphs for higher-order flow analysis. They faithfully model the call/return structure of a program, which results in fewer spurious flows and increased precision. However, pushdown models require that calls and returns in the analyzed program nest properly. As a result, they cannot be used to analyze language constructs that break call/return nesting such as generators, coroutines, call/cc, etc. In this paper, we extend the CFA2 flow analysis to create the first pushdown flow analysis for languages with first-class control. We modify the abstract semantics of CFA2 to allow continuations to escape to, and be restored from, the heap. We then present a summarization algorithm that handles escaping continuations via a new kind of summary edge. We prove that the algorithm is sound with respect to the abstract semantics. Copyright © 2011 ACM.",First-class continuations | Higher-order functional language | Pushdown flow analysis | Restricted continuation-passing style | Summarization,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Vardoulakis, Dimitrios;Shivers, Olin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054083183,10.1145/2034773.2034804,Deriving an efficient FPGA implementation of a Low Density Parity Check Forward error corrector,"Creating correct hardware is hard. Though there is much talk of using formal and semi-formal methods to develop designs and implementations, in practice most implementations are written without the support of any formal or semi-formal methodology. Having such a methodology brings many benefits, including improved likelihood of a correct implementation, lowering the cost of design exploration and lowering the cost of certification. In this paper, we introduce a semi-formal methodology for connecting executable specifications written in the functional language Haskell to efficient VHDL implementations. The connection is performed by manual edits, using semi-formal equational reasoning facilitated by the worker/wrapper transformation, and directed using commutable functors. We explain our methodology on a full-scale example, an efficient Low-Density Parity Check forward error correcting code, which has been implemented on a Virtex-5 FPGA. Copyright © 2011 ACM.",DSL | Error correction | Lava | LDPC,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Gill, Andy;Farmer, Andrew",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054062487,10.1145/2034773.2034814,Forest: A language and toolkit for programming with filestores,"A filestore is a structured collection of data files housed in a conventional hierarchical file system. Many applications use filestores as a poor-man's database, and the correct execution of these applications requires that the collection of files, directories, and symbolic links stored on disk satisfy a variety of precise invariants. Moreover, all of these structures must have acceptable ownership, permission, and timestamp attributes. Unfortunately, current programming languages do not provide support for documenting assumptions about filestores, detecting errors in them, or safely loading from and storing to them. This paper describes the design, implementation, and semantics of Forest, a new domain-specific language for describing filestores. The language uses a type-based metaphor to specify the expected structure, attributes, and invariants of filestores. Forest generates loading and storing functions that make it easy to connect data on disk to an isomorphic representation in memory that can be manipulated as if it were any other data structure. Forest also generates metadata that describes the degree to which the structures on the disk conform to the specification, making error detection easy. In a nutshell, Forest extends the rigorous discipline of typed programming languages to the untyped world of file systems. We have implemented Forest as an embedded domain-specific language in Haskell. In addition to generating infrastructure for reading, writing and checking file systems, our implementation generates type class instances that make it easy to build generic tools that operate over arbitrary filestores.We illustrate the utility of this infrastructure by building a file system visualizer, a file access checker, a generic query interface, description-directed variants of several standard UNIX shell tools and (circularly) a simple Forest description inference engine. Finally, we formalize a core fragment of Forest in a semantics inspired by classical tree logics and prove round-tripping laws showing that the loading and storing functions behave sensibly. Copyright © 2011 ACM.",Ad hoc data | Bidirectional transformations | Data description languages | Domain-specific languages | File systems | Filestores | Generic programming | Haskell,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Fisher, Kathleen;Foster, Nate;Walker, David;Zhu, Kenny Q.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054069455,10.1145/2034773.2034818,Binders UNBOUND,"Implementors of compilers, program refactorers, theorem provers, proof checkers, and other systems that manipulate syntax know that dealing with name binding is difficult to do well. Operations such as α-equivalence and capture-avoiding substitution seem simple, yet subtle bugs often go undetected. Furthermore, their implementations are tedious, requiring "" boilerplate"" code that must be updated whenever the object language definition changes. Many researchers have therefore sought to specify binding syntax declaratively, so that tools can correctly handle the details behind the scenes. This idea has been the inspiration for many new systems (such as Beluga, Delphin, FreshML, FreshOCaml, Cαml, FreshLib, and Ott) but there is still room for improvement in expressivity, simplicity and convenience. In this paper, we present a new domain-specific language, UNBOUND, for specifying binding structure. Our language is particularly expressive-it supports multiple atom types, pattern binders, type annotations, recursive binders, and nested binding (necessary for telescopes, a feature found in dependently-typed languages). However, our specification language is also simple, consisting of just five basic combinators. We provide a formal semantics for this language derived from a locally nameless representation and prove that it satisfies a number of desirable properties. We also present an implementation of our binding specification language as a GHC Haskell library implementing an embedded domain specific language (EDSL). By using Haskell type constructors to represent binding combinators, we implement the EDSL succinctly using datatype-generic programming. Our implementation supports a number of features necessary for practical programming, including flexibility in the treatment of user-defined types, besteffort name preservation (for error messages), and integration with Haskell's monad transformer library. Copyright © 2011 ACM.",Generic programming | Haskell | Name binding | Patterns,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Weirich, Stephanie;Yorgey, Brent A.;Sheard, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2034773.2034775,,,,,,,,Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80054087154,10.1145/2034773.2034785,Pushdown flow analysis of first-class control,"Pushdown models are better than control-flow graphs for higher-order flow analysis. They faithfully model the call/return structure of a program, which results in fewer spurious flows and increased precision. However, pushdown models require that calls and returns in the analyzed program nest properly. As a result, they cannot be used to analyze language constructs that break call/return nesting such as generators, coroutines, call/cc, etc. In this paper, we extend the CFA2 flow analysis to create the first pushdown flow analysis for languages with first-class control. We modify the abstract semantics of CFA2 to allow continuations to escape to, and be restored from, the heap. We then present a summarization algorithm that handles escaping continuations via a new kind of summary edge. We prove that the algorithm is sound with respect to the abstract semantics. Copyright © 2011 ACM.",First-class continuations | Higher-order functional language | Pushdown flow analysis | Restricted continuation-passing style | Summarization,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Vardoulakis, Dimitrios;Shivers, Olin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054083183,10.1145/2034773.2034804,Deriving an efficient FPGA implementation of a Low Density Parity Check Forward error corrector,"Creating correct hardware is hard. Though there is much talk of using formal and semi-formal methods to develop designs and implementations, in practice most implementations are written without the support of any formal or semi-formal methodology. Having such a methodology brings many benefits, including improved likelihood of a correct implementation, lowering the cost of design exploration and lowering the cost of certification. In this paper, we introduce a semi-formal methodology for connecting executable specifications written in the functional language Haskell to efficient VHDL implementations. The connection is performed by manual edits, using semi-formal equational reasoning facilitated by the worker/wrapper transformation, and directed using commutable functors. We explain our methodology on a full-scale example, an efficient Low-Density Parity Check forward error correcting code, which has been implemented on a Virtex-5 FPGA. Copyright © 2011 ACM.",DSL | Error correction | Lava | LDPC,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Gill, Andy;Farmer, Andrew",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054062487,10.1145/2034773.2034814,Forest: A language and toolkit for programming with filestores,"A filestore is a structured collection of data files housed in a conventional hierarchical file system. Many applications use filestores as a poor-man's database, and the correct execution of these applications requires that the collection of files, directories, and symbolic links stored on disk satisfy a variety of precise invariants. Moreover, all of these structures must have acceptable ownership, permission, and timestamp attributes. Unfortunately, current programming languages do not provide support for documenting assumptions about filestores, detecting errors in them, or safely loading from and storing to them. This paper describes the design, implementation, and semantics of Forest, a new domain-specific language for describing filestores. The language uses a type-based metaphor to specify the expected structure, attributes, and invariants of filestores. Forest generates loading and storing functions that make it easy to connect data on disk to an isomorphic representation in memory that can be manipulated as if it were any other data structure. Forest also generates metadata that describes the degree to which the structures on the disk conform to the specification, making error detection easy. In a nutshell, Forest extends the rigorous discipline of typed programming languages to the untyped world of file systems. We have implemented Forest as an embedded domain-specific language in Haskell. In addition to generating infrastructure for reading, writing and checking file systems, our implementation generates type class instances that make it easy to build generic tools that operate over arbitrary filestores.We illustrate the utility of this infrastructure by building a file system visualizer, a file access checker, a generic query interface, description-directed variants of several standard UNIX shell tools and (circularly) a simple Forest description inference engine. Finally, we formalize a core fragment of Forest in a semantics inspired by classical tree logics and prove round-tripping laws showing that the loading and storing functions behave sensibly. Copyright © 2011 ACM.",Ad hoc data | Bidirectional transformations | Data description languages | Domain-specific languages | File systems | Filestores | Generic programming | Haskell,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Fisher, Kathleen;Foster, Nate;Walker, David;Zhu, Kenny Q.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054069455,10.1145/2034773.2034818,Binders UNBOUND,"Implementors of compilers, program refactorers, theorem provers, proof checkers, and other systems that manipulate syntax know that dealing with name binding is difficult to do well. Operations such as α-equivalence and capture-avoiding substitution seem simple, yet subtle bugs often go undetected. Furthermore, their implementations are tedious, requiring "" boilerplate"" code that must be updated whenever the object language definition changes. Many researchers have therefore sought to specify binding syntax declaratively, so that tools can correctly handle the details behind the scenes. This idea has been the inspiration for many new systems (such as Beluga, Delphin, FreshML, FreshOCaml, Cαml, FreshLib, and Ott) but there is still room for improvement in expressivity, simplicity and convenience. In this paper, we present a new domain-specific language, UNBOUND, for specifying binding structure. Our language is particularly expressive-it supports multiple atom types, pattern binders, type annotations, recursive binders, and nested binding (necessary for telescopes, a feature found in dependently-typed languages). However, our specification language is also simple, consisting of just five basic combinators. We provide a formal semantics for this language derived from a locally nameless representation and prove that it satisfies a number of desirable properties. We also present an implementation of our binding specification language as a GHC Haskell library implementing an embedded domain specific language (EDSL). By using Haskell type constructors to represent binding combinators, we implement the EDSL succinctly using datatype-generic programming. Our implementation supports a number of features necessary for practical programming, including flexibility in the treatment of user-defined types, besteffort name preservation (for error messages), and integration with Haskell's monad transformer library. Copyright © 2011 ACM.",Generic programming | Haskell | Name binding | Patterns,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2011-10-19,Conference Paper,"Weirich, Stephanie;Yorgey, Brent A.;Sheard, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81255143812,10.1109/ICGSE.2011.21,Exploring the role of instant messaging in a global software development project,"Communication plays a vital role in software development projects. Globally distributed teams use a mix of different communication channels to get the work done. In this paper, we report on an empirical study of a team distributed across Denmark and India. This paper explores the integration of formal documentation, bug-tracking systems and email with informal communication on Instant Messaging (IM), screen sharing, and audio conversations. Whenever overlap times occur, informal communication can take place at the same time in different sites, and it can effectively complement formal documentation. Our analysis provides an indication that IM can play a special role in such socio-technical communication systems: IM acts as a real time glue between different channels. The communication through IM also provides a means to build trust and social relationships with co-workers. © 2011 IEEE.",ethnographic research | informal communication | Instant messaging | skype | social software,"Proceedings - 2011 6th IEEE International Conference on Global Software Engineering, ICGSE 2011",2011-11-21,Conference Paper,"Dittrich, Yvonne;Giuffrida, Rosalba",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81255143812,10.1109/ICGSE.2011.21,Exploring the role of instant messaging in a global software development project,"Communication plays a vital role in software development projects. Globally distributed teams use a mix of different communication channels to get the work done. In this paper, we report on an empirical study of a team distributed across Denmark and India. This paper explores the integration of formal documentation, bug-tracking systems and email with informal communication on Instant Messaging (IM), screen sharing, and audio conversations. Whenever overlap times occur, informal communication can take place at the same time in different sites, and it can effectively complement formal documentation. Our analysis provides an indication that IM can play a special role in such socio-technical communication systems: IM acts as a real time glue between different channels. The communication through IM also provides a means to build trust and social relationships with co-workers. © 2011 IEEE.",ethnographic research | informal communication | Instant messaging | skype | social software,"Proceedings - 2011 6th IEEE International Conference on Global Software Engineering, ICGSE 2011",2011-11-21,Conference Paper,"Dittrich, Yvonne;Giuffrida, Rosalba",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80052400207,10.1109/ICPC.2011.21,Obstacles in using frameworks and APIs: An exploratory study of programmers' newsgroup discussions,"Large software frameworks and APIs can be hard to learn and use, impeding software productivity. But what are the specific challenges that programmers actually face when using frameworks and APIs in practice? What makes APIs hard to use, and what can be done to alleviate the problems associated with API usability and learnability? To explore these questions, we conducted an exploratory study in which we manually analyzed a set of newsgroup discussions about specific challenges that programmers had about a software framework. Based on this set of data, we identified several categories of obstacles in using APIs. We discussed what could be done to help overcome these obstacles. © 2011 IEEE.",APIs | AWT/Swing | Case Studies | Frameworks | Usability,IEEE International Conference on Program Comprehension,2011-09-09,Conference Paper,"Hou, Daqing;Li, Lin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-80052400747,10.1109/ICPC.2011.36,A lightweight approach to uncover technical artifacts in unstructured data,"Developer communication through email, chat, or issue report comments consists mostly of largely unstructured data, i.e., natural language text, mixed with technical artifacts such as project-specific jargon, abbreviations, source code patches, stack traces and identifiers. These technical artifacts represent a valuable source of knowledge on the technical part of the system, with a wide range of applications from establishing traceability links to creating project-specific vocabularies. However, the lack of well-defined boundaries between natural language and technical content make the automated mining of technical artifacts challenging. As a first step towards a general-purpose technique to extracting technical artifacts from unstructured data, we present a lightweight approach to untangle technical artifacts and natural language text. Our approach is based on existing spell checking tools, which are well-understood, fast, readily available across platforms and impartial to different kinds of textual data. Through a handcrafted benchmark, we demonstrate that our approach is able to successfully uncover a wide range of technical artifacts in unstructured data. © 2011 IEEE.",language analysis | technical artifacts | text mining | unstructured data,IEEE International Conference on Program Comprehension,2011-09-09,Conference Paper,"Bettenburg, Nicolas;Adams, Bram;Hassan, Ahmed E.;Smidt, Michel",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80052403087,10.1109/ICPC.2011.33,Toward intuitive programming languages,"Modern text-based computer programming languages use syntax, semantics, and APIs to represent actions a computer will perform. Historically, the design of these languages has largely escaped the critical gaze of peer review, relying more on expert opinions than robust scientific methodologies. In this paper, we pose a question - is it possible to create a programming language where the syntax, semantics, and API design is based upon rigorous data collection and the scientific method? We have undertaken a long-term project to develop a computer programming language, called Hop, where each language decision is based upon empirical metrics gathered from human studies. While such a design may not universally benefit all programmers, our hope is that such a procedure may make our community's language design decisions more objective and transparent. © 2011 IEEE.",empirical studies | intuitiveness | programming languages | syntax,IEEE International Conference on Program Comprehension,2011-09-09,Conference Paper,"Stefik, Andreas;Siebert, Susanna;Slattery, Kim;Stefik, Melissa",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80052426026,10.1109/ICPC.2011.16,Satisfying programmers' information needs in API-based programming,"Programmers encounter many difficulties in using an API to solve a programming task. To cope with these difficulties, they browse the Internet for code samples, tutorials, and API documentation. In general, it is time-consuming to find relevant help from the plethora of information on the web. While programmers can use search-based tools to help locate code snippets or applications that may be relevant to the APIs they are using, they still face the significant challenge of understanding and assessing the quality of the search results. We propose to investigate a proactive help system that is integrated in a development environment to provide contextual suggestions to the programmers as the code is being read and edited in the editor. © 2011 IEEE.",abstract interpretation | API-based programming | inter-procedural analysis | rule-based inferences,IEEE International Conference on Program Comprehension,2011-09-09,Conference Paper,"Rupakheti, Chandan Raj;Hou, Daqing",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80052402206,10.1109/ICPC.2011.37,Context and vision: Studying two factors impacting program comprehension,"Linguistic information derived from identifiersand comments has a paramount role in program comprehension. Indeed, very often, program documentation is scarce and when available, it is almost always outdated. Previous research works showed that program comprehension is often solely grounded on identifiers and comments and that, ultimately, it is the quality of comments and identifiers that impact the accuracy and efficiency of program comprehension. Previous works also investigated the factors in?uencing program com-prehension. However, they are limited by the available tools used to establish relations between cognitive processes and program comprehension. The goal of our research work is to foster our understanding of program comprehension by better understanding its implied underlying cognitive processes. We plan to study vision as the fundamental mean used by developers to understand a code in the context of a given program. Vision is indeed the trigger mechanism starting any cognitive process, in particular in program comprehension. We want to provide supporting evidence that context guides the cognitive process toward program comprehension. Therefore, we will perform a series of empirical studies to collect observations related to the use of context and vision in program comprehension. Then, we will propose laws and then derive a theory to explain the observable facts and predict new facts. The theory could be used in future empirical studies and will provide the relation between program comprehension and cognitive processes. © 2011 IEEE.",cognitive process | Program comprehension | program context | vision science,IEEE International Conference on Program Comprehension,2011-09-09,Conference Paper,"Soh, Zéphyrin",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80052421395,10.1109/ICPC.2011.43,Modeling framework API evolution as a multi-objective optimization problem,"Today's software development depends greatly on frameworks and libraries. When their APIs evolve, developers must update their programs accordingly. Existing approaches facilitate the upgrading process by generating change - rules based on various input data, such call dependency, text similarity, software metrics, etc. However, existing approaches do not provide 100% precision and recall because of the limited set of input data that they use to generate change - rules. For example, an approach only considering text similarity usually discovers less change - rules then that considering both text similarity and call dependency with similar precision. But adding more input data may increase the complexity of the change - rule generating algorithms and make them unpractical. We propse MOFAE (Multi-Objective Framework API Evolution) by modeling framework API evolution as multi-objective optimization problem to take more input data into account while generating change - rules and to control the algorithmic complexity. © 2011 IEEE.",API evolution | framework evolution | multi-object problem | search based software engineering,IEEE International Conference on Program Comprehension,2011-09-09,Conference Paper,"Wu, Wei",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959900039,10.1145/1985793.1985812,Static extraction of program configuration options,"Many programs use a key-value model for configuration options. We examined how this model is used in seven open source Java projects totaling over a million lines of code. We present a static analysis that extracts a list of configuration options for a program. Our analysis finds 95% of the options read by the programs in our sample, making it more complete than existing documentation. Most configuration options we saw fall into a small number of types. A dozen types cover 90% of options. We present a second analysis that exploits this fact, inferring a type for most options. Together, these analyses enable more visibility into program configuration, helping reduce the burden of configuration documentation and configuration debugging. © 2011 ACM.",configuration | documentation | experiences | static analysis,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Rabkin, Ariel;Katz, Randy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959887906,10.1145/1985793.1985813,An empirical study of build maintenance effort,"The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system specifications require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of different sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the difference in scale, the build system churn rate is comparable to that of the source code, and build changes induce more relative churn on the build system than source code changes induce on the source code. Furthermore, build maintenance yields up to a 27% overhead on source code development and a 44% overhead on test development. Up to 79% of source code developers and 89% of test code developers are significantly impacted by build maintenance, yet investment in build experts can reduce the proportion of impacted developers to 22% of source code developers and 24% of test code developers. © 2011 ACM.",build systems | empirical software engineering | mining software repositories,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"McIntosh, Shane;Adams, Bram;Nguyen, Thanh H.D.;Kamei, Yasutaka;Hassan, Ahmed E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959907623,10.1145/1985793.1985815,An empirical investigation into the role of API-level refactorings during software evolution,"It is widely believed that refactoring improves software quality and programmer productivity by making it easier to maintain and understand software systems. However, the role of refactorings has not been systematically investigated using fine-grained evolution history. We quantitatively and qualitatively studied API-level refactorings and bug fixes in three large open source projects, totaling 26523 revisions of evolution. The study found several surprising results: One, there is an increase in the number of bug fixes after API-level refactorings. Two, the time taken to fix bugs is shorter after API-level refactorings than before. Three, a large number of refactoring revisions include bug fixes at the same time or are related to later bug fix revisions. Four, API-level refactorings occur more frequently before than after major software releases. These results call for re-thinking refactoring's true benefits. Furthermore, frequent floss refactoring mistakes observed in this study call for new software engineering tools to support safe application of refactoring and behavior modifying edits together. © 2011 ACM.",defects | empirical study | refactoring | release cycle | software evolution,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Kim, Miryung;Cai, Dongxiang;Kim, Sunghun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959861140,10.1145/1985793.1985819,On-demand feature recommendations derived from mining public product descriptions,"We present a recommender system that models and recommends product features for a given domain. Our approach mines product descriptions from publicly available online specifications, utilizes text mining and a novel incremental diffusive clustering algorithm to discover domain-specific features, generates a probabilistic feature model that represents commonalities, variants, and cross-category features, and then uses association rule mining and the k-Nearest-Neighbor machine learning strategy to generate product specific feature recommendations. Our recommender system supports the relatively labor-intensive task of domain analysis, potentially increasing opportunities for re-use, reducing time-to-market, and delivering more competitive software products. The approach is empirically validated against 20 different product categories using thousands of product descriptions mined from a repository of free software applications. © 2011 ACM.",clustering | domain analysis | recommender systems,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Dumitru, Horatiu;Gibiec, Marek;Hariri, Negar;Cleland-Huang, Jane;Mobasher, Bamshad;Castro-Herrera, Carlos;Mirakhorli, Mehdi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959867950,10.1145/1985793.1985820,Inferring better contracts,"Considerable progress has been made towards automatic support for one of the principal techniques available to enhance program reliability: equipping programs with extensive contracts. The results of current contract inference tools are still often unsatisfactory in practice, especially for programmers who already apply some kind of basic Design by Contract discipline, since the inferred contracts tend to be simple assertions - the very ones that programmers find easy to write. We present new, completely automatic inference techniques and a supporting tool, which take advantage of the presence of simple programmer-written contracts in the code to infer sophisticated assertions, involving for example implication and universal quantification. Applied to a production library of classes covering standard data structures such as linked lists, arrays, stacks, queues and hash tables, the tool is able, entirely automatically, to infer 75% of the complete contracts - contracts yielding the full formal specification of the classes - with very few redundant or irrelevant clauses. © 2011 ACM.",contract inference | data mining | invariants | random testing,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Wei, Yi;Furia, Carlo A.;Kazmin, Nikolay;Meyer, Bertrand",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959907333,10.1145/1985793.1985823,Synthesis of live behaviour models for fallible domains,"We revisit synthesis of live controllers for event-based operational models. We remove one aspect of an idealised problem domain by allowing to integrate failures of controller actions in the environment model. Classical treatment of failures through strong fairness leads to a very high computational complexity and may be insufficient for many interesting cases. We identify a realistic stronger fairness condition on the behaviour of failures. We show how to construct controllers satisfying liveness specifications under these fairness conditions. The resulting controllers exhibit the only possible behaviour in face of the given topology of failures: they keep retrying and never give up. We then identify some well-structure conditions on the environment. These conditions ensure that the resulting controller will be eager to satisfy its goals. Furthermore, for environments that satisfy these conditions and have an underlying probabilistic behaviour, the measure of traces that satisfy our fairness condition is 1, giving a characterisation of the kind of domains in which the approach is applicable. © 2011 ACM.",behavioural modelling | controller synthesis,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"D'Ippolito, Nicolás;Braberman, Victor;Piterman, Nir;Uchitel, Sebastián",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959892356,10.1145/1985793.1985834,Model projection: Simplifying models in response to restricting the environment,"This paper introduces Model Projection. Finite state models such as Extended Finite State Machines are being used in an ever increasing number of software engineering activities. Model projection facilitates model development by specializing models for a specific operating environment. A projection is useful in many design-level applications including specification reuse and property verification. The applicability of model projection rests upon three critical concerns: correctness, effectiveness, and efficiency, all of which are addressed in this paper. We introduce four related algorithms for model projection and prove each correct. We also present an empirical study of effectiveness and efficiency using ten models, including widely studied benchmarks as well as industrial models. Results show that a typical projection includes about half of the states and a third of the transitions from the original model. © 2011 ACM.",extended finite state machines | model projection | slicing,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Androutsopoulos, Kelly;Binkley, David;Clark, David;Gold, Nicolas;Harman, Mark;Lano, Kevin;Li, Zheng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959902128,10.1145/1985793.1985852,Leveraging software architectures to guide and verify the development of sense/compute/control applications,"A software architecture describes the structure of a computing system by specifying software components and their interactions. Mapping a software architecture to an implementation is a well known challenge. A key element of this mapping is the architecture's description of the data and control-flow interactions between components. The characterization of these interactions can be rather abstract or very concrete, providing more or less implementation guidance, programming support, and static verification. In this paper, we explore one point in the design space between abstract and concrete component interaction specifications. We introduce a notion of interaction contract that expresses allowed interactions between components, describing both data and control-flow constraints. This declaration is part of the architecture description, allows generation of extensive programming support, and enables various verifications. We instantiate our approach in an architecture description language for Sense/Compute/Control applications, and describe associated compilation and verification strategies © 2011 ACM.",architectural conformance | generative programming,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Cassou, Damien;Balland, Emilie;Consel, Charles;Lawall, Julia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959876878,10.1145/1985793.1985862,Interface decomposition for service compositions,"Service-based applications can be realized by composing existing services into new, added-value composite services. The external services with which a service composition interacts are usually known by means of their syntactical interface. However, an interface providing more information, such as a behavioral specification, could be more useful to a service integrator for assessing that a certain external service can contribute to fulfill the functional requirements of the composite application. Given the requirements specification of a composite service, we present a technique for obtaining the behavioral interfaces - in the form of labeled transition systems - of the external services, by decomposing the global interface specification that characterizes the environment of the service composition. The generated interfaces guarantee that the service composition fulfills its requirements during the execution. Our approach has been implemented in the LTSA tool and has been applied to two case studies. © 2011 ACM.",behavioral interface | interface decomposition | service compositions | services,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Bianculli, Domenico;Giannakopoulou, Dimitra;Pǎsǎreanu, Corina S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959909737,10.1145/1985793.1985863,Unifying execution of imperative and declarative code,"We present a unified environment for running declarative specifications in the context of an imperative object-Oriented programming language. Specifications are Alloy-like, written in first-order relational logic with transitive closure, and the imperative language is Java. By being able to mix imperative code with executable declarative specifications, the user can easily express constraint problems in place, i.e., in terms of the existing data structures and objects on the heap. After a solution is found, the heap is updated to reflect the solution, so the user can continue to manipulate the program heap in the usual imperative way. We show that this approach is not only convenient, but, for certain problems can also outperform a standard imperative implementation. We also present an optimization technique that allowed us to run our tool on heaps with almost 2000 objects. © 2011 ACM.",constraint-based languages | declarative programming | executable specifications | formal methods,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Milicevic, Aleksandar;Rayside, Derek;Yessenov, Kuat;Jackson, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959870257,10.1145/1985793.1985866,Improving requirements quality using essential use case interaction patterns,"Requirements specifications need to be checked against the 3C's - Consistency, Completeness and Correctness - in order to achieve high quality. This is especially difficult when working with both natural language requirements and associated semi-formal modelling representations. We describe a technique and support tool that allows us to perform semi-automated checking of natural language and semi-formal requirements models, supporting both consistency management between representations but also correctness and completeness analysis. We use a concept of essential use case interaction patterns to perform the correctness and completeness analysis on the semi-formal representation. We highlight potential inconsistencies, incompleteness and incorrectness using visual differencing in our support tool. We have evaluated our approach via an end user study which focused on the tool's usefulness, ease of use, ease of learning and user satisfaction and provided data for cognitive dimensions of notations analysis of the tool. © 2011 ACM.",consistency management | essential use cases | requirements engineering | requirements patterns | tool support,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Kamalrudin, Massila;Hosking, John;Grundy, John",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959890862,10.1145/1985793.1985874,Mining parametric specifications,"Specifications carrying formal parameters that are bound to concrete data at runtime can effectively and elegantly capture multi-object behaviors or protocols. Unfortunately, parametric specifications are not easy to formulate by nonexperts and, consequently, are rarely available. This paper presents a general approach for mining parametric specifications from program executions, based on a strict separation of concerns: (1) a trace slicer first extracts sets of independent interactions from parametric execution traces; and (2) the resulting non-parametric trace slices are then passed to any conventional non-parametric property learner. The presented technique has been implemented in jMiner, which has been used to automatically mine many meaningful and non-trivial parametric properties of OpenJDK 6. © 2011 ACM.",dynamic analysis | parametric specifications,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Lee, Choonghwan;Chen, Feng;Roşu, Grigore",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959861136,10.1145/1985793.1985887,Bringing domain-specific languages to digital forensics,"Digital forensics investigations often consist of analyzing large quantities of data. The software tools used for analyzing such data are constantly evolving to cope with a multiplicity of versions and variants of data formats. This process of customization is time consuming and error prone. To improve this situation we present Derric, a domain-specific language (DSL) for declaratively specifying data structures. This way, the specification of structure is separated from data processing. The resulting architecture encourages customization and facilitates reuse. It enables faster development through a division of labour between investigators and software engineers. We have performed an initial evaluation of Derric by constructing a data recovery tool. This so-called carver has been automatically derived from a declarative description of the structure of JPEG files. We compare it to existing carvers, and show it to be in the same league both with respect to recovered evidence, and runtime performance. © 2011 ACM.",data description languages | digital forensics | domain-specific languages | model-driven engineering,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Van Den Bos, Jeroen;Van Der Storm, Tijs",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959884238,10.1145/1985793.1985893,An evaluation of the internal quality of business applications: Does size matter?,"This study summarizes results of a study of the internal, structural quality of 288 business applications comprising 108 million lines of code collected from 75 companies in 8 industry segments. These applications were submitted to a static analysis that evaluates quality within and across application components that may be coded in different languages. The analysis consists of evaluating the application against a repository of over 900 rules of good architectural and coding practice. Results are presented for measures of security, performance, and changeability. The effect of size on quality is evaluated, and the ability of modularity to reduce the impact of size is suggested by the results. © 2011 ACM.",benchmarking | internal quality | maintainability performance efficiency | software metrics | software quality | static analysis,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Curtis, Bill;Sappidi, Jay;Subramanyam, Jitendra",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959868269,10.1145/1985793.1985901,A comparison of model-based and judgment-based release planning in incremental software projects,"Numerous factors are involved when deciding when to implement which features in incremental software development. To facilitate a rational and efficient planning process, release planning models make such factors explicit and compute release plan alternatives according to optimization principles. However, experience suggests that industrial use of such models is limited. To investigate the feasibility of model and tool support, we compared input factors assumed by release planning models with factors considered by expert planners. The former factors were cataloged by systematically surveying release planning models, while the latter were elicited through repertory grid interviews in three software organizations. The findings indicate a substantial overlap between the two approaches. However, a detailed analysis reveals that models focus on only select parts of a possibly larger space of relevant planning factors. Three concrete areas of mismatch were identified: (1) continuously evolving requirements and specifications, (2) continuously changing prioritization criteria, and (3) authority-based decision processes. With these results in mind, models, tools and guidelines can be adjusted to address better real-life development processes. © 2011 ACM.",agile | case study | large development projects | practitioners' mental models | repertory grid | scrum,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Benestad, Hans Christian;Hannay, Jo E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959871512,10.1145/1985793.1985907,How do programmers ask and answer questions on the web? (NIER track),"Question and Answer (Q&A) websites, such as Stack Overflow, use social media to facilitate knowledge exchange between programmers and fill archives with millions of entries that contribute to the body of knowledge in software development. Understanding the role of Q&A websites in the documentation landscape will enable us to make recommendations on how individuals and companies can leverage this knowledge effectively. In this paper, we analyze data from Stack Overflow to categorize the kinds of questions that are asked, and to explore which questions are answered well and which ones remain unanswered. Our preliminary findings indicate that Q&A websites are particularly effective at code reviews and conceptual questions. We pose research questions and suggest future work to explore the motivations of programmers that contribute to Q&A websites, and to understand the implications of turning Q&A exchanges into technical mini-blogs through the editing of questions and answers. © 2011 ACM.",q&a | questions | social media | stack overflow,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Treude, Christoph;Barzilay, Ohad;Storey, Margaret Anne",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959902122,10.1145/1985793.1985920,Design and implementation of a data analytics infrastructure in support of crisis informatics research (NIER track),"Crisis informatics is an emerging research area that studies how information and communication technology (ICT) is used in emergency response. An important branch of this area includes investigations of how members of the public make use of ICT to aid them during mass emergencies. Data collection and analytics during crisis events is a critical pre-requisite for performing such research, as the data generated during these events on social media networks are ephemeral and easily lost. We report on the current state of a crisis informatics data analytics infrastructure that we are developing in support of a broader, interdisciplinary research program. We also comment on the role that software engineering research plays in these increasingly common, highly-interdisciplinary research efforts. © 2011 ACM.",crisis informatics | scalability | software frameworks,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Anderson, Kenneth M.;Schram, Aaron",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959915053,10.1145/1985793.1985924,Learning to adapt requirements specifications of evolving systems (NIER track),"We propose a novel framework for adapting and evolving software requirements models. The framework uses model checking and machine learning techniques for verifying properties and evolving model descriptions. The paper offers two novel contributions and a preliminary evaluation and application of the ideas presented. First, the framework is capable of coping with errors in the specification process so that performance degrades gracefully. Second, the framework can also be used to re-engineer a model from examples only, when an initial model is not available. We provide a preliminary evaluation of our framework by applying it to a Pump System case study, and integrate our prototype tool with the NuSMV model checker. We show how the tool integrates verification and evolution of abstract models, and also how it is capable of re-engineering partial models given examples from an existing system. © 2011 ACM.",adaptation | machine learning in software engineering,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Borges, Rafael V.;D'Avila Garcez, Artur;Lamb, Luis C.;Nuseibeh, Bashar",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959909732,10.1145/1985793.1985928,Matching logic: A new program verification approach (NIER track),"Matching logic is a new program verification logic, which builds upon operational semantics. Matching logic specifications are constrained symbolic program configurations, called patterns, which can be matched by concrete configurations. By building upon an operational semantics of the language and allowing specifications to directly refer to the structure of the configuration, matching logic has at least three benefits: (1) One's familiarity with the formalism reduces to one's familiarity with the operational semantics of the language, that is, with the language itself; (2) The verification process proceeds the same way as the program execution, making debugging failed proof attempts manageable because one can always see the ""current configuration"" and ""what went wrong', same like in a debugger; and (3) Nothing is lost in translation, that is, there is no gap between the language itself and its verifier. Moreover, direct access to the structure of the configuration facilitates defining subpatterns that one may reason about, such as disjoint lists or trees in the heap, as well as supporting framing in various components of the configuration at no additional costs. © 2011 ACM.",matching logic | maude | rewriting logic,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Roşu, Grigore;Ştefǎnescu, Andrei",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959877497,10.1145/1985793.1985940,A study of ripple effects in software ecosystems (NIER track),"When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation - known as a ripple effect - is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes. Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on early results of such an empirical study of API changes that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating aroung the Squeak and Pharo software ecosystems: six years of evolution, nearly 3,000 contributors, and close to 2,500 distinct systems. © 2011 ACM.",empirical studies | mining software repositories | software ecosystems,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Robbes, Romain;Lungu, Mircea",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959861762,10.1145/1985793.1985946,Flexible generators for software reuse and evolution (NIER track),"Developers tend to use models and generators during initial development, but often abandon them later in software evolution and reuse. One reason for that is that code generated from models (e.g., UML) is often manually modified, and changes cannot be easily propagated back to models. Once models become out of sync with code, any future re-generation of code overrides manual modifications. We propose a flexible generator solution that alleviates the above problem. The idea is to let developers weave arbitrary manual modifications into the generation process, rather than modify already generated code. A flexible generator stores specifications of manual modifications in executable form, so that weaving can be automatically re-done any time code is regenerated from modified models. In that way, models and manual modification can evolve independently but in sync with each other, and the generated code never gets directly changed. As a proof of concept, we have already built a flexible generator prototype by a merger of conventional generation system and variability technique to handle manual modifications. We believe a flexible generator approach alleviates an important problem that hinders wide spread adoption of MDD in software practice. © 2011 ACM.",domain-specific languages | generators | software product lines | software reuse,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Jarzabek, Stan;Trung, Ha Duy",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959897525,10.1145/1985793.1985948,Towards architectural information in implementation (NIER track),"Agile development methods favor speed and feature producing iterations. Software architecture, on the other hand, is ripe with techniques that are slow and not oriented directly towards implementation of costumers' needs. Thus, there is a major challenge in retaining architectural information in a fast-faced agile project. We propose to embed as much architectural information as possible in the central artefact of the agile universe, the code. We argue that thereby valuable architectural information is retained for (automatic) documentation, validation, and further analysis, based on a relatively small investment of effort. We outline some preliminary examples of architectural annotations in Java and Python and their applicability in practice. © 2011 ACM.",agile methods | architectural reconstruction | software architecture,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Christensen, Henrik Bærbak;Hansen, Klaus Marius",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959899054,10.1145/1985793.1985956,Dynamic shape analysis of program heap using graph spectra (Nier track),Programs written in languages such as Java and C# maintain most of their state on the heap. The size and complexity of these programs pose a challenge in understanding and maintaining them; Heap analysis by summarizing the state of heap graphs assists programmers in these tasks. In this paper we present a novel dynamic heap analysis technique that uses spectra of the heap graphs to summarize them. These summaries capture the shape of recursive data structures as dynamic invariants or likely properties of these structures that must be preserved after any destructive update. Initial experiments show that this approach can generate meaningful summaries for a range of subject structures. © 2011 ACM.,deryaft | shape analysis | structural invariant generation,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Malik, Muhammad Zubair",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959867340,10.1145/1985793.1985994,"Impact of software resource estimation research on practice: A preliminary report on achievements, synergies, and challenges","This paper is a contribution to the Impact Project in the area of software resource estimation. The objective of the Impact Project has been to analyze the impact of software engineering research investments on software engineering practice. The paper begins by summarizing the motivation and context for analyzing software resource estimation; and by summarizing the study's purpose, scope, and approach. The approach includes analyses of the literature; interviews of leading software resource estimation researchers, practitioners, and users; and value/impact surveys of estimators and users. The study concludes that research in software resource estimation has had a significant impact on the practice of software engineering, but also faces significant challenges in addressing likely future software trends. © 2011 ACM.",impact project | software resource estimation,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Boehm, Barry;Valerdi, Ricardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959890855,10.1145/1985793.1985999,"Exploring, exposing, and exploiting emails to include human factors in software engineering","Researchers mine software repositories to support software maintenance and evolution. The analysis of the structured data, mainly source code and changes, has several benefits and offers precise results. This data, however, leaves communication in the background, and does not permit a deep investigation of the human factor, which is crucial in software engineering. Software repositories also archive documents, such as emails or comments, that are used to exchange knowledge among people - we call it ""people-centric information."" By covering this data, we include the human factor in our analysis, yet its unstructured nature makes it currently sub-exploited. Our work, by focusing on email communication and by implementing the necessary tools, investigates methods for exploring, exposing, and exploiting unstructured data. We believe it is possible to close the gap between development and communication, extract opinions, habits, and views of developers, and link implementation to its rationale; we see in a future where software analysis and development is routinely augmented with people-centric information. © 2011 Author.",email communication | toolset | unstructured data,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"Bacchelli, Alberto",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79959878959,10.1145/1985793.1986031,Build system maintenance,"The build system, i.e., the infrastructure that converts source code into deliverables, plays a critical role in the development of a software project. For example, developers rely upon the build system to test and run their source code changes. Without a working build system, development progress grinds to a halt, as the source code is rendered useless. Based on experiences reported by developers, we conjecture that build maintenance for large software systems is considerable, yet this maintenance is not well understood. A firm understanding of build maintenance is essential for project managers to allocate personnel and resources to build maintenance tasks effectively, and reduce the build maintenance overhead on regular development tasks, such as fixing defects and adding new features. In our work, we empirically study build maintenance in one proprietary and nine open source projects of different sizes and domain. Our case studies thus far show that: (1) similar to Lehman's first law of software evolution, build system specifications tend to grow unless effort is invested into restructuring them, (2) the build system accounts for up to 31% of the code files in a project, and (3) up to 27% of development tasks that change the source code also require build maintenance. Currently, we are working on identifying concrete measures that projects can take to reduce the build maintenance overhead. © 2011 ACM.",build systems | empirical software engineering | mining software repositories,Proceedings - International Conference on Software Engineering,2011-07-07,Conference Paper,"McIntosh, Shane",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-83455214120,10.1109/ICSM.2011.6080778,Expanding identifiers to normalize source code vocabulary,"Maintaining modern software requires significant tool support. Effective tools exploit a variety of information and techniques to aid a software maintainer. One area of recent interest in tool development exploits the natural language information found in source code. Such Information Retrieval (IR) based tools compliment traditional static analysis tools and have tackled problems, such as feature location, that otherwise require considerable human effort. To reap the full benefit of IR-based techniques, the language used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. Unfortunately, there is a significant proportion of invented vocabulary in source code. Vocabulary normalization aligns the vocabulary found in the source code with that found in other software artifacts. Most existing work related to normalization has focused on splitting an identifier into its constituent parts. The next step is to expand each part into a (dictionary) word that matches the vocabulary used in other software artifacts. Building on a successful approach to splitting identifiers, an implementation of an expansion algorithm is presented. Experiments on two systems find that up to 66% of identifiers are correctly expanded, which is within about 20% of the current system's best-case performance. Not only is this performance comparable to previous techniques, but the result is achieved in the absence of special purpose rules and not limited to restricted syntactic contexts. Results from these experiments also show the impact that varying levels of documentation (including both internal documentation such as the requirements and design, and external, or user-level, documentation) have on the algorithm's performance. © 2011 IEEE.",natural language processing | program comprehension | source code analysis tools,"IEEE International Conference on Software Maintenance, ICSM",2011-12-19,Conference Paper,"Lawrie, Dawn;Binkley, Dave",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-83455169642,10.1109/ICSM.2011.6080785,Graph-based detection of library API imitations,"It has been a common practice nowadays to employ third-party libraries in software projects. Software libraries encapsulate a large number of useful, well-tested and robust functions, so that they can help improve programmers' productivity and program quality. To interact with libraries, programmers only need to invoke Application Programming Interfaces (APIs) exported from libraries. However, programmers do not always use libraries as effectively as expected in their application development. One commonly observed phenomenon is that some library behaviors are re-implemented by client code. Such re-implementation, or imitation, is not just a waste of resource and energy, but its failure to abstract away similar code also tends to make software error-prone. In this paper, we propose a novel approach based on trace subsumption relation of data dependency graphs to detect imitations of library APIs for achieving better software maintainability. Furthermore, we have implemented a prototype of this approach and applied it to ten large real-world open-source projects. The experiments show 313 imitations of explicitly imported libraries with high precision average of 82%, and 116 imitations of static libraries with precision average of 75%. © 2011 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2011-12-19,Conference Paper,"Sun, Chengnian;Khoo, Siau Cheng;Zhang, Shao Jie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-83455169499,10.1109/ICSM.2011.6080812,"Precise detection of un-initialized variables in large, real-life COBOL programs in presence of unrealizable paths","Using variables before assigning any values to them are known to result in critical failures in an application. Few compilers warn about the use of some, but not all uses of un-initialized variables. The problem persists, especially in COBOL systems, due to lack of reliable program analysis tools. A critical reason is the presence of large number of control flow paths due to the use of un-structured constructs of the language. We present the problems faced by one of our big clients in his large, COBOL based software system due to the use of un-initialized variables. Using static data and control-flow analysis to detect them, we observed large number of false positives (imprecision) introduced due to the unrealizable paths in the un-structured COBOL code. We propose a solution to address the realizability issue. The solution is based on the summary based function analysis, which is adapted for COBOL Paragraphs and Sections, to handle the perform-through and fall-through control-flow, and is significantly engineered to scale for large programs (single COBOL program extending to tens of thousands of lines). Using this technique, we noted very large reduction, 45% on an average, in the number of false positives for the un-initialized variables. © 2011 IEEE.",defect detection | program analysis | un-initialzed variables,"IEEE International Conference on Software Maintenance, ICSM",2011-12-19,Conference Paper,"Jiresal, Rahul;Contractor, Adnan;Naik, Ravindra",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-83455200857,10.1109/ICSM.2011.6080814,Source code comprehension strategies and metrics to predict comprehension effort in software maintenance and evolution tasks - An empirical study with industry practitioners,"The goal of this research was to assess the consistency of source code comprehension strategies and comprehension effort estimation metrics, such as LOC, across different types of modification tasks in software maintenance and evolution. We conducted an empirical study with software development practitioners using source code from a small paint application written in Java, along with four semantics-preserving modification tasks (refactoring, defect correction) and four semantics-modifying modification tasks (enhancive and modification). Each task has a change specification and corresponding source code patch. The subjects were asked to comprehend the original source code and then judge whether each patch meets the corresponding change specification in the modification task. The subjects recorded the time to comprehend and described the comprehension strategies used and their reason for the patch judgments. The 24 subjects used similar comprehension strategies. The results show that the comprehension strategies and effort estimation metrics are not consistent across different types of modification tasks. The recorded descriptions indicate the subjects scanned through the original source code and the patches when trying to comprehend patches in the semantics-modifying tasks while the subjects only read the source code of the patches in semantics-preserving tasks. An important metric for estimating comprehension efforts of the semantics-modifying tasks is the Code Clone Subtracted from LOC(CCSLOC), while that of semantics-preserving tasks is the number of referred variables. © 2011 IEEE.",comprehension effort estimation metrics | semantics-modifying | semantics-preserving | software maintenance and evolution | source code comprehension,"IEEE International Conference on Software Maintenance, ICSM",2011-12-19,Conference Paper,"Nishizono, Kazuki;Morisaki, Shuji;Vivanco, Rodrigo;Matsumoto, Kenichi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-83455213856,10.1109/ICSM.2011.6080816,Toward a metrics suite for source code lexicons,"In this paper we present an empirical study of relationships between three source code lexicons: the identifier, comment, and literal lexicons. We conjecture that shared and unique properties of these lexicons for the given subject system can inform the configuration of a source code retrieval technique for a particular software understanding activity or software evolution task. Thus, we seek to discover these lexicon properties, and so we investigate five lexicon measures that consider term frequency, term density, and term provenance. © 2011 IEEE.",software lexicon | software metrics | text retrieval,"IEEE International Conference on Software Maintenance, ICSM",2011-12-19,Conference Paper,"Biggers, Lauren R.;Eddy, Brian P.;Kraft, Nicholas A.;Etzkorn, Letha H.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-83455262898,10.1109/ICSM.2011.6080830,A logic meta-programming foundation for example-driven pattern detection in object-oriented programs,"This paper summarizes the doctoral dissertation in which we introduced an example-driven approach to pattern detection. This approach enables specifying pattern characteristics in a familiar language: through a code excerpt that corresponds to their prototypical implementation. Such excerpts are matched against the program under investigation according to various matching strategies that vary in leniency. Each match is quantified by the extent to which it exhibits the exemplified characteristics. The smaller this extent, the more likely the match is a false positive - thus establishing a ranking which facilitates assessing a large amount of matches. Unique to the matching process is that it incorporates whole-program analyses in its comparison of individual program elements. This way, we are able to recall implicit implementation variants (i.e., those implied by the semantics of the programming language) of a pattern of which only the prototypical implementation has been exemplified. © 2011 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2011-12-19,Conference Paper,"Roover, Coen De",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84856935547,10.1109/ISSRE.2011.32,Statistical evaluation of complex input-output transformations,"This paper presents a new, statistical approach to evaluating software products that transform complex inputs into complex outputs. This approach, called multistage stratified input/output (MSIO) sampling, combines automatic clustering of multidimensional I/O data with multistage sampling and manual examination of data elements, in order to accurately and economically estimate summary measures of output data quality. We report results of two case studies in which MSIO sampling was successfully applied to evaluating complex graphical outputs. © 2011 IEEE.",bioinformatics | complex data | estimation | MSIO sampling | multistage sampling | software reliability | software testing | stratified sampling,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2011-12-01,Conference Paper,"Shu, Gang;Bai, Zhuofu;Podgurski, Andy",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-78049529048,10.1016/j.infsof.2010.09.002,Applying CIM-to-PIM model transformations for the service-oriented development of information systems,"Context: Model-driven approaches deal with the provision of models, transformations between them and code generators to address software development. This approach has the advantage of defining a conceptual structure, where the models used by business managers and analysts can be mapped into more detailed models used by software developers. This alignment between high-level business specifications and the lower-level information technologies (ITs) models is crucial to the field of service-oriented development, where meaningful business services and process specifications are those relevant to real business scenarios. Objective: This paper presents a model-driven approach which, starting from high-level computational-independent business models (CIMs) - the business view - sets out guidelines for obtaining lower-level platform-independent behavioural models (PIMs) - the information system view. A key advantage of our approach is the use of real high-level business models, not just requirements models, which, by means of model transformations, helps software developers to make the most of the business knowledge for specifying and developing business services. Method: This proposal is framed in a method for service-oriented development of information systems whose main characteristic is the use of services as first-class objects. The method follows an MDA-based approach, proposing a set of models at different levels of abstraction and model transformations to connect them. Results: The paper present the complete set of CIM and PIM metamodels and the specification of the mappings between them, which clear advantage is the support for the alignment between high-level business view and ITs. The proposed model-driven process is being implemented in an MDA tool. A first prototype has been used to develop a travel agency case study that illustrates the proposal. Conclusion: This study shows how a model-driven approach helps to solve the alignment problem between the business view and the information system view that arises when adopting service-oriented approaches for software development. © 2010 Elsevier B.V. All rights reserved.",Business models | Information systems | Model transformations | Model-driven development | Service-oriented engineering,Information and Software Technology,2011-01-01,Article,"De Castro, Valeria;Marcos, Esperanza;Vara, Juan Manuel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79957472291,10.1016/j.infsof.2011.03.003,Cycle elimination for invocation graph-based context-sensitive pointer analysis,"Context: Pointer analysis is an important building block of optimizing compilers and program analyzers for C language. Various methods with precision and performance trade-offs have been proposed. Among them, cycle elimination has been successfully used to improve the scalability of context-insensitive pointer analyses without losing any precision. Objective: In this article, we present a new method on context-sensitive pointer analysis with an effective application of cycle elimination. Method: To obtain similar benefits of cycle elimination for context-sensitive analysis, we propose a novel constraint-based formulation that uses sets of contexts as annotations. Our method is not based on binary decision diagram (BDD). Instead, we directly use invocation graphs to represent context sets and apply a hash-consing technique to deal with the exponential blow-up of contexts. Result: Experimental results on C programs ranging from 20,000 to 290,000 lines show that applying cycle elimination to our new formulation results in 4.5 ×speedup over the previous BDD-based approach. Conclusion: We showed that cycle elimination is an effective method for improving the scalability of context-sensitive pointer analysis. © 2011 Elsevier B.V. All rights reserved.",Binary decision diagram | Constraint-based analysis | Context-sensitive pointer analysis | Cycle elimination,Information and Software Technology,2011-08-01,Conference Paper,"Choi, Woongsik;Choe, Kwang Moo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79952453471,10.1016/j.infsof.2010.10.007,Beyond the customer: Opening the agile systems development process,"Context: A particular strength of agile systems development approaches is that they encourage a move away from 'introverted' development, involving the customer in all areas of development, leading to more innovative and hence more valuable information system. However, a move toward open innovation requires a focus that goes beyond a single customer representative, involving a broader range of stakeholders, both inside and outside the organisation in a continuous, systematic way. Objective: This paper provides an in-depth discussion of the applicability and implications of open innovation in an agile environment. Method: We draw on two illustrative cases from industry. Results: We highlight some distinct problems that arose when two project teams tried to combine agile and open innovation principles. For example, openness is often compromised by a perceived competitive element and lack of transparency between business units. In addition, minimal documentation often reduce effective knowledge transfer while the use of short iterations, stand-up meetings and presence of on-site customer reduce the amount of time for sharing ideas outside the team. Conclusion: A clear understanding of the inter- and intra-organisational applicability and implications of open innovation in agile systems development is required to address key challenges for research and practice. © 2010 Elsevier B.V. All rights reserved.",Agile systems development | Inter-organisational | Intra-organisational | Networking | Open innovation,Information and Software Technology,2011-05-01,Conference Paper,"Conboy, Kieran;Morgan, Lorraine",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79957507487,10.1016/j.infsof.2011.02.007,Usability evaluation methods for the web: A systematic mapping study,"Context: In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers' usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring. Objective: The objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14 years. Method: A systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner. Results: The results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners. Conclusions: From an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted. © 2011 Elsevier B.V. All rights reserved.",Systematic mapping | Usability evaluation methods | Web development,Information and Software Technology,2011-08-01,Conference Paper,"Fernandez, Adrian;Insfran, Emilio;Abrahão, Silvia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79952485151,10.1016/j.infsof.2010.12.012,Evaluating software engineering techniques for developing complex systems with multiagent approaches,"Context: Multiagent systems (MAS) allow complex systems to be developed in which autonomous and heterogeneous entities interact. Currently, there are a great number of methods and frameworks for developing MAS. The selection of one or another development environment is a crucial part of the development process. Therefore, the evaluation and comparison of MAS software engineering techniques is necessary in order to make the selection of the development environment easier. Objective: The main goal of this paper is to define an evaluation framework that will help in facilitating, standardizing, and simplifying the evaluation, analysis, and comparison of MAS development environments. Moreover, the final objective of the proposed tool is to provide a repository of the most commonly used MAS software engineering methods and tools. Method: The proposed framework analyzes methods and tools through a set of criteria that are related to both system engineering dimensions and MAS features. Also, the support for developing organizational and service-oriented MAS is studied. This framework is implemented as an online application to improve its accessibility. Results: In this paper, we present Masev, which is an evaluation framework for MAS software engineering. It allows MAS methods, techniques and environments to be analyzed and compared. A case study of the analysis of four methodologies is presented. Conclusion: It is concluded that Masev simplifies the evaluation and comparison task and summarizes the most important issues for developing MAS, organizational MAS, and service-oriented MAS. Therefore, it could help developers to select the most appropriate MAS method and tools for developing a specific system, and it could be used for MAS software engineering developers to detect and deficiencies in their methods and tools. Also, developers of new tools can understand this application as a way to publish their tools and demonstrate what their contributions are to the state of the art. © 2010 Elsevier B.V. All rights reserved.",Methodology | Multiagent systems development tool | Software engineering,Information and Software Technology,2011-05-01,Conference Paper,"Garcia, Emilia;Giret, Adriana;Botti, Vicente",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953687366,10.1016/j.infsof.2010.12.009,A systematic review of research on open source software in commercial software product development,"Context: The popularity of the open source software development in the last decade, has brought about an increased interest from the industry on how to use open source components, participate in the open source community, build business models around this type of software development, and learn more about open source development methodologies. There is a need to understand the results of research in this area. Objective: Since there is a need to understand conducted research, the aim of this study is to summarize the findings of research that has ben carried out on usage of open source components and development methodologies by the industry, as well as companies' participation in the open source community. Method: Systematic review through searches in library databases and manual identification of articles from the open source conference. The search was first carried out in May 2009 and then once again in May 2010. Results: In 2009, 237 articles were first found, from which 19 were selected based on content and quality, and in 2010, 76 new articles were found from which four were selected. Twenty three articles were identified in total. Conclusions: The articles could be divided into four categories: open source as part of component based software engineering, business models with open source in commercial organization, company participation in open source development communities, and usage of open source processes within a company. © 2010 Elsevier B.V. All rights reserved.",Business models | Commercial | Component based software engineering | Open source software | Proprietary,Information and Software Technology,2011-01-01,Article,"Höst, Martin;Oručević-Alagić, Alma",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79951809409,10.1016/j.infsof.2010.12.002,Problems in the interplay of development and IT operations in system development projects: A Delphi study of Norwegian IT experts,"Context: The assumption of the presented work is that the ability of system developers and IT operations personnel to cooperate effectively in system development projects has great impact on the quality of the final system solution, as well as on the service level of its subsequent operation. Objective: The present research explores the interplay of system development and IT operations and the challenges they are meeting. We are especially interested in identifying problems encountered between these two parties in system development projects. Method: We identify and rank problems by using a ranking-type Delphi study. We involved 42 Norwegian IT experts and split them into three expert panels: system developers, IT operations personnel and system owners. We then guided them through the three phases of the Delphi method - brainstorming, reduction and ranking. Results: A comprehensive list of 66 problems, organized into seven groups, is compiled. Through a selection and ranking procedure, the panels found the following to be the six most serious problems in the interplay of system development and IT operations: (1) IT operations not being involved in the requirements specification; (2) poor communication and information flow; (3) unsatisfactory test environments; (4) lack of knowledge transfer; (5) systems being put into production before they are complete; and (6) operational routines not being established prior to deployment. Conclusion: The sheer amount and variety of problems mentioned and the respondents' explanations confirm that this interplay needs attention; the parties agree that they do not cooperate effectively in development projects. The results imply that IT operations should be regarded as an important stakeholder throughout several systems development activities, especially requirements analysis, testing and deployment. Moreover, such involvement should be facilitated by an increased focus on enhancing cooperation and communication. © 2010 Elsevier B.V. All rights reserved.",Interplay | IT operations | Problems | System development | The Delphi method,Information and Software Technology,2011-04-01,Article,"Iden, Jon;Tessem, Bjørnar;Päivärinta, Tero",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953724329,10.1016/j.infsof.2010.12.011,Using mapping studies as the basis for further research - A participant-observer case study,"Context: We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research. Objective: This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic. Method: We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies. Results: Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers. Conclusion: Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research. © 2010 Elsevier B.V. All rights reserved.",Case study | Mapping studies | Software engineering | Systematic literature review,Information and Software Technology,2011-06-01,Article,"Kitchenham, Barbara A.;Budgen, David;Pearl Brereton, O.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79952455512,10.1016/j.infsof.2010.12.005,Process models for service-based applications: A systematic literature review,"Context: Service-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of SBAs have been addressed, however, there are still outstanding questions relating to the processes required to develop them. Objective: The objective of this study is to systematically identify process models for developing service-based applications (SBAs) and review the processes within them. This will provide a useful starting point for any further research in the area. A secondary objective of the study is to identify process models which facilitate the adaptation of SBAs. Method: In order to achieve this objective a systematic literature review (SLR) of the existing software engineering literature is conducted. Results: During this research 722 studies were identified using a predefined search strategy, this number was narrowed down to 57 studies based on a set of strict inclusion and exclusion criteria. The results are reported both quantitatively in the form of a mapping study, as well as qualitatively in the form of a narrative summary of the key processes identified. Conclusion: There are many process models reported for the development of SBAs varying in detail and maturity, this review has identified and categorised the processes within those process models. The review has also identified and evaluated process models which facilitate the adaptation of SBAs. © 2010 Elsevier B.V. All rights reserved.",Service-based application | SOA | Software process | Systematic literature review,Information and Software Technology,2011-05-01,Conference Paper,"Lane, Stephen;Richardson, Ita",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-78049527806,10.1016/j.infsof.2010.07.004,A formal approach for the development of reactive systems,"Context: This paper deals with the development and verification of liveness properties on reactive systems using the Event-B method. By considering the limitation of the Event-B method to invariance properties, we propose to apply the language TLA+ to verify liveness properties on Event-B models. Objective: This paper deals with the use of two verification approaches: theorem proving and model-checking, in the construction and verification of safe reactive systems. The theorem prover concerned is part of the Click-n-Prove tool associated to the Event-B method and the model checker is TLC for TLA + models. Method: To verify liveness properties on Event-B systems, we extend first the expressivity and the semantics of a B model (called temporal B model) to deal with the specification of fairness and eventuality properties. Second, we propose semantics of the extension over traces, in the same spirit as TLA+ does. Third, we give verification rules in the axiomatic way of the Event-B method. Finally, we give transformation rules from a temporal B model into a TLA+ module. We present in particular, our prototype system called B2TLA+, that we have developed to support this transformation; then we can verify liveness properties thanks to the model checker TLC on finite state systems. For the verification of infinite-state systems, we propose the use of the predicate diagrams and its associated tool DIXIT. As the B refinement preserves invariance properties through refinement steps, we propose some rules to get the preservation of liveness properties by the B refinement. Results: The proposed approach is applied for the development of some reactive systems examples and our prototype system B2TLA+ is successfully used to transform a temporal B model into a TLA+ module. Conclusion: The paper successfully defines an approach for the specification and verification of safety and liveness properties for the development of reactive systems using the Event-B method, the language TLA+ and the predicate diagrams with their associated tools. The approach is illustrated on a case study of a parcel sorting system. © 2010 Elsevier B.V. All rights reserved.",Event-B method | Language TLA + | Liveness properties | Reactive systems | Refinement | Verification,Information and Software Technology,2011-01-01,Article,"Mosbahi, Olfa;Jemni Ben Ayed, Leila;Khalgui, Mohamed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79957479773,10.1016/j.infsof.2011.02.004,Improving the reliability of transaction identification in use cases,"Context: The concept of transactions is used in Use Case Points (UCP), and in many other functional size measurement methods, to capture the smallest unit of functionality that should be considered while measuring the size of a system. Unfortunately, in the case of the UCP method at least four methods for use-case transaction identification have been proposed so far. The different approaches to transaction identification and difficulties related to the analysis of requirements expressed in natural language can lead to problems in the reliability of functional size measurement. Objective: The goal of this study was to evaluate reliability of transaction identification in use cases (with the methods mentioned in the literature), analyze their weaknesses, and propose some means for their improvement. Method: A controlled experiment on a group of 120 students was performed to investigate if the methods for transaction identification, known from the literature, provide similar results. In addition, a qualitative analysis of the experiment data was performed to investigate the potential problems related to transaction identification in use cases. During the experiment a use-case benchmark specification was used. The automatic methods for transaction identification, proposed in the paper have been validated using the same benchmark by comparing the outcomes provided by these methods to on-average number of transactions identified by the participants of the experiment. Results: A significant difference in the median number of transactions was observed between groups using different methods of transaction identification. The Kruskal-Wallis test was performed with the significance level α set to 0.05 and followed by the post-hoc analysis performed according to the procedure proposed by Conover. Also a large intra-method variability was observed. The ratios between the maximum and minimum number of transactions identified by the participants using the same method were equal to 1.96, 3.83, 2.03, and 2.21. The proposed automatic methods for transaction identification provided results consistent with those provided by the participants of the experiment and functional measurement experts. The relative error between the number of transaction identified by the tool and on-average number of transactions identified by the participants of the experiment ranged from 3% to 7%. Conclusions: Human-performed transaction identification is error prone and quite subjective. Its reliability can be improved by automating the process with the use of natural language processing techniques. © 2011 Elsevier B.V. All rights reserved.",Functional size measurement | Natural language processing | Use Case Points | Use-case transactions,Information and Software Technology,2011-08-01,Conference Paper,"Ochodek, M.;Alchimowicz, B.;Jurkiewicz, J.;Nawrocki, J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79251617525,10.1016/j.infsof.2010.11.001,A two-stage framework for UML specification matching,"Context: Specification matching techniques are crucial for effective retrieval processes. Despite the prevalence for object-oriented methodologies, little attention has been given to Unified Modeling Language (UML) for matching. Objective: This paper presents a two-stage framework for matching two UML specifications and quantifying the results based on the systematic integration of their structural and behavioral similarities in order to identify the candidate component set for reuse. Method: The first stage in the framework is an evaluation of the similarities between UML class diagrams using the Structure-Mapping Engine (SME), a simulation of the analogical reasoning approach known as the structure-mapping theory. The second stage, performed on the components identified in the first stage, is based on a graph-similarity scoring algorithm in which UML class diagrams and sequence diagrams are transformed into an SME representation and a Message-Object-Order Graph (MOOG). The effectiveness of the proposed framework was evaluated using a case study. Results: The experimental results showed a reduction in potential mismatches and an overall high precision and recall. Conclusion: It is concluded that the two-stage framework is capable of performing more precise matching compared to those of other single-stage matching frameworks. Moreover, the two-stage framework could be utilized within a reuse process, bypassing the need for extra information for retrieval of the components described by UML. © 2010 Elsevier B.V. All rights reserved.",Graph similarity scoring | Specification matching | Structure mapping | UML,Information and Software Technology,2011-03-01,Article,"Park, Wei Jin;Bae, Doo Hwan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-78751581634,10.1016/j.infsof.2010.09.004,Assessing PSP effect in training disciplined software development: A Plan-Track-Review model,"Context: In training disciplined software development, the PSP is said to result in such effect as increased estimation accuracy, better software quality, earlier defect detection, and improved productivity. But a systematic mechanism that can be easily adopted to assess and interpret PSP effect is scarce within the existing literature. Objective: The purpose of this study is to explore the possibility of devising a feasible assessment model that ties up critical software engineering values with the pertinent PSP metrics. Method: A systematic review of the literature was conducted to establish such an assessment model (we called a Plan-Track-Review model). Both mean and median approaches along with a set of simplified procedures were used to assess the commonly accepted PSP training effects. A set of statistical analyses further followed to increase understanding of the relationships among the PSP metrics and to help interpret the application results. Results: Based on the results of this study, PSP training effect on the controllability, manageability, and reliability of a software engineer is quite positive and largely consistent with the literature. However, its effect on one's predictability on project in general (and on project size in particular) is not implied as said in the literature. As for one's overall project efficiency, our results show a moderate improvement. Our initial finding also suggests that a prior stage PSP effect could have an impact on later stage training outcomes. Conclusion: It is concluded that this Plan-Track-Review model with the associated framework can be used to assess PSP effect regarding a disciplined software development. The generated summary report serves to provide useful feedback for both PSP instructors and students based on internal as well as external standards. © 2010 Elsevier B.V. All rights reserved.",Effect assessment | Personal software process (PSP) | Plan-Track-Review | Software process improvement (SPI),Information and Software Technology,2011-02-01,Article,"Shen, Wen Hsiang;Hsueh, Nien Lin;Lee, Wei Mann",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80051669402,10.1016/j.infsof.2011.05.008,The value of software sizing,"Context: One of the difficulties faced by software development Project Managers is estimating the cost and schedule for new projects. Previous industry surveys have concluded that software size and cost estimation is a significant technical area of concern. In order to estimate cost and schedule it is important to have a good understanding of the size of the software product to be developed. There are a number of techniques used to derive software size, with function points being amongst the most documented. Objective: In this paper we explore the utility of function point software sizing techniques when applied to two levels of software requirements documentation in a commercial software development organisation. The goal of the research is to appraise the value (cost/benefit) which functional sizing techniques can bring to the project planning and management of software projects within a small-to-medium sized software development enterprise (SME). Method: Functional counts were made at the bid and detailed functional specification stages for each of five commercial projects used in the research. Three variants of the NESMA method were used to determine these function counts. Through a structured interview session, feedback on the sizing results was obtained to evaluate its feasibility and potential future contribution to the company. Results: The results of our research suggest there is value in performing size estimates at two appropriate stages in the software development lifecycle, with simplified methods providing the optimal return on effort expended. Conclusion: The 'Estimated NESMA' is the most appropriate tool for use in size estimation for the company studied. The use of software sizing provides a valuable contribution which would augment, but not replace, the company's existing cost estimation approach. © 2011 Elsevier B.V. All rights reserved.",Empirical software engineering | Function points | NESMA | Project planning | Size metrics | Software size estimation,Information and Software Technology,2011-11-01,Article,"Wilkie, F. G.;McChesney, I. R.;Morrow, P.;Tuxworth, C.;Lester, N. G.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-78649326944,10.1016/j.jss.2010.08.048,A formal approach for the specification and verification of trustworthy component-based systems,"Software systems are increasingly becoming ubiquitous affecting the way we experience the world. Embedded software systems, especially those used in smart devices, have become an essential constituent of the technological infrastructure of modern societies. Such systems, in order to be trusted in society, must be proved to be trustworthy. Trustworthiness is a composite non-functional property that implies safety, timeliness, security, availability, and reliability. This paper presents a formal approach for the development of trustworthy component-based systems. The approach involves a formal component model for the specification of component's structure, functional, and non-functional (trustworthiness) properties, a model transformation technique for the automatic generation of component behavior using the specified structure and restricted by the specified properties, and a unified formal verification method for safety, security, reliability and availability properties using model checking. © 2010 Elsevier Inc.",Component model | Component-based development | Formal verification | Trustworthiness,Journal of Systems and Software,2011-01-01,Conference Paper,"Mohammad, Mubarak;Alagar, Vangalur",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-78649333454,10.1016/j.jss.2010.09.036,An assessment of systems and software engineering scholars and institutions (2003-2007 and 2004-2008),"An ongoing, annual survey of publications in systems and software engineering identifies the top 15 scholars and institutions in the field over a 5-year period. Each ranking is based on the weighted scores of the number of papers published in TSE, TOSEM, JSS, SPE, EMSE, IST, and Software of the corresponding period. This report summarizes the results for 2003-2007 and 2004-2008. The top-ranked institution is Korea Advanced Institute of Science and Technology, Korea for 2003-2007, and Simula Research Laboratory, Norway for 2004-2008, while Magne Jørgensen is the top-ranked scholar for both periods. © 2010 Elsevier Inc.",Research publications | Systems and software engineering | Top institutions | Top scholars,Journal of Systems and Software,2011-01-01,Conference Paper,"Wong, W. Eric;Tse, T. H.;Glass, Robert L.;Basili, Victor R.;Chen, T. Y.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953717390,10.1016/j.jss.2011.01.048,"The reliability estimation, prediction and measuring of component-based software","Reliability is a key driver of safety-critical systems such as health-care systems and traffic controllers. It is also one of the most important quality attributes of the systems embedded into our surroundings, e.g. sensor networks that produce information for business processes. Therefore, the design decisions that have a great impact on the reliability of a software system, i.e. architecture and components, need to be thoroughly evaluated. This paper addresses software reliability evaluation during the design and implementation phases; it provides a coherent approach by combining both predicted and measured reliability values with heuristic estimates in order to facilitate a smooth reliability evaluation process. The approach contributes by integrating the component-level reliability evaluation activities (i.e. the heuristic reliability estimation, model-based reliability prediction and model-based reliability measuring of components) and the system-level reliability prediction activity to support the incremental and iterative development of reliable component-based software systems. The use of the developed reliability evaluation approach with the supporting tool chain is illustrated by a case study. The paper concludes with a summary of lessons learnt from the case studies. © 2011 Elsevier Inc. All rights reserved.",Architecture | ComponentBee | Evaluation | Prediction | Rap | Tool | Uml,Journal of Systems and Software,2011-06-01,Article,"Palviainen, Marko;Evesti, Antti;Ovaska, Eila",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79956194649,10.1016/j.jss.2011.02.005,Dynamic deployment of context-aware access control policies for constrained security devices,"Securing the access to a server, guaranteeing a certain level of protection over an encrypted communication channel, executing particular counter measures when attacks are detected are examples of security requirements. Such requirements are identified based on organizational purposes and expectations in terms of resource access and availability and also on system vulnerabilities and threats. All these requirements belong to the so-called security policy. Deploying the policy means enforcing, i.e., configuring, those security components and mechanisms so that the system behavior be finally the one specified by the policy. The deployment issue becomes more difficult as the growing organizational requirements and expectations generally leave behind the integration of new security functionalities in the information system: the information system will not always embed the necessary security functionalities for the proper deployment of contextual security requirements. To overcome this issue, our solution is based on a central entity approach which takes in charge unmanaged contextual requirements and dynamically redeploys the policy when context changes are detected by this central entity. We also present an improvement over the OrBAC (Organization-Based Access Control) model. Up to now, a controller based on a contextual OrBAC policy is passive, in the sense that it assumes policy evaluation triggered by access requests. Therefore, it does not allow reasoning about policy state evolution when actions occur. The modifications introduced by our work overcome this limitation and provide a proactive version of the model by integrating concepts from action specification languages. © 2011 Elsevier Inc.",Authorization | IT security | Network security | OrBAC,Journal of Systems and Software,2011-07-01,Article,"Preda, Stere;Cuppens, Frédéric;Cuppens-Boulahia, Nora;Garcia-Alfaro, Joaquin;Toutain, Laurent",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79956224689,10.1016/j.jss.2011.02.025,Parsed use case descriptions as a basis for object-oriented class model generation,"Object-oriented analysis and design has become a major approach in the design of software systems. Recent developments in CASE tools provide help in documenting the analysis and design stages and in detecting incompleteness and inconsistency in analysis. However, these tools do not contribute to the initial and difficult stage of the analysis process of identifying the objects/classes, attributes and relationships used to model the problem domain. This paper presents a tool, Class-Gen, which can partially automate the identification of objects/classes from natural language requirement specifications for object identification. Use case descriptions (UCDs) provide the input to Class-Gen which parses and analyzes the text written in English. A parsed use case description (PUCD) is generated which is then used as the basis for the construction of an initial UML class model representing object classes and relationships identified in the requirements. PUCDs enable the extraction of nouns, verbs, adjectives and adverbs from traditional UCDs for the identification process. Finally Class-Gen allows the initial class model to be refined manually. Class-Gen has been evaluated against a collection of unseen requirements. The results of the evaluation are encouraging as they demonstrate the potential for such tools to assist with the software development process. © 2011 Published by Elsevier Inc.",analysis | Natural language processing | Object-oriented | Parsing | Requirements specifications,Journal of Systems and Software,2011-07-01,Article,"Elbendak, Mosa;Vickers, Paul;Rossiter, Nick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960453174,10.1016/j.jss.2010.11.908,Defining and documenting execution viewpoints for a large and complex software-intensive system,"An execution view is an important asset for developing large and complex systems. An execution view helps practitioners to describe, analyze, and communicate what a software system does at runtime and how it does it. In this paper, we present an approach to define and document viewpoints that guide the construction and use of execution views for an existing large and complex software-intensive system. This approach includes the elicitation of the organization's requirements for execution views, the initial definition and validation of a set of execution viewpoints, and the documentation of the execution viewpoints. The validation and application of the approach have helped us to produce mature viewpoints that are being used to support the construction and use of execution views of the Philips Healthcare MRI scanner, a representative large software-intensive system in the healthcare domain. © 2010 Elsevier Inc. All rights reserved.",Architecture | Runtime | Viewpoints | Views,Journal of Systems and Software,2011-09-01,Conference Paper,"Callo Arias, Trosky B.;America, Pierre;Avgeriou, Paris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960835977,10.1016/j.jss.2011.03.032,Formal analysis of an electronic voting system: An experience report,"We have seen that several currently deployed e-voting systems share critical failures in their design and implementation that render their technical and procedural controls insufficient to guarantee trustworthy voting. The application of formal methods would greatly help to better address problems associated with assurance against requirements and standards. More specifically, it would help to thoroughly specify and analyze the underlying assumptions and security specific properties, and it would improve the trustworthiness of the final systems. In this article, we show how such techniques can be used to model and reason about the security of one of the currently deployed e-voting systems in the U.S.A named ES&S. We used the ASTRAL language to specify the voting process of ES&S machines and the critical security requirements for the system. Proof obligations that verify that the specified system meets the critical requirements were automatically generated by the ASTRAL Software Development Environment (SDE). The PVS interactive theorem prover was then used to apply the appropriate proof strategies and discharge the proof obligations. We also believe that besides analyzing the system against its requirements, it is equally important to perform an analysis under malicious circumstances where the execution model is augmented with attack behaviors. Thus, we extend the formal specification of the system by specifying attacks that have been shown to successfully compromise the system, and we then repeat the formal verification. This is helpful in detecting missing requirements or unwarranted assumptions about the specification of the system. In addition, this allows one to sketch countermeasure strategies to be used when the system behaves differently than it should and to build confidence about the system under development. Finally, we acknowledge the main problem that arises in e-voting system specification and verification: modeling attacks is very difficult because the different types of attack often cut across the structure of the original behavior models, thus making (incremental or compositional) verification very difficult. © 2011 Elsevier Inc.",Critical requirements | Electronic voting systems | ES&S system | Formal specification and verification,Journal of Systems and Software,2011-01-01,Article,"Weldemariam, Komminist;Kemmerer, Richard A.;Villafiorita, Adolfo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960841146,10.1016/j.jss.2011.05.025,Engineering the authoring of usable service front ends,"This paper presents a method and the associated authoring tool for supporting the development of interactive applications able to access multiple Web Services, even from different types of interactive devices. We show how model-based descriptions are useful for this purpose and describe the associated automatic support along with the underlying rules. The proposed environment is able to aid in the design of new interactive applications that access pre-existing Web Services, which may contain annotations supporting the user interface development. This is achieved through the use of task models as a starting point for the design and development of the corresponding implementations. We also provide an example to better illustrate the features of the approach, and report on two evaluations conducted to assess the support tool. © 2011 Elsevier Inc.",HCI model-based design | Service front ends | Web Services,Journal of Systems and Software,2011-10-01,Article,"Patern, Fabio;Santoro, Carmen;Spano, Lucio Davide",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053563844,10.1016/j.jss.2011.07.006,Understanding the relevance of micro-structures for design patterns detection,"One important issue concerning design patterns in reverse engineering is their detection to support program comprehension, design recovery, system (re-)documentation, and software evolution. The objectives of this paper are to identify and analyze different types of building blocks of design patterns and to evaluate if the detection of these building blocks (called micro-structures) is relevant for the detection of occurrences of the design patterns. This analysis is useful to understand how the different types of micro-structures can be combined to better comprehend design patterns and to improve their detection. To achieve the objectives, the paper provides a description of different micro-structures, an analysis of their relevance in different design motifs, and a statistical analysis on the number and types of micro-structures present in different design patterns. Finally, we investigate if the detection of some design patterns can be performed only through the detection of a combined set of micro-structures, or other techniques should be exploited. © 2011 Elsevier Inc. All rights reserved.",Design motif detection | Design patterns | Micro-structures,Journal of Systems and Software,2011-12-01,Article,"Arcelli Fontana, F.;Maggioni, S.;Raibulet, C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053560498,10.1016/j.jss.2011.07.027,P2P-based multidimensional indexing methods: A survey,"P2P-based multidimensional index (MI) is a hotspot which absorbs many researchers to dedicate them into. However, no summarization or review on this technology has been made at present. To the best of our knowledge, this is the first work on reviewing P2P-based MI. This paper innovatively adopts visualization technique to show the research groups and then analyzes investigating style of research groups. Based on evolution of P2P-based MI inheriting from centralized MI and P2P, we divide P2P-based MI methods into 4 categories: extending centralized MI, extending P2P, combining centralized MI and P2P, and miscellaneous. For each category, the paper selects classical techniques and describes them in detail. This is the first time of doing the classification job over massive related works. Finally, load balancing and update strategies are described and discussed for they are important factors related to performance. We believe many researchers will get benefits from our work for further studies. © 2011 Elsevier Inc. All rights reserved.",Multidimensional index | Peer-to-peer computing | Survey,Journal of Systems and Software,2011-01-01,Article,"Zhang, Chong;Xiao, Weidong;Tang, Daquan;Tang, Jiuyang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959283675,10.1145/1985441.1985447,A study of language usage evolution in open source software,"The use of programming languages such as Java and C in Open Source Software (OSS) has been well studied. However, many other popular languages such as XSL or XML have received minor attention. In this paper, we discuss some trends in OSS development that we observed when considering multiple programming language evolution of OSS. Based on the revision data of 22 OSS projects, we tracked the evolution of language usage and other artefacts such as documentation files, binaries and graphics files. In these systems several different languages and artefact types including C/C++, Java, XML, XSL, Makefile, Groovy, HTML, Shell scripts, CSS, Graphics files, JavaScript, JSP, Ruby, Phyton, XQuery, OpenDocument files, PHP, etc. have been used. We found that the amount of code written in different languages differs substantially. Some of our findings can be summarized as follows: (1) JavaScript and CSS files most often co-evolve with XSL; (2) Most Java developers but only every second C/C++ developer work with XML; (3) and more generally, we observed a significant increase of usage of XML and XSL during recent years and found that Java or C are hardly ever the only language used by a developer. In fact, a developer works with more than 5 different artefact types (or 4 different languages) in a project on average. © 2011 ACM.",evolution | open source software | programming language | software archives,Proceedings - International Conference on Software Engineering,2011-06-22,Conference Paper,"Karus, Siim;Gall, Harald",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959279588,10.1145/1985441.1985466,Automated topic naming to support cross-project analysis of software maintenance activities,"Researchers have employed a variety of techniques to extract underlying topics that relate to software development artifacts. Typically, these techniques use semi-unsupervised machine-learning algorithms to suggest candidate word-lists. However, word-lists are difficult to interpret in the absence of meaningful summary labels. Current topic modeling techniques assume manual labelling and do not use domainspecific knowledge to improve, contextualize, or describe results for the developers. We propose a solution: automated labelled topic extraction. Topics are extracted using Latent Dirichlet Allocation (LDA) from commit-log comments recovered from source control systems such as CVS and Bit-Keeper. These topics are given labels from a generalizable cross-project taxonomy, consisting of non-functional requirements. Our approach was evaluated with experiments and case studies on two large-scale RDBMS projects: MySQL and MaxDB. The case studies show that labelled topic extraction can produce appropriate, context-sensitive labels relevant to these projects, which provides fresh insight into their evolving software development activities. © 2011 ACM.",lda | non-functional requirements | topic analysis,Proceedings - International Conference on Software Engineering,2011-06-22,Conference Paper,"Hindle, Abram;Ernst, Neil A.;Godfrey, Michael W.;Mylopoulos, John",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959216965,10.1145/1985441.1985471,Improving identifier informativeness using part of speech information,"Recent software development tools have exploited the mining of natural language information found within software and its supporting documentation. To make the most of this information, researchers have drawn upon the work of the natural language processing community for tools and techniques. One such tool provides part-of-speech information, which finds application in improving the searching of software repositories and extracting domain information found in identifiers. Unfortunately, the natural language found is software differs from that found in standard prose. This difference potentially limits the effectiveness of off-the-shelf tools. An empirical investigation finds that with minimal guidance an existing tagger was correct 88% of the time when tagging the words found in source code identifiers. The investigation then uses the improved part-of-speech information to tag a large corpus of over 145,000 structure-field names. From patterns in the tags several rules emerge that seek to understand past usage and to improve future naming. © 2011 ACM.",identifier analysis | natural language processing | program comprehension,Proceedings - International Conference on Software Engineering,2011-06-22,Conference Paper,"Binkley, Dave;Hearn, Matthew;Lawrie, Dawn",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-81455154924,10.1145/2048066.2048070,SHERIFF: Precise detection and automatic mitigation of false sharing,"False sharing is an insidious problem for multithreaded programs running on multicore processors, where it can silently degrade performance and scalability. Previous tools for detecting false sharing are severely limited: they cannot distinguish false sharing from true sharing, have high false positive rates, and provide limited assistance to help programmers locate and resolve false sharing. This paper presents two tools that attack the problem of false sharing: SHERIFF-DETECT and SHERIFF-PROTECT. Both tools leverage a framework we introduce here called SHERIFF. SHERIFF breaks out threads into separate processes, and exposes an API that allows programs to perform per-thread memory isolation and tracking on a per-page basis. We believe SHERIFF is of independent interest. SHERIFF-DETECT finds instances of false sharing by comparing updates within the same cache lines by different threads, and uses sampling to rank them by performance impact. SHERIFF-DETECT is precise (no false positives), runs with low overhead (on average, 20%), and is accurate, pinpointing the exact objects involved in false sharing. We present a case study demonstrating SHERIFF-DETECT's effectiveness at locating false sharing in a variety of benchmarks. Rewriting a program to fix false sharing can be infeasible when source is unavailable, or undesirable when padding objects would unacceptably increase memory consumption or further worsen runtime performance. SHERIFF-PROTECT mitigates false sharing by adaptively isolating shared updates from different threads into separate physical addresses, effectively eliminating most of the performance impact of false sharing.We show that SHERIFF-PROTECT can improve performance for programs with catastrophic false sharing by up to 9x, without pr rogrammer intervention. Copyright is held by the author / owner(s).",False sharing | Multi-threaded,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2011-11-23,Conference Paper,"Liu, Tongping;Berger, Emery D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81455141863,10.1145/2048066.2048096,Immutable specifications for more concise and precise verification,"In the current work, we investigate the benefits of immutability guarantees for allowing more flexible handling of aliasing, as well as more precise and concise specifications. Our approach supports finer levels of control that can mark data structures as being immutable through the use of immutability annotations. By using such annotations to encode immutability guarantees, we expect to obtain better specifications that can more accurately describe the intentions, as well as prohibitions, of the method. Ultimately, our goal is improving the precision of the verification process, as well as making the specifications more readable, more precise and as an enforceable program documentation.We have designed and implemented a new entailment procedure to formally and automatically reason about immutability enhanced specifications. We have also formalised the soundness for our new procedure through an operational semantics with mutability assertions on the heap. Lastly, we have carried out a set of experiments to both validate and affirm the utility of our current proposal on immutability enhanced specification mechanism. Copyright is held by the author / owner(s).",Languages | Verification,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2011-11-23,Conference Paper,"David, Cristina;Chin, Wei Ngan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81455159339,10.1145/2048066.2048126,JIT compilation policy for modern machines,"Dynamic or Just-in-Time (JIT) compilation is crucial to achieve acceptable performance for applications (written in managed languages, such as Java and C#) distributed as intermediate language binary codes for a virtual machine (VM) architecture. Since it occurs at runtime, JIT compilation needs to carefully tune its compilation policy to make effective decisions regarding if and when to compile different program regions to achieve the best overall program performance. Past research has extensively tuned JIT compilation policies, but mainly for VMs with a single compiler thread and for execution on single-processor machines. This work is driven by the need to explore the most effective JIT compilation strategies in their modern operational environment, where (a) processors have evolved from single to multi/many cores, and (b) VMs provide support for multiple concurrent compiler threads. Our results confirm that changing if and when methods are compiled have significant performance impacts. We construct several novel configurations in the HotSpot JVM to facilitate this study. The new configurations are necessitated by modern Java benchmarks that impede traditional static whole-program discovery, analysis and annotation, and are required for simulating future many-core hardware that is not yet widely available. We study the effects on performance of increasing compiler aggressiveness for VMs with multiple compiler threads running on existing single/multi-core and future many-core machines. Our results indicate that although more aggressive JIT compilation policies show no benefits on single-core machines, these can often improve program performance for multi/many-core machines. However, accurately prioritizing JIT method compilations is crucial to realize such benefits. Copyright is held by the author / owner(s).",Dynamic compilation | Java | Multicore | Virtual machines,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2011-11-23,Conference Paper,"Kulkarni, Prasad A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81455159314,10.1145/2048066.2048142,Freedom before commitment: A lightweight type system for object initialisation,"One of the main purposes of object initialisation is to establish invariants such as a field being non-null or an immutable data structure containing specific values. These invariants are then implicitly assumed by the rest of the implementation, for instance, to ensure that a field may be safely dereferenced or that immutable data may be accessed concurrently. Consequently, letting an object escape from its constructor is dangerous; the escaping object might not yet satisfy its invariants, leading to errors in code that relies on them. Nevertheless, preventing objects entirely from escaping from their constructors is too restrictive; it is often useful to call auxiliary methods on the object under initialisation or to pass it to another constructor to set up mutually-recursive structures. We present a type system that tracks which objects are fully initialised and which are still under initialisation. The system can be used to prevent objects from escaping, but also to allow safe escaping by making explicit which objects might not yet satisfy their invariants. We designed, formalised and implemented our system as an extension to a non-null type system, but it is not limited to this application. Our system is conceptually simple and requires little annotation overhead; it is sound and sufficiently expressive for many common programming idioms. Therefore, we believe it to be the first such system suitable for mainstream use. Copyright is held by the author / owner(s).",Design | Languages | Reliability,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2011-11-23,Conference Paper,"Summers, Alexander J.;Müller, Peter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81455159309,10.1145/2048066.2048145,F4F: Taint analysis of framework-based web applications,"This paper presents F4F (Framework For Frameworks), a system for effective taint analysis of framework-based web applications. Most modern web applications utilize one or more web frameworks, which provide useful abstractions for common functionality. Due to extensive use of reflective language constructs in framework implementations, existing static taint analyses are often ineffective when applied to framework-based applications. While previous work has included ad hoc support for certain framework constructs, adding support for a large number of frameworks in this manner does not scale from an engineering standpoint. F4F employs an initial analysis pass in which both application code and configuration files are processed to generate a specification of framework-related behaviors. A taint analysis engine can leverage these specifications to perform a much deeper, more precise analysis of framework-based applications. Our specification language has only a small number of simple but powerful constructs, easing analysis engine integration. With this architecture, new frameworks can be handled with no changes to the core analysis engine, yielding significant engineering benefits. We implemented specification generators for several web frameworks and added F4F support to a state-of-the-art taint-analysis engine. In an experimental evaluation, the taint analysis enhanced with F4F discovered 525 new issues across nine benchmarks, a harmonic mean of 2.10X more issues per benchmark. Furthermore, manual inspection of a subset of the new issues showed that many were exploitable or reflected bad security practice. Copyright is held by the author / owner(s).",Languages | Security,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2011-11-23,Conference Paper,"Sridharan, Manu;Artzi, Shay;Pistoia, Marco;Guarnieri, Salvatore;Tripp, Omer;Berg, Ryan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-81455154871,10.1145/2048066.2048146,Rolecast: Finding missing security checks when you do not know what checks are,"Web applications written in languages such as PHP and JSP are notoriously vulnerable to accidentally omitted authorization checks and other security bugs. Existing techniques that find missing security checks in library and system code assume that (1) security checks can be recognized syntactically and (2) the same pattern of checks applies universally to all programs. These assumptions do not hold for Web applications. EachWeb application uses different variables and logic to check the user's permissions. Even within the application, security logic varies based on the user's role, e.g., regular users versus administrators. This paper describes ROLECAST, the first system capable of statically identifying security logic that mediates securitysensitive events (such as database writes) in Web applications, rather than taking a specification of this logic as input. We observe a consistent software engineering pattern-the code that implements distinct user role functionality and its security logic resides in distinct methods and files-and develop a novel algorithm for discovering this pattern in Web applications. Our algorithm partitions the set of file contexts (a coarsening of calling contexts) on which securitysensitive events are control dependent into roles. Roles are based on common functionality and security logic. ROLECAST identifies security-critical variables and applies rolespecific variable consistency analysis to find missing security checks. ROLECAST discovered 13 previously unreported, remotely exploitable vulnerabilities in 11 substantial PHP and JSP applications, with only 3 false positives. This paper demonstrates that (1) accurate inference of application- and role-specific security logic improves the security of Web applications without specifications, and (2) static analysis can discover security logic automatically by exploiting distinctive software engineering features. Copyright is held by the author / owner(s).",Access control | Interprocedural analysis | Jsp | Php | Security | Security checks | Static analysis | User roles,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2011-11-23,Conference Paper,"Son, Sooel;McKinley, Kathryn S.;Shmatikov, Vitaly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959873913,10.1145/1993498.1993500,Commutative set: A language extension for implicit parallel programming,"Sequential programming models express a total program order, of which a partial order must be respected. This inhibits parallelizing tools from extracting scalable performance. Programmer written semantic commutativity assertions provide a natural way of relaxing this partial order, thereby exposing parallelism implicitly in a program. Existing implicit parallel programming models based on semantic commutativity either require additional programming extensions, or have limited expressiveness. This paper presents a generalized semantic commutativity based programming extension, called Commutative Set (COMMSET), and associated compiler technology that enables multiple forms of parallelism. COMMSET expressions are syntactically succinct and enable the programmer to specify commutativity relations between groups of arbitrary structured code blocks. Using only this construct, serializing constraints that inhibit parallelization can be relaxed, independent of any particular parallelization strategy or concurrency control mechanism. COMMSET enables well performing parallelizations in cases where they were inapplicable or non-performing before. By extending eight sequential programs with only 8 annotations per program on average, COMMSET and the associated compiler technology produced a geomean speedup of 5.7x on eight cores compared to 1.5x for the best non-COMMSET parallelization. © 2011 ACM.",automatic parallelization | implicit parallelism | programming model | semantic commutativity | static analysis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Prabhu, Prakash;Ghosh, Soumyadeep;Zhang, Yun;Johnson, Nick P.;August, David I.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959876215,10.1145/1993498.1993504,Data representation synthesis,"We consider the problem of specifying combinations of data structures with complex sharing in a manner that is both declarative and results in provably correct code. In our approach, abstract data types are specified using relational algebra and functional dependencies. We describe a language of decompositions that permit the user to specify different concrete representations for relations, and show that operations on concrete representations soundly implement their relational specification. It is easy to incorporate data representations synthesized by our compiler into existing systems, leading to code that is simpler, correct by construction, and comparable in performance to the code it replaces. © 2011 ACM.",composite data structures | synthesis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Hawkins, Peter;Aiken, Alex;Fisher, Kathleen;Rinard, Martin;Sagiv, Mooly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959904195,10.1145/1993498.1993516,Automatic CPU-GPU communication management and optimization,"The performance benefits of GPU parallelism can be enormous, but unlocking this performance potential is challenging. The applicability and performance of GPU parallelizations is limited by the complexities of CPU-GPU communication. To address these communications problems, this paper presents the first fully automatic system for managing and optimizing CPU-GPU communcation. This system, called the CPU-GPU Communication Manager (CGCM), consists of a run-time library and a set of compiler transformations that work together to manage and optimize CPU-GPU communication without depending on the strength of static compile-time analyses or on programmer-supplied annotations. CGCM eases manual GPU parallelizations and improves the applicability and performance of automatic GPU parallelizations. For 24 programs, CGCM-enabled automatic GPU parallelization yields a whole program geomean speedup of 5.36x over the best sequential CPU-only execution. © 2011 ACM.",communication | gpu | management | optimization,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Jablin, Thomas B.;Prabhu, Prakash;Jablin, James A.;Johnson, Nick P.;Beard, Stephen R.;August, David I.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959878920,10.1145/1993498.1993518,EnerJ: Approximate data types for safe and general low-power computation,"Energy is increasingly a first-order concern in computer systems. Exploiting energy-accuracy trade-offs is an attractive choice in applications that can tolerate inaccuracies. Recent work has explored exposing this trade-off in programming models. A key challenge, though, is how to isolate parts of the program that must be precise from those that can be approximated so that a program functions correctly even as quality of service degrades. We propose using type qualifiers to declare data that may be subject to approximate computation. Using these types, the system automatically maps approximate variables to low-power storage, uses low-power operations, and even applies more energy-efficient algorithms provided by the programmer. In addition, the system can statically guarantee isolation of the precise program component from the approximate component. This allows a programmer to control explicitly how information flows from approximate data to precise data. Importantly, employing static analysis eliminates the need for dynamic checks, further improving energy savings. As a proof of concept, we develop EnerJ, an extension to Java that adds approximate data types. We also propose a hardware architecture that offers explicit approximate storage and computation. We port several applications to EnerJ and show that our extensions are expressive and effective; a small number of annotations lead to significant potential energy savings (10%-50%) at very little accuracy cost. © 2011 ACM.",accuracy-aware computing | critical data | energy | power-aware computing | soft errors,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Sampson, Adrian;Dietl, Werner;Fortuna, Emily;Gnanapragasam, Danushen;Ceze, Luis;Grossman, Dan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959908491,10.1145/1993498.1993524,"Probabilistic, modular and scalable inference of typestate specifications","Static analysis tools aim to find bugs in software that correspond to violations of specifications. Unfortunately, for large and complex software, these specifications are usually either unavailable or sophisticated, and hard to write. This paper presents ANEK, a tool and accompanying methodology for inferring specifications useful for modular typestate checking of programs. In particular, these specifications consist of pre and postconditions along with aliasing annotations known as access permissions. A novel feature of ANEK is that it can generate program specifications even when the code under analysis gives rise to conflicting constraints, a situation that typically occurs when there are bugs. The design of ANEK also makes it easy to add heuristic constraints that encode intuitions gleaned from several years of experience writing such specifications, and this allows it to infer specifications that are better in a subjective sense. The ANEK algorithm is based on a modular analysis that makes it fast and scalable, while producing reliable specifications. All of these features are enabled by its underlying probabilistic analysis that produces specifications that are very likely. Our implementation of ANEK infers access permissions specifications used by the PLURAL [5] modular typestate checker for Java programs. We have run ANEK on a number of Java benchmark programs, including one large open-source program(approximately 38K lines of code), to infer specifications that were then checked using PLURAL. The results for the large benchmark show that ANEK can quickly infer specifications that are both accurate and qualitatively similar to those written by hand, and at 5% of the time taken to manually discover and hand-code the specifications. © 2011 ACM.",aliasing | inference | object protocol | ownership | permission | specification | typestate,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Beckman, Nels E.;Nori, Aditya V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959871475,10.1145/1993498.1993526,Mostly-automated verification of low-level programs in computational separation logic,"Several recent projects have shown the feasibility of verifying low-level systems software. Verifications based on automated theorem-proving have omitted reasoning about first-class code pointers, which is critical for tasks like certifying implementations of threads and processes. Conversely, verifications that deal with first-class code pointers have featured long, complex, manual proofs. In this paper, we introduce the Bedrock framework, which supports mostly-automated proofs about programs with the full range of features needed to implement, e.g., language runtime systems. The heart of our approach is in mostly-automated discharge of verification conditions inspired by separation logic. Our take on separation logic is computational, in the sense that function specifications are usually written in terms of reference implementations in a purely functional language. Logical quantifiers are the most challenging feature for most automated verifiers; by relying on functional programs (written in the expressive language of the Coq proof assistant), we are able to avoid quantifiers almost entirely. This leads to some dramatic improvements compared to both past work in classical verification, which we compare against with implementations of data structures like binary search trees and hash tables; and past work in verified programming with code pointers, which we compare against with examples like function memoization and a cooperative threading library. © 2011 ACM.",functional programming | interactive proof assistants | low-level programming languages | separation logic,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Chlipala, Adam",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959892905,10.1145/1993498.1993530,LeakChaser: Helping programmers narrow down causes of memory leaks,"In large programs written in managed languages such as Java and C#, holding unnecessary references often results in memory leaks and bloat, degrading significantly their run-time performance and scalability. Despite the existence of many leak detectors for such languages, these detectors often target low-level objects; as a result, their reports contain many false warnings and lack sufficient semantic information to help diagnose problems. This paper introduces a specification-based technique called LeakChaser that can not only capture precisely the unnecessary references leading to leaks, but also explain, with high-level semantics, why these references become unnecessary. At the heart of LeakChaser is a three-tier approach that uses varying levels of abstraction to assist programmers with different skill levels and code familiarity to find leaks. At the highest tier of the approach, the programmer only needs to specify the boundaries of coarse-grained activities, referred to as transactions. The tool automatically infers liveness properties of these transactions, by monitoring the execution, in order to find unnecessary references. Diagnosis at this tier can be performed by any programmer after inspecting the APIs and basic modules of a program, without understanding of the detailed implementation of these APIs. At the middle tier, the programmer can introduce application-specific semantic information by specifying properties for the transactions. At the lowest tier of the approach is a liveness checker that does not rely on higher-level semantic information, but rather allows a programmer to assert lifetime relationships for pairs of objects. This task could only be performed by skillful programmers who have a clear understanding of data structures and algorithms in the program. We have implemented LeakChaser in Jikes RVM and used it to help us diagnose several real-world leaks. The implementation incurs a reasonable overhead for debugging and tuning. Our case studies indicate that the implementation is powerful in guiding programmers with varying code familiarity to find the root causes of several memory leaks - -even someone who had not studied a leaking program can quickly find the cause after using LeakChaser's iterative process that infers and checks properties with different levels of semantic information. © 2011 ACM.",leakchaser | memory leak detection | object lifetime assertions,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Xu, Guoqing;Bond, Michael D.;Qin, Feng;Rountev, Atanas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959910753,10.1145/1993498.1993539,A security policy oracle: Detecting security holes using multiple API implementations,"Even experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. Neither approach guarantees that the policy used for verification is correct. In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flow- and context-sensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and security-sensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. Security-policy differencing has no intrinsic false positives: implementations of the same API must enforce the same policy, or at least one of them is wrong! Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, well-studied libraries. Multiple API implementations are proliferating due to cloud-based software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering ""deep"" bugs in them. © 2011 ACM.",access control | authorization | java class libraries | security | static analysis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Srivastava, Varun;Bond, Michael D.;McKinley, Kathryn S.;Shmatikov, Vitaly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959913663,10.1145/1993498.1993555,ALTER: Exploiting breakable dependences for parallelization,"For decades, compilers have relied on dependence analysis to determine the legality of their transformations. While this conservative approach has enabled many robust optimizations, when it comes to parallelization there are many opportunities that can only be exploited by changing or re-ordering the dependences in the program. This paper presents Alter: a system for identifying and enforcing parallelism that violates certain dependences while preserving overall program functionality. Based on programmer annotations, Alter exploits new parallelism in loops by reordering iterations or allowing stale reads. Alter can also infer which annotations are likely to benefit the program by using a test-driven framework. Our evaluation of Alter demonstrates that it uncovers parallelism that is beyond the reach of existing static and dynamic tools. Across a selection of 12 performance-intensive loops, 9 of which have loop-carried dependences, Alter obtains an average speedup of 2.0x on 4 cores. © 2011 ACM.",automatic parallelization | dependences | deterministic | program annotations | software transactional memory | speculation,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Udupa, Abhishek;Rajan, Kaushik;Thies, William",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959887153,10.1145/1993498.1993561,Verification of semantic commutativity conditions and inverse operations on linked data structures,"We present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not necessarily identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions. Many speculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state. Together, the commutativity conditions and inverse operations provide a key resource that language designers, developers of program analysis systems, and implementors of software systems can draw on to build languages, program analyses, and systems with strong correctness guarantees. © 2011 ACM.",commutativity condition | data structure | inverse operation | verification,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Kim, Deokhwan;Rinard, Martin C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959900605,10.1145/1993498.1993562,Exploiting the commutativity lattice,"Speculative execution is a promising approach for exploiting parallelism in many programs, but it requires efficient schemes for detecting conflicts between concurrently executing threads. Prior work has argued that checking semantic commutativity of method invocations is the right way to detect conflicts for complex data structures such as kd-trees. Several ad hoc ways of checking commutativity have been proposed in the literature, but there is no systematic approach for producing implementations. In this paper, we describe a novel framework for reasoning about commutativity conditions: the commutativity lattice. We show how commutativity specifications from this lattice can be systematically implemented in one of three different schemes: abstract locking, forward gatekeeping and general gatekeeping. We also discuss a disciplined approach to exploiting the lattice to find different implementations that trade off precision in conflict detection for performance. Finally, we show that our novel conflict detection schemes are practical and can deliver speedup on three real-world applications. © 2011 ACM.",commutativity lattice | optimistic parallelism | transactions,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Kulkarni, Milind;Nguyen, Donald;Prountzos, Dimitrios;Sui, Xin;Pingali, Keshav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959888753,10.1145/1993498.1993565,Precise and compact modular procedure summaries for heap manipulating programs,"We present a strictly bottom-up, summary-based, and precise heap analysis targeted for program verification that performs strong updates to heap locations at call sites. We first present a theory of heap decompositions that forms the basis of our approach; we then describe a full analysis algorithm that is fully symbolic and efficient. We demonstrate the precision and scalability of our approach for verification of real C and C++ programs. © 2011 ACM.",pointer analysis | summary-based analysis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Dillig, Isil;Dillig, Thomas;Aiken, Alex;Sagiv, Mooly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959878355,10.1145/1993498.1993566,On inter-procedural analysis of programs with lists and data,"We address the problem of automatic synthesis of assertions on sequential programs with singly-linked lists containing data over infinite domains such as integers or reals. Our approach is based on an accurate abstract inter-procedural analysis. Program configurations are represented by graphs where nodes represent list segments without sharing. The data in these list segments are characterized by constraints in abstract domains. We consider a domain where constraints are in a universally quantified fragment of the first-order logic over sequences, as well as a domain constraining the multisets of data in sequences. Our analysis computes the effect of each procedure in a local manner, by considering only the reachable part of the heap from its actual parameters. In order to avoid losses of information, we introduce a mechanism based on unfolding/folding operations allowing to strengthen the analysis in the domain of first-order formulas by the analysis in the multisets domain. The same mechanism is used for strengthening the sound (but incomplete) entailment operator of the domain of first-order formulas. We have implemented our techniques in a prototype tool and we have shown that our approach is powerful enough for automatic (1) generation of non-trivial procedure summaries, (2) pre/post-condition reasoning, and (3) procedure equivalence checking. © 2011 ACM.",assertion synthesis | dynamic data structures | interprocedural analysis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Bouajjani, Ahmed;Drǎgoi, Cezara;Enea, Constantin;Sighireanu, Mihaela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959886562,10.1145/1993498.1993569,Taming the wildcards: Combining definition- and use-site variance,"Variance allows the safe integration of parametric and subtype polymorphism. Two flavors of variance, definition-site versus use-site variance, have been studied and have had their merits hotly debated. Definition-site variance (as in Scala and C#) offers simple type-instantiation rules, but causes fractured definitions of naturally invariant classes; Use-site variance (as in Java) offers simplicity in class definitions, yet complex type-instantiation rules that elude most programmers. We present a unifying framework for reasoning about variance. Our framework is quite simple and entirely denotational, that is, it evokes directly the definition of variance with a small core calculus that does not depend on specific type systems. This general framework can have multiple applications to combine the best of both worlds: for instance, it can be used to add use-site variance annotations to the Scala type system. We show one such application in detail: we extend the Java type system with a mechanism that modularly infers the definition-site variance of type parameters, while allowing use-site variance annotations on any type-instantiation. Applying our technique to six Java generic libraries (including the Java core library) shows that 20-58 (depending on the library) of generic definitions are inferred to have single-variance; 8-63% of method signatures can be relaxed through this inference, and up to 91% of existing wildcard annotations are unnecessary and can be elided. © 2011 ACM.",definition-site variance | language extensions | use-site variance | variance | wildcards,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Altidor, John;Huang, Shan Shan;Smaragdakis, Yannis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/1926385.1926406,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/1926385.1926416,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/1926385.1926417,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82055191935,10.1109/SCAM.2011.24,Counting bugs is harder than you think,"Software Assurance Metrics and Tool Evaluation (SAMATE) is a broad, inclusive project at the U.S. National Institute of Standards and Technology (NIST) with the goal of improving software assurance by developing materials, specifications, and methods to test tools and techniques and measure their effectiveness. We review some SAMATE sub-projects: web application security scanners, malware research protocol, electronic voting systems, the SAMATE Reference Dataset, a public repository of thousands of example programs with known weaknesses, and the Static Analysis Tool Exposition (SATE). Along the way we list over two dozen possible research questions, which are also collaboration opportunities. Software metrics are incomplete without metrics of what is variously called bugs, flaws, or faults. We detail numerous critical research problems related to such metrics. For instance, is a warning from a source code scanner a real bug, a false positive, or something else? If a numeric overflow leads to buffer overflow, which leads to command injection, what is the error? How many bugs are there if two sources call two sinks: 1, 2, or 4? Where is a missing feature? We conclude with a list of concepts which may be a useful basis of bug metrics. © 2011 IEEE.",software debugging | software engineering | software metrics | software tools,"Proceedings - 11th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2011",2011-11-29,Conference Paper,"Black, Paul E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82055200236,10.1109/SCAM.2011.22,Program analysis too loopy? Set the loops aside,"Among the many obstacles in efficient and sound program analysis, loops may be the most prevalent. In program analyses that traverse paths, loops introduce a variable, possibly infinite, number of paths. This paper looks at whether loops could be analyzed separately and replaced with a summary. First, the complexity of a loop is estimated by counting the paths through the body of the loop. 85% of the loops have fewer than ten paths and less than 1% have more than 10,000 paths. Second, the number of paths is computed by analyzing loops separately to assess the feasibility of such an analysis approach. While the number of paths is decreased in many cases, it is typically not sufficient for long, complex functions. Finally, loops are classified based on their stopping condition and further analyzed for programming elements that may make loop analysis more difficult. Nearly half of the loops are array traversals and over half of the loops contain a function call. © 2011 IEEE.",loops | paths | program analysis,"Proceedings - 11th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2011",2011-11-29,Conference Paper,"Larson, Eric",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82055200258,10.1109/SCAM.2011.12,Analyzing the effect of preprocessor annotations on code clones,"The C preprocessor cpp is a powerful and language-independent tool, widely used to implement variable software in different programming languages (C, C++) using conditional compilation. Preprocessor annotations can used on different levels of granularity such as functions or statements. In this paper, we investigate whether there is a relation between code clones and preprocessor annotations. Specifically, we address the question whether the discipline of annotation has an effect on code clones. To this end, we perform a case study on fifteen different C programs and analyze them regarding code clones and #ifdef occurrences. We found only minor effects of annotations on code clones, but a relationship between annotations that align with the code structure (and code clones). With this work, we provide new insights why code clones occur in C programs. Furthermore, the results can support the decision whether or not it is beneficial to remove clones. © 2011 IEEE.",C preprocessor | code clones | empirical study | software maintenance,"Proceedings - 11th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2011",2011-11-29,Conference Paper,"Schulze, Sandro;Jürgens, Elmar;Feigenspan, Janet",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82055183819,10.1109/SCAM.2011.14,A comparative study of code query technologies,"When analyzing software systems we face the challenge of how to implement a particular analysis for different programming languages. A solution for this problem is to write a single analysis using a code query language, abstracting from the specificities of languages being analyzed. Over the past ten years many code query technologies have been developed, based on different formalisms. Each technology comes with its own query language and set of features. To determine the state of the art of code querying we compare the languages and tools for seven code query technologies: Grok, Rscript, JRelCal, Semmle Code, JGraLab, CrocoPat and JTransformer. The specification of a package stability metric is used as a running example to compare the languages. The comparison involves twelve criteria, some of which are concerned with properties of the query language (paradigm, types, parametrization, polymorphism, modularity, and libraries), and some of which are concerned with the tool itself (output formats, interactive interface, API support, interchange formats, extraction support, and licensing). We contextualize the criteria in two usage scenarios: interactive and tool integration. We conclude that there is no particularly weak or dominant tool. As important improvement points, we identify the lack of library mechanisms, interchange formats, and possibilities for integration with source code extractors. © 2011 IEEE.",Code query | comparative study | CrocoPat | Grok | JGraLab | JRelCal | JTransformer | Rscript | SemmleCode | software analysis,"Proceedings - 11th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2011",2011-11-29,Conference Paper,"Alves, Tiago L.;Hage, Jurriaan;Rademaker, Peter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82055196653,10.1109/SCAM.2011.5,I2SD: Reverse engineering sequence diagrams from enterprise java beans with interceptors,"An Enterprise Java Beans (EJB) interceptor is a software mechanism that provides for introducing behavior implemented as separate code into the execution of a Java application. In this way EJB interceptors provide a clear separation of the core functionality of the bean and other concerns, such as logging or performance analysis. Despite the beauty of the idea behind the interceptors, developing, testing and managing dependencies introduced by the interceptors are considered to be daunting tasks. For example, the developers can specify interceptors at multiple locations and by multiple means. However, different locations and specification means influence the order of the interceptor invocation, which is governed by more than fifteen different intertwined rules according to the EJB standard. To facilitate development of EJB applications we have designed I2SD, Interceptors to Sequence Diagrams, a tool for reverse engineering EJB applications with interceptors to UML sequence diagrams. I2SD provides the developer with a visual feedback and can be used by quality managers to get a broader understanding of the way interceptors are used in their project. © 2011 IEEE.",,"Proceedings - 11th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2011",2011-11-29,Conference Paper,"Roubtsov, Serguei;Serebrenik, Alexander;Mazoyer, Aurélien;Brand, Mark Van Den",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054782605,10.1145/2038476.2038489,Game description language and frameworks for Langrid gaming,"Langrid Gaming, which provides multilingual communication for gaming simulations using translation services provided on the Language Grid, is a useful educational tool and can contribute to the resolution of international social conflicts. This tool has been enhanced on the basis of the experience and know-how obtained from the participants' comments and suggestions. However, the object and the model object of the gaming tool are usually defined as Java or PHP objects, which makes it difficult for non-programmers to develop a new game. In this study, we propose a framework to implement various types of Langrid Gaming easily. In particular, we designed a new domain specific language (DSL) called 'Karina' to describe targeted games, and implemented a translator to convert Karina script into CGI programs. This script development environment makes game building easy and simplifies the development procedures, regardless of whether the gaming developer is a programmer or not. © 2011 ACM.",gaming simulation | intercultural collaboration | language grid,SIGDOC'11 - Proceedings of the 29th ACM International Conference on Design of Communication,2011-10-25,Conference Paper,"Suzuki, Itaru;Tsunoda, Keisuke;Hishiyama, Reiko",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054787059,10.1145/2038476.2038497,On the effect of visual refinement upon user feedback in the context of video prototyping,"There has been extensive discussion and research surrounding fidelity or refinement of prototypes in paper and software form, especially focusing on how the nature of prototypes influences the feedback that this prototype can help elicit during user testing. We extend this debate to the domain of video prototypes, where use scenarios are acted out on video. This study examines how the visual refinement (a.k.a. visual fidelity) of design representations presented in such videos impacts user feedback. An experiment was performed where two video prototypes were compared, one where the product is portrayed with high visual refinement and the other looking rough and sketchy. Our results could not identify any significant effects upon the number or type of comments returned by users. This finding contrasts widely held contentions relating to fidelity of software and paper prototypes, though it agrees with similar experiments done with non video prototypes. In practice our results support the validity of testing with low fidelity videos and suggest that the choice of visual fidelity in video prototypes should be based on pragmatic project concerns, e.g., whether the video should be used also for communication and the resources that are available for prototyping. © 2011 ACM.",fidelity | video prototyping | visual refinement,SIGDOC'11 - Proceedings of the 29th ACM International Conference on Design of Communication,2011-10-25,Conference Paper,"Bojic, Miroslav;Goulati, Areti;Szostak, Dalila;Markopoulos, Panos",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80054785040,10.1145/2038476.2038509,Necessary and neglected? An empirical study of internal documentation in agile software development teams,"When compared to traditional development methods, agile development practices are associated with more direct communication and less documentation. However, few empirical studies exist that investigate the role of documentation in agile development teams. We thus employed a questionnaire to measure the perceptions of a group of agile practitioners with regard to the documentation in their projects. We obtained responses from 79 agile software development professionals and 8 teams in 13 different countries. Our findings include that over half of developers in our data set find documentation important or even very important but that too little documentation is available in their projects. Agile practitioners do not seem to agree with the agile principle that ""The most efficient and effective method of conveying information to and within a development team is face-to-face conversation."" We were able to validate this result for a set of dissimilar agile teams in various domains. © 2011 ACM.",agile software development | documentation | empirical study,SIGDOC'11 - Proceedings of the 29th ACM International Conference on Design of Communication,2011-10-25,Conference Paper,"Stettina, Christoph Johann;Heijstek, Werner",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80054784947,10.1145/2038476.2038517,A cognitive perspective on developer comprehension of software design documentation,"Software design documentation is an important aid for communication during software development and maintenance. Nevertheless, little empirical evidence exists regarding the use of software documentation, and effective software design representation in particular. In an experimental setting, we used documentation from industry in which aspects of a software design were modeled in both a (UML) diagram and text. We recorded and analysed how participants used these media to answer various design-related questions and collected additional information in various questionnaires. By having participants think aloud, we set out to understand the underlying cognitive processes of developer design comprehension by applying the grounded theory method. We validated the results with concepts from the cognitive theory of multimedia learning. Results show a positive correlation between developer certainty and correctness of answers, whereas the opposite was not found. Also, self-rated experience and self-rated skill coincide with higher levels of certainty. We found that participants rated information based on perceived importance and that their ""common sense"" plays a significant role. Surprisingly, more than 60 percent of the answers were based on the consultation of a single medium. These results clearly ask for further investigation. We propose corresponding future work. © 2011 ACM.",cognitive theory of multimedia learning | software design comprehension,SIGDOC'11 - Proceedings of the 29th ACM International Conference on Design of Communication,2011-10-25,Conference Paper,"Schoonewille, Hugo H.;Heijstek, Werner;Chaudron, Michel R.V.;Kühne, Thomas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79960173379,10.1145/1993744.1993763,Structure-aware sampling on data streams,"The massive data streams observed in network monitoring, data processing and scientific studies are typically too large to store. For many applications over such data, we must obtain compact summaries of the stream. These summaries should allow accurate answering of post hoc queries with estimates which approximate the true answers over the original stream. The data often has an underlying structure which makes certain subset queries, in particular range queries, more relevant than arbitrary subsets. Applications such as access control, change detection, and heavy hitters typically involve subsets that are ranges or unions thereof. Random sampling is a natural summarization tool, being easy to implement and flexible to use. Known sampling methods are good for arbitrary queries but fail to optimize for the common case of range queries. Meanwhile, specialized summarization algorithms have been proposed for range-sum queries and related problems. These can outperform sampling giving fixed space resources, but lack its flexibility and simplicity. Particularly, their accuracy degrades when queries span multiple ranges. We define new stream sampling algorithms with a smooth and tunable trade-off between accuracy on range-sum queries and arbitrary subset-sum queries. The technical key is to relax requirements on the variance over all subsets to enable better performance on the ranges of interest. This boosts the accuracy on range queries while retaining the prime benefits of sampling, in particular flexibility and accuracy, with tail bounds guarantees. Our experimental study indicates that structure-aware summaries can drastically improve range-sum accuracy with respect to state-of-the-art stream sampling algorithms and outperform deterministic methods on range-sum queries and hierarchical heavy hitter queries. © Copyright 2011 ACM.",Approximate query processing | Data streams | Structure-aware sampling | VarOpt,Performance Evaluation Review,2011-01-01,Conference Paper,"Cohen, Edith;Cormode, Graham;Duffield, Nick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/1824760.1824763,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80052319925,10.1145/2000791.2000792,Prime: A methodology for developing provenance-aware applications,"Provenance refers to the past processes that brought about a given (version of an) object, item or entity. By knowing the provenance of data, users can often better understand, trust, reproduce, and validate it. A provenance-aware application has the functionality to answer questions regarding the provenance of the data it produces, by using documentation of past processes. Prime is a software engineering technique for adapting application designs to enable them to interact with a provenance middleware layer, thereby making them provenance-aware. In this article, we specify the steps involved in applying Prime, analyze its effectiveness, and illustrate its use with two case studies, in bioinformatics and medicine. © 2011 ACM.",Methodology | Provenance,ACM Transactions on Software Engineering and Methodology,2011-08-01,Article,"Miles, Simon;Groth, Paul;Munroe, Steve;Moreau, Luc",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053470521,10.1145/2000799.2000804,A compiler for multimodal scenarios: Transforming LSCs into aspectJ,"We exploit the main similarity between the aspect-oriented programming paradigm and the inter-object, scenario-based approach to specification, in order to construct a new way of executing systems based on the latter. Specifically, we transform multimodal scenario-based specifications, given in the visual language of live sequence charts (LSC), into what we call scenario aspects, implemented in AspectJ. Unlike synthesis approaches, which attempt to take the inter-object scenarios and construct intra-object state-based per-object specifications or a single controller automaton, we follow the ideas behind the LSC play-out algorithm to coordinate the simultaneous monitoring and direct execution of the specified scenarios. Thus, the structure of the specification is reflected in the structure of the generated code; the high-level inter-object requirements and their structure are not lost in the translation. The transformation/compilation scheme is fully implemented in a UML2-compliant tool we term the S2A compiler (for Scenarios to Aspects), which provides full code generation of reactive behavior from inter-object multimodal scenarios. S2A supports advanced scenario-based programming features, such as multiple instances and exact and symbolic parameters. We demonstrate our work with an application whose interobject behaviors are specified using LSCs. We discuss advantages and challenges of the compilation scheme in the context of the more general vision of scenario-based programming. © 2011 ACM.",Aspect oriented programming | Code generation | Inter-object approach | Live sequence charts | Scenario-based programming | Scenarios | UML sequence diagrams | Visual formalisms,ACM Transactions on Software Engineering and Methodology,2011-09-01,Article,"Maoz, Shahar;Harel, David;Kleinbort, Asaf",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79551508935,10.1109/TSE.2010.27,Bristlecone: Language support for robust software applications,"We present Bristlecone, a programming language for robust software systems. Bristlecone applications have two components: a high-level organization specification that describes how the application's conceptual operations interact and a low-level operational specification that describes the sequence of instructions that comprise an individual conceptual operation. Bristlecone uses the high-level organization specification to recover the software system from an error to a consistent state and to reason how to safely continue the software system's execution after the error. We have implemented a compiler and runtime for Bristlecone. We have evaluated this implementation on three benchmark applications: a Web crawler, a Web server, and a multiroom chat server. We developed both a Bristlecone version and a Java version of each benchmark application. We used injected failures to evaluate the robustness of each version of the application. We found that the Bristlecone versions of the benchmark applications more successfully survived the injected failures. The Bristlecone compiler contains a static analysis that operates on the organization specification to generate a set of diagrams that graphically present the task interactions in the application. We have used the analysis to help understand the high-level structure of three Bristlecone applications: a game server, a Web server, and a chat server. © 2011 IEEE.",Software robustness,IEEE Transactions on Software Engineering,2011-01-01,Article,"Demsky, Brian;Sundaramurthy, Sivaji",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79551549673,10.1109/TSE.2010.13,Deriving a slicing algorithm via FermaT transformations,"In this paper, we present a case study in deriving an algorithm from a formal specification via FermaT transformations. The general method (which is presented in a separate paper) is extended to a method for deriving an implementation of a program transformation from a specification of the program transformation. We use program slicing as an example transformation since this is of interest outside the program transformation community. We develop a formal specification for program slicing in the form of a WSL specification statement which is refined into a simple slicing algorithm by applying a sequence of general purpose program transformations and refinements. Finally, we show how the same methods can be used to derive an algorithm for semantic slicing. The main novel contributions of this paper are: 1) developing a formal specification for slicing, 2) expressing the definition of slicing in terms of a WSL specification statement, and 3) by applying correctness preserving transformations to the specification, we can derive a simple slicing algorithm. © 2011 IEEE.",algorithm derivation | formal methods | Program slicing | program transformations,IEEE Transactions on Software Engineering,2011-02-07,Article,"Ward, Martin P.;Zedan, Hussein",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79551515140,10.1109/TSE.2010.10,From UML to Petri nets: The PCM-based methodology,"In this paper, we present an evaluation methodology to validate the performance of a UML model, representing a software architecture. The proposed approach is based on open and well-known standards: UML for software modeling and the OMG Profile for Schedulability, Performance, and Time Specification for the performance annotations into UML models. Such specifications are collected in an intermediate model, called the Performance Context Model (PCM). The intermediate model is translated into a performance model which is subsequently evaluated. The paper is focused on the mapping from the PCM to the performance domain. More specifically, we adopt Petri nets as the performance domain, specifying a mapping process based on a compositional approach we have entirely implemented in the ArgoPerformance tool. All of the rules to derive a Petri net from a PCM and the performance measures assessable from the former are carefully detailed. To validate the proposed technique, we provide an in-depth analysis of a web application for music streaming. © 2011 IEEE.",performances evaluation | Petri nets | Software engineering | software performance engineering | UML,IEEE Transactions on Software Engineering,2011-01-01,Article,"Distefano, Salvatore;Scarpa, Marco;Puliafito, Antonio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79551538969,10.1109/TSE.2010.80,Verifying the evolution of probability distributions governed by a DTMC,"We propose a new probabilistic temporal logic, iLTL, which captures properties of systems whose state can be represented by probability mass functions (pmfs). Using iLTL, we can specify reachability to a state (i.e., a pmf), as well as properties representing the aggregate (expected) behavior of a system. We then consider a class of systems whose transitions are governed by a Markov Chain-in this case, the set of states a system may be in is specified by the transitions of pmfs from all potential initial states to the final state. We then provide a model checking algorithm to check iLTL properties of such systems. Unlike existing model checking techniques, which either compute the portions of the computational paths that satisfy a specification or evaluate properties along a single path of pmf transitions, our model checking technique enables us to do a complete analysis on the expected behaviors of large-scale systems. Desirable system parameters may also be found as a counterexample of a negated goal. Finally, we illustrate the usefulness of iLTL model checking by means of two examples: assessing software reliability and ensuring the results of administering a drug. © 2011 IEEE.",Discrete Time Markov Chain | linear temporal logic | pharmacokinetics | Probabilistic model checking,IEEE Transactions on Software Engineering,2011-01-01,Article,"Kwon, Youngmin;Agha, Gul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79953231975,10.1109/TSE.2010.89,Improving source code lexicon via traceability and information retrieval,"The paper presents an approach helping developers to maintain source code identifiers and comments consistent with high-level artifacts. Specifically, the approach computes and shows the textual similarity between source code and related high-level artifacts. Our conjecture is that developers are induced to improve the source code lexicon, i.e., terms used in identifiers or comments, if the software development environment provides information about the textual similarity between the source code under development and the related high-level artifacts. The proposed approach also recommends candidate identifiers built from high-level artifacts related to the source code under development and has been implemented as an Eclipse plug-in, called COde Comprehension Nurturant Using Traceability (COCONUT). The paper also reports on two controlled experiments performed with master's and bachelor's students. The goal of the experiments is to evaluate the quality of identifiers and comments (in terms of their consistency with high-level artifacts) in the source code produced when using or not using COCONUT. The achieved results confirm our conjecture that providing the developers with similarity between code and high-level artifacts helps to improve the quality of source code lexicon. This indicates the potential usefulness of COCONUT as a feature for software development environments. © 2006 IEEE.",empirical software engineering | information retrieval | software development environments | Software traceability | source code comprehensibility | source code identifier quality,IEEE Transactions on Software Engineering,2011-03-15,Article,"De Lucia, Andrea;Di Penta, Massimiliano;Oliveto, Rocco",Include,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79957803658,10.1109/TSE.2010.96,Efficient consistency measurement based on behavioral profiles of process models,"Engineering of process-driven business applications can be supported by process modeling efforts in order to bridge the gap between business requirements and system specifications. However, diverging purposes of business process modeling initiatives have led to significant problems in aligning related models at different abstract levels and different perspectives. Checking the consistency of such corresponding models is a major challenge for process modeling theory and practice. In this paper, we take the inappropriateness of existing strict notions of behavioral equivalence as a starting point. Our contribution is a concept called behavioral profile that captures the essential behavioral constraints of a process model. We show that these profiles can be computed efficiently, i.e., in cubic time for sound free-choice Petri nets w.r.t. their number of places and transitions. We use behavioral profiles for the definition of a formal notion of consistency which is less sensitive to model projections than common criteria of behavioral equivalence and allows for quantifying deviation in a metric way. The derivation of behavioral profiles and the calculation of a degree of consistency have been implemented to demonstrate the applicability of our approach. We also report the findings from checking consistency between partially overlapping models of the SAP reference model. © 2011 IEEE.",Behavioral abstraction | Consistency checking | Consistency measures | Process model alignment | Process model analysis,IEEE Transactions on Software Engineering,2011-06-06,Article,"Weidlich, Matthias;Mendling, Jan;Weske, Mathias",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79961069824,10.1109/TSE.2010.93,Dynamic analysis for diagnosing integration faults,"Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies. © 2006 IEEE.",diagnosis | Dynamic Analysis | false positive filters | fault localization | field failure analysis | regression failure analysis,IEEE Transactions on Software Engineering,2011-08-08,Article,"Mariani, Leonardo;Pastore, Fabrizio;Pezzè, Mauro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79961031075,10.1109/TSE.2010.64,Zebu: A language-based approach for network protocol message processing,"A network application communicates with other applications according to a set of rules known as a protocol. This communication is managed by the part of the application known as the protocol-handling layer, which enables the manipulation of protocol messages. The protocol-handling layer is a critical component of a network application since it represents the interface between the application and the outside world. It must thus satisfy two constraints: It must be efficient to be able to treat a large number of messages and it must be robust to face various attacks targeting the application itself or the underlying platform. Despite these constraints, the development process of this layer still remains rudimentary and requires a high level of expertise. It includes translating the protocol specification written in a high-level formalism such as ABNF toward low-level code such as C. The gap between these abstraction levels can entail many errors. This paper proposes a new language-based approach to developing protocol-handling layers, to improve their robustness without compromising their performance. Our approach is based on the use of a domain-specific language, Zebu, to specify the protocol-handling layer of network applications that use textual HTTP-like application protocols. The Zebu syntax is very close to that of ABNF, facilitating the adoption of Zebu by domain experts. By annotating the original ABNF specification of a protocol, the Zebu user can dedicate the protocol-handling layer to the needs of a given application. The Zebu compiler first checks the annotated specification for inconsistencies, and then generates a protocol-handling layer according to the annotations. This protocol-handling layer is made up of a set of data structures that represent a message, a parser that fills in these data structures, and various stub functions to access these data structures or drive the parsing of a message. © 2006 IEEE.",domain-specific languages | message composing | message parsing | Network protocols,IEEE Transactions on Software Engineering,2011-01-01,Article,"Burgy, Laurent;Réveillère, Laurent;Lawall, Julia;Muller, Gilles",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80053586433,10.1109/TSE.2010.78,The impact of irrelevant and misleading information on software development effort estimates: A randomized controlled field experiment,"Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects. © 2011 IEEE.",Cost estimation | requirements/specifications. | software psychology,IEEE Transactions on Software Engineering,2011-09-07,Article,"Jørgensen, Magne;Grimstad, Stein",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-83555163833,10.1109/TSE.2011.4,Putting preemptive Time Petri Nets to work in a V-Model SW life cycle,"Preemptive Time Petri Nets (pTPNs) support modeling and analysis of concurrent timed SW components running under fixed priority preemptive scheduling. The model is supported by a well-established theory based on symbolic state space analysis through Difference Bounds Matrix (DBM) zones, with specific contributions on compositional modularization, trace analysis, and efficient overapproximation and cleanup in the management of suspension deriving from preemptive behavior. In this paper, we devise and implement a framework that brings the theory to application. To this end, we cast the theory into an organic tailoring of design, coding, and testing activities within a V-Model SW life cycle in respect of the principles of regulatory standards applied to the construction of safety-critical SW components. To implement the toolchain subtended by the overall approach into a Model Driven Development (MDD) framework, we complement the theory of state space analysis with methods and techniques supporting semiformal specification and automated compilation into pTPN models and real-time code, measurement-based Execution Time estimation, test case selection and execution, coverage evaluation. © 2006 IEEE.",automated code generation | automated model transformation | coverage analysis. | Execution Time estimation | model driven development | preemptive Time Petri Nets | Real-time systems | real-time testing | safety-critical SW components | SW life cycle | symbolic state space analysis | test case selection and execution | V-Model,IEEE Transactions on Software Engineering,2011-11-24,Article,"Carnevali, Laura;Ridi, Lorenzo;Vicario, Enrico",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-82055193725,10.1109/VISSOF.2011.6069460,An interactive differential and temporal approach to visually analyze software evolution,"Software evolution is one of the most important topics in modern software engineering research. It deals with complex information and large amounts of data. Software visualization can be helpful in this scenario, helping to summarize, analyze and understand software evolution data. This paper presents SourceMiner Evolution (SME), a software tool that uses an interactive differential and temporal approach to visualize software evolution. The tool is implemented as an Eclipse plug-in and has four views that are assembled directly from the IDE AST. The views portray the software from different perspectives. One view shows how metrics of a chosen software entity evolves over time. The other three views show differential comparisons of any two versions of a system structure, dependency and inheritance properties. © 2011 IEEE.",differential views | multi-perspectives | Software evolution | Software visualization,Proceedings of VISSOFT 2011 - 6th IEEE International Workshop on Visualizing Software for Understanding and Analysis,2011-11-29,Conference Paper,"Novais, Renato L.;Lima, Caio A.N.;De F. Carneiro, Glauco;Paulo, R. M.S.;Mendonça, Manoel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-83455213327,10.1109/WCRE.2011.26,"Useful, but usable? Factors affecting the usability of APIs","Software development today has been largely dependent on the use of API libraries, frameworks, and reusable components. However, the API usability issues often increase the development cost (e.g., time, effort) and lower code quality. In this regard, we study 1,513 bug-posts across five different bug repositories, using both qualitative and quantitative analysis. We identify the API usability issues that are reflected in the bug-posts from the API users, and distinguish relative significance of the usability factors. Moreover, from the lessons learned by manual investigation of the bug-posts, we provide further insight into the most frequent API usability issues. © 2011 IEEE.",API | Application Programming Interface | Usability,"Proceedings - Working Conference on Reverse Engineering, WCRE",2011-12-19,Conference Paper,"Zibran, Minhaz F.;Eishita, Farjana Z.;Roy, Chanchal K.",Include,
10.1016/j.jss.2022.111515,2-s2.0-82855169268,10.1109/VLHCC.2011.6070395,The role of conceptual knowledge in API usability,"While many studies have investigated the challenges that developers face in finding and using API documentation, few have considered the role of developers' conceptual knowledge in these tasks. We designed a study in which developers were asked to explore the feasibility of two requirements concerning networking protocols and application platforms that most participants were unfamiliar with, observing the effect that a lack of conceptual knowledge had on their use of documentation. Our results show that without conceptual knowledge, developers struggled to formulate effective queries and to evaluate the relevance or meaning of content they found. Our results suggest that API documentation should not only include detailed examples of API use, but also thorough introductions to the concepts, standards, and ideas manifested in an API's data structures and functionality. © 2011 IEEE.",API usability | documentation | feasibility,"Proceedings - 2011 IEEE Symposium on Visual Languages and Human Centric Computing, VL/HCC 2011",2011-12-09,Conference Paper,"Ko, Andrew J.;Riche, Yann",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-80052962210,10.1007/s10515-011-0082-3,Inferring specifications for resources from natural language API documentation,"Many software libraries, especially those commercial ones, provide API documentation in natural languages to describe correct API usages. However, developers may still write code that is inconsistent with API documentation, partially because many developers are reluctant to carefully read API documentation as shown by existing research. As these inconsistencies may indicate defects, researchers have proposed various detection approaches, and these approaches need many known specifications. As it is tedious to write specifications manually for all APIs, various approaches have been proposed to mine specifications automatically. In the literature, most existing mining approaches rely on analyzing client code, so these mining approaches would fail to mine specifications when client code is not sufficient. Instead of analyzing client code, we propose an approach, called Doc2Spec, that infers resource specifications from API documentation in natural languages. We evaluated our approach on the Javadocs of five libraries. The results show that our approach performs well on real scale libraries, and infers various specifications with relatively high precisions, recalls, and F-scores. We further used inferred specifications to detect defects in open source projects. The results show that specifications inferred by Doc2Spec are useful to detect real defects in existing projects. © Springer Science+Business Media, LLC 2011.",API documentation | Inferring specifications,Automated Software Engineering,2011-12-01,Conference Paper,"Zhong, Hao;Zhang, Lu;Xie, Tao;Mei, Hong",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-79961162141,10.1007/978-3-642-22655-7_5,Using structure-based recommendations to facilitate discoverability in APIs,"Empirical evidence indicates that developers face significant hurdles when the API elements necessary to implement a task are not accessible from the types they are working with. We propose an approach that leverages the structural relationships between API elements to make API methods or types not accessible from a given API type more discoverable. We implemented our approach as an extension to the content assist feature of the Eclipse IDE, in a tool called API Explorer. API Explorer facilitates discoverability in APIs by recommending methods or types, which although not directly reachable from the type a developer is currently working with, may be relevant to solving a programming task. In a case study evaluation, participants experienced little difficulty selecting relevant API elements from the recommendations made by API Explorer, and found the assistance provided by API Explorer helpful in surmounting discoverability hurdles in multiple tasks and various contexts. The results provide evidence that relevant API elements not accessible from the type a developer is working with could be efficiently located through guidance based on structural relationships. © 2011 Springer-Verlag Berlin Heidelberg.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-08-10,Conference Paper,"Duala-Ekoko, Ekwa;Robillard, Martin P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-83455213308,10.1109/WCRE.2011.18,Does the documentation of design pattern instances impact on source code comprehension? results from two controlled experiments,"We present the results of a controlled experiment and a differentiated replication that have been carried out to assess the effect of the documentation of design patterns on the comprehension of source code. The two experiments involved Master Students in Computer Science at the University of Basilicas a and at University of Salerno, respectively. The participants to the original experiment performed a comprehension task with and without graphically- documented design patterns. Textually documented design patterns were provided or not to the participants to perform a comprehension task within the replication. The data analysis revealed that participants employing graphically documented design patterns achieved significantly better performances than the participants provided with source code alone. Conversely, the effect of textually documented design patterns was not statistically significant. A further analysis revealed that the documentation type (textual and graphical) does not significantly affect the performance, when participants correctly recognize design pattern instances. © 2011 IEEE.",Controlled Experiment | Design Patterns | Maintenance,"Proceedings - Working Conference on Reverse Engineering, WCRE",2011-12-19,Conference Paper,"Gravino, Carmine;Risi, Michele;Scanniello, Giuseppe;Tortora, Genoveffa",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-82855169268,10.1109/VLHCC.2011.6070395,The role of conceptual knowledge in API usability,"While many studies have investigated the challenges that developers face in finding and using API documentation, few have considered the role of developers' conceptual knowledge in these tasks. We designed a study in which developers were asked to explore the feasibility of two requirements concerning networking protocols and application platforms that most participants were unfamiliar with, observing the effect that a lack of conceptual knowledge had on their use of documentation. Our results show that without conceptual knowledge, developers struggled to formulate effective queries and to evaluate the relevance or meaning of content they found. Our results suggest that API documentation should not only include detailed examples of API use, but also thorough introductions to the concepts, standards, and ideas manifested in an API's data structures and functionality. © 2011 IEEE.",API usability | documentation | feasibility,"Proceedings - 2011 IEEE Symposium on Visual Languages and Human Centric Computing, VL/HCC 2011",2011-12-09,Conference Paper,"Ko, Andrew J.;Riche, Yann",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.21236/ada547371,,,,,,,,Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79958153465,10.1145/1978942.1979263,HyperSource: Bridging the gap between source and code-related web sites,"Programmers frequently use the Web while writing code: they search for libraries, code examples, tutorials, and documentation. This link between code and visited Web pages remains implicit today. Connecting source code and browsing histories might help programmers maintain context, reduce the cost of Web page re-retrieval, and enhance understanding when code is shared. This note introduces HyperSource, an IDE augmentation that associates browsing histories with source code edits. HyperSource comprises a browser extension that logs visited pages; an IDE that tracks user activity and maps pages to code edits; a source document model that tracks visited pages at a character level; and a user interface that enables interaction with these histories. We discuss relevance heuristics and privacy issues inherent in this approach. Informal log analyses and user feedback suggest that our annotation model is promising for code editing and might also apply to other document authoring tasks after refinement. Copyright 2011 ACM.",Browsing history | Code editors | Edit wear,Conference on Human Factors in Computing Systems - Proceedings,2011-01-01,Conference Paper,"Hartmann, Björn;Dhillon, Mark;Chan, Matthew K.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-79961162141,10.1007/978-3-642-22655-7_5,Using structure-based recommendations to facilitate discoverability in APIs,"Empirical evidence indicates that developers face significant hurdles when the API elements necessary to implement a task are not accessible from the types they are working with. We propose an approach that leverages the structural relationships between API elements to make API methods or types not accessible from a given API type more discoverable. We implemented our approach as an extension to the content assist feature of the Eclipse IDE, in a tool called API Explorer. API Explorer facilitates discoverability in APIs by recommending methods or types, which although not directly reachable from the type a developer is currently working with, may be relevant to solving a programming task. In a case study evaluation, participants experienced little difficulty selecting relevant API elements from the recommendations made by API Explorer, and found the assistance provided by API Explorer helpful in surmounting discoverability hurdles in multiple tasks and various contexts. The results provide evidence that relevant API elements not accessible from the type a developer is working with could be efficiently located through guidance based on structural relationships. © 2011 Springer-Verlag Berlin Heidelberg.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-08-10,Conference Paper,"Duala-Ekoko, Ekwa;Robillard, Martin P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80052141018,10.1109/LICS.2011.33,Temporal specifications with accumulative values,"There is recently a significant effort to add quantitative objectives to formal verification and synthesis. We introduce and investigate the extension of temporal logics with quantitative atomic assertions, aiming for a general and flexible framework for quantitative-oriented specifications. In the heart of quantitative objectives lies the accumulation of values along a computation. It is either the accumulated summation, as with the energy objectives, or the accumulated average, as with the mean-payoff objectives. We investigate the extension of temporal logics with the prefix-accumulation assertions Sum(v) ≥ c and Avg(v) ≥ c, where v is a numeric variable of the system, c is a constant rational number, and Sum(v) and Avg(v) denote the accumulated sum and average of the values of v from the beginning of the computation up to the current point of time. We also allow the path-accumulation assertions LimInfAvg(v) ≥ c and LimSupAvg(v) ≥ c, referring to the average value along an entire computation. We study the border of decidability for extensions of various temporal logics. In particular, we show that extending the fragment of CTL that has only the EX, EF, AX, and AG temporal modalities by prefix-accumulation assertions and extending LTL with path-accumulation assertions, result in temporal logics whose model-checking problem is decidable. The extended logics allow to significantly extend the currently known energy and mean-payoff objectives. Moreover, the prefix-accumulation assertions may be refined with ""controlled-accumulation"", allowing, for example, to specify constraints on the average waiting time between a request and a grant. On the negative side, we show that the fragment we point to is, in a sense, the maximal logic whose extension with prefix-accumulation assertions permits a decidable model-checking procedure. Extending a temporal logic that has the EG or EU modalities, and in particular CTL and LTL, makes the problem undecidable. © 2011 IEEE.",,Proceedings - Symposium on Logic in Computer Science,2011-09-02,Conference Paper,"Boker, Udi;Chatterjee, Krishnendu;Henzinger, Thomas A.;Kupferman, Orna",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79956369890,10.1016/j.scico.2010.09.007,Property specifications for workflow modelling,"Previously we provided two formal behavioural semantics for the Business Process Modelling Notation (BPMN) in the process algebra CSP. By exploiting CSP's refinement orderings, developers may formally compare their BPMN models. However, BPMN is not a specification language, and it is difficult and sometimes impossible to use it to construct behavioural properties against which other BPMN models may be verified. This paper considers a pattern-based approach to expressing behavioural properties. We describe a property specification language PL for capturing a generalisation of Dwyer et al.'s Property Specification Patterns, and present a translation from PL into a bounded, positive fragment of linear temporal logic, which can then be automatically translated into CSP for simple refinement checking. We present a detailed example studying the behavioural properties of an airline ticket reservation business process. Using the same example we also describe some recent results on expressing behavioural compatibility within our semantic models. These results lead to a compositional approach for ensuring deadlock freedom of interacting business processes. © 2010 Elsevier B.V. All rights reserved.",Compatibility | CSP | Linear temporal logic | Property specification patterns | Workflow specification | Workflow verification,Science of Computer Programming,2011-10-01,Article,"Wong, Peter Y.H.;Gibbons, Jeremy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79959550320,10.1145/1984701.1984706,Measuring API documentation on the Web,"Software development blogs, developer forums and Q&A websites are changing the way software is documented. With these tools, developers can create and communicate knowledge and experiences without relying on a central authority to provide official documentation. Instead, any content created by a developer is just a web search away. To understand whether documentation via social media can replace or augment more traditional forms of documentation, we study the extent to which the methods of one particular API - jQuery - are documented on the Web. We analyze 1,730 search results and show that software development blogs in particular cover 87.9% of the API methods, mainly featuring tutorials and personal experiences about using the methods. Further, this effort is shared by a large group of developers contributing just a few blog posts. Our findings indicate that social media is more than a niche in software documentation, that it can provide high levels of coverage and that it gives readers a chance to engage with authors. © 2011 ACM.",Api documentation | Crowd documentation ABSTRACT | Social media,Proceedings - International Conference on Software Engineering,2011-06-29,Article,"Parnin, Chris;Treude, Christoph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/MS.2009.116,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84866916142,10.1145/2351676.2351683,Improving the effectiveness of spectra-based fault localization using specifications,"Fault localization i.e., locating faulty lines of code, is a key step in removing bugs and often requires substantial manual effort. Recent years have seen many automated localization techniques, specifically using the program's passing and failing test runs, i.e., test spectra. However, the effectiveness of these approaches is sensitive to factors such as the type and number of faults, and the quality of the test-suite. This paper presents a novel technique that applies spectra-based localization in synergy with specification-based analysis to more accurately locate faults. Our insight is that unsatisfiability analysis of violated specifications, enabled by SAT technology, could be used to (1) compute unsatisfiable cores that contain likely faulty statements and (2) generate tests that help spectrabased localization. Our technique is iterative and driven by a feedback loop that enables more precise fault localization. Our framework SAT-TAR embodies our technique for Java programs, including those with multiple faults. An experimental evaluation using a suite of widely-studied data structure programs, including the ANTLR and JTopas parser applications, shows that our technique localizes faults more accurately than state-of-the-art approaches. Copyright 2012 ACM.",Alloy | Automated debugging | Fault localization | KODKOD | Minimal unsat cores | Tarantula,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Gopinath, Divya;Zaeem, Razieh Nokhbeh;Khurshid, Sarfraz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866913797,10.1145/2351676.2351691,Supporting automated vulnerability analysis using formalized vulnerability signatures,"Adopting publicly accessible platforms such as cloud computing model to host IT systems has become a leading trend. Although this helps to minimize cost and increase availability and reachability of applications, it has serious implications on applications' security. Hackers can easily exploit vulnerabilities in such publically accessible services. In addition to, 75% of the total reported application vulnerabilities are web application specific. Identifying such known vulnerabilities as well as newly discovered vulnerabilities is a key challenging security requirement. However, existing vulnerability analysis tools cover no more than 47% of the known vulnerabilities. We introduce a new solution that supports automated vulnerability analysis using formalized vulnerability signatures. Instead of depending on formal methods to locate vulnerability instances where analyzers have to be developed to locate specific vulnerabilities, our approach incorporates a formal vulnerability signature described using OCL. Using this formal signature, we perform program analysis of the target system to locate signature matches (i.e. signs of possible vulnerabilities). A newly-discovered vulnerability can be easily identified in a target program provided that a formal signature for it exists. We have developed a prototype static vulnerability analysis tool based on our formalized vulnerability signatures specification approach. We have validated our approach in capturing signatures of the OWSAP Top10 vulnerabilities and applied these signatures in analyzing a set of seven benchmark applications. Copyright 2012 ACM.",Common weaknesses enumeration (CWE) | Formal vulnerability specification | Software security | Vulnerability analysis,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Almorsy, Mohamed;Grundy, John;Ibrahim, Amani S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866920089,10.1145/2351676.2351720,Fast and precise points-to analysis with incremental CFL-reachability summarisation,"We describe our preliminary experience in the design and implementation of a points-to analysis for Java, called EMU, that enables developers to perform pointer-related queries in programs undergoing constant changes in IDEs. EMU achieves fast response times by adopting a modular approach to incrementally updating method summaries upon small code changes: the points-to information in a method is summarised indirectly by CFL reachability rather than directly by points-to sets. Thus, the impact of a small code change made in a method is localised, requiring only its affected part to be re-summarised just to reflect the change. EMU achieves precision by being context-sensitive (for both method invocation and heap abstraction) and field-sensitive. Our evaluation shows that EMU can be promisingly deployed in IDEs where the changes are small. Copyright 2012 ACM.",CFL reachability | Points-to analysis | Summarisation,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Shang, Lei;Lu, Yi;Xue, Jingling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866896680,10.1145/2351676.2351727,Automated API migration in a user-extensible refactoring tool for Erlang programs,"Wrangler is a refactoring and code inspection tool for Erlang programs. Apart from providing a set of built-in refactorings and code inspection functionalities, Wrangler allows users to define refactorings, code inspections, and general program transformations for themselves to suit their particular needs. These are defined using a template- and rule-based program transformation and analysis framework built into Wrangler. This paper reports an extension to Wrangler's extension framework, supporting the automatic generation of API migration refactorings from a user-defined adapter module. Copyright 2012 ACM.",API migration | Erlang | Refactoring | Rewrite rule | Software engineering | Template | Wrangler,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Li, Huiqing;Thompson, Simon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866949717,10.1145/2351676.2351731,Adaptability of model comparison tools,"Modern model-based development methodologies require a large number of efficient, high-quality model comparison tools. They must be carefully adapted to the specific model type, user preferences and application context. Implementing a large number of dedicated, monolithic tools is infeasible, the only viable approach are generic, adaptable tools. Generic tools currently available provide only partial or lowquality solutions to this challenge; their results are not satisfactory for model types such as state machines or block diagrams. This paper presents the SiDiff approach to model comparison which includes a set of highly configurable incremental matchers and a specification language to control their application. Copyright 2012 ACM.",Model comparsion | Model configuration management | Model matching | Model versioning,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Kehrer, Timo;Kelter, Udo;Pietsch, Pit;Schmidt, Maik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866946832,10.1145/2351676.2351748,CHESS: A model-driven engineering tool environment for aiding the development of complex industrial systems,"Modern software systems require advanced design support capable of mastering rising complexity, as well as of automating as many development tasks as possible. Model- Driven Engineering (MDE) is earning consideration as a solid response to those challenges on account of its support for abstraction and domain specialisation. However, MDE adoption often shatters industrial practice because its novelty opposes the need to preserve vast legacy and to not disband the skills matured in pre-MDE or alternative development solutions. This work presents the CHESS tool environment, a novel approach for cross-domain modelling of industrial complex systems. It leverages on UML profiling and separation of concerns realised through the specification of well-defined design views, each of which addresses a particular aspect of the problem. In this way, extra-functional, functional, and deployment descriptions of the system can be given in a focused manner, avoiding issues pertaining to distinct concerns to interfere with one another. Copyright 2012 ACM.",Back propagation | Code generation | Separation of concerns,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Cicchetti, Antonio;Ciccozzi, Federico;Mazzini, Silvia;Puri, Stefano;Panunzio, Marco;Zovi, Alessandro;Vardanega, Tullio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866926570,10.1145/2351676.2351750,Quokka: Visualising interactions of enterprise software environment emulators,"Enterprise software systems operate in large-scale, heterogeneous, distributed environments which makes assessment of non-functional properties, such as scalability and robustness, of those systems particularly challenging. Enterprise environment emulators can provide test-beds representative of real environments using only a few physical hosts thereby allowing assessment of the non-functional properties of enterprise software systems. To date, analysing outcomes of these tests has been an ad hoc and somewhat tedious affair; largely based on manual and/or script-assisted inspection of interaction logs. Quokka visualises emulations significantly aiding analysis and comprehension. Emulated interactions can be viewed live (in real-time) as well as be replayed at a later stage, furthermore, basic charts are used to aggregate and summarise emulations, helping to identify performance and scalability issues. Copyright 2012 ACM.",Enterprise software emulation | Scalability | Service virtualization | Visualisation,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Hine, Cameron;Schneider, Jean Guy;Han, Jun;Versteeg, Steve",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866923869,10.1145/2351676.2351753,Semantic patch inference,"We propose a tool for inferring transformation specifications from a few examples of original and updated code. These transformation specifications may contain multiple code fragments from within a single function, all of which must be present for the transformation to apply. This makes the inferred transformations context sensitive. Our algorithm is based on depth-first search, with pruning. Because it is applied locally to a collection of functions that contain related changes, it is efficient in practice. We illustrate the approach on an example drawn from recent changes to the Linux kernel. Copyright 2012 ACM.",Collateral evolution | Semantic patch | Software evolution,"2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",2012-10-05,Conference Paper,"Andersen, Jesper;Nguyen, Anh Cuong;Lo, David;Lawall, Julia;Khoo, Siau Cheng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867492179,10.1145/2372251.2372264,"Using a follow-on survey to investigate why use of the visitor, singleton & facade patterns is controversial","Context: A previous study has shown that software developers who are experienced with using design patterns hold some conicting opinions about three of the more popular design patterns: Facade, Singleton and Visitor. Aim: To identify the characteristics of these three patterns that have caused them to generate such differing views. Method: We employed a qualitative follow-on survey of those developers who had taken part in the earlier survey about design patterns. Results: We received 46 usable responses from a possible total of 188, with nearly 85% of respondents having six or more years of experience with design patterns. Of these, 27 also provided comments and descriptions of experiences about the patterns, which we categorised. Conclusions: All three patterns can easily be misused and in each case, the consequences of misuse are regarded as being particularly significant. Copyright 2012 ACM.",Design pattern | Empirical | Survey,International Symposium on Empirical Software Engineering and Measurement,2012-10-22,Conference Paper,"Zhang, Cheng;Budgen, David;Drummond, Sarah",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867543629,10.1145/2372251.2372273,Plat-Forms 2011: Finding emergent properties of web application development platforms,"Empirical evidence on emergent properties of different web development platforms when used in a non-trivial setting is rare to non-existent. In this paper we report on an experiment called Plat Forms 2011 where teams of professional software developers implemented the same specification of a small to medium sized web application using different web development platforms, with 3 to 4 teams per platform. We define platforms by the main programming language used, in our case Java, Perl, PHP, or Ruby. In order to find properties that are similar within a web development platform but different across platforms, we analyzed several characteristics of the teams and their solutions, such as completeness, robustness, structure and aspects of the team's development process. We found certain characteristics that can be attributed to the platforms used but others that cannot. Our findings also indicate that for some characteristics the programming language might not be the best attribute by which to define the platform anymore. Copyright 2012 ACM.",Comparison | Emergent properties | Empirical software engineering | Experiment | Languages | Platforms | Web development,International Symposium on Empirical Software Engineering and Measurement,2012-10-22,Conference Paper,"Stärk, Ulrich;Prechelt, Lutz;Jolevski, Ilija",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867515377,10.1145/2372251.2372274,"Dispersion, coordination and performance in global software teams: A systematic review","Effective team coordination is crucial for successful global software projects. Although considerable research effort has been made in this area, no agreement has been reached on the influence of dispersion on team coordination and performance. The objective of this paper is to summarize the evidence on the relationship among context dispersion, team coordination and performance in global software projects. We have performed a Systematic literature review (SLR) to collect relevant studies and a thematic analysis to synthesize the extracted data. We found 28 primary studies reporting the impact of five dispersion dimensions on team performance. Previously, only two primary studies considered and distinguished all of these dispersion dimensions in studying dispersed team performance. The dispersion dimensions affect team outcomes indirectly through influencing organic and mechanistic coordination processes. Empirical evidence show that geographical dispersion impacts negatively and temporal dispersion has a mixed effect on team performance. While studies with teams working across different time zones shows a tendency that the team performance is pessimistically perceived, studies that use direct measure on task performance shows a positive association to temporal dispersion. The paper provides implications for future research and practitioners in establishing effective distributed team coordination. Copyright 2012 ACM.",Communication | Distribution | Global software development | Performance | Systematic literature review | Team coordination,International Symposium on Empirical Software Engineering and Measurement,2012-10-22,Conference Paper,"Anh, Nguyen Duc;Cruzes, Daniela S.;Conradi, Reidar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867508448,10.1145/2372251.2372280,"A Study of reusability, complexity, and reuse design principles","A study is reported on the relationship of complexity and reuse design principles with the reusability of code components. Reusability of a component is measured as the ease of reuse as perceived by the subjects reusing the component. Thirty-four subjects participated in the study with each subject reusing 5 components, resulting in 170 cases of reuse. The components were randomly assigned to the subjects from a pool of 25 components which were designed and built for reuse. The relationship between the complexity of a component and the ease of reuse was analyzed by a regression analysis. It was observed that the higher the complexity the lower the ease of reuse, but the correlation is not significant. An analysis of the relationship between a set of reuse design principles, used in designing and building the components, and the ease of reuse is also reported. The reuse design principles: well-defined interface, and clarity and understandability significantly increase the ease of reuse, while documentation does not have a significant impact on the ease of reuse. Copyright 2012 ACM.",Empirical study | Reusability | Reuse design | Software reuse,International Symposium on Empirical Software Engineering and Measurement,2012-10-22,Conference Paper,"Anguswamy, Reghu;Frakes, William B.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84871309582,10.1145/2393596.2393606,Searching connected API subgraph via text phrases,"Reusing APIs of existing libraries is a common practice during software development, but searching suitable APIs and their usages can be time-consuming [6]. In this paper, we study a new and more practical approach to help users find usages of APIs given only simple text phrases, when users have limited knowledge about an API library. We model API invocations as an API graph and aim to find an optimum connected subgraph that meets users' search needs. The problem is challenging since the search space in an API graph is very huge. We start with a greedy subgraph search algorithm which returns a connected subgraph containing nodes with high textual similarity to the query phrases. Two refinement techniques are proposed to improve the quality of the returned subgraph. Furthermore, as the greedy subgraph search algorithm relies on online query of shortest path between two graph nodes, we propose a space-efficient compressed shortest path indexing scheme that can efficiently recover the exact shortest path. We conduct extensive experiments to show that the proposed subgraph search approach for API recommendation is very effective in that it boosts the average F1-measure of the state-of-the-art approach, Portfolio [15], on two groups of real-life queries by 64% and 36% respectively. © 2012 ACM.",API graph | API recommendation | subgraph searching,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",2012-12-24,Conference Paper,"Chan, Wing Kwan;Cheng, Hong;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871292962,10.1145/2393596.2393660,How do developers use parallel libraries?,"Parallel programming is hard. The industry leaders hope to convert the hard problem of using parallelism into the easier problem of using a parallel library. Yet, we know little about how programmers adopt these libraries in practice. Without such knowledge, other programmers cannot educate themselves about the state of the practice, library designers are unaware of API misusage, researchers make wrong assumptions, and tool vendors do not support common usage of library constructs. We present the first study that analyzes the usage of parallel libraries in a large scale experiment. We analyzed 655 open-source applications that adopted Microsoft's new parallel libraries - Task Parallel Library (TPL) and Parallel Language Integrated Query (PLINQ) - comprising 17.6M lines of code written in C#. These applications are developed by 1609 programmers. Using this data, we answer 8 research question and we uncover some interesting facts. For example, (i) for two of the fundamental parallel constructs, in at least 10% of the cases developers misuse them so that the code runs sequentially instead of concurrently, (ii) developers make their parallel code unnecessarily complex, (iii) applications of different size have different adoption trends. The library designers confirmed that our finding are useful and will influence the future development of the libraries. © 2012 ACM.",C# | empirical study | multi-core | parallel libraries,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",2012-12-24,Conference Paper,"Okur, Semih;Dig, Danny",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871345410,10.1145/2393596.2393661,Seeking the ground truth: A retroactive study on the evolution and migration of software libraries,"Application programming interfaces (APIs) are a common and industrially-relevant means for third-party software developers to reuse external functionality. Several techniques have been proposed to help migrate client code between library versions with incompatible APIs, but it is not clear how well these perform in an absolute sense. We present a retroactive study into the presence and nature of API incompatibilities between several versions of a set of Java-based software libraries; for each, we perform a detailed, manual analysis to determine what the correct adaptations are to migrate from the older to the newer version. In addition, we investigate whether any of a set of adaptation recommender techniques is capable of identifying the correct adaptations for library migration. We find that a given API incompatibility can typically be addressed by only one or two recommender techniques, but sometimes none serve. Furthermore, those techniques give correct recommendations, on average, in only about 20% of cases. © 2012 ACM.",adaptive change | API | recommendation systems,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",2012-12-24,Conference Paper,"Cossette, Bradley E.;Walker, Robert J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871299619,10.1145/2393596.2393662,How do developers react to API deprecation? The case of a smalltalk ecosystem,"When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation - known as a ripple effect - is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes. Although studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on an empirical study of API deprecations that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating around the Squeak and Pharo software ecosystems: seven years of evolution, more than 3,000 contributors, and more than 2,600 distinct systems. We analyzed 577 methods and 186 classes that were deprecated, and answer research questions regarding the frequency, magnitude, duration, adaptation, and consistency of the ripple effects triggered by API changes. © 2012 ACM.",ecosystems | empirical studies | mining software repositories,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",2012-12-24,Conference Paper,"Robbes, Romain;Lungu, Mircea;Röthlisberger, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871343569,10.1145/2393596.2393667,Rubicon: Bounded verification of web applications,"Rubicon is a verifier for web applications. Specifications are written in an embedded domain-specific language and are checked fully automatically. Rubicon is designed to fit with current practices: its language is based on RSpec, a popular testing framework, and its analysis leverages the standard Ruby interpreter to perform symbolic execution (generating verification conditions that are checked by the Alloy Analyzer). Rubicon has been evaluated on five open-source applications; in one, a widely used customer relationship management system, a previously unknown security flaw was revealed. © 2012 ACM.",Alloy analyzer | Alloy language | lightweight formal methods | programming languages | RSpec | Ruby on rails | symbolic execution | web applications,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",2012-12-24,Conference Paper,"Near, Joseph P.;Jackson, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867515576,10.1145/2364527.2364551,On the complexity of equivalence of specifications of infinite objects,"We study the complexity of deciding the equality of infinite objects specified by systems of equations, and of infinite objects specified by λ-terms. For equational specifications there are several natural notions of equality: equality in all models, equality of the sets of solutions, and equality of normal forms for productive specifications. For λ-terms we investigate Böhm-tree equality and various notions of observational equality. We pinpoint the complexity of each of these notions in the arithmetical or analytical hierarchy. We show that the complexity of deciding equality in all models subsumes the entire analytical hierarchy. This holds already for the most simple infinite objects, viz. streams over {0,1}, and stands in sharp contrast to the low arithmetical II 0 2-completeness of equality of equationally specified streams derived in [17] employing a different notion of equality. © 2012 ACM.",complexity | equality | equational specifications | infinite objects | lambda terms | semantics,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2012-10-22,Conference Paper,"Endrullis, Jörg;Hendriks, Dimitri;Bakhshi, Rena",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867491922,10.1145/2364527.2364575,Automatic amortised analysis of dynamic memory allocation for lazy functional programs,"This paper describes the first successful attempt, of which we are aware, to define an automatic, type-based static analysis of resource bounds for lazy functional programs. Our analysis uses the automatic amortisation approach developed by Hofmann and Jost, which was previously restricted to eager evaluation. In this paper, we extend this work to a lazy setting by capturing the costs of unevaluated expressions in type annotations and by amortising the payment of these costs using a notion of lazy potential. We present our analysis as a proof system for predicting heap allocations of a minimal functional language (including higher-order functions and recursive data types) and define a formal cost model based on Launchbury's natural semantics for lazy evaluation. We prove the soundness of our analysis with respect to the cost model. Our approach is illustrated by a number of representative and non-trivial examples that have been analysed using a prototype implementation of our analysis. © 2012 ACM.",amortisation | lazy evaluation | resource analysis | type systems,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2012-10-22,Conference Paper,"Simões, Hugo;Vasconcelos, Pedro;Florido, Mário;Jost, Steffen;Hammond, Kevin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2364527.2364566,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865012209,10.1109/icpc.2012.6240506,A lightweight visualization of interprocedural data-flow paths for source code reading,"To understand the behavior of a program, developers must read source code fragments in various modules. For developers investigating data-flow paths among modules, a call graph is too abstract since it does not visualize how parameters of method calls are related to each other. On the other hand, a system dependence graph is too fine-grained to investigate interprocedural data-flow paths. In this research, we propose an intermediate-level of visualization; we visualize interprocedural data-flow paths among method parameters and fields with summarized intraprocedural data-flow paths. We have implemented our visualization as an Eclipse plug-in for Java. The tool comprises a lightweight data-flow analysis and an interactive graph viewer using fractal value to extract a small subgraph of data-flow related to variables specified by a developer. A case study has shown our visualization enabled developers to investigate more data-flow paths in a fixed time slot. In addition, we report our lightweight data-flow analysis can generate precise data-flow paths for 98% of Java methods. © 2012 IEEE.",data-flow analysis | program comprehension | software visualization | static analysis,IEEE International Conference on Program Comprehension,2012-01-01,Conference Paper,"Ishio, Takashi;Etsuda, Shogo;Inoue, Katsuro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865007043,10.1109/icpc.2012.6240509,Evaluating forum discussions to inform the design of an API critic,"Learning to use a software framework and its API (Application Programming Interfaces) can be a major endeavor for novices. To help, we have built a critic to advise the use of an API based on the formal semantics of the API. Specifically, the critic offers advice when the symbolic state of the API client code triggers any API usage rules. To assess to what extent our critic can help solve practical API usage problems and what kinds of API usage rules can be formulated, we manually analyzed 150 discussion threads from the Java Swing forum. We categorize the discussion threads according to how they can be helped by the critic. We find that API problems of the same nature appear repeatedly in the forum, and that API problems of the same nature can be addressed by implementing a new API usage rule for the critic. We characterize the set of discovered API usage rules as a whole. Unlike past empirical studies that focus on answering why frameworks and APIs are hard to learn, ours is the first designed to produce systematic data that have been directly used to build an API support tool. © 2012 IEEE.",APIs | AWT/Swing | Critic | Java | Online Forum Discussions | Software Frameworks | Symbolic Execution,IEEE International Conference on Program Comprehension,2012-01-01,Conference Paper,"Rupakheti, Chandan R.;Hou, Daqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865045102,10.1109/icpc.2012.6240510,Mining source code descriptions from developer communications,"Very often, source code lacks comments that adequately describe its behavior. In such situations developers need to infer knowledge from the source code itself or to search for source code descriptions in external artifacts. We argue that messages exchanged among contributors/developers, in the form of bug reports and emails, are a useful source of information to help understanding source code. However, such communications are unstructured and usually not explicitly meant to describe specific parts of the source code. Developers searching for code descriptions within communications face the challenge of filtering large amount of data to extract what pieces of information are important to them. We propose an approach to automatically extract method descriptions from communications in bug tracking systems and mailing lists. We have evaluated the approach on bug reports and mailing lists from two open source systems (Lucene and Eclipse). The results indicate that mailing lists and bug reports contain relevant descriptions of about 36% of the methods from Lucene and 7% from Eclipse, and that the proposed approach is able to extract such descriptions with a precision of up to 79% for Eclipse and 87% for Lucene. The extracted method descriptions can help developers in understanding the code and could also be used as a starting point for source code re-documentation. © 2012 IEEE.",Code re-documentation | mining e-mails | program comprehension,IEEE International Conference on Program Comprehension,2012-01-01,Conference Paper,"Panichella, Sebastiano;Aponte, Jairo;Di Penta, Massimiliano;Marcus, Andrian;Canfora, Gerardo",Exclude,
10.1016/j.jss.2022.111515,,10.1109/ICPC.2012.6240477,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864212368,10.1109/ICSE.2012.6227209,Developer prioritization in bug repositories,"Developers build all the software artifacts in development. Existing work has studied the social behavior in software repositories. In one of the most important software repositories, a bug repository, developers create and update bug reports to support software development and maintenance. However, no prior work has considered the priorities of developers in bug repositories. In this paper, we address the problem of the developer prioritization, which aims to rank the contributions of developers. We mainly explore two aspects, namely modeling the developer prioritization in a bug repository and assisting predictive tasks with our model. First, we model how to assign the priorities of developers based on a social network technique. Three problems are investigated, including the developer rankings in products, the evolution over time, and the tolerance of noisy comments. Second, we consider leveraging the developer prioritization to improve three predicted tasks in bug repositories, i.e., bug triage, severity identification, and reopened bug prediction. We empirically investigate the performance of our model and its applications in bug repositories of Eclipse and Mozilla. The results indicate that the developer prioritization can provide the knowledge of developer priorities to assist software tasks, especially the task of bug triage. © 2012 IEEE.",bug triage | developer prioritization | reopened bug prediction | severity identification | software evolution,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Xuan, Jifeng;Jiang, He;Ren, Zhilei;Zou, Weiqin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864217582,10.1109/ICSE.2012.6227207,Recovering traceability links between an API and its learning resources,"Large frameworks and libraries require extensive developer learning resources, such as documentation and mailing lists, to be useful. Maintaining these learning resources is challenging partly because they are not explicitly linked to the frameworks' API, and changes in the API are not reflected in the learning resources. Automatically recovering traceability links between an API and learning resources is notoriously difficult due to the inherent ambiguity of unstructured natural language. Code elements mentioned in documents are rarely fully qualified, so readers need to understand the context in which a code element is mentioned. We propose a technique that identifies code-like terms in documents and links these terms to specific code elements in an API, such as methods. In an evaluation study with four open source systems, we found that our technique had an average recall and precision of 96%. © 2012 IEEE.",,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Dagenais, Barthélémy;Robillard, Martin P.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84864219659,10.1109/ICSE.2012.6227205,"Graph-based pattern-oriented, context-sensitive source code completion","Code completion helps improve developers' programming productivity. However, the current support for code completion is limited to context-free code templates or a single method call of the variable on focus. Using software libraries for development, developers often repeat API usages for certain tasks. Thus, a code completion tool could make use of API usage patterns. In this paper, we introduce GraPacc, a graph-based, pattern-oriented, context-sensitive code completion approach that is based on a database of such patterns. GraPacc represents and manages the API usage patterns of multiple variables, methods, and control structures via graph-based models. It extracts the context-sensitive features from the code under editing, e.g. the API elements on focus and their relations to other code elements. Those features are used to search and rank the patterns that are most fitted with the current code. When a pattern is selected, the current code will be completed via a novel graph-based code completion algorithm. Empirical evaluation on several real-world systems shows that GraPacc has a high level of accuracy in code completion. © 2012 IEEE.",API usage pattern | pattern-based code completion,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Nguyen, Anh Tuan;Nguyen, Tung Thanh;Nguyen, Hoan Anh;Tamrawi, Ahmed;Nguyen, Hung Viet;Al-Kofahi, Jafar;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864221521,10.1109/ICSE.2012.6227201,The impacts of software process improvement on developers: A systematic review,"This paper presents the results of a systematic review on the impacts of Software Process Improvement (SPI) on developers. This review selected 26 studies from the highest quality journals, conferences, and workshop in the field. The results were compiled and organized following the grounded theory approach. Results from the grounded theory were further categorized using the Ishikawa (or fishbone) diagram. The Ishikawa Diagram models all the factors potentially impacting software developers, and shows both the positive and negative impacts. Positive impacts include a reduction in the number of crises, and an increase in team communications and morale, as well as better requirements and documentation. Negative impacts include increased overhead on developers through the need to collect data and compile documentation, an undue focus on technical approaches, and the fact that SPI is oriented toward management and process quality, and not towards developers and product quality. This systematic review should support future practice through the identification of important obstacles and opportunities for achieving SPI success. Future research should also benefit from the problems and advantages of SPI identified by developers. © 2012 IEEE.",developer | impact | software process | systematic review,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Lavallée, Mathieu;Robillard, Pierre N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864191006,10.1109/ICSE.2012.6227188,How do professional developers comprehend software?,"Research in program comprehension has considerably evolved over the past two decades. However, only little is known about how developers practice program comprehension under time and project pressure, and which methods and tools proposed by researchers are used in industry. This paper reports on an observational study of 28 professional developers from seven companies, investigating how developers comprehend software. In particular we focus on the strategies followed, information needed, and tools used. We found that developers put themselves in the role of end users by inspecting user interfaces. They try to avoid program comprehension, and employ recurring, structured comprehension strategies depending on work context. Further, we found that standards and experience facilitate comprehension. Program comprehension was considered a subtask of other maintenance tasks rather than a task by itself. We also found that face-to-face communication is preferred to documentation. Overall, our results show a gap between program comprehension research and practice as we did not observe any use of state of the art comprehension tools and developers seem to be unaware of them. Our findings call for further careful analysis and for reconsidering research agendas. © 2012 IEEE.",context awareness | empirical studies | maintenance | program comprehension | software documentation,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Roehm, Tobias;Tiarks, Rebecca;Koschke, Rainer;Maalej, Walid",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84864264465,10.1109/ICSE.2012.6227187,Asking and answering questions about unfamiliar APIs: An exploratory study,"The increasing size of APIs and the increase in the number of APIs available imply developers must frequently learn how to use unfamiliar APIs. To identify the types of questions developers want answered when working with unfamiliar APIs and to understand the difficulty they may encounter answering those questions, we conducted a study involving twenty programmers working on different programming tasks, using unfamiliar APIs. Based on the screen captured videos and the verbalization of the participants, we identified twenty different types of questions programmers ask when working with unfamiliar APIs, and provide new insights to the cause of the difficulties programmers encounter when answering questions about the use of APIs. The questions we have identified and the difficulties we observed can be used for evaluating tools aimed at improving API learning, and in identifying areas of the API learning process where tool support is missing, or could be improved. © 2012 IEEE.",,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Duala-Ekoko, Ekwa;Robillard, Martin P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864204974,10.1109/ICSE.2012.6227155,Does organizing security patterns focus architectural choices?,"Security patterns can be a valuable vehicle to design secure software. Several proposals have been advanced to improve the usability of security patterns. They often describe extra annotations to be included in the pattern documentation. This paper presents an empirical study that validates whether those proposals provide any real benefit for software architects. A controlled experiment has been executed with 90 master students, who have performed several design tasks involving the hardening of a software architecture via security patterns. The results show that annotations produce benefits in terms of a reduced number of alternatives that need to be considered during the selection of a suitable pattern. However, they do not reduce the time spent in the selection process. © 2012 IEEE.",experiment | secure software engineering | security patterns | software architecture,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Yskout, Koen;Scandariato, Riccardo;Joosen, Wouter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864226089,10.1109/ICSE.2012.6227140,Synthesizing API usage examples,"Key program interfaces are sometimes documented with usage examples: concrete code snippets that characterize common use cases for a particular data type. While such documentation is known to be of great utility, it is burdensome to create and can be incomplete, out of date, or not representative of actual practice. We present an automatic technique for mining and synthesizing succinct and representative human-readable documentation of program interfaces. Our algorithm is based on a combination of path sensitive dataflow analysis, clustering, and pattern abstraction. It produces output in the form of well-typed program snippets which document initialization, method calls, assignments, looping constructs, and exception handling. In a human study involving over 150 participants, 82% of our generated examples were found to be at least as good at human-written instances and 94% were strictly preferred to state of the art code search. © 2012 IEEE.",,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Buse, Raymond P.L.;Weimer, Westley",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864240782,10.1109/ICSE.2012.6227138,Temporal analysis of API usage concepts,"Software reuse through Application Programming Interfaces (APIs) is an integral part of software development. The functionality offered by an API is not always accessed uniformly throughout the lifetime of a client program. We propose Temporal API Usage Pattern Mining to detect API usage patterns in terms of their time of introduction into client programs. We detect concepts as distinct groups of API functionality from the change history of a client program. We locate those concepts in the client change history and detect temporal usage patterns, where a pattern contains a set of concepts that were added into the client program in a specific temporal order. We investigated the properties of temporal API usage patterns through a multiple-case study of three APIs and their use in up to 19 client software projects. Our technique was able to detect a number of valuable patterns in two out of three of the APIs investigated. Further investigation showed some patterns to be relatively consistent between clients, produced by multiple developers, and not trivially derivable from program structure or API documentation. © 2012 IEEE.",API Usability | API Usage | Mining Software Repositories | Software Reuse | Usage Pattern,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Uddin, Gias;Dagenais, Barthélémy;Robillard, Martin P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864270587,10.1109/ICSE.2012.6227137,Inferring method specifications from natural language API descriptions,"Application Programming Interface (API) documents are a typical way of describing legal usage of reusable software libraries, thus facilitating software reuse. However, even with such documents, developers often overlook some documents and build software systems that are inconsistent with the legal usage of those libraries. Existing software verification tools require formal specifications (such as code contracts), and therefore cannot directly verify the legal usage described in natural language text in API documents against code using that library. However, in practice, most libraries do not come with formal specifications, thus hindering tool-based verification. To address this issue, we propose a novel approach to infer formal specifications from natural language text of API documents. Our evaluation results show that our approach achieves an average of 92% precision and 93% recall in identifying sentences that describe code contracts from more than 2500 sentences of API documents. Furthermore, our results show that our approach has an average 83% accuracy in inferring specifications from over 1600 sentences describing code contracts. © 2012 IEEE.",,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Pandita, Rahul;Xiao, Xusheng;Zhong, Hao;Xie, Tao;Oney, Stephen;Paradkar, Amit",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84864211699,10.1109/ICSE.2012.6227136,Automatic parameter recommendation for practical API usage,"Programmers extensively use application programming interfaces (APIs) to leverage existing libraries and frameworks. However, correctly and efficiently choosing and using APIs from unfamiliar libraries and frameworks is still a non-trivial task. Programmers often need to ruminate on API documentations (that are often incomplete) or inspect code examples (that are often absent) to learn API usage patterns. Recently, various techniques have been proposed to alleviate this problem by creating API summarizations, mining code examples, or showing common API call sequences. However, few techniques focus on recommending API parameters. In this paper, we propose an automated technique, called Precise, to address this problem. Differing from common code completion systems, Precise mines existing code bases, uses an abstract usage instance representation for each API usage example, and then builds a parameter usage database. Upon a request, Precise queries the database for abstract usage instances in similar contexts and generates parameter candidates by concretizing the instances adaptively. The experimental results show that our technique is more general and applicable than existing code completion systems, specially, 64% of the parameter recommendations are useful and 53% of the recommendations are exactly the same as the actual parameters needed. We have also performed a user study to show our technique is useful in practice. © 2012 IEEE.",API | argument | code completion | parameter | recommendation,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Zhang, Cheng;Yang, Juyuan;Zhang, Yi;Fan, Jing;Zhang, Xin;Zhao, Jianjun;Ou, Peizhao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864272324,10.1109/ICSE.2012.6227133,Active code completion,"Code completion menus have replaced standalone API browsers for most developers because they are more tightly integrated into the development workflow. Refinements to the code completion menu that incorporate additional sources of information have similarly been shown to be valuable, even relative to standalone counterparts offering similar functionality. In this paper, we describe active code completion, an architecture that allows library developers to introduce interactive and highly-specialized code generation interfaces, called palettes, directly into the editor. Using several empirical methods, we examine the contexts in which such a system could be useful, describe the design constraints governing the system architecture as well as particular code completion interfaces, and design one such system, named Graphite, for the Eclipse Java development environment. Using Graphite, we implement a palette for writing regular expressions as our primary example and conduct a small pilot study. In addition to showing the feasibility of this approach, it provides further evidence in support of the claim that integrating specialized code completion interfaces directly into the editor is valuable to professional developers. © 2012 IEEE.",code completion | development environments,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Omar, Cyrus;Yoon, Young Seok;LaToza, Thomas D.;Myers, Brad A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864258262,10.1109/ICSE.2012.6227127,Statically checking API protocol conformance with mined multi-object specifications,"Programmers using an API often must follow protocols that specify when it is legal to call particular methods. Several techniques have been proposed to find violations of such protocols based on mined specifications. However, existing techniques either focus on single-object protocols or on particular kinds of bugs, such as missing method calls. There is no practical technique to find multi-object protocol bugs without a priori known specifications. In this paper, we combine a dynamic analysis that infers multi-object protocols and a static checker of API usage constraints into a fully automatic protocol conformance checker. The combined system statically detects illegal uses of an API without human-written specifications. Our approach finds 41 bugs and code smells in mature, real-world Java programs with a true positive rate of 51%. Furthermore, we show that the analysis reveals bugs not found by state of the art approaches. © 2012 IEEE.",Specification mining | Static analysis | Typestate,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Pradel, Michael;Jaspan, Ciera;Aldrich, Jonathan;Gross, Thomas R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864224057,10.1109/ICSE.2012.6227126,Behavioral validation of JFSL specifications through model synthesis,"Contracts are a popular declarative specification technique to describe the behavior of stateful components in terms of pre/post conditions and invariants. Since each operation is specified separately in terms of an abstract implementation, it may be hard to understand and validate the resulting component behavior from contracts in terms of method interactions. In particular, properties expressed through algebraic axioms, which specify the effect of sequences of operations, require complex theorem proving techniques to be validated. In this paper, we propose an automatic small-scope based approach to synthesize incomplete behavioral abstractions for contracts expressed in the JFSL notation. The proposed abstraction technique enables the possibility to check that the contract behavior is coherent with behavioral properties expressed as axioms of an algebraic specifications. We assess the applicability of our approach by showing how the synthesis methodology can be applied to some classes of contract-based artifacts like specifications of data abstractions and requirement engineering models. © 2012 IEEE.",behavioral validation | contracts | model synthesis | specifications,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Ghezzi, Carlo;Mocci, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864225072,10.1109/ICSE.2012.6227125,Specification patterns from research to industry: A case study in service-based applications,"Specification patterns have proven to help developers to state precise system requirements, as well as formalize them by means of dedicated specification languages. Most of the past work has focused its applicability area to the specification of concurrent and real-time systems, and has been limited to a research setting. In this paper we present the results of our study on specification patterns for service-based applications (SBAs). The study focuses on industrial SBAs in the banking domain. We started by performing an extensive analysis of the usage of specification patterns in published research case studies - representing almost ten years of research in the area of specification, verification, and validation of SBAs. We then compared these patterns with a large body of specifications written by our industrial partner over a similar time period. The paper discusses the outcome of this comparison, indicating that some needs of the industry, especially in the area of requirements specification languages, are not fully met by current software engineering research. © 2012 IEEE.",requirements specifications | services | specification languages | specification patterns,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Bianculli, Domenico;Ghezzi, Carlo;Pautasso, Cesare;Senti, Patrick",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84864252214,10.1109/ICSE.2012.6227121,Software analytics in practice: Mini tutorial,"A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services and the dynamics of software development. Software analytics is to develop and apply data exploration and analysis technologies, such as pattern recognition, machine learning, and information visualization, on software data to obtain insightful and actionable information for modern software and services. This tutorial presents latest research and practice on principles, techniques, and applications of software analytics in practice, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research and practice directions in software analytics. The attendees can acquire the skills and knowledge needed to perform industrial research or conduct industrial practice in the field of software analytics and to integrate analytics in their own industrial research, practice, and training. © 2012 IEEE.",,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Zhang, Dongmei;Xie, Tao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864257712,10.1109/ICSE.2012.6227119,Constructing parser for industrial software specifications containing formal and natural language description,"This paper describes a novel framework for creating a parser to process and analyze texts written in a ""partially structured"" natural language. In many projects, the contents of document artifacts tend to be described as a mixture of formal parts (i.e. the text constructs follow specific conventions) and parts written in arbitrary free text. Formal parsers, typically defined and used to process a description with rigidly defined syntax such as program source code are very precise and efficient in processing the formal part, while parsers developed for natural language processing (NLP) are good at robustly interpreting the free-text part. Therefore, combining these parsers with different characteristics can allow for more flexible and practical processing of various project documents. Unfortunately, conventional approaches to constructing a parser from multiple small parsers were studied extensively only for formal language parsers and are not directly applicable to NLP parsers due to the differences in the way the input text is extracted and evaluated. We propose a method to configure and generate a combined parser by extending an approach based on parser combinator, the operators for composing multiple formal parsers, to support both NLP and formal parsers. The resulting text parser is based on Parsing Expression Grammars, and it benefits from the strength of both parser types. We demonstrate an application of such combined parser in practical situations and show that the proposed approach can efficiently construct a parser for analyzing project-specific industrial specification documents. © 2012 IEEE.",Document Analysis | Parser Combinator | Parsing Expression Grammars | Requirement Engineering,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Iwama, Futoshi;Nakamura, Taiga;Takeuchi, Hironori",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864229964,10.1109/ICSE.2012.6227231,JavaMOP: Efficient parametric runtime monitoring framework,"Runtime monitoring is a technique usable in all phases of the software development cycle, from initial testing, to debugging, to actually maintaining proper function in production code. Of particular importance are parametric monitoring systems, which allow the specification of properties that relate objects in a program, rather than only global properties. In the past decade, a number of parametric runtime monitoring systems have been developed. Here we give a demonstration of our system, JavaMOP. It is the only parametric monitoring system that allows multiple differing logical formalisms. It is also the most efficient in terms of runtime overhead, and very competitive with respect to memory usage. © 2012 IEEE.",aspect-oriented programming | debugging | runtime monitoring | runtime verification | testing,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Jin, Dongyun;Meredith, Patrick O.Neil;Lee, Choonghwan;Rosu, Grigore",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864225729,10.1109/ICSE.2012.6227057,Using structural and semantic information to support software refactoring,"In the software life cycle the internal structure of the system undergoes continuous modifications. These changes push away the source code from its original design, often reducing its quality. In such cases refactoring techniques can be applied to improve the design quality of the system. Approaches existing in literature mainly exploit structural relationships present in the source code, e.g., method calls, to support the software engineer in identifying refactoring solutions. However, also semantic information is embedded in the source code by the developers, e.g., the terms used in the comments. This research investigates about the usefulness of combining structural and semantic information to support software refactoring. © 2012 IEEE.",refactoring | semantic information,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Bavota, Gabriele",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84864227810,10.1109/ICSE.2012.6227055,Using machine learning to enhance automated requirements model transformation,"Textual specification documents do not represent a suitable starting point for software development. This issue is due to the inherent problems of natural language such as ambiguity, impreciseness and incompleteness. In order to overcome these shortcomings, experts derive analysis models such as requirements models. However, these models are difficult and costly to create manually. Furthermore, the level of abstraction of the models is too low, thus hindering the automated transformation process. We propose a novel approach which uses high abstraction requirements models in the form of Object System Models (OSMs) as targets for the transformation of natural language specifications in conjunction with appropriate text mining and machine learning techniques. OSMs allow the interpretation of the textual specification based on a small set of facts and provide structural and behavioral information. This approach will allow both (1) the enhancement of minimal specifications, and in the case of comprehensive specifications (2) the determination of the most suitable structure of reusable requirements. © 2012 IEEE.",machine learning | Natural language specification | Object System Models | text mining,Proceedings - International Conference on Software Engineering,2012-07-30,Conference Paper,"Chioaşcǎ, Erol Valeriu",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ICSM.2012.6405244,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ICSM.2012.6405245,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84873122920,10.1109/ICSM.2012.6405249,What makes a good code example?: A study of programming Q&A in StackOverflow,"Programmers learning how to use an API or a programming language often rely on code examples to support their learning activities. However, what makes for an effective ode example remains an open question. Finding the haracteristics of the effective examples is essential in improving the appropriateness of these learning aids. To help answer this question we have onducted a qualitative analysis of the questions and answers posted to a programming Q&A web site called StackOverflow. On StackOverflow answers can be voted on, indicating which answers were found helpful by users of the site. By analyzing these well-received answers we identified haracteristics of effective examples. We found that the explanations acompanying examples are as important as the examples themselves. Our findings have implications for the way the API documentation and example set should be developed and evolved as well as the design of the tools assisting the development of these materials. © 2012 IEEE.",API | code example | documentation | social learning,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Nasehi, Seyed Mehdi;Sillito, Jonathan;Maurer, Frank;Burns, Chris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873206471,10.1109/ICSM.2012.6405271,An empirical study on requirements traceability using eye-tracking,"Requirements traceability (RT) links help developers to understand programs and ensure that their source code is consistent with its documentation. Creating RT links is a laborious and resource-consuming task. Information Retrieval (IR) techniques are useful to automatically recover traceability links. However, IR-based approaches typically have low accuracy (precision and recall) and, thus, creating RT links remains a human intensive process. We conjecture that understanding how developers verify RT links could help improve the accuracy of IR-based approaches to recover RT links. Consequently, we perform an empirical study consisting of two controlled experiments. First, we use an eye-tracking system to capture developers' eye movements while they verify RT links. We analyse the obtained data to identify and rank developers' preferred source code entities (SCEs), e.g., class names, method names. Second, we use the ranked SCEs to propose two new weighting schemes called SE/IDF (source code entity/inverse document frequency) and DOI/IDF (domain or implementation/inverse document frequency) to recover RT links combined with an IR technique. SE/IDF is based on the developers preferred SCEs to verify RT links. DOI/IDF is an extension of SE/IDF distinguishing domain and implementation concepts. We use LSI combined with SE/IDF, DOI/IDF, and TF/IDF to show, using two systems, iTrust and Pooka, that LSIDOI/IDF statistically improves the accuracy of the recovered RT links over LSI TF/IDF. © 2012 IEEE.",eye tracking | LDA | LSI | Requirements traceability | source code,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Ali, Nasir;Sharafl, Zohreh;Gueheneuc, Yann Gael;Antoniol, Giuliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873104174,10.1109/ICSM.2012.6405295,Survival of Eclipse third-party plug-ins,"Today numerous software systems are being developed on top of frameworks. In this study, we analyzed the survival of 467 Eclipse third-party plug-ins altogether having 1,447 versions. We classify these plug-ins into two categories: those that depend on only stable and supported Eclipse APIs and those that depend on at least one of the potentially unstable, discouraged and unsupported Eclipse non-APIs. Comparing the two categories of plug-ins, we observed that the plug-ins depending solely on APIs have a very high source compatibility success rate compared to those that depend on at least one of the non-APIs. However, we have also observed that recently released plug-ins that depend on non-APIs also have a very high forward source compatibility success rate. This high source compatibility success rate is due to the dependency structure of these plug-ins: recently released plug-ins that depend on non-APIs predominantly depend on old Eclipse nonAPIs rather than on newly introduced ones. Finally, we showed that the majority of plug-ins hosted on SourceForge do not evolve beyond the first year of release. © 2012 IEEE.",APIs | Eclipse | non-APIs | Third-party plug-ins,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Businge, John;Serebrenik, Alexander;Van Den Brand, Mark",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873163600,10.1109/ICSM.2012.6405297,Inferring the data access from the clients of generic APIs,"Many programs access external data sources through generic APIs. The class hierarchy of such a generic API does not reflect the schema of any particular data source, and thus it is hard to clarify what data an API client accesses and how it obtains them. This makes it difficult to maintain the API clients. In this paper, we show that the data access of an API client can be recovered through static analysis on the client's source code. We provide a formal and intuitive way to represent the data access, as a graph of so-called summoning snippets. Each snippet stands for a type of data accessed by the client, and carries the code slice from the client about how to obtain the data via the API. We provide an automated approach to inferring a complete and well-simplified set of summoning snippets from the client source code, based on points-to analysis and code slicing. We implement this approach as a development assistant tool, and evaluate it on eight open source data processing programs, with average precision and recall of 89% and 95%, respectively. Further inspection of these clients, as well as a user study about writing data accessing code on their data sources, show that the inference results are useful in the inspection of existing clients and the development of new data access logics. © 2012 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Song, Hui;Huang, Gang;Xiong, Yingfei;Sun, Yanchun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873170472,10.1109/ICSM.2012.6405303,Modelling the 'Hurried' bug report reading process to summarize bug reports,"Although bug reports are frequently consulted project assets, they are communication logs, by-products of bug resolution, and not artifacts created with the intent of being easy to follow. To facilitate bug report digestion, we propose a new, unsupervised, bug report summarization approach that estimates the attention a user would hypothetically give to different sentences in a bug report, when pressed with time. We pose three hypotheses on what makes a sentence relevant: discussing frequently discussed topics, being evaluated or assessed by other sentences, and keeping focused on the bug report's title and description. Our results suggest that our hypotheses are valid, since the summaries have as much as 12% improvement in standard summarization evaluation metrics compared to the previous approach. Our evaluation also asks developers to assess the quality and usefulness of the summaries created for bug reports they have worked on. Feedback from developers not only show the summaries are useful, but also point out important requirements for this, and any bug summarization approach, and indicates directions for future work. © 2012 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Lotufo, Rafael;Malik, Zeeshan;Czarnecki, Krzysztof",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873175078,10.1109/ICSM.2012.6405306,"Triaging incoming change requests: Bug or commit history, or code authorship?","There is a tremendous wealth of code authorship information available in source code. Motivated with the presence of this information, in a number of open source projects, an approach to recommend expert developers to assist with a software change request (e.g., a bug fixes or feature) is presented. It employs a combination of an information retrieval technique and processing of the source code authorship information. The relevant source code files to the textual description of a change request are first located. The authors listed in the header comments in these files are then analyzed to arrive at a ranked list of the most suitable developers. The approach fundamentally differs from its previously reported counterparts, as it does not require software repository mining. Neither does it require training from past bugs/issues, which is often done with sophisticated techniques such as machine learning, nor mining of source code repositories, i.e., commits. An empirical study to evaluate the effectiveness of the approach on three open source systems, ArgoUML, JEdit, and MuCommander, is reported. Our approach is compared with two representative approaches: 1) using machine learning on past bug reports, and 2) based on commit logs. The presented approach is found to provide recommendation accuracies that are equivalent or better than the two compared approaches. These findings are encouraging, as it opens up a promising and orthogonal possibility of recommending developers without the need of any historical change information. © 2012 IEEE.",change request | code authorship | expert developer recommendations | information retrieval | triaging,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Linares-Vasquez, Mario;Hossen, Kamal;Dang, Hoang;Kagdi, Huzefa;Gethers, Malcom;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873198298,10.1109/ICSM.2012.6405313,Incremental reengineering and migration of a 40 year old airport operations system,"This report describes the challenges and experiences with the incremental re-engineering and migration of a 40 year old airport operations system. The undocumented COBOL legacy system has to be replaced within given constraints such as limited downtime. A 3-step technical strategy is derived and successfully applied to the re-engineering task in this project. The incremental approach and resulting parallel operations of both systems are the most significant technical drivers for complexity in this environment. Furthermore, this report describes the process for planning, analyzing and designing a replacement system that is backed by strong user acceptance. The user interface design task of taking the system from VT100 to a web interface was a critical success factor, as well as live testing with actual production data and actual user interactions. Other aspects such as training and end user documentation are discussed. © 2012 IEEE.",airport operations | AODB | incremental | migration | reengineering,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Bernhart, Mario;Mauczka, Andreas;Fiedler, Michael;Strobl, Stefan;Grechenig, Thomas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873108293,10.1109/ICSM.2012.6405320,Automated architectural reviews with Semmle,"Keeping code at a high level of quality and in sync with the defined architecture is not a trivial matter when you are participating in a competitive market in which you would rather provide solutions today than tomorrow. Code checkers help, but most are too generic to be of real value and so end up being ignored. Customisable code checkers can provide real benefit at reasonable cost. By encoding company specific (or even project specific) rules, actual results become much more relevant. With the additional integration of UML diagrams and specifications into the code checker, it even becomes possible to manage the gap between the actual code and the architecture from day to day at reasonable cost. © 2012 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"De Schutter, Kris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873142716,10.1109/ICSM.2012.6405331,Detecting similar applications with collaborative tagging,"Detecting similar applications are useful for various purposes ranging from program comprehension, rapid prototyping, plagiarism detection, and many more. McMillan et al. have proposed a solution to detect similar applications based on common Java API usage patterns. Recently, collaborative tagging has impacted software development practices. Various sites allow users to give various tags to software systems. In this study, we would like to complement the study by McMillan et al. by leveraging another source of information aside from API usage patterns, namely software tags. We have performed a user study involving several participants and the results show that collaborative tagging is a promising source of information useful for detecting similar software applications. © 2012 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Thung, Ferdian;Lo, David;Jiang, Lingxiao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873120035,10.1109/ICSM.2012.6405349,ReBPMN: Recovering and reducing business processes,"Specification models recovered from existing software applications can support developers in comprehending and checking the applications during maintenance and evolution operations. Often, in fact, a huge amount of business knowledge is embedded in the application implementation while documentation is not available or not aligned with the actual software implementation. In order to (re)acquire and preserve the business knowledge, specifications recovery techniques are adopted. In this paper we present reBPMN, a tool that recovers business process models from execution traces of target applications. It recovers the process exposed by means of Web interfaces and it applies a multi-objective process reduction technique, which minimizes at the same time process complexity, non-conformances, and loss of business content. This allows us to obtain processes having high readability by decreasing their structural complexity, while preserving the completeness of the described business and domain-specific information. A case study shows the effectiveness of reBPMN in recovering readable and business-meaningful processes. © 2012 IEEE.",Business Process Recovery | Multi-Objective Optimization | Ontology,"IEEE International Conference on Software Maintenance, ICSM",2012-12-01,Conference Paper,"Tomasi, Alex;Marchetto, Alessandro;Di Francescomarino, Chiara;Susi, Angelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84876384943,10.1109/ISSRE.2012.14,Revolution: Automatic evolution of mined specifications,"Specifications mined from execution traces are largely used to support testing and analysis of software applications with little runtime variability. However, when models are mined from applications that evolve at runtime, the resulting models become quickly obsolete, and thus of little support for any testing and analysis activity. To cope with such systems, mined specifications must be consistently updated every time the software changes. In principle, models can be periodically mined from scratch, but in many cases this solution is too expensive or even impossible. In this paper we describe Revolution, an approach for the automatic evolution of specifications mined by applying state abstraction techniques. Revolution produces models that are continuously updated and thus remain aligned with the actual implementation. Empirical results show that Revolution can suitably address run-time evolving applications. © 2012 IEEE.",Finite state machine | Model-based analysis | Specification mining,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2012-12-01,Conference Paper,"Mariani, Leonardo;Marchetto, Alessandro;Nguyen, Cu D.;Tonella, Paolo;Baars, Arthur",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84876366272,10.1109/ISSRE.2012.11,An assertions-based approach to verifying the absence property pattern,"Temporal properties are very common in various classes of systems, including information systems and security policies. This paper investigates two verification methods, proof and model checking, for one of the most frequent patterns of temporal property, the absence pattern. We explore two model-based specification techniques, B and Alloy, because of their adequacy for easily specifying systems with complex data structures, like information systems. We propose a first-order, assertion-based, sound and complete strategy to verify the absence pattern. This enables the proof of the absence pattern using conventional first-order provers. We show that the use of assertions significantly increases the size of the models that can be checked, when compared to traditional LTL model checking techniques. The approach is illustrated throughout a case study. © 2012 IEEE.",Absence properties | Alloy | B method | Model checking | Proofs | Temporal properties | Verification,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2012-12-01,Conference Paper,"Frappier, Marc;Mammar, Amel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865023884,10.1109/MISE.2012.6226020,Extent and characteristics of dependencies between vehicle functions in automotive software systems,"Functional dependencies and feature interactions are a major source of erroneous and unwanted behavior in software-intensive systems. To overcome these problems, many approaches exist that focus on modeling these functional dependencies in advance, i.e., in the specification or the design of a system. However, there is little empirical data on the amount of such interactions between system functions in realistic systems. In this paper, we analyze structural models of a modern realistic automotive vehicle system with the aim to assess the extent and characteristics of interactions between system functions. Our results show that at least 69% of the analyzed system functions depend on each other or influence each other. These dependencies stretch all over the system whereby single system functions have dependencies to up to 40% of all system functions. These results challenge the current development methods and processes that treat system functions more or less as independent units of functionality. © 2012 IEEE.",automotive | empirical studies | feature interaction | functional specifications | model-based development,"2012 4th International Workshop on Modeling in Software Engineering, MiSE 2012 - Proceedings",2012-08-20,Conference Paper,"Vogelsang, Andreas;Teuchert, Stefan;Girard, Jean François",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864976405,10.1109/MISE.2012.6226011,Community-driven language development,"Software development processes are becoming more collaborative, trying to integrate end-users as much as possible. The idea is to advance towards a community-driven process where all actors (both technical and nontechnical) work together to ensure that the system-to-be will satisfy all expectations. This seems specially appropriate in the field of Domain-Specific Languages (DSLs) typically designed to facilitate the development of software for a particular domain. DSLs offer constructs closer to the vocabulary of the domain which simplifies the adoption of the DSL by endusers. Interestingly enough, the development of DSLs is not a collaborative process itself. In this sense, the goal of this paper is to propose a collaborative infrastructure for the development of DSLs where end-users have a direct and active participation in the evolution of the language. This infrastructure is based on Collaboro, a DSL to represent change proposals, possible solutions and comments arisen during the development and evolution of a language. © 2012 IEEE.",,"2012 4th International Workshop on Modeling in Software Engineering, MiSE 2012 - Proceedings",2012-08-20,Conference Paper,"Izquierdo, Javier Luis Cánovas;Cabot, Jordi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864985386,10.1109/MISE.2012.6226015,A feature model for model-to-text transformation languages,"Model-to-text (M2T) transformation is an important model management operation, as it is used to implement code and documentation generation; model serialisation (enabling model interchange); and model visualisation and exploration. Despite the creation of the MOF Model-To-Text Transformation Language (MOFM2T) in 2008, many very different M2T languages exist today. Because there is little interoperability between M2T languages and rewriting an existing M2T transformation in a new language is costly, developers face a difficult choice when selecting a M2T language. In this paper, we use domain analysis to identify a preliminary feature model for M2T languages. We demonstrate the appropriateness of the feature model by describing two different M2T languages, and discuss potential applications for a tool-supported and model-driven approach to describing the features of M2T languages. © 2012 IEEE.",,"2012 4th International Workshop on Modeling in Software Engineering, MiSE 2012 - Proceedings",2012-08-20,Conference Paper,"Rose, Louis M.;Matragkas, Nicholas;Kolovos, Dimitrios S.;Paige, Richard F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864989344,10.1109/MISE.2012.6226019,A feature-based approach to system deployment and adaptation,"Building large scale systems involves many design decisions, both at specification and implementation levels. This is due to numerous variants in the description of the task to achieve and its execution context as well as in the assembly of software components. We have modeled variability for large scale systems using feature diagrams, a formalism well suited for modeling variablility. These models are built with a clear separation of concerns between specification and implementation aspects. They are used at design and deployment time as well as at execution time. Our test application domain is video surveillance systems, from a software engineering perspective. These are good candidates to put model driven engineering to the test, because of the huge variability in both the surveillance tasks and the video analysis algorithms. They are also dynamically adaptive systems, thus suitable for models at run time approaches. We propose techniques and tools to define the models, to operate on them, and to transform specification requirements into an effective implementation of a processing chain. We also define a run time architecture to integrate models into the adaptation loop. © 2012 IEEE.",feature model | models at run time | software variability | video surveillance,"2012 4th International Workshop on Modeling in Software Engineering, MiSE 2012 - Proceedings",2012-08-20,Conference Paper,"Moisan, Sabine;Rigault, Jean Paul;Acher, Mathieu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865142761,10.1109/MSR.2012.6224293,Towards improving bug tracking systems with game mechanisms,"Low bug report quality and human conflicts pose challenges to keep bug tracking systems productive. This work proposes to address these issues by applying game mechanisms to bug tracking systems. We investigate the use of game mechanisms in Stack Overflow, an online community organized to resolve computer programming related problems, for which the improvements we seek for bug tracking systems also turn out to be relevant. The results of our Stack Overflow investigation show that its game mechanisms could be used to address these issues by motivating contributors to increase contribution frequency and quality, by filtering useful contributions, and by creating an agile and dependable moderation system. We proceed by mapping these mechanisms to open-source bug tracking systems, and find that most benefits are applicable. Additionally, our results motivate tailoring a reward and reputation system and summarizing bug reports as future directions for increasing the benefits of game mechanisms in bug tracking systems. © 2012 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2012-08-23,Conference Paper,"Lotufo, Rafael;Passos, Leonardo;Czarnecki, Krzysztof",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865116062,10.1109/MSR.2012.6224268,Trendy bugs: Topic trends in the Android bug reports,"Studying vast volumes of bug and issue discussions can give an understanding of what the community has been most concerned about, however the magnitude of documents can overload the analyst. We present an approach to analyze the development of the Android open source project by observing trends in the bug discussions in the Android open source project public issue tracker. This informs us of the features or parts of the project that are more problematic at any given point of time. In turn, this can be used to aid resource allocation (such as time and man power) to parts or features. We support these ideas by presenting the results of issue topic distributions over time using statistical analysis of the bug descriptions and comments for the Android open source project. Furthermore, we show relationships between those time distributions and major development releases of the Android OS. © 2012 IEEE.",Android | bug logs | statistical trend analysis | topics,IEEE International Working Conference on Mining Software Repositories,2012-08-23,Conference Paper,"Martie, Lee;Palepu, Vijay Krishna;Sajnani, Hitesh;Lopes, Cristina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869804067,10.1145/2384616.2384624,Reducing the barriers to writing verified specifications,"Formally verifying a program requires significant skill not only because of complex interactions between program subcomponents, but also because of deficiencies in current verification interfaces. These skill barriers make verification economically unattractive by preventing the use of lessskilled (less-expensive) workers and distributed workflows (i.e., crowdsourcing). This paper presents VeriWeb, a web-based IDE for verification that decomposes the task of writing verifiable specifications into manageable subproblems. To overcome the information loss caused by task decomposition, and to reduce the skill required to verify a program, VeriWeb incorporates several innovative user interface features: drag and drop condition construction, concrete counterexamples, and specification inlining. To evaluate VeriWeb, we performed three experiments. First, we show that VeriWeb lowers the time and monetary cost of verification by performing a comparative study of VeriWeb and a traditional tool using 14 paid subjects contracted hourly from Exhedra Solution's vWorker online marketplace. Second, we demonstrate the dearth and insufficiency of current ad-hoc labor marketplaces for verification by recruiting workers from Amazon's Mechanical Turk to perform verification with VeriWeb. Finally, we characterize the minimal communication overhead incurred when Veri- Web is used collaboratively by observing two pairs of developers each use the tool simultaneously to verify a single program.",Crowdsourcing | Human factors | Program verification,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Schiller, Todd W.;Ernst, Michael D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869800778,10.1145/2384616.2384633,An abstract interpretation framework for refactoring with application to extract methods with contracts,"Method extraction is a common refactoring feature provided by most modern IDEs. It replaces a user-selected piece of code with a call to an automatically generated method. We address the problem of automatically inferring contracts (precondition, postcondition) for the extracted method. We require the inferred contract: (a) to be valid for the extracted method (validity); (b) to guard the language and programmer assertions in the body of the extracted method by an opportune precondition (safety); (c) to preserve the proof of correctness of the original code when analyzing the new method separately (completeness); and (d) to be the most general possible (generality). These requirements rule out trivial solutions (e.g., inlining, projection, etc). We propose two theoretical solutions to the problem. The first one is simple and optimal. It is valid, safe, complete and general but unfortunately not effectively computable (except for unrealistic finiteness/decidability hypotheses). The second one is based on an iterative forward/backward method. We show it to be valid, safe, and, under reasonable assumptions, complete and general. We prove that the second solution subsumes the first. All justifications are provided with respect to a new, set-theoretic version of Hoare logic (hence without logic), and abstract of Hoare logic, revisited to avoid surprisingly unsound inference rules. We have implemented the new algorithms on the top of two industrial-strength tools (CCCheck and the Microsoft Roslyn CTP). Our experience shows that the analysis is both fast enough to be used in an interactive environment and precise enough to generate good annotations.",Design by contract | Interpretation | Method extraction | Program transformation | Refactoring | Static analysis,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Cousot, Patrick;Cousot, Radhia;Logozzo, Francesco;Barnett, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869751828,10.1145/2384616.2384644,Elixir: A system for synthesizing concurrent graph programs ?,"Algorithms in new application areas like machine learning and network analysis use ""irregular"" data structures such as graphs, trees and sets. Writing efficient parallel code in these problem domains is very challenging because it requires the programmer to make many choices: a given problem can usually be solved by several algorithms, each algorithm may have many implementations, and the best choice of algorithm and implementation can depend not only on the characteristics of the parallel platform but also on properties of the input data such as the structure of the graph. One solution is to permit the application programmer to experiment with different algorithms and implementations without writing every variant from scratch. Auto-tuning to find the best variant is a more ambitious solution. These solutions require a system for automatically producing efficient parallel implementations from high-level specifications. Elixir, the system described in this paper, is the first step towards this ambitious goal. Application programmers write specifications that consist of an operator, which describes the computations to be performed, and a schedule for performing these computations. Elixir uses sophisticated inference techniques to produce efficient parallel code from such specifications. We used Elixir to automatically generate many parallel implementations for three irregular problems: breadthfirst search, single source shortest path, and betweennesscentrality computation. Our experiments show that the best generated variants can be competitive with handwritten code for these problems from other research groups; for some inputs, they even outperform the handwritten versions.",Amorphous Data-parallelism | Compiler Optimization | Concurrency | Irregular Programs | Optimistic Parallelization. | Parallelism | Synthesis,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Prountzos, Dimitrios;Manevich, Roman;Pingali, Keshav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869824477,10.1145/2384616.2384664,Talk versus work: Characteristics of developer collaboration on the jazz platform,"IBM's Jazz initiative offers a state-of-the-art collaborative development environment (CDE) facilitating developer interactions around interdependent units of work. In this paper, we analyze development data across two versions of a major IBM product developed on the Jazz platform, covering in total 19 months of development activity, including 17,000+ work items and 61,000+ comments made by more than 190 developers in 35 locations. By examining the relation between developer talk and work, we find evidence that developers maintain a reasonably high level of connectivity with peer developers with whom they share work dependencies, but the span of a developer's communication goes much beyond the known dependencies of his/her work items. Using multiple linear regression models, we find that the number of defects owned by a developer is impacted by the number of other developers (s)he is connected through talk, his/her interpersonal influence in the network of work dependencies, the number of work items (s)he comments on, and the number work items (s)he owns. These effects are maintained even after controlling for workload, role, work dependency, and connection related factors. We discuss the implications of our results for collaborative software development and project governance.",Collaboration | Defects | Jazz | Models | Social network analysis | Software teams,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Datta, Subhajit;Sindhgatta, Renuka;Sengupta, Bikram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869751795,10.1145/2384616.2384666,An empirical study of the influence of static type systems on the usability of undocumented software,"Although the study of static and dynamic type systems plays a major role in research, relatively little is known about the impact of type systems on software development. Perhaps one of the more common arguments for static type systems in languages such as Java or C++ is that they require developers to annotate their code with type names, which is thus claimed to improve the documentation of software. In contrast, one common argument against static type systems is that they decrease flexibility, which may make them harder to use. While these arguments are found in the literature, rigorous empirical evidence is lacking. We report on a controlled experiment where 27 subjects performed programming tasks on an undocumented API with a static type system (requiring type annotations) as well as a dynamic type system (which does not). Our results show that for some tasks, programmers had faster completion times using a static type system, while for others, the opposite held. We conduct an exploratory study to try and theorize why.",Empirical research | Programming languages | Type systems,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Mayer, Clemens;Hanenberg, Stefan;Robbes, Romain;Tanter, Éric;Stefik, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869744336,10.1145/2384616.2384680,ReIm and reiminfer: Checking and inference of reference immutability and method purity,"Reference immutability ensures that a reference is not used to modify the referenced object, and enables the safe sharing of object structures. A pure method does not cause side-effects on the objects that existed in the pre-state of the method execution. Checking and inference of reference immutability and method purity enables a variety of program analyses and optimizations. We present ReIm, a type system for reference immutability, and ReImInfer, a corresponding type inference analysis. The type system is concise and context-sensitive. The type inference analysis is precise and scalable, and requires no manual annotations. In addition, we present a novel application of the reference immutability type system: method purity inference. To support our theoretical results, we implemented the type system and the type inference analysis for Java. We include a type checker to verify the correctness of the inference result. Empirical results on Java applications and libraries of up to 348kLOC show that our approach achieves both scalability and precision.",,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Huang, Wei;Milanova, Ana;Dietl, Werner;Ernst, Michael D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869751528,10.1145/2384616.2384685,Chaperones and impersonators: Run-time support for reasonable interposition,"Chaperones and impersonators provide run-time support for interposing on primitive operations such as function calls, array access and update, and structure field access and update. Unlike most interposition support, chaperones and impersonators are restricted so that they constrain the behavior of the interposing code to reasonable interposition, which in practice preserves the 2Department of Computer Science University of Massachusetts Amherst, MA, USA 3School of Computer Science University of Waterloo Waterloo, ON, Canadaion mechanisms and reasoning that programmers and compiler analyses rely on. Chaperones and impersonators are particularly useful for implementing contracts, and our implementation in Racket allows us to improve both the expressiveness and the performance of Racket's contract system. Specifically, contracts on mutable data can be enforced without changing the API to that data; contracts on large data structures can be checked lazily on only the accessed parts of the structure; contracts on objects and classes can be implemented with lower overhead; and contract wrappers can preserve object equality where appropriate. With this extension, gradual typing systems, such as Typed Racket, that rely on contracts for interoperation with untyped code can now pass mutable values safely between typed and untyped modules.",Contracts | Intercession | Interposition | Proxies,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Strickland, T. Stephen;Tobin-Hochstadt, Sam;Findler, Robert Bruce;Flatt, Matthew",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869757438,10.1145/2384616.2384689,Typestate-based semantic code search over partial programs,"We present a novel code search approach for answering queries focused on API-usage with code showing how the API should be used. To construct a search index, we develop new techniques for statically mining and consolidating temporal API specifications from code snippets. In contrast to existing semanticbased techniques, our approach handles partial programs in the form of code snippets. Handling snippets allows us to consume code from various sources such as parts of open source projects, educational resources (e.g. tutorials), and expert code sites. To handle code snippets, our approach (i) extracts a possibly partial temporal specification from each snippet using a relatively precise static analysis tracking a generalized notion of typestate, and (ii) consolidates the partial temporal specifications, combining consistent partial information to yield consolidated temporal specifications, each of which captures a full(er) usage scenario. To answer a search query, we define a notion of relaxed inclusion matching a query against temporal specifications and their corresponding code snippets. We have implemented our approach in a tool called PRIME and applied it to search for API usage of several challenging APIs. PRIME was able to analyze and consolidate thousands of snippets per tested API, and our results indicate that the combination of a relatively precise analysis and consolidation allowed PRIME to answer challenging queries effectively.",Code Search Engine | Ranking Code Samples | Specification Mining | Static Analysis | Typestate,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Mishne, Alon;Shoham, Sharon;Yahav, Eran",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869837624,10.1145/2384616.2384690,Finding reusable data structures,"A big source of run-time performance problems in largescale, object-oriented applications is the frequent creation of data structures (by the same allocation site) whose lifetimes are disjoint, and whose shapes and data content are always the same. Constructing these data structures and computing the same data values many times is expensive; significant performance improvements can be achieved by reusing their instances, shapes, and/or data values rather than reconstructing them. This paper presents a run-time technique that can be used to help programmers find allocation sites that create such data structures to improve performance. At the heart of the technique are three reusability definitions and novel summarization approaches that compute summaries for data structures based on these definitions. The computed summaries are used subsequently to find data structures that have disjoint lifetimes, and/or that have the same shapes and content. We have implemented this technique in the Jikes RVM and performed extensive studies on large-scale, real-world programs. We describe our experience using six case studies, in which we have achieved large performance gains by fixing problems reported by our tool.",Data structure encoding | Memory management | Object reuse | Performance optimization,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2012-11-27,Conference Paper,"Xu, Guoqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863471732,10.1145/2254064.2254091,Parallelizing top-down interprocedural analyses,"Modularity is a central theme in any scalable program analysis. The core idea in a modular analysis is to build summaries at procedure boundaries, and use the summary of a procedure to analyze the effect of calling it at its calling context. There are two ways to perform a modular program analysis: (1) top-down and (2) bottomup. A bottom-up analysis proceeds upwards from the leaves of the call graph, and analyzes each procedure in the most general calling context and builds its summary. In contrast, a top-down analysis starts from the root of the call graph, and proceeds downward, analyzing each procedure in its calling context. Top-down analyses have several applications in verification and software model checking. However, traditionally, bottom-up analyses have been easier to scale and parallelize than top-down analyses. In this paper, we propose a generic framework, BOLT, which uses MapReduce style parallelism to scale top-down analyses. In particular, we consider top-down analyses that are demand driven, such as the ones used for software model checking. In such analyses, each intraprocedural analysis happens in the context of a reachability query. A query Q over a procedure P results in query tree that consists of sub-queries over the procedures called by P . The key insight in BOLT is that the query tree can be explored in parallel using MapReduce style parallelism - the map stage can be used to run a set of enabled queries in parallel, and the reduce stage can be used to manage inter-dependencies between queries. Iterating the map and reduce stages alternately, we can exploit the parallelism inherent in top-down analyses. Another unique feature of BOLT is that it is parameterized by the algorithm used for intraprocedural analysis. Several kinds of analyses, including may-analyses, mustanalyses, and may-must-analyses can be parallelized using BOLT. We have implemented the BOLT framework and instantiated the intraprocedural parameter with a may-must-analysis. We have run BOLT on a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties. Our results demonstrate an average speedup of 3.71x and a maximum speedup of 7.4x (with 8 cores) over a sequential analysis. Moreover, in several checks where a sequential analysis fails, BOLT is able to successfully complete its analysis. Copyright © 2012 ACM.",Abstraction refinement | Interprocedural analysis | Software model checking,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Albarghouthi, Aws;Kumar, Rahul;Nori, Aditya V.;Rajamani, Sriram K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863429283,10.1145/2254064.2254098,Type-directed completion of partial expressions,"Modern programming frameworks provide enormous libraries arranged in complex structures, so much so that a large part of modern programming is searching for APIs that ""surely exist"" somewhere in an unfamiliar part of the framework. We present a novel way of phrasing a search for an unknown API: the programmer simply writes an expression leaving holes for the parts they do not know.We call these expressions partial expressions. We present an efficient algorithm that produces likely completions ordered by a ranking scheme based primarily on the similarity of the types of the APIs suggested to the types of the known expressions. This gives a powerful language for both API discovery and code completion with a small impedance mismatch from writing code. In an automated experiment on mature C# projects, we show our algorithm can place the intended expression in the top 10 choices over 80% of the time. Copyright © 2012 ACM.",Code completion | Partial expressions | Program synthesis | Ranking | Type-based analysis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Perelman, Daniel;Gulwani, Sumit;Ball, Thomas;Grossman, Dan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863439046,10.1145/2254064.2254099,Self-stabilizing java,Self-stabilizing programs automatically recover from state corruption caused by software bugs and other sources to reach the correct state. A number of applications are inherently self-stabilizing- such programs typically overwrite all non-constant data with new input data. We present a type system and static analyses that together check whether a program is self-stabilizing. We combine this with a code generation strategy that ensures that a program continues executing long enough to self-stabilize. Our experience using SJava indicates that (1) SJava annotations are easy to write once one understands a program and (2) SJava successfully checked that several benchmarks were self-stabilizing. Copyright © 2012 ACM.,Self-stabilization | Software robustness,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Eom, Yong Hun;Demsky, Brian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863455863,10.1145/2254064.2254100,Type-directed automatic incrementalization,"Application data often changes slowly or incrementally over time. Since incremental changes to input often result in only small changes in output, it is often feasible to respond to such changes asymptotically more efficiently than by re-running the whole computation. Traditionally, realizing such asymptotic efficiency improvements requires designing problem-specific algorithms known as dynamic or incremental algorithms, which are often significantly more complicated than conventional algorithms to design, analyze, implement, and use. A long-standing open problem is to develop techniques that automatically transform conventional programs so that they correctly and efficiently respond to incremental changes. In this paper, we describe a significant step towards solving the problem of automatic incrementalization: a programming language and a compiler that can, given a few type annotations describing what can change over time, compile a conventional program that assumes its data to be static (unchanging over time) to an incremental program. Based on recent advances in self-adjusting computation, including a theoretical proposal for translating purely functional programs to self-adjusting programs, we develop techniques for translating conventional Standard ML programs to selfadjusting programs. By extending the Standard ML language, we design a fully featured programming language with higher-order features, a module system, and a powerful type system, and implement a compiler for this language. The resulting programming language, LML, enables translating conventional programs decorated with simple type annotations into incremental programs that can respond to changes in their data correctly and efficiently. We evaluate the effectiveness of our approach by considering a range of benchmarks involving lists, vectors, and matrices, as well as a ray tracer. For these benchmarks, our compiler incrementalizes existing code with only trivial amounts of annotation. The resulting programs are often asymptotically more efficient, leading to orders of magnitude speedups in practice. Copyright © 2012 ACM.",Compiler optimization | Incrementalization | Performance | Self-adjusting computation | Type annotations,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-01-01,Conference Paper,"Chen, Yan;Dunfield, Jana;Acar, Umut A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863494447,10.1145/2254064.2254112,Synthesizing software verifiers from proof rules,"Automatically generated tools can significantly improve programmer productivity. For example, parsers and dataflow analyzers can be automatically generated from declarative specifications in the form of grammars, which tremendously simplifies the task of implementing a compiler. In this paper, we present a method for the automatic synthesis of software verification tools. Our synthesis procedure takes as input a description of the employed proof rule, e.g., program safety checking via inductive invariants, and produces a tool that automatically discovers the auxiliary assertions required by the proof rule, e.g., inductive loop invariants and procedure summaries. We rely on a (standard) representation of proof rules using recursive equations over the auxiliary assertions. The discovery of auxiliary assertions, i.e., solving the equations, is based on an iterative process that extrapolates solutions obtained for finitary unrollings of equations. We show how our method synthesizes automatic safety and liveness verifiers for programs with procedures, multi-threaded programs, and functional programs. Our experimental comparison of the resulting verifiers with existing state-of-the-art verification tools confirms the practicality of the approach. Copyright © 2012 ACM.",Proof rules | Software model checking | Software verification | Verification tool synthesis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Grebenshchikov, Sergey;Lopes, Nuno P.;Popeea, Corneliu;Rybalchenko, Andrey",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863451085,10.1145/2254064.2254114,Concurrent data representation synthesis,"We describe an approach for synthesizing data representations for concurrent programs. Our compiler takes as input a program written using concurrent relations and synthesizes a representation of the relations as sets of cooperating data structures as well as the placement and acquisition of locks to synchronize concurrent access to those data structures. The resulting code is correct by construction: individual relational operations are implemented correctly and the aggregate set of operations is serializable and deadlock free. The relational specification also permits a high-level optimizer to choose the best performing of many possible legal data representations and locking strategies, which we demonstrate with an experiment autotuning a graph benchmark. Copyright © 2012 ACM.",Lock placement | Synthesis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Hawkins, Peter;Aiken, Alex;Fisher, Kathleen;Rinard, Martin;Sagiv, Mooly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863494854,10.1145/2254064.2254115,Dynamic synthesis for relaxed memory models,"Modern architectures implement relaxed memory models which may reorder memory operations or execute them non-atomically. Special instructions called memory fences are provided, allowing control of this behavior. To implement a concurrent algorithm for a modern architecture, the programmer is forced to manually reason about subtle relaxed behaviors and figure out ways to control these behaviors by adding fences to the program. Not only is this process time consuming and error-prone, but it has to be repeated every time the implementation is ported to a different architecture. In this paper, we present the first scalable framework for handling real-world concurrent algorithms running on relaxed architectures. Given a concurrent C program, a safety specification, and a description of the memory model, our framework tests the program on the memory model to expose violations of the specification, and synthesizes a set of necessary ordering constraints that prevent these violations. The ordering constraints are then realized as additional fences in the program. We implemented our approach in a tool called DFENCE based on LLVM and used it to infer fences in a number of concurrent algorithms. Using DFENCE, we perform the first in-depth study of the interaction between fences in real-world concurrent C programs, correctness criteria such as sequential consistency and linearizability, and memory models such as TSO and PSO, yielding many interesting observations. We believe that this is the first tool that can handle programs at the scale and complexity of a lock-free memory allocator. Copyright © 2012 ACM.",Concurrency | Relaxed memory models | Synthesis | Weak memory models,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Liu, Feng;Nedev, Nayden;Prisadnikov, Nedyalko;Vechev, Martin;Yahav, Eran",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863465080,10.1145/2254064.2254126,Fully automatic and precise detection of thread safety violations,"Concurrent, object-oriented programs often use thread-safe library classes. Existing techniques for testing a thread-safe class either rely on tests using the class, on formal specifications, or on both. Unfortunately, these techniques often are not fully automatic as they involve the user in analyzing the output. This paper presents an automatic testing technique that reveals concurrency bugs in supposedly thread-safe classes. The analysis requires as input only the class under test and reports only true positives. The key idea is to generate tests in which multiple threads call methods on a shared instance of the tested class. If a concurrent test exhibits an exception or a deadlock that cannot be triggered in any linearized execution of the test, the analysis reports a thread safety violation. The approach is easily applicable, because it is independent of handwritten tests and explicit specifications. The analysis finds 15 concurrency bugs in popular Java libraries, including two previously unknown bugs in the Java standard library. Copyright © 2012 ACM.",Concurrent test generation | Testing | Thread safety,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2012-07-09,Conference Paper,"Pradel, Michael;Gross, Thomas R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2103656.2103663,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857805008,10.1145/2103656.2103673,Recursive proofs for inductive tree data-structures,"We develop logical mechanisms and procedures to facilitate the verification of full functional properties of inductive tree datastructures using recursion that are sound, incomplete, but terminating. Our contribution rests in a new extension of first-order logic with recursive definitions called Dryad, a syntactical restriction on pre- and post-conditions of recursive imperative programs using Dryad, and a systematic methodology for accurately unfolding the footprint on the heap uncovered by the program that leads to finding simple recursive proofs using formula abstraction and calls to SMT solvers. We evaluate our methodology empirically and show that several complex tree data-structure algorithms can be checked against full functional specifications automatically, given pre- and post-conditions. This results in the first automatic terminating methodology for proving a wide variety of annotated algorithms on tree data-structures correct, including max-heaps, treaps, red-black trees, AVL trees, binomial heaps, and B-trees. Copyright © 2012 ACM.",Heap analysis | Recursive program | SMT solver | Tree,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Madhusudan, P.;Qiu, Xiaokang;Stefanescu, Andrei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857837117,10.1145/2103656.2103674,Symbolic finite state transducers: Algorithms and applications,"Finite automata and finite transducers are used in a wide range of applications in software engineering, from regular expressions to specification languages. We extend these classic objects with symbolic alphabets represented as parametric theories. Admitting potentially infinite alphabets makes this representation strictly more general and succinct than classical finite transducers and automata over strings. Despite this, the main operations, including composition, checking that a transducer is single-valued, and equivalence checking for single-valued symbolic finite transducers are effective given a decision procedure for the background theory. We provide novel algorithms for these operations and extend composition to symbolic transducers augmented with registers. Our base algorithms are unusual in that they are nonconstructive, therefore, we also supply a separate model generation algorithm that can quickly find counterexamples in the case two symbolic finite transducers are not equivalent. The algorithms give rise to a complete decidable algebra of symbolic transducers. Unlike previous work, we do not need any syntactic restriction of the formulas on the transitions, only a decision procedure. In practice we leverage recent advances in satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on four case studies, covering a wide range of applications. Our techniques can synthesize string pre-images in excess of 8, 000 bytes in roughly a minute, and we find that our new encodings significantly outperform previous techniques in succinctness and speed of analysis. Copyright © 2012 ACM.",Automata | Composition | Equivalence | SMT,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Veanes, Margus;Hooimeijer, Pieter;Livshits, Benjamin;Molnar, David;Bjørner, Nikolaj;Research, Microsoft",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857871513,10.1145/2103656.2103675,Constraints as control,"We present an extension of Scala that supports constraint programming over bounded and unbounded domains. The resulting language, Kaplan, provides the benefits of constraint programming while preserving the existing features of Scala. Kaplan integrates constraint and imperative programming by using constraints as an advanced control structure; the developers use the monadic 'for' construct to iterate over the solutions of constraints or branch on the existence of a solution. The constructs we introduce have simple semantics that can be understood as explicit enumeration of values, but are implemented more efficiently using symbolic reasoning. Kaplan programs can manipulate constraints at run-time, with the combined benefits of type-safe syntax trees and first-class functions. The language of constraints is a functional subset of Scala, supporting arbitrary recursive function definitions over algebraic data types, sets, maps, and integers. Our implementation runs on a platform combining a constraint solver with a standard virtual machine. For constraint solving we use an algorithm that handles recursive function definitions through fair function unrolling and builds upon the state-of-the art SMT solver Z3. We evaluate Kaplan on examples ranging from enumeration of data structures to execution of declarative specifications. We found Kaplan promising because it is expressive, supporting a range of problem domains, while enabling full-speed execution of programs that do not rely on constraint programming. Copyright © 2012 ACM.",Constraint programming | Embedded domain-specific languages | Executable specifications | Non-determinism | Satisfiability modulo theories | Scala,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Köksal, Ali Sinan;Kuncak, Viktor;Suter, Philippe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857813189,10.1145/2103656.2103680,Deciding choreography realizability,"Since software systems are becoming increasingly more concurrent and distributed, modeling and analysis of interactions among their components is a crucial problem. In several application domains, message-based communication is used as the interaction mechanism, and the communication contract among the components of the system is specified semantically as a state machine. In the service-oriented computing domain such communication contracts are called ""choreography"" specifications. A choreography specification identifies allowable ordering of message exchanges in a distributed system. A fundamental question about a choreography specification is determining its realizability, i.e., given a choreography specification, is it possible to build a distributed system that communicates exactly as the choreography specifies? Checking realizability of choreography specifications has been an open problem for several years and it was not known if this was a decidable problem. In this paper we give necessary and sufficient conditions for realizability of choreographies. We implemented the proposed realizability check and our experiments show that it can efficiently determine the realizability of 1) web service choreographies, 2) Singularity OS channel contracts, and 3) UML collaboration (communication) diagrams. Copyright© 2012 ACM.",Choreography | Message-based Interactions | Realizability,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Basu, Samik;Bultan, Tevfik;Ouederni, Meriem",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857878490,10.1145/2103656.2103694,Resource-sensitive synchronization inference by abduction,"We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly-synchronized parallelized program and proof of that program. Unlike previous work, ours is not an independence analysis; we insert synchronization constructs to preserve relevant dependencies found in the sequential program that may otherwise be violated by a naïve translation. Separation logic allows us to parallelize fine-grained patterns of resource-usage, moving beyond straightforward points-to analysis. Our analysis works by using the sequential proof to discover dependencies between different parts of the program. It leverages these discovered dependencies to guide the insertion of synchronization primitives into the parallelized program, and to ensure that the resulting parallelized program satisfies the same specification as the original sequential program, and exhibits the same sequential behaviour. Our analysis is built using frame inference and abduction, two techniques supported by an increasing number of separation logic tools. Copyright © 2012 ACM.",Abduction | Deterministic parallelism | Frame inference | Separation logic,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Botinčan, Matko;Dodds, Mike;Jagannathan, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857876312,10.1145/2103656.2103698,Algebraic foundations for effect-dependent optimisations,"We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity). Copyright © 2012 ACM.",Algebraic theory of effects | Call-by-Push-Value | Code transformations | Compiler optimisations | Computational effects | Denotational semantics | Domain theory | Inequational logic | Relevant and affine monads | Sum and tensor | Type and effect systems,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Kammar, Ohad;Plotkin, Gordon D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857806005,10.1145/2103656.2103714,The ins and outs of gradual type inference,"Gradual typing lets programmers evolve their dynamically typed programs by gradually adding explicit type annotations, which confer benefits like improved performance and fewer run-time failures. However, we argue that such evolution often requires a giant leap, and that type inference can offer a crucial missing step. If omitted type annotations are interpreted as unknown types, rather than the dynamic type, then static types can often be inferred, thereby removing unnecessary assumptions of the dynamic type. The remaining assumptions of the dynamic type may then be removed by either reasoning outside the static type system, or restructuring the code. We present a type inference algorithm that can improve the performance of existing gradually typed programs without introducing any new run-time failures. To account for dynamic typing, types that flow in to an unknown type are treated in a fundamentally different manner than types that flow out. Furthermore, in the interests of backward-compatibility, an escape analysis is conducted to decide which types are safe to infer. We have implemented our algorithm for ActionScript, and evaluated it on the SunSpider and V8 benchmark suites. We demonstrate that our algorithm can improve the performance of unannotated programs as well as recover most of the type annotations in annotated programs. Copyright © 2012 ACM.",ActionScript | Gradual typing | Type inference,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2012-03-12,Conference Paper,"Rastogi, Aseem;Chaudhuri, Avik;Hosmer, Basil",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84872340802,10.1109/SCAM.2012.23,AccessAnalysis - A tool for measuring the appropriateness of access modifiers in Java systems,"Access modifiers allow Java developers to define package and class interfaces tailored for different groups of clients. According to the principles of information hiding and encapsulation, the accessibility of types, methods, and fields should be as restrictive as possible. However, in programming practice, the potential of the given possibilities seems not always be fully exploited. Access Analysis is a plug-in for the Eclipse IDE that measures the usage of access modifiers for types and methods in Java. It calculates two metrics, Inappropriate Generosity with Accessibility of Types (IGAT) and Inappropriate Generosity with Accessibility of Methods (IGAM), which represent the degree of deviation between actual and necessary access modifiers. As an approximation for the necessary access modifier, we introduce the notion of minimal access modifiers. The minimal access modifier is the most restrictive access modifier that allows all existing references to a type or method in the entire source code of a system. Access Analysis determines minimal access modifiers by static source code analysis using the build-in Java DOM/AST API of Eclipse. © 2012 IEEE.",,"Proceedings - 2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation, SCAM 2012",2012-12-01,Conference Paper,"Zoller, Christian;Schmolitzky, Axel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84872294567,10.1109/SCAM.2012.27,Combining conceptual and domain-based couplings to detect database and code dependencies,"Knowledge of software dependencies plays an important role in program comprehension and other maintenance activities. Traditionally, dependencies are derived by source code analysis, however, such an approach can be difficult to use in multi-tier hybrid software systems, or legacy applications where conventional code analysis tools simply do not work as is. In this paper, we propose a hybrid approach to detecting software dependencies by combining conceptual and domain-based coupling metrics. In recent years, a great deal of research focused on deriving various coupling metrics from these sources of information with the aim of assisting software maintainers. Conceptual metrics specify underlying relationships encoded by developers in identifiers and comments of source code classes whereas domain metrics exploit coupling manifested in domain-level information of software components and it is independent from software implementation. The proposed approach is independent from programming language, as such it can be used in multi-tier hybrid systems or legacy applications. We report the results of an empirical case study on a large-scale enterprise system where we demonstrate that the combined approach is able to detect database and source code dependencies with higher precision and recall as compared to its standalone constituents. © 2012 IEEE.",,"Proceedings - 2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation, SCAM 2012",2012-12-01,Conference Paper,"Gethers, Malcom;Aryani, Amir;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84872311480,10.1109/SCAM.2012.10,Compatibility prediction of eclipse third-party plug-ins in new eclipse releases,"Incompatibility between applications developed on top of frameworks with new versions of the frameworks is a big nightmare to both developers and users of the applications. Understanding the factors that cause incompatibilities is a step to solving them. One such direction is to analyze and identify parts of the reusable code of the framework that are prone to change. In this study we carried out an empirical investigation on 11 Eclipse SDK releases (1.0 to 3.7) and 288 Eclipse third-party plug-ins (ETPs) with two main goals: First, to determine the relationship between the age of Eclipse non-APIs (internal implementations) used by an ETP and the compatibility of the ETP. We found that third-party plug-in that use only old non-APIs have a high chance of compatibility success in new SDK releases compared to those that use at least one newly introduced non-API. Second, our goal was to build and test a predictive model for the compatibility of an ETP, supported in a given SDK release in a newer SDK release. Our findings produced 23 statistically significant prediction models having high values of the strength of the relationship between the predictors and the prediction (logistic regression R2 of up to 0.810). In addition, the results from model testing indicate high values of up to 100% of precision and recall and up to 98% of accuracy of the predictions. Finally, despite the fact that SDK releases with API breaking changes, i.e., 1.0, 2.0 and 3.0, have got nothing to do with non-APIs, our findings reveal that non-APIs introduced in these releases have a significant impact on the compatibility of the ETPs that use them. © 2012 IEEE.",Eclipse | non-API | Plugin | Prediction,"Proceedings - 2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation, SCAM 2012",2012-12-01,Conference Paper,"Businge, John;Serebrenik, Alexander;Van Den Brand, Mark",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864658547,10.1145/2254756.2254798,Don't let the negatives bring you down: Sampling from streams of signed updates,"Random sampling has been proven time and time again to be a powerful tool for working with large data. Queries over the full dataset are replaced by approximate queries over the smaller (and hence easier to store and manipulate) sample. The sample constitutes a flexible summary that supports a wide class of queries. But in many applications, datasets are modified with time, and it is desirable to update samples without requiring access to the full underlying datasets. In this paper, we introduce and analyze novel techniques for sampling over dynamic data, modeled as a stream of modifications to weights associated with each key. While sampling schemes designed for stream applications can often readily accommodate positive updates to the dataset, much less is known for the case of negative updates, where weights are reduced or items deleted altogether. We primarily consider the turnstile model of streams, and extend classic schemes to incorporate negative updates. Perhaps surprisingly, the modifications to handle negative updates turn out to be natural and seamless extensions of the well-known positive update-only algorithms. We show that they produce unbiased estimators, and we relate their performance to the behavior of corresponding algorithms on insert-only streams with different parameters. A careful analysis is necessitated, in order to account for the fact that sampling choices for one key now depend on the choices made for other keys. In practice, our solutions turn out to be efficient and accurate. Compared to recent algorithms for L p sampling which can be applied to this problem, they are significantly more reliable, and dramatically faster. © 2012 ACM.",data streams | deletions | sampling | updates,Performance Evaluation Review,2012-08-13,Conference Paper,"Cohen, Edith;Cormode, Graham;Duffield, Nick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864700460,10.1145/2254756.2254800,Fair sampling across network flow measurements,"Sampling is crucial for controlling resource consumption by internet traffic flow measurements. Routers use Packet Sampled NetFlow, and completed flow records are sampled in the measurement infrastructure. Recent research, motivated by the need of service providers to accurately measure both small and large traffic subpopulations, has focused on distributing a packet sampling budget amongst subpopulations. But long timescales of hardware development and lower bandwidth costs motivate post-measurement analysis of complete flow records at collectors instead. Sampling in collector databases then manages data volumes, yielding general purpose summaries that are rapidly queried to trigger drill-down analysis on a time limited window of full data. These are sufficiently small to be archived. This paper addresses the problem of distributing a sampling budget over subpopulations of flow records. Estimation accuracy goals are met by fairly sharing the budget. We establish a correspondence between the type of accuracy goal, and the flavor of fair sharing used. A streaming Max-Min Fair Sampling algorithm fairly shares the sampling budget across subpopulations, with sampling as a mechanism to deallocate budget. This provides timely samples and is robust against uncertainties in configuration and demand. We illustrate using flow records from an access router of a large ISP, where rates over interface traffic subpopulations vary over several orders of magnitude. We detail an implementation whose computational cost is no worse than subpopulation-oblivious sampling. © 2012 ACM.",estimation | IP flows | max-min fairness | sampling | streaming,Performance Evaluation Review,2012-08-13,Conference Paper,"Duffield, Nick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84870700608,10.1109/ICGSE.2012.30,"An exploratory study of software evolution and quality: Before, during and after a transfer","In the light of globalization it is not uncommon that different teams from different locations get involved in the development of a software product during its evolution. However, empirical evidence that demonstrates the effect of changing team members on software quality is scarce. In this paper, we investigate quality of a software product, across subsequent software releases, that was first developed in one location of Ericsson, a large multinational corporation, then jointly with an offshore location of the same company, and finally transferred offshore. To get a better understanding multiple data sources are used in the analysis: qualitative data, consisting of interviews and documentation, and quantitative data, consisting of release history and defect statistics. Our findings confirm an initial decline in efficiency and quality after a transfer, and highlight the areas of concern for companies that are considering transferring their product development from experienced teams to those having limited or no previous engagement with the product. © 2012 IEEE.",Empirical case study | Global Software Development | Software evolution | Software quality | Software transfer,"Proceedings - 2012 IEEE 7th International Conference on Global Software Engineering, ICGSE 2012",2012-12-12,Conference Paper,"Jabangwe, Ronald;Šmite, Darja",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84870723249,10.1109/ICGSE.2012.29,Systematic literature reviews in distributed software development: A tertiary study,"Distributed Software Development (DSD) emerged from the need to achieve geographically distant customers and currently, allows organizations have global customers and other benefits. This scenario has given rise to new Software Engineering challenges resulting from DSD particularities. Several Systematic Reviews were conducted to address these new challenges. The objective of this paper is to categorize systematic reviews conducted in DSD context. We used the systematic review method to identify SLRs (Systematic Literature Reviews) that address DSD aspects. This study is categorized as a tertiary review. Of fourteen SLRs, seven address aspects of managing distributed development. Four SLRs addressed topics of engineering process. The three remaining are related to Requirements, Design and Software Engineering Education in DSD. The topic areas covered by SLRs are limited, where the majority are focused on summarize the current knowledge concerning a research question. Despite the number of SLRs, the amount of empirical studies is relatively small. © 2012 IEEE.",Distributed Software Development | Empirical Evidence | Global Software Engineering | Systematic Review | Tertiary Study,"Proceedings - 2012 IEEE 7th International Conference on Global Software Engineering, ICGSE 2012",2012-12-12,Conference Paper,"Marques, Anna Beatriz;Rodrigues, Rosiane;Conte, Tayana",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864056961,10.1007/978-3-642-31424-7_9,ACTL ∩ LTL synthesis,"We study the synthesis problem for specifications of the common fragment of ACTL (computation tree logic with only universal path quantification) and LTL (linear-time temporal logic). Key to this setting is a novel construction for translating properties from LTL to very-weak automata, whenever possible. Such automata are structurally simple and thus amenable to optimizations as well as symbolic implementations. Based on this novel construction, we describe a synthesis approach that inherits the efficiency of generalized reactivity(1) synthesis [27], but is significantly richer in terms of expressivity. © 2012 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-07-25,Conference Paper,"Ehlers, Rüdiger",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864027414,10.1007/978-3-642-31424-7_18,Leveraging interpolant strength in model checking,"Craig interpolation is a well known method of abstraction successfully used in both hardware and software model checking. The logical strength of interpolants can affect the quality of approximations and consequently the performance of the model checkers. Recently, it was observed that for the same resolution proof a complete lattice of interpolants ordered by strength can be derived. Most state-of-the-art model checking techniques based on interpolation subject the interpolants to constraints that ensure efficient verification as, for example, in transition relation approximation for bounded model checking, counterexample-guided abstraction refinement and function summarization for software update checking. However, in general, these verification-specific constraints are not satisfied by all possible interpolants. The paper analyzes the restrictions within the lattice of interpolants under which the required constraints are satisfied. This enables investigation of the effect of the strength of interpolants on the particular techniques, while preserving their soundness. As an additional benefit, combination of this result with proof manipulation procedures allows the use of optimized solvers to generate interpolants of different strengths for various model checking techniques. © 2012 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-07-25,Conference Paper,"Rollini, Simone Fulvio;Sery, Ondrej;Sharygina, Natasha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864028638,10.1007/978-3-642-31424-7_25,Assume-guarantee abstraction refinement for probabilistic systems,"We describe an automated technique for assume-guarantee style checking of strong simulation between a system and a specification, both expressed as non-deterministic Labeled Probabilistic Transition Systems (LPTSes). We first characterize counterexamples to strong simulation as stochastic trees and show that simpler structures are insufficient. Then, we use these trees in an abstraction refinement algorithm that computes the assumptions for assume-guarantee reasoning as conservative LPTS abstractions of some of the system components. The abstractions are automatically refined based on tree counterexamples obtained from failed simulation checks with the remaining components. We have implemented the algorithms for counterexample generation and assume-guarantee abstraction refinement and report encouraging results. © 2012 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-07-25,Conference Paper,"Komuravelli, Anvesh;Pǎsǎreanu, Corina S.;Clarke, Edmund M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864050468,10.1007/978-3-642-31424-7_28,Approximately bisimilar symbolic models for digital control systems,"Symbolic approaches to control hybrid systems construct a discrete approximately-bisimilar abstraction of a continuous control system and apply automata-theoretic techniques to construct controllers enforcing given specifications. For the class of digital control systems (i.e., whose control signals are piecewise constant) satisfying incremental input-to-state stability (δ-ISS), existing techniques to compute discrete abstractions begin with a quantization of the state and input sets, and show that the quantized system is approximately bisimilar to the original if the sampling time is sufficiently large or if the Lyapunov functions of the system decrease fast enough. If the sampling time is not sufficiently large, the former technique fails to apply. While abstraction based on Lyapunov functions may be applicable, because of the conservative nature of Lyapunov functions in practice, the size of the discrete abstraction may be too large for subsequent analyses. In this paper, we propose a technique to compute discrete approximately-bisimilar abstractions of δ-ISS digital control systems. Our technique quantizes the state and input sets, but is based on multiple sampling steps: instead of requiring that the sampling time is sufficiently large (which may not hold), the abstract transition system relates states multiple sampling steps apart. We show on practical examples that the discrete state sets computed by our procedure can be several orders of magnitude smaller than existing approaches, and can compute symbolic approximate-bisimilar models even when other existing approaches do not apply or time-out. Since the size of the discrete state set is the main limiting factor in the application of symbolic control, our results enable symbolic control of larger systems than was possible before. © 2012 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-07-25,Conference Paper,"Majumdar, Rupak;Zamani, Majid",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864056988,10.1007/978-3-642-31424-7_36,An axiomatic memory model for POWER multiprocessors,"The growing complexity of hardware optimizations employed by multiprocessors leads to subtle distinctions among allowed and disallowed behaviors, posing challenges in specifying their memory models formally and accurately, and in understanding and analyzing the behavior of concurrent software. This complexity is particularly evident in the IBM® Power Architecture®, for which a faithful specification was published only in 2011 using an operational style. In this paper we present an equivalent axiomatic specification, which is more abstract and concise. Although not officially sanctioned by the vendor, our results indicate that this axiomatic specification provides a reasonable basis for reasoning about current IBM® POWER® multiprocessors. We establish the equivalence of the axiomatic and operational specifications using both manual proof and extensive testing. To demonstrate that the constraint-based style of axiomatic specification is more amenable to computer-aided verification, we develop a SAT-based tool for evaluating possible outcomes of multi-threaded test programs, and we show that this tool is significantly more efficient than a tool based on an operational specification. © 2012 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-01-01,Conference Paper,"Mador-Haim, Sela;Maranget, Luc;Sarkar, Susmit;Memarian, Kayvan;Alglave, Jade;Owens, Scott;Alur, Rajeev;Martin, Milo M.K.;Sewell, Peter;Williams, Derek",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864051872,10.1007/978-3-642-31424-7_62,Joogie: Infeasible code detection for Java,"We present Joogie, a tool that detects infeasible code in Java programs. Infeasible code is code that does not occur on feasible control-flow paths and thus has no feasible execution. Infeasible code comprises many errors detected by static analysis in modern IDEs such as guaranteed null-pointer dereference or unreachable code. Unlike existing techniques, Joogie identifies infeasible code by proving that a particular statement cannot occur on a terminating execution using techniques from static verification. Thus, Joogie is able to detect infeasible code which is overlooked by existing tools. Joogie works fully automatically, it does not require user-provided specifications and (almost) never produces false warnings. © 2012 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-07-25,Conference Paper,"Arlt, Stephan;Schäf, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867515576,10.1145/2364527.2364551,On the complexity of equivalence of specifications of infinite objects,"We study the complexity of deciding the equality of infinite objects specified by systems of equations, and of infinite objects specified by λ-terms. For equational specifications there are several natural notions of equality: equality in all models, equality of the sets of solutions, and equality of normal forms for productive specifications. For λ-terms we investigate Böhm-tree equality and various notions of observational equality. We pinpoint the complexity of each of these notions in the arithmetical or analytical hierarchy. We show that the complexity of deciding equality in all models subsumes the entire analytical hierarchy. This holds already for the most simple infinite objects, viz. streams over {0,1}, and stands in sharp contrast to the low arithmetical II 0 2-completeness of equality of equationally specified streams derived in [17] employing a different notion of equality. © 2012 ACM.",complexity | equality | equational specifications | infinite objects | lambda terms | semantics,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2012-10-22,Conference Paper,"Endrullis, Jörg;Hendriks, Dimitri;Bakhshi, Rena",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867491922,10.1145/2364527.2364575,Automatic amortised analysis of dynamic memory allocation for lazy functional programs,"This paper describes the first successful attempt, of which we are aware, to define an automatic, type-based static analysis of resource bounds for lazy functional programs. Our analysis uses the automatic amortisation approach developed by Hofmann and Jost, which was previously restricted to eager evaluation. In this paper, we extend this work to a lazy setting by capturing the costs of unevaluated expressions in type annotations and by amortising the payment of these costs using a notion of lazy potential. We present our analysis as a proof system for predicting heap allocations of a minimal functional language (including higher-order functions and recursive data types) and define a formal cost model based on Launchbury's natural semantics for lazy evaluation. We prove the soundness of our analysis with respect to the cost model. Our approach is illustrated by a number of representative and non-trivial examples that have been analysed using a prototype implementation of our analysis. © 2012 ACM.",amortisation | lazy evaluation | resource analysis | type systems,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2012-10-22,Conference Paper,"Simões, Hugo;Vasconcelos, Pedro;Florido, Mário;Jost, Steffen;Hammond, Kevin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2364527.2364566,,,,,,,,Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84870700608,10.1109/ICGSE.2012.30,"An exploratory study of software evolution and quality: Before, during and after a transfer","In the light of globalization it is not uncommon that different teams from different locations get involved in the development of a software product during its evolution. However, empirical evidence that demonstrates the effect of changing team members on software quality is scarce. In this paper, we investigate quality of a software product, across subsequent software releases, that was first developed in one location of Ericsson, a large multinational corporation, then jointly with an offshore location of the same company, and finally transferred offshore. To get a better understanding multiple data sources are used in the analysis: qualitative data, consisting of interviews and documentation, and quantitative data, consisting of release history and defect statistics. Our findings confirm an initial decline in efficiency and quality after a transfer, and highlight the areas of concern for companies that are considering transferring their product development from experienced teams to those having limited or no previous engagement with the product. © 2012 IEEE.",Empirical case study | Global Software Development | Software evolution | Software quality | Software transfer,"Proceedings - 2012 IEEE 7th International Conference on Global Software Engineering, ICGSE 2012",2012-12-12,Conference Paper,"Jabangwe, Ronald;Šmite, Darja",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84870723249,10.1109/ICGSE.2012.29,Systematic literature reviews in distributed software development: A tertiary study,"Distributed Software Development (DSD) emerged from the need to achieve geographically distant customers and currently, allows organizations have global customers and other benefits. This scenario has given rise to new Software Engineering challenges resulting from DSD particularities. Several Systematic Reviews were conducted to address these new challenges. The objective of this paper is to categorize systematic reviews conducted in DSD context. We used the systematic review method to identify SLRs (Systematic Literature Reviews) that address DSD aspects. This study is categorized as a tertiary review. Of fourteen SLRs, seven address aspects of managing distributed development. Four SLRs addressed topics of engineering process. The three remaining are related to Requirements, Design and Software Engineering Education in DSD. The topic areas covered by SLRs are limited, where the majority are focused on summarize the current knowledge concerning a research question. Despite the number of SLRs, the amount of empirical studies is relatively small. © 2012 IEEE.",Distributed Software Development | Empirical Evidence | Global Software Engineering | Systematic Review | Tertiary Study,"Proceedings - 2012 IEEE 7th International Conference on Global Software Engineering, ICGSE 2012",2012-12-12,Conference Paper,"Marques, Anna Beatriz;Rodrigues, Rosiane;Conte, Tayana",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2063239.2063244,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84859418056,10.1145/2089116.2089122,Symbolic message sequence charts,"Message sequence charts (MSCs) are a widely used visual formalism for scenario-based specifications of distributed reactive systems. In its conventional usage, an MSC captures an interaction snippet between concrete objects in the system. This leads to voluminous specifications when the system contains several objects that are behaviorally similar. MSCs also play an important role in the model-based testing of reactive systems, where they may be used for specifying (partial) system behaviors, describing test generation criteria, or representing test cases. However, since the number of processes in a MSC specification are fixed, model-based testing of systems consisting of process classes may involve a significant amount of rework: for example, reconstructing system models, or regenerating test cases for systems differing only in the number of processes of various types. In this article we propose a scenario-based notation, called symbolic message sequence charts (SMSCs), for modeling, simulation, and testing of process classes. SMSCs are a lightweight syntactic and semantic extension of MSCs where, unlike MSCs, a SMSC lifeline can denote some/all objects from a collection. Our extensions give us substantially more modeling power. Moreover, we present an abstract execution semantics for (structured collections of) SMSCs. This allows us to validate MSC-based system models capturing interactions between large, or even unbounded, number of objects. Finally, we describe a SMSC-based testing methodology for process classes, which allows generation of test cases for new object configurations with minimal rework. Since our SMSC extensions are only concerned with MSC lifelines, we believe that they can be integrated into existing standards such as UML 2.0. We illustrate our SMSC-based framework for modeling, simulation, and testing of process classes using a weather-update controller case-study from NASA. © 2012 ACM.",Message sequence charts | Symbolic execution | Test generation | Unified modeling language (UML),ACM Transactions on Software Engineering and Methodology,2012-03-01,Article,"Roychoudhury, Abhik;Goel, Ankit;Sengupta, Bikram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863934994,10.1145/2211616.2211619,A logical verification methodology for service-oriented computing,"We introduce a logical verification methodology for checking behavioral properties of service-oriented computing systems. Service properties are described by means of SocL, a branching-Time temporal logic that we have specifically designed for expressing in an effective way distinctive aspects of services, such as, acceptance of a request, provision of a response, correlation among service requests and responses, etc. Our approach allows service properties to be expressed in such a way that they can be independent of service domains and specifications. We show an instantiation of our general methodology that uses the formal language COWS to conveniently specify services and the expressly developed software tool CMC to assist the user in the task of verifying SocL formulas over service specifications. We demonstrate the feasibility and effectiveness of our methodology by means of the specification and analysis of a case study in the automotive domain. © 2012 ACM.",,ACM Transactions on Software Engineering and Methodology,2012-06-01,Article,"Fantechi, Alessandro;Gnesi, Stefania;Lapadula, Alessandro;Mazzanti, Franco;Pugliese, Rosario;Tiezzi, Francesco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873684136,10.1145/2377656.2377659,Validation of requirements for hybrid systems: A formal approach,"Flaws in requirements may have unacceptable consequences in the development of safety-critical applications. Formal approaches may help with a deep analysis that takes care of the precise semantics of the requirements. However, the proposed solutions often disregard the problem of integrating the formalization with the analysis, and the underlying logical framework lacks either expressive power, or automation. We propose a new, comprehensive approach for the validation of functional requirements of hybrid systems, where discrete components and continuous components are tightly intertwined. the proposed solution allows to tackle problems of conversion from informal to formal, traceability, automation, user acceptance, and scalability. We build on a new language, OTHELLO which is expressive enough to represent various domains of interest, yet allowing efficient procedures for checking the satisfiability. Around this, we propose a structured methodology where: informal requirements are fragmented and categorized according to their role; each fragment is formalized based on its category; specialized formal analysis techniques, optimized for requirements analysis, are finally applied. the approach was the basis of an industrial project aiming at the validation of the European Train Control System (ETCS) requirements specification. During the project a realistic subset of the ETCS specification was formalized and analyzed. the approach was positively assessed by domain experts. © 2012 ACM.",European train control system | Formal languages | Methodology | Requirements validation | Safety-critical applications,ACM Transactions on Software Engineering and Methodology,2012-11-01,Article,"Cimatti, Alessandro;Roveri, Marco;Susi, Angelo;Tonetta, Stefano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873637463,10.1145/2377656.2377661,Validating software metrics: A spectrum of philosophies,"Context. Researchers proposing a new metric have the burden of proof to demonstrate to the research community that the metric is acceptable in its intended use. This burden of proof is provided through the multi-faceted, scientific, and objective process of software metrics validation. Over the last 40 years, however, researchers have debated what constitutes a ""valid"" metric. Aim. the debate over what constitutes a valid metric centers on software metrics validation criteria. the objective of this article is to guide researchers in making sound contributions to the field of software engineering metrics by providing a practical summary of the metrics validation criteria found in the academic literature. Method.We conducted a systematic literature review that began with 2,288 papers and ultimately focused on 20 papers. After extracting 47 unique validation criteria from these 20 papers, we performed a comparative analysis to explore the relationships amongst the criteria. Results. Our 47 validation criteria represent a diverse view of what constitutes a valid metric.We present an analysis of the criteria's categorization, conflicts, common themes, and philosophical motivations behind the validation criteria. Conclusions. Although the 47 validation criteria are not conflict-free, the diversity of motivations and philosophies behind the validation criteria indicates that metrics validation is complex. Researchers proposing new metrics should consider the applicability of the validation criteria in terms of our categorization and analysis. Rather than arbitrarily choosing validation criteria for each metric, researchers should choose criteria that can confirm that the metric is appropriate for its intended use. We conclude that metrics validation criteria provide answers to questions that researchers have about the merits and limitations of a metric. © 2012 ACM.",Software metrics | Systematic literature review | Validation criterion,ACM Transactions on Software Engineering and Methodology,2012-11-01,Article,"Meneely, Andrew;Smith, Ben;Williams, Laurie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873660420,10.1145/2377656.2377662,"HAMPI: A solver for word equations over strings, regular expressions, and context-free grammars","Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint-generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable software reliability tools. the increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis of string-manipulating programs, so researchers end up implementing their own ad-hoc solvers. To fulfill this need, we designed and implementedHAMPI, a solver for string constraints over bounded string variables. Users of HAMPI specify constraints using regular expressions, context-free grammars, equality between string terms, and typical string operations such as concatenation and substring extraction. HAMPI then finds a string that satisfies all the constraints or reports that the constraints are unsatisfiable. We demonstrate HAMPI's expressiveness and efficiency by applying it to program analysis and automated testing. We used HAMPI in static and dynamic analyses for finding SQL injection vulnerabilities in Web applications with hundreds of thousands of lines of code. We also used HAMPI in the context of automated bug finding in C programs using dynamic systematic testing (also known as concolic testing). We then compared HAMPI with another string solver, CFGAnalyzer, and show that HAMPI is several times faster. HAMPI's source code, documentation, and experimental data are available at http://people.csail.mit. edu/akiezun/hampi1. © 2012 ACM.",Concolic testing | Context-free languages | Program analysis | Regular languages | String constraints | Word equations,ACM Transactions on Software Engineering and Methodology,2012-11-01,Article,"Kiezun, Adam;Ganesh, Vijay;Artzi, Shay;Guo, Philip J.;Hooimeijer, Pieter;Ernst, Michael D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84856746279,10.1109/TSE.2011.104,GenProg: A generic method for automatic software repair,"This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that repair the defect, are not fragile input memorizations, and do not lead to serious degradation in functionality. © 2006 IEEE.",Automatic programming | corrections | testing and debugging,IEEE Transactions on Software Engineering,2012-02-13,Article,"Le Goues, Claire;Nguyen, Thanh Vu;Forrest, Stephanie;Weimer, Westley",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84856523145,10.1109/TSE.2011.6,Aspectizing Java access control,"It is inevitable that some concerns crosscut a sizeable application, resulting in code scattering and tangling. This issue is particularly severe for security-related concerns: It is difficult to be confident about the security of an application when the implementation of its security-related concerns is scattered all over the code and tangled with other concerns, making global reasoning about security precarious. In this study, we consider the case of access control in Java, which turns out to be a crosscutting concern with a nonmodular implementation based on runtime stack inspection. We describe the process of modularizing access control in Java by means of Aspect-Oriented Programming (AOP). We first show a solution based on AspectJ, the most popular aspect-oriented extension to Java, that must rely on a separate automata infrastructure. We then put forward a novel solution via dynamic deployment of aspects and scoping strategies. Both solutions, apart from providing a modular specification of access control, make it possible to easily express other useful policies such as the Chinese wall policy. However, relying on expressive scope control results in a compact implementation, which, at the same time, permits the straightforward expression of even more interesting policies. These new modular implementations allowed by AOP alleviate maintenance and evolution issues produced by the crosscutting nature of access control. © 2006 IEEE.",access control | aspect-oriented programming | Programming languages | security,IEEE Transactions on Software Engineering,2012-02-07,Article,"Toledo, Rodolfo;Núñez, Angel;Tanter, Éric;Noyé, Jacques",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84856529647,10.1109/TSE.2010.98,Automated abstractions for contract validation,"Pre/postcondition-based specifications are commonplace in a variety of software engineering activities that range from requirements through to design and implementation. The fragmented nature of these specifications can hinder validation as it is difficult to understand if the specifications for the various operations fit together well. In this paper, we propose a novel technique for automatically constructing abstractions in the form of behavior models from pre/postcondition-based specifications. Abstraction techniques have been used successfully for addressing the complexity of formal artifacts in software engineering; however, the focus has been, up to now, on abstractions for verification. Our aim is abstraction for validation and hence, different and novel trade-offs between precision and tractability are required. More specifically, in this paper, we define and study enabledness-preserving abstractions, that is, models in which concrete states are grouped according to the set of operations that they enable. The abstraction results in a finite model that is intuitive to validate and which facilitates tracing back to the specification for debugging. The paper also reports on the application of the approach to two industrial strength protocol specifications in which concerns were identified. © 2006 IEEE.",automated abstraction | Requirements/specifications | validation,IEEE Transactions on Software Engineering,2012-01-09,Article,"De Caso, Guido;Braberman, Víctor;Garbervetsky, Diego;Uchitel, Sebastián",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84856524160,10.1109/TSE.2011.5,Measuring code quality to improve specification mining,"Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few false positives in practice (63 percent when balancing precision and recall, 3 percent when focused on precision), while still finding useful specifications (e.g., those that find many bugs) on over 1.5 million lines of code. © 2006 IEEE.",code metrics | machine learning | program understanding | software engineering | Specification mining,IEEE Transactions on Software Engineering,2012-01-01,Article,"Le Goues, Claire;Weimer, Westley",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84861840303,10.1109/TSE.2011.38,Specifying dynamic analyses by extending language semantics,"Dynamic analysis is increasingly attracting attention for debugging, profiling, and program comprehension. Ten to twenty years ago, many dynamic analyses investigated only simple method execution traces. Today, in contrast, many sophisticated dynamic analyses exist, for instance, for detecting memory leaks, analyzing ownership properties, measuring garbage collector performance, or supporting debugging tasks. These analyses depend on complex program instrumentations and analysis models, making it challenging to understand, compare, and reproduce the proposed approaches. While formal specifications and proofs are common in the field of static analysis, most dynamic analyses are specified using informal, textual descriptions. In this paper, we propose a formal framework using operational semantics that allows researchers to precisely specify their dynamic analysis. Our goal is to provide an accessible and reusable basis on which researchers who may not be familiar with rigorous specifications of dynamic analyses can build. By extending the provided semantics, one can concisely specify how runtime events are captured and how this data is transformed to populate the analysis model. Furthermore, our approach provides the foundations to reason about properties of a dynamic analysis. © 2012 IEEE.",debugging | formal definitions and theory | tracing | ynamic analysis,IEEE Transactions on Software Engineering,2012-06-11,Article,"Lienhard, Adrian;Gîrba, Tudor;Nierstrasz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864610088,10.1109/TSE.2011.62,Adaptation of service protocols using process algebra and on-the-fly reduction techniques,"Reuse and composition are increasingly advocated and put into practice in modern software engineering. However, the software entities that are to be reused to build an application, e.g., services, have seldom been developed to integrate and to cope with the application requirements. As a consequence, they present mismatch, which directly hampers their reusability and the possibility of composing them. Software Adaptation has become a hot topic as a nonintrusive solution to work mismatch out using corrective pieces named adaptors. However, adaptation is a complex issue, especially when behavioral interfaces, or conversations, are taken into account. In this paper, we present state-of-the-art techniques to generate adaptors given the description of reused entities' conversations and an abstract specification of the way mismatch can be solved. We use a process algebra to encode the adaptation problem, and propose on-the-fly exploration and reduction techniques to compute adaptor protocols. Our approach follows the model-driven engineering paradigm, applied to service-oriented computing as a representative field of composition-based software engineering. We take service description languages as inputs of the adaptation process and we implement adaptors as centralized service compositions, i.e., orchestrations. Our approach is completely tool supported. © 2012 IEEE.",adaptation contracts | interfaces | mismatch | on - the-fly generation | process algebra | protocols | Service composition | software adaptation | tools | verification,IEEE Transactions on Software Engineering,2012-07-17,Article,"Mateescu, Radu;Poizat, Pascal;Salaün, Gwen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863109141,10.1109/TSE.2011.84,Exemplar: A source code search engine for finding highly relevant applications,"A fundamental problem of finding software applications that are highly relevant to development tasks is the mismatch between the high-level intent reflected in the descriptions of these tasks and low-level implementation details of applications. To reduce this mismatch we created an approach called EXEcutable exaMPLes ARchive (Exemplar) for finding highly relevant software projects from large archives of applications. After a programmer enters a natural-language query that contains high-level concepts (e.g., MIME, datasets), Exemplar retrieves applications that implement these concepts. Exemplar ranks applications in three ways. First, we consider the descriptions of applications. Second, we examine the Application Programming Interface (API) calls used by applications. Third, we analyze the dataflow among those API calls. We performed two case studies (with professional and student developers) to evaluate how these three rankings contribute to the quality of the search results from Exemplar. The results of our studies show that the combined ranking of application descriptions and API documents yields the most-relevant search results. We released Exemplar and our case study data to the public. © 2012 IEEE.",concept location | information retrieval | mining software repositories | open source software | software reuse | Source code search engines,IEEE Transactions on Software Engineering,2012-10-16,Article,"McMillan, Collin;Grechanik, Mark;Poshyvanyk, Denys;Fu, Chen;Xie, Qing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867303904,10.1109/TSE.2011.102,Formal specification-based inspection for verification of programs,"Software inspection is a static analysis technique that is widely used for defect detection, but which suffers from a lack of rigor. In this paper, we address this problem by taking advantage of formal specification and analysis to support a systematic and rigorous inspection method. The aim of the method is to use inspection to determine whether every functional scenario defined in the specification is implemented correctly by a set of program paths and whether every program path of the program contributes to the implementation of some functional scenario in the specification. The method is comprised of five steps: deriving functional scenarios from the specification, deriving paths from the program, linking scenarios to paths, analyzing paths against the corresponding scenarios, and producing an inspection report, and allows for a systematic and automatic generation of a checklist for inspection. We present an example to show how the method can be used, and describe an experiment to evaluate its performance by comparing it to perspective-based reading (PBR). The result shows that our method may be more effective in detecting function-related defects than PBR but slightly less effective in detecting implementation-related defects. We also describe a prototype tool to demonstrate the supportability of the method, and draw some conclusions about our work. © 2012 IEEE.",formal specification | program verification | software inspection | Specification-based program inspection,IEEE Transactions on Software Engineering,2012-10-16,Article,"Liu, Shaoying;Chen, Yuting;Nagoya, Fumiko;McDermid, John A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84870554847,10.1109/TSE.2011.112,Matching and merging of variant feature specifications,"Model Management addresses the problem of managing an evolving collection of models by capturing the relationships between models and providing well-defined operators to manipulate them. In this paper, we describe two such operators for manipulating feature specifications described using hierarchical state machine models: Match, for finding correspondences between models, and Merge, for combining models with respect to known or hypothesized correspondences between them. Our Match operator is heuristic, making use of both static and behavioral properties of the models to improve the accuracy of matching. Our Merge operator preserves the hierarchical structure of the input models, and handles differences in behavior through parameterization. This enables us to automatically construct merges that preserve the semantics of hierarchical state machines. We report on tool support for our Match and Merge operators, and illustrate and evaluate our work by applying these operators to a set of telecommunication features built by AT&T. © 2012 IEEE.",behavior preservation | hierarchical state machines | match | merge | Model management | parameterization | statecharts | variability modeling,IEEE Transactions on Software Engineering,2012-12-10,Article,"Nejati, Shiva;Sabetzadeh, Mehrdad;Chechik, Marsha;Easterbrook, Steve;Zave, Pamela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84870527314,10.1109/TSE.2011.110,Programmer-friendly refactoring errors,"Refactoring tools, common to many integrated development environments, can help programmers to restructure their code. These tools sometimes refuse to restructure the programmer's code, instead giving the programmer a textual error message that she must decode if she wishes to understand the reason for the tool's refusal and what corrective action to take. This paper describes a graphical alternative to textual error messages called Refactoring Annotations. It reports on two experiments, one using an integrated development environment and the other using paper mockups, that show that programmers can use Refactoring Annotations to quickly and accurately understand the cause of refactoring errors. © 2012 IEEE.",programmers | Refactoring | refactoring errors | tools | usability,IEEE Transactions on Software Engineering,2012-12-10,Article,"Murphy-Hill, Emerson;Black, Andrew P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84870511476,10.1109/TSE.2011.77,Two studies of framework-usage templates extracted from dynamic traces,"Object-oriented frameworks are widely used to develop new applications. They provide reusable concepts that are instantiated in application code through potentially complex implementation steps such as subclassing, implementing interfaces, and calling framework operations. Unfortunately, many modern frameworks are difficult to use because of their large and complex APIs and frequently incomplete user documentation. To cope with these problems, developers often use existing framework applications as a guide. However, locating concept implementations in those sample applications is typically challenging due to code tangling and scattering. To address this challenge, we introduce the notion of concept-implementation templates, which summarize the necessary concept-implementation steps and identify them in the sample application code, and a technique, named FUDA, to automatically extract such templates from dynamic traces of sample applications. This paper further presents the results of two experiments conducted to evaluate the quality and usefulness of FUDA templates. The experimental evaluation of FUDA with 14 concepts in five widely used frameworks suggests that the technique is effective in producing templates with relatively few false positives and false negatives for realistic concepts by using two sample applications. Moreover, we observed in a user study with 28 programmers that the use of templates reduced the concept-implementation time compared to when documentation was used. © 2012 IEEE.",application programming interface (API) | concept location | concept-implementation templates | dynamic analysis | feature identification | framework comprehension | framework documentation | Object-oriented application frameworks,IEEE Transactions on Software Engineering,2012-12-10,Article,"Heydarnoori, Abbas;Czarnecki, Krzysztof;Binder, Walter;Bartolomei, Thiago Tonelli",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869001448,10.1145/2379057.2379077,"QualiCES, a method for verifying the consistency among documents of the requirement engineering phase","During the initial software specification phase, requirement document, use cases description and interface prototypes can be generated as a way to aid in the construction of system data. The consistency among these documents is a quality attribute which must be emphasized at this phase of the software development process. The QualiCES method is presented herein; it allows assessing the consistency among these software documents, and is supported by a checklist and by a consistency metrics developed to this end. As benefits, there is defect detection and a software quality warranty from the beginning of software development. The method was executed in a case study. Based on the results, the viability for applying the method can be verified, as well as the proposal innovation degree. Copyright © 2012 ACM.",Consistency | Requirement engineering | Software inspection | Software quality,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Muriana, Luã Marcelo;MacIel, Cristiano;Mendes, Fabiana Freitas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869037200,10.1145/2379057.2379088,Instant Annotation: Early design experiences in supporting cross-cultural group chat,"Cross-cultural group chat is an important option for supporting communication in both industry and education settings. However, studies of such interactions have reported persistent communication problems that appear to be due to mismatches in non-native and native speakers' language proficiency. With this problem in mind, we have been exploring a conceptual design called Instant Annotation. Our design concept supports a kind of threading in chat using annotation, thus offering para-communication support in cross-cultural group chat. As part of this design investigation, we studied native and non-native speakers in a group chat activity, shared the new design concept, and interviewed users to gather their feedback about the Instant Annotation concept. The results pointed to three different design use cases and led us to envision four general design features that we will explore in our ongoing work. We discuss the cross-cultural communication problem, findings from the interview study, the current design and future directions. Copyright © 2012 ACM.",Annotation | Cross-cultural | CSCW | Group discussion | Native speaker | Non-native speaker | Online chat | Scenario-based design | Tagging,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Li, Na;Rosson, Mary Beth",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869030345,10.1145/2379057.2379101,"Investigating usability and ""meaningful use"" of Electronic Medical Records","In this paper, I summarize research regarding known issues with Electronic Medical Record (EMR) software design and subsequent implementation. I consider the Centers for Medicare and Medicaid Services' cash incentive for EMR adoption, and the ""meaningful use"" criteria that mediate that incentive (Table 1). Based on this research and my own small-scale study of real time EMR use, I outline the ways that the same problems had by EMR usability researchers are also had by actual EMR users, themselves. Specifically, questions about how to account for both embodied and cognitive effects, how to discern noise from useful information, and how to make useful what is available are all concerns shared by both care providers using EMRs and those who study EMR usability. As a result, I propose that communication design researchers design usability studies that use Mol et al's [9] construct of ""care"" (as a practice) as the gold standard for ""meaningful use."" That is, meaningful use of EMR software ought to be articulated less in terms of task-oriented record-keeping practices and the time it takes to accomplish them, but in terms of Mol et al's three components of good care: embodied practices, attuned attentiveness, and adaptive tinkering. Copyright © 2012 ACM.",Care | Design | Electronic Medical Records | Meaningful use | Usability,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Teston, Christa B.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869034789,10.1145/2379057.2379111,A process documentation model for DCMI,"The Dublin Core Metadata Initiative (DCMI) often must develop instructional materials associated with its mission to provide essential metadata vocabularies. DCMI undertook an effort to create a consistent framework for documentation that would streamline creation of instructional resources specific to internal tools, processes, and activities. Extensive documentation also describes applications of the Dublin Core metadata schema, but those materials require their own structure and content priorities. DCMI's preferred meeting management tool, Open Conference System (OCS), was used as an exemplar to develop a comprehensive and flexible structure for documentation of internal tools, procedures, and activities. The model rests on a solid foundation of theory and experience accumulated by researchers. The resulting integrative review informed development of a theoretically justified and practically useful template for internal DCMI documentation. Copyright © 2012 ACM.",DCMI | Dublin core | Minimalist documentation | Process documentation | Streamlined step model,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Talley, David W.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869074991,10.1145/2379057.2379112,Development and application of a heuristic to assess trends in API documentation,"Computer technology has made amazing advances in the past few decades; however, the software documentation of today still looks strikingly similar to the software documentation used 30 years ago. If this continues into the 21st century, more and more soft-ware developers could be using 20 th-century-style documentation to solve 21 st-century problems with 21 st-century technologies. Is 20 th-century- style documentation up to the challenge? How can that be measured? This paper seeks to answer those questions by developing a heuristic to identify whether the documentation set for an application programming interface (API) contains the key elements of API reference documentation that help software developers learn an API. The resulting heuristic was tested on a collection of software documentation that was chosen to provide a diverse set of examples with which to validate the heuristic. In the course of testing the heuristic, interesting patterns in the API documentation were observed. For example, twenty-five percent of the documentation sets studied did not have any overview information, which, according to studies, is one of the most basic elements an API documentation set needs to help software developers learn to use the API. The heuristic produced by this research can be used to evaluate large sets of API documentation, track trends in API documentation, and facilitate additional research. Copyright © 2012 ACM.",API | API reference documentation | Application programming interface | Software documentation | Software libraries,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Watson, Robert",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84869074706,10.1145/2379057.2379115,Designing for selective reading with QuikScan views,"Many people-especially knowledge workers-experience information overload, lack sufficient time to read, and therefore choose to read selectively within texts. QuikScan Views is a new Web-based reading environment that provides extensive support for selective reading. It is an enhancement of QuikScan, an empirically validated document format that employs a multiple summary approach to facilitate selective reading, enable quick access to specific ideas in the body of the document, and improve text recall. QuikScan Views provides a hyperlinked table of contents for global navigation, displays QuikScan summaries in a scrolling window (as well as within the body of the document), and adds an extra level of summarization by means of a hyperlinked structured abstract. A QuikScan Views document gives the reader choices of pathways through the document corresponding to the time the reader wishes to invest and the reader's desire to increase their recall of the document. Copyright © 2012 ACM.",Literacy | Reading | Selective reading | Summarization,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Farkas, David K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869062849,10.1145/2379057.2379123,Structured authoring meets technical comics in TechCommix,"TechCommix is an XML grammar and GUI that allows technical communicators to build comics based on the principles of structured authoring. TechCommix XML uses elements of two markup languages-ComicsML and DITA-the combination of which offers a means of tagging elements connected to a comics narrative (such as speech, action, narration) and to structured technical documentation (such as context, step, example). The resulting language allows a technical writer to differentiate between instructional and entertainment content, facilitating content analysis and reuse. Additionally, the TechCommix GUI provides assisted means of building web comics from DITA input. In this online environment, a technical writer can transform an XML file into an HTML deliverable with multiple presentation options-extending usability and accessibility beyond the current standard of image-based web comics. Future work will examine the efficacy of these comics in communicating procedural information. Copyright © 2012 ACM.",Comics | DITA | Structured authoring | XML,SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication,2012-11-19,Conference Paper,"Evia, Carlos;Scerbo, Siroberto;Stewart, Michael;Perez-Quiñones, Manuel;Lockridge, Tim",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84857620649,10.1145/2071389.2071392,Flow logic for process calculi,"Flow Logic is an approach to statically determining the behavior of programs and processes. It borrows methods and techniques from Abstract Interpretation, Data Flow Analysis and Constraint Based Analysis while presenting the analysis in a style more reminiscent of Type Systems. Traditionally developed for programming languages, this article provides a tutorial development of the approach of Flow Logic for process calculi based on a decade of research. We first develop a simple analysis for the π-calculus; this consists of the specification, semantic soundness (in the form of subject reduction and adequacy results), and a Moore Family result showing that a least solution always exists, as well as providing insights on how to implement the analysis.We then show how to strengthen the analysis technology by introducing reachability components, interaction points, and localized environments, and finally, we extend it to a relational analysis. A Flow Logic is a program logic-in the same sense that a Hoare's logic is. We conclude with an executive summary presenting the highlights of the approach from this perspective including a discussion of theoretical properties as well as implementation considerations. The electronic supplements present an application of the analysis techniques to a version of the π-calculus incorporating distribution and code mobility; also the proofs of the main results can be found in the electronic supplements. © 2012 ACM.",Adequacy | Flow logic | Moore family | Process calculi | Static analysis | Subject reduction,ACM Computing Surveys,2012-01-01,Article,"Nielson, Hanne Riis;Nielson, Flemming;Pilegaard, Henrik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-79955134463,10.1145/2089125.2089127,Free/Libre Open-Source Software development: What we know and what we do not know,"We review the empirical research on Free/Libre and Open-Source Software (FLOSS) development and assess the state of the literature.We develop a framework for organizing the literature based on the input-mediatoroutput- input (IMOI) model from the small groups literature. We present a quantitative summary of articles selected for the review and then discuss findings of this literature categorized into issues pertaining to inputs (e.g., member characteristics, technology use, and project characteristics), processes (software development practices, social processes, and firm involvement practices), emergent states (e.g., social states and taskrelated states), and outputs (e.g. team performance, FLOSS implementation, and project evolution). Based on this review, we suggest topics for future research, as well as identify methodological and theoretical issues for future inquiry in this area, including issues relating to sampling and the need for more longitudinal studies. © 2012 ACM.",Computer-mediated communication | Development | Distributed work | Free/Libre open-source software,ACM Computing Surveys,2012-02-01,Review,"Crowston, Kevin;Wei, Kangning;Howison, James;Wiggins, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863832368,10.1145/2187671.2187675,Recent thermal management techniques for microprocessors,"Microprocessor design has recently encountered many constraints such as power, energy, reliability, and temperature. Among these challenging issues, temperature-related issues have become especially important within the past several years. We summarize recent thermal management techniques for microprocessors, focusing on those that affect or rely on themicroarchitecture. We categorize thermal management techniques into sixmain categories: temperaturemonitoring, microarchitectural techniques, floorplanning, OS/compiler techniques, liquid cooling techniques, and thermal reliability/security. Temperature monitoring, a requirement for Dynamic Thermal Management (DTM), includes temperature estimation and sensor placement techniques for accurate temperature measurement or estimation. Microarchitectural techniques include both static and dynamic thermal management techniques that control hardware structures. Floorplanning covers a range of thermal-aware floorplanning techniques for 2D and 3D microprocessors. OS/compiler techniques include thermal-aware task scheduling and instruction scheduling techniques. Liquid cooling techniques are higher-capacity alternatives to conventional air cooling techniques. Thermal reliability/security issues cover temperature-dependent reliability modeling, Dynamic Reliability Management (DRM), and malicious codes that specifically cause overheating. Temperature-related issues will only become more challenging as process technology continues to evolve and transistor densities scale up faster than power per transistor scales down. The overall objective of this survey is to give microprocessor designers a broad perspective on various aspects of designing thermal-aware microprocessors and to guide future thermal management studies. © 2012 ACM.",Microprocessor | Performance and reliability | Thermal management,ACM Computing Surveys,2012-06-01,Article,"Kong, Joonho;Chung, Sung Woo;Skadron, Kevin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863823506,10.1145/2187671.2187678,Behavioral interface specification languages,"Behavioral interface specification languages provide formal code-level annotations, such as preconditions, postconditions, invariants, and assertions that allow programmers to express the intended behavior of program modules. Such specifications are useful for precisely documenting program behavior, for guiding implementation, and for facilitating agreement between teams of programmers in modular development of software. When used in conjunction with automated analysis and program verification tools, such specifications can support detection of common code vulnerabilities, capture of light-weight application-specific semantic properties, generation of test cases and test oracles, and full formal program verification. This article surveys behavioral interface specification languages with a focus toward automatic program verification and with a view towards aiding the Verified Software Initiative-a fifteen-year, cooperative, international project directed at the scientific challenges of large-scale software verification. © 2012 ACM.",Abstraction | Assertion | Behavioral subtyping | Frame conditions | Interface specification language | Invariant | JML | Postcondition | Precondition | Separation logic | SPARK | Spec#,ACM Computing Surveys,2012-06-01,Article,"Hatcliff, John;Leavens, Gary T.;Leino, K. Rustan M.;Müller, Peter;Parkinson, Matthew",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84866499982,10.1145/2333112.2333113,A taxonomy and survey of SCTP research,"The Stream Control Transmission Protocol (SCTP) is a relatively recent general-purpose transport layer protocol for IP networks that has been introduced as a complement to the well-established TCP and UDP transport protocols. Although initially conceived for the transport of PSTN signaling messages over IP networks, the introduction of key features in SCTP, such as multi-homing and multi-streaming, has spurred considerable research interest surrounding SCTP and its applicability to different networking scenarios. This article aims to provide a detailed survey of one of these new features-multi-homing-which, as it is shown, is the subject of evaluation in more than half of all published SCTP-related articles. To this end, the article first summarizes and organizes SCTP-related research conducted so far by developing a four-dimensional taxonomy reflecting the (1) protocol feature examined, (2) application area, (3) network environment, and (4) study approach. Over 430 SCTP-related publications have been analyzed and classified according to the proposed taxonomy. As a result, a clear perspective on this research area in the decade since the first protocol standardization in 2000 is given, covering both current and future research trends. On continuation, a detailed survey of the SCTP multi-homing feature is provided, examining possible applications of multi-homing, such as robustness, handover support, and loadsharing. © 2012 ACM.",Handoff management | Loadsharing | Multi-homing | Robustness | SCTP | Taxonomy,ACM Computing Surveys,2012-08-01,Review,"Budzisz, Łukasz;Garcia, Johan;Brunstrom, Anna;Ferrús, Ramon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2333112.2333120,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857356474,10.1007/s10664-011-9171-y,An exploratory study of the impact of antipatterns on class change- and fault-proneness,"Antipatterns are poor design choices that are conjectured to make object-oriented systems harder to maintain. We investigate the impact of antipatterns on classes in object-oriented systems by studying the relation between the presence of antipatterns and the change- and fault-proneness of the classes. We detect 13 antipatterns in 54 releases of ArgoUML, Eclipse, Mylyn, and Rhino, and analyse (1) to what extent classes participating in antipatterns have higher odds to change or to be subject to fault-fixing than other classes, (2) to what extent these odds (if higher) are due to the sizes of the classes or to the presence of antipatterns, and (3) what kinds of changes affect classes participating in antipatterns. We show that, in almost all releases of the four systems, classes participating in antipatterns are more change-and fault-prone than others. We also show that size alone cannot explain the higher odds of classes with antipatterns to underwent a (fault-fixing) change than other classes. Finally, we show that structural changes affect more classes with antipatterns than others. We provide qualitative explanations of the increase of change- and fault-proneness in classes participating in antipatterns using release notes and bug reports. The obtained results justify a posteriori previous work on the specification and detection of antipatterns and could help to better focus quality assurance and testing activities. © 2011 Springer Science+Business Media, LLC.",Antipatterns | Empirical software engineering | Mining software repositories,Empirical Software Engineering,2012-06-01,Article,"Khomh, Foutse;Penta, Massimiliano Di;Guéhéneuc, Yann Gaël;Antoniol, Giuliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863725502,10.1007/s10664-011-9169-5,The evolution of Java build systems,"Build systems are responsible for transforming static source code artifacts into executable software. While build systems play such a crucial role in software development and maintenance, they have been largely ignored by software evolution researchers. However, a firm understanding of build system aging processes is needed in order to allow project managers to allocate personnel and resources to build system maintenance tasks effectively, and reduce the build maintenance overhead on regular development activities. In this paper, we study the evolution of build systems based on two popular Java build languages (i.e.,ANT and Maven) from two perspectives: (1) a static perspective, where we examine the complexity of build system specifications using software metrics adopted from the source code domain; and (2) a dynamic perspective, where the complexity and coverage of representative build runs are measured. Case studies of the build systems of six open source build projects with a combined history of 172 releases show that build system and source code size are highly correlated, with source code restructurings often requiring build system restructurings. Furthermore, we find that Java build systems evolve dynamically in terms of duration and recursive depth of the directory hierarchy. © Springer Science+Business Media, LLC 2011.",ANT | Build systems | Maven | Software complexity | Software evolution,Empirical Software Engineering,2012-08-01,Conference Paper,"McIntosh, Shane;Adams, Bram;Hassan, Ahmed E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84868370117,10.1007/s10664-011-9186-4,What should developers be aware of? An empirical study on the directives of API documentation,"Application Programming Interfaces (API) are exposed to developers in order to reuse software libraries. API directives are natural-language statements in API documentation that make developers aware of constraints and guidelines related to the usage of an API. This paper presents the design and the results of an empirical study on the directives of API documentation of object-oriented libraries. Its main contribution is to propose and extensively discuss a taxonomy of 23 kinds of API directives. © Springer Science+Business Media, LLC 2011.",,Empirical Software Engineering,2012-12-01,Article,"Monperrus, Martin;Eichberg, Michael;Tekes, Elif;Mezini, Mira",Include,
10.1016/j.jss.2022.111515,2-s2.0-80755189031,10.1016/j.jss.2011.02.034,InRob: An approach for testing interoperability and robustness of real-time embedded software,"Advances in digital technologies have contributed for significant reduction in accidents caused by hardware failures. However, the growing complexity of functions performed by embedded software has increased the number of accidents caused by software faults in critical systems. Moreover, due to the highly competitive market, software intensive subsystems are usually developed by different suppliers. Often these subsystems are required to interact with each other in order to provide a collaborative service. Testing approaches for subsystems integration support verification of the quality of service, focusing on the subsystems interfaces. The increasing complexity and tight coupling of real-time subsystems make integration testing unmanageable. The ad-hoc approach for testing is becoming less effective and more expensive. This article presents an integration testing approach denominated InRob, designed to verify the interoperability and robustness related to timing constraints of real-time embedded software. InRob guides the construction of services, based on formal models, aiming at the specifications of interoperability and robustness of test cases related to delays and time-outs of the messages exchanged in the interfaces of interconnected subsystems. The proposed formalism supports automatic test cases generation by verifying the relevant properties in the service behavioral model. As timing constraints are critical properties of aerospace systems, the feasibility of InRob is showed in the integration testing process of a telescope onboard in a satellite. The process is instantiated with existing testing tools and the case study is the software embedded in the telescope. © 2011 Elsevier Inc.",Automatic test case generation | Integration testing | Interoperability test | Robustness test | Service model | Timing deviations,Journal of Systems and Software,2012-01-01,Article,"Mattiello-Francisco, Fátima;Martins, Eliane;Cavalli, Ana Rosa;Yano, Edgar Toshiro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80755172214,10.1016/j.jss.2011.07.028,SimFuzz: Test case similarity directed deep fuzzing,"Fuzzing is widely used to detect software vulnerabilities. Blackbox fuzzing does not require program source code. It mutates well-formed inputs to produce new ones. However, these new inputs usually do not exercise deep program semantics since the possibility that they can satisfy the conditions of a deep program state is low. As a result, blackbox fuzzing is often limited to identify vulnerabilities in input validation components of a program. Domain knowledge such as input specifications can be used to mitigate these limitations. However, it is often expensive to obtain such knowledge in practice. Whitebox fuzzing employs heavy analysis techniques, i.e.; dynamic symbolic execution, to systematically generate test inputs and explore as many paths as possible. It is powerful to explore new program branches so as to identify more vulnerabilities. However, it has fundamental challenges such as unsolvable constraints and is difficult to scale to large programs due to path explosion. This paper proposes a novel fuzzing approach that aims to produce test inputs to explore deep program semantics effectively and efficiently. The fuzzing process comprises two stages. At the first stage, a traditional blackbox fuzzing approach is applied for test data generation. This process is guided by a novel test case similarity metric. At the second stage, a subset of the test inputs generated at the first stage is selected based on the test case similarity metric. Then, combination testing is applied on these selected test inputs to further generate new inputs. As a result, less redundant test inputs, i.e.; inputs that just explore shallow program paths, are created at the first stage, and more distinct test inputs, i.e.; inputs that explore deep program paths, are produced at the second stage. A prototype tool SimFuzz is developed and evaluated on real programs, and the experimental results are promising. © 2011 Elsevier Inc.",Fuzzing | Software testing | Software vulnerability,Journal of Systems and Software,2012-01-01,Article,"Zhang, Dazhi;Liu, Donggang;Lei, Yu;Kung, David;Csallner, Christoph;Nystrom, Nathaniel;Wang, Wenhua",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80755172137,10.1016/j.jss.2011.07.037,Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study,"Ambient Assisted Living (AAL) investigates the development of systems involving the use of different types of sensors, which monitor activities and vital signs of lonely elderly people in order to detect emergency situations or deviations from desirable medical patterns. Instead of requiring the elderly person to manually push a button to request assistance, state-of-the-art AAL solutions automate the process by 'perceiving' lonely elderly people in their home environment through various sensors and performing appropriate actions under the control of the underlying software. Dependability in the AAL domain is a critical requirement, since poor system availability, reliability, safety, or integrity may cause inappropriate emergency assistance to potentially have fatal consequences. Nevertheless, contemporary research has not focused on assessing dependability in this domain. This work attempts to fill this gap presenting an approach which relies on modern quantitative and qualitative dependability analysis techniques based on software architecture. The analysis method presented in this paper consists of conversion patterns from Unified Modeling Language (UML) behavior models of the AAL software architecture into a formal executable specification, based on a probabilistic process algebra description language, which enables a sound quantitative and qualitative analysis. The UML models specify system component interactions and are annotated with component failure probabilities and system usage profile information. The resulting formal specification is executed on PRISM, a model checking tool adequate for the purpose of our analysis in order to identify a set of domain-specific dependability properties expressed declaratively in Probabilistic Computational Tree Logic (PCTL). The benefits of using these techniques are twofold. Firstly, they allow us to seamlessly integrate the analysis during subsequent software lifecycle stages in critical scenarios. Secondly, we identify the components which have the highest impact on software system dependability, and therefore, be able to address software architecture and individual software component problems prior to implementation and the occurrence of critical errors. © 2011 Elsevier Inc.",Ambient Assisted Living | Component-based system | Dependability analysis,Journal of Systems and Software,2012-01-01,Article,"Rodrigues, Genaína Nunes;Alves, Vander;Silveira, Renato;Laranjeira, Luiz A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84355162988,10.1016/j.jss.2011.08.031,Design patterns selection: An automatic two-phase method,"Over many years of research and practices in software development, hundreds of software design patterns have been invented and published. Now, a question which naturally arises is how software developers select the right design patterns from all relevant patterns to solve design problems in the software design phase. To address this issue, in this paper, we propose a two-phase method to select a right design pattern. The proposed method is based on a text classification approach that aims to show an appropriate way to suggest the right design pattern(s) to developers for solving each given design problem. There are two advantages of the proposed method in comparison to previous works. First, there is no need for semi-formal specifications of design patterns and second, the suitable design patterns are suggested with their degree of similarity to the design problem. To evaluate the proposed method, we apply it on real problems and several case studies. The experimental results show that the proposed method is promising and effective. © 2011 Elsevier Inc.",Automatic pattern selection | Machine learning | Software design pattern | Text classification,Journal of Systems and Software,2012-02-01,Conference Paper,"Hasheminejad, Seyed Mohammad Hossein;Jalili, Saeed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857365900,10.1016/j.jss.2011.05.047,Changing attitudes towards the generation of architectural models,"Architectural design is an important activity, but the understanding of how it is related to requirements modeling is rather limited. It is worth noting that goal orientation is an increasingly recognized paradigm for eliciting, modeling, specifying, and analyzing software requirements. However, it is not clear how goal models are related to architectural models. In this paper we present an approach based on model transformations to derive architectural structural specifications from system goals. The source and target languages are respectively the i* (iStar) modeling language and the Acme architectural description language. A real case study is used to show the feasibility of our approach. © 2011 Elsevier Inc. All rights reserved.",Architectural design | Model driven development | Model transformations | Requirements engineering,Journal of Systems and Software,2012-03-01,Article,"Castro, Jaelson;Lucena, Marcia;Silva, Carla;Alencar, Fernanda;Santos, Emanuel;Pimentel, João",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857371236,10.1016/j.jss.2011.05.022,Enabling correct design and formal analysis of Ambient Assisted Living systems,"Ambient Assisted Living (AAL) systems intend to provide services that enable people with specific needs to live an independent and safe life. Emergency treatment services are critical, time-constrained, and require compliance to numerous non-functional (or quality) requirements. In conventional approaches, often, non-functional requirements are kept outside the modeling scope and as such, their verification is also overlooked. For this reason, the specification and verification of Non-functional requirements (NFR) in this kind of services is a key issue. This paper presents a verification approach based on timed traces semantics and a methodology based on UML-RT models (MEDISTAM-RT) to check the fulfillment of non-functional requirements, such as timeliness and safety (deadlock freeness), and to assure the correct functioning of the AAL systems. We validate this approach by its application to an Emergency Assistance System for monitoring people suffering from cardiac alteration with syncope. © 2011 Elsevier Inc. All rights reserved.",AAL | Ambient Assisted Living | Formal methods | Time-constraints | Validation | Verification,Journal of Systems and Software,2012-03-01,Article,"Benghazi, Kawtar;Hurtado, María V.;Hornos, Miguel J.;Rodríguez, María L.;Rodríguez-Domínguez, Carlos;Pelegrina, Ana B.;Rodríguez-Fórtiz, María J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857369375,10.1016/j.jss.2011.05.036,Towards supporting the software architecture life cycle,"Software architecture is a central element during the whole software life cycle. Among other things, software architecture is used for communication and documentation, for design, for reasoning about important system properties, and as a blueprint for system implementation. This is expressed by the software architecture life cycle, which emphasizes architecture-related activities like architecture design, implementation, and analysis in the context of a software life cycle. While individual activities of the software architecture life cycle are supported very well, a seamless approach for supporting the whole life cycle is still missing. Such an approach requires the integration of disparate information, artifacts, and tools into one consistent information model and environment. In this article we present such an approach. It is based on a semi-formal architecture model, which is used in all activities of the architecture life cycle, and on a set of extensible and integrated tools supporting these activities. Such an integrated approach provides several benefits. Potentially redundant activities like the creation of multiple architecture descriptions are avoided, the captured information is always consistent and up-to-date, extensive tracing between different information is possible, and interleaving activities in incremental development and design are supported. © 2011 Elsevier Inc. All rights reserved.",Software architecture | Software architecture analysis | Software architecture design | Software architecture evaluation | Software architecture knowledge management | Software architecture life cycle | Software architecture model | Software architecture tools,Journal of Systems and Software,2012-03-01,Article,"Weinreich, Rainer;Buchgeher, Georg",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857366776,10.1016/j.jss.2011.09.046,Bridging the gap between requirements and design: An approach based on Problem Frames and SysML,"The relation between the requirements specification and the design has been widely investigated with the aim to bridge the gap between the two artifacts. The goal is to find effective mechanisms to generate the system design starting from the analysis and specification of the requirements. This paper contributes to this research stream with an approach to create early design models from requirement artifacts. The approach weaves together the analysis and design phases favoring a tight collaboration between analysts and designers. It is based on Problem Frames, decomposition and re-composition patterns and supported by the System Modeling Language. The proposed solution has the potentiality of easing the development, shortening the development cycle and reducing the associated cost. The proposed design generation guidelines have been implemented as ATLAS Transformation Language rules in a model-based transformation process. The entire approach is model driven, allowing for the generation of the design model through transformations applied to the requirements model. The design model is automatically generated through the application of the transformation rules described in the paper. The proposed rules are fairly general and can be applied to any analysis model built according to the proposed analysis guidelines. The transformation process can be easily re-implemented using any suitable modeling tool that includes the ATLAS Transformation Language interpretation engine. © 2011 Elsevier Inc. All rights reserved.",Architectural patterns | ATLAS Transformation Language | Blackboard | Decomposition criteria | Model based transformations | Problem Frames | System Modeling Language,Journal of Systems and Software,2012-03-01,Article,"Colombo, Pietro;Khendek, Ferhat;Lavazza, Luigi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857356116,10.1016/j.jss.2011.10.017,A documentation framework for architecture decisions,"In this paper, we introduce a documentation framework for architecture decisions. This framework consists of four viewpoint definitions using the conventions of ISO/IEC/IEEE 42010, the new international standard for the description of system and software architectures. The four viewpoints, a Decision Detail viewpoint, a Decision Relationship viewpoint, a Decision Chronology viewpoint, and a Decision Stakeholder Involvement viewpoint satisfy several stakeholder concerns related to architecture decision management. With the exception of the Decision Stakeholder Involvement viewpoint, the framework was evaluated in an industrial case study. The results are promising, as they show that decision views can be created with reasonable effort while satisfying many of the stakeholder concerns in decision documentation. © 2011 Elsevier Inc. All rights reserved.",Architectural viewpoints | Architecture decisions | Architecture framework | Architecture knowledge management | Case study | Software architecture,Journal of Systems and Software,2012-01-01,Article,"Van Heesch, U.;Avgeriou, P.;Hilliard, R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84857356760,10.1016/j.jss.2011.11.1021,Handling timing constraints violations in soft real-time applications as exceptions,"In this paper, an exception-based programming paradigm is envisioned to deal with timing constraints violations occurring in soft real-time and multimedia applications written in the C language. In order to prove viability of the approach, a mechanism allowing to use such paradigm has been designed and implemented as an open-source library of C macros making use of the standard POSIX API (a few Linux-specific optimizations are also briefly discussed). The proposed approach has been validated by modifying mplayer, one of the most widely used multimedia player for Linux, so as to use the introduced library. An extensive experimental evaluation has been made, both when running the player alone and when mixing it with a workload of other synthetic real-time applications. In the latter case, different scheduling policies have been used, including both standard priority-based ones as available on the mainline Linux, and an experimental deadline-based one available as a separate patch. The shown results demonstrate how the exception-based paradigm is effective in improving the audio/video delay exhibited by the player achieving a superior performance and a dramatically better quality of experience as compared to the original heuristic frame-dropping mechanism of the player. © 2011 Elsevier Inc. All rights reserved.",Exception handling | Multimedia | Soft real-time,Journal of Systems and Software,2012-04-01,Article,"Cucinotta, Tommaso;Faggioli, Dario",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865755126,10.1016/j.jss.2011.12.043,An adaptive model-free resource and power management approach for multi-tier cloud environments,"With the development of cloud environments serving as a unified infrastructure, the resource management and energy consumption issues become more important in the operations of such systems. In this paper, we investigate adaptive model-free approaches for resource allocation and energy management under time-varying workloads and heterogeneous multi-tier applications. Specifically, we make use of measurable metrics, including throughput, rejection amount, queuing state, and so on, to design resource adjustment schemes and to make control decisions adaptively. The ultimate objective is to guarantee the summarized revenue of the resource provider while saving energy and operational costs. To validate the effectiveness, performance evaluation experiments are performed in a simulated environment, with realistic workloads considered. Results show that with the combination of long-term adaptation and short-term adaptation, the fluctuation of unpredictable workloads can be captured, and thus the total revenue can be preserved while balancing the power consumption as needed. Furthermore, the proposed approach can achieve better effect and efficiency than the model-based approaches in dealing with real-world workloads. © 2011 Elsevier Inc. All rights reserved.",Adaptive model-free approach | Cloud environment | Power management | Resource management,Journal of Systems and Software,2012-01-01,Article,"Wang, Xiaoying;Du, Zhihui;Chen, Yinong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84859527852,10.1016/j.jss.2012.02.033,A decade of agile methodologies: Towards explaining agile software development,"Ever since the agile manifesto was created in 2001, the research community has devoted a great deal of attention to agile software development. This article examines publications and citations to illustrate how the research on agile has progressed in the 10 years following the articulation of the manifesto. Specifically, we delineate the conceptual structure underlying agile scholarship by performing an analysis of authors who have made notable contributions to the field. Further, we summarize prior research and introduce contributions in this special issue on agile software development. We conclude by discussing directions for future research and urging agile researchers to embrace a theory-based approach in their scholarship. © 2012 Elsevier Inc. All rights reserved.","Agile software development | Crystal method | eXtreme programming, XP | Feature-driven development | Information systems | Lean software development | Scrum | Software engineering | Theory",Journal of Systems and Software,2012-01-01,Editorial,"Dingsøyr, Torgeir;Nerur, Sridhar;Balijepally, Venugopal;Moe, Nils Brede",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863098864,10.1016/j.jss.2012.04.054,Collaborative prioritization of architectural concerns,"Efficient architecture work involves balancing the degree of architecture documentation with attention to needs, costs, agility and other factors. This paper presents a method for prioritizing architectural concerns in the presence of heterogeneous stakeholder groups in large organizations that need to evolve existing architecture. The method involves enquiry, analysis, and deliberation using collaborative and analytical techniques. Method outcomes are action principles directed to managers, and assessment of user needs directed to architects, along with evidence. The method results from 4 years of action research at Ericsson AB with the purpose of adding missing views to architecture documentation and removing superfluous ones. It is illustrated on a case where 29 senior engineers and managers within Ericsson prioritized 37 architectural concerns areas to arrive at 8 action principles, 5 prioritized improvement areas, and 24 improvement suggestions. Feedback from the organization is that the method has been effective in prioritizing architectural concerns, that data collection and analysis is more extensive compared to traditional prioritization practices, but that extensive analysis seems inevitable in architecture improvement work. © 2012 Elsevier Inc. All rights reserved.",Requirement prioritization | Software architecture documentation | Software architecture evolution,Journal of Systems and Software,2012-09-01,Conference Paper,"Pareto, Lars;Sandberg, Anna Börjesson;Eriksson, Peter;Ehnebom, Staffan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863091724,10.1016/j.jss.2012.04.076,Benefits of supplementing use case narratives with activity diagrams - An exploratory study,"Use case narratives modeling the complex functionality of a given system often extend for several pages due to the need to include numerous alternative scenario specifications. In such situations, it is difficult to ensure the completeness and validity of the process logic embedded in such lengthy text narratives. This exploratory study investigates the benefits of supplementing each complex and lengthy use case narrative with an activity diagram for analysts and clients during requirements gathering and analysis. Our findings indicate that the process logic in corresponding activity diagrams is more complete and offers a greater degree of validity than that used in use case narratives. In addition, the quality of the process logic in these artifacts is not negatively affected by a use case narrative's length or complexity when they are used together to capture system requirements. Our research provides empirical evidence of beneficial improvements in the quality of these widely used artifacts that subsequently help eliminate or minimize inconsistencies among the requirements specified in different artifacts. © 2012 Elsevier Inc. All rights reserved.",Activity diagrams | Process logic | Quality improvement | Unified Modeling Language | Use case narratives,Journal of Systems and Software,2012-09-01,Conference Paper,"Bolloju, Narasimha;Sun, Sherry X.Y.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863620476,10.1016/j.jss.2011.09.019,On the relationship between comment update practices and Software Bugs,"When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future bugs than other indicators like the number of prior bugs or changes. Our findings suggest that inconsistent changes are not necessarily correlated with more bugs. Instead, a change in which a function and its comment are suddenly updated inconsistently, whereas they are usually updated consistently (or vice versa), is risky (high probability of introducing a bug) and should be reviewed carefully by practitioners. © 2011 Elsevier Inc. All rights reserved.",Code quality | Empirical studies | Software bugs | Software evolution | Source code comments,Journal of Systems and Software,2012-10-01,Article,"Ibrahim, Walid M.;Bettenburg, Nicolas;Adams, Bram;Hassan, Ahmed E.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84863612271,10.1016/j.jss.2012.04.080,A framework for model-driven development of information systems: Technical decisions and lessons learned,"In recent years, the impact of the model-driven engineering (MDE) paradigm has resulted in the advent of a number of model-based methodological proposals that leverage the use of models at any stage of the development cycle. Apart from promoting the role of models, MDE is notable for leveraging the level of automation along the development process. For this to be achieved there is a need of supporting frameworks, tools or environments. This way, while accompanying any methodological proposal of the corresponding technical support has been traditionally recognized as a good practice, it becomes a mandatory requirement in MDE contexts. To address this task, this work presents in a systematic and reasoned way the set of methodological and technical decisions that drove the specification of M2DAT, a technical solution for model-driven development of Information Systems and its reference implementation: M2DAT-DB, a DSL toolkit for model-driven development of modern DB schemas. The objective of this work is to put forward the conclusions and decisions derived from the experience of the authors when designing and building such framework. As a result, this work will help not only MDE practitioners, but also SE practitioners wishing to bring the advantages of MDE to their fields of interest. © 2012 Elsevier Inc.",Domain-specific modeling | Model-driven engineering | Software development frameworks,Journal of Systems and Software,2012-10-01,Article,"Vara, Juan Manuel;Marcos, Esperanza",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867331419,10.1016/j.jss.2012.03.007,HPobSAM for modeling and analyzing IT Ecosystems - Through a case study,"The next generation of software systems includes systems composed of a large number of distributed, decentralized, autonomous, interacting, cooperating, organically grown, heterogeneous, and continually evolving subsystems, which we call IT Ecosystems. Clearly, we need novel models and approaches to design and develop such systems which can tackle the long-term evolution and complexity problems. In this paper, our framework to model IT Ecosystems is a combination of centralized control (top-down) and self-organizing (bottom-up) approach. We use a flexible formal model, HPobSAM, that supports both behavioral and structural adaptation/evolution. We use a detailed, close to real-life, case study of a smart airport to show how we can use HPobSAM in modeling, analyzing and developing an IT Ecosystem. We provide an executable formal specification of the model in Maude, and use LTL model checking and bounded state space search provided by Maude to analyze the model. We develop a prototype of our case study designed by HPobSAM using Java and Ponder2. Due to the complexity of the model, we cannot check all properties at design time using Maude. We propose a new approach for run-time verification of our case study, and check different types of properties which we could not verify using model checking. As our model uses dynamic policies to control the behavior of systems which can be modified at runtime, it provides us a suitable capability to react to the property violation by modification of policies.© 2012 Elsevier Inc. All rights reserved.",Formal modeling | Large-scale software systems | Self-adaptive systems | Self-organizing systems | Verification,Journal of Systems and Software,2012-12-01,Article,"Khakpour, Narges;Jalili, Saeed;Sirjani, Marjan;Goltz, Ursula;Abolhasanzadeh, Bahareh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867332389,10.1016/j.jss.2012.06.026,Technology flexibility as enabler of robust application development in community source: The case of Kuali and Sakai,"Technology flexibility has been an important topic in software engineering since the start of computerized business applications, which require frequent changes to system specifications due to ever changing business requirements. Achieving a higher degree of technology flexibility has been a long-running challenge to software engineers and project managers. Recently, there has been a new software development approach called ""community source"" consisting of numerous development partners that are also users of the software. In community source, technology flexibility is even more important than usual due to the increase in complexity and uncertainty of software requirements by its many development partners in the community. In this paper, we investigate two community source cases, i.e.; Kuali and Sakai, and examine how technology flexibility is achieved in application software engineering. The principles generated from this study should offer useful insights to the continuous efforts toward making more robust business applications in support of agile enterprises. © 2012 Elsevier Inc. All rights reserved.",Agile enterprise | Community source | Open source | Service oriented architecture | Technology flexibility | Workflow technology,Journal of Systems and Software,2012-01-01,Article,"Liu, Manlu;Wang, Harry Jiannan;Leon Zhao, J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84861955129,10.1016/j.infsof.2012.04.001,Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment,"Context: Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process. Objective: We present an empirical study conducted to evaluate a Pattern-based method for Secure Development - PbSD - that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design. Method: The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in SQL and Oracle's VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods. Results: We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use. Conclusion: The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations. © 2012 Elsevier B.V. All rights reserved.",Authorization | Controlled experiment | Database design | Model driven development | Secure software development | Security patterns,Information and Software Technology,2012-09-01,Article,"Abramov, Jenny;Sturm, Arnon;Shoval, Peretz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865748053,10.1016/j.infsof.2012.07.015,Guest Editorial: Special section on software reliability and security,,,Information and Software Technology,2012-12-01,Editorial,"Baik, Jongmoon;Massacci, Fabio;Zulkernine, Mohammad",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84860226417,10.1016/j.infsof.2012.01.010,Interactive specification and verification of behavioral adaptation contracts,"Context: Adaptation is a crucial issue when building new applications by reusing existing software services which were not initially designed to interoperate with each other. Adaptation contracts describe composition constraints and adaptation requirements among these services. The writing of this specification by a designer is a difficult and error-prone task, especially when interaction protocols are considered in service interfaces. Objective: In this article, we propose a tool-based, interactive approach to support the contract design process. Method: Our approach includes: (i) a graphical notation to define port bindings, and an interface compatibility measure to compare protocols and suggest some port connections to the designer, (ii) compositional and hierarchical techniques to facilitate the specification of adaptation contracts by building them incrementally, (iii) validation and verification techniques to check that the contract will make the involved services work correctly and as expected by the designer. Results: Our results show a reduction both in the amount of effort that the designer has to put into building the contract, as well as in the number of errors present in the final result (noticeably higher in the case of manual specification). Conclusion: We conclude that it is important to provide integrated tool support for the specification and verification of adaptation contracts, since their incorrect specification induces erroneous executions of the system. To the best of our knowledge, such tool support has not been provided by any other approach so far, and hence we consider the techniques described in this paper as an important contribution to the area of behavioral software adaptation. © 2012 Elsevier B.V. All rights reserved.",Formal specification | Interaction protocol | Software adaptation | Software reusability,Information and Software Technology,2012-07-01,Article,"Cámara, Javier;Salaün, Gwen;Canal, Carlos;Ouederni, Meriem",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84863478607,10.1016/j.infsof.2012.04.008,A systematic review of code generation proposals from state machine specifications,"Context: Model Driven Development (MDD) encourages the use of models for developing complex software systems. Following a MDD approach, modelling languages are used to diagrammatically model the structure and behaviour of object-oriented software, among which state-based languages (including UML state machines, finite state machines and Harel statecharts) constitute the most widely used to specify the dynamic behaviour of a system. However, generating code from state machine models as part of the final system constitutes one of the most challenging tasks due to its dynamic nature and because many state machine concepts are not supported by the object-oriented programming languages. Therefore, it is not surprising that such code generation has received great attention over the years. Objective: The overall objective of this paper is to plot the landscape of published proposals in the field of object oriented code generation from state machine specifications, restricting the search neither to a specific context nor to a particular programming language. Method: We perform a systematic, accurate literature review of published studies focusing on the object oriented implementation of state machine specifications. Results: The systematic review is based on a comprehensive set of 53 resources in all, which we have classified into two groups: pattern-based and not pattern-based. For each proposal, we have analysed both the state machine specification elements they support and the means the authors propose for their implementation. Additionally, the review investigates which proposals take into account desirable features to be considered in software development such as maintenance or reusability. Conclusions: One of the conclusions drawn from the review is that most of the analysed works are based on a software design pattern. Another key finding is that many papers neither support several of the main components of the expressive richness of state machine specifications nor provide an implementation strategy that considers relevant qualitative aspects in software development. © 2012 Elsevier B.V. All rights reserved.",Code generation | Finite state machines | Statecharts | Systematic review | UML state machines,Information and Software Technology,2012-10-01,Review,"Domínguez, Eladio;Pérez, Beatriz;Rubio, Ángel L.;Zapata, María A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864762828,10.1016/j.infsof.2012.06.003,Software quality assurance economics,"Context: Software companies invest in quality assurance in order to lower software development and maintenance cost, and to increase revenue and profit margins. To contribute to increase of net income, a quality assurance organization has to consider cost and value of the testware involved in assuring quality of software artifacts, such as requirements, specifications, designs, and code. Objective: This paper proposes a set of economic metrics: testware return on investment, inflation, and cost and value sensitivity to artifact changes and time passage. The paper proposes a set of guidelines on lowering testware cost, on increasing value, on maximizing return on investment, and on when to release. Method: This paper presents an industrial case study data on the relation between test case cost and value, and on cost and value sensitivity to time passage and artifact changes. Results: The industrial case study showed return on investment on test cases of up to 200%, deflation of up to -2% per month, undesirable economic effects, such as test case cost outpacing test case value and rapid test case value depreciation based on time passage. Conclusion: A viable QA organization should measure and improve test case return on investment, inflation, and cost and value sensitivity to artifact changes and time passage. © 2012 Elsevier B.V. All rights reserved.",Defects | Economic release criterion | Inflation | ROI | Sensitivity to time passage | Test cases,Information and Software Technology,2012-11-01,Article,"Nikolik, Borislav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84864766220,10.1016/j.infsof.2012.05.003,Static analysis of Android programs,"Context: Android is a programming language based on Java and an operating system for embedded and mobile devices, whose upper layers are written in the Android language itself. As a language, it features an extended event-based library and dynamic inflation of graphical views from declarative XML layout files. A static analyzer for Android programs must consider such features, for correctness and precision. Objective: Our goal is to extend the Julia static analyzer, based on abstract interpretation, to perform formally correct analyses of Android programs. This article is an in-depth description of such an extension, of the difficulties that we faced and of the results that we obtained. Method: We have extended the class analysis of the Julia analyzer, which lies at the heart of many other analyses, by considering some Android key specific features such as the potential existence of many entry points to a program and the inflation of graphical views from XML through reflection. We also have significantly improved the precision of the nullness analysis on Android programs. Results: We have analyzed with Julia most of the Android sample applications by Google and a few larger open-source programs. We have applied tens of static analyses, including classcast, dead code, nullness and termination analysis. Julia has found, automatically, bugs, flaws and inefficiencies both in the Google samples and in the open-source applications. Conclusion: Julia is the first sound static analyzer for Android programs, based on a formal basis such as abstract interpretation. Our results show that it can analyze real third-party Android applications, without any user annotation of the code, yielding formally correct results in at most 7 min and on standard hardware. Hence it is ready for a first industrial use. © 2012 Elsevier B.V. All rights reserved.",Abstract interpretation | Android | Program verification | Static analysis,Information and Software Technology,2012-11-01,Article,"Payet, Étienne;Spoto, Fausto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865713100,10.1016/j.infsof.2012.06.009,Validation of SDL-based architectural design models using communication-based coverage criteria,"Context: As the capability to automatically generate code from different models becomes more sophisticated, it is critical that these models be adequately tested for quality assurance prior to code generation. Objective: Although simulation-based black-box testing strategies exist for these models, it is important that we also employ white-box testing strategies similar to those used to test implementation code to further validate the quality of these models. Method: We apply coverage testing to architectural design models represented by SDL, a Specification and Description Language. Our previous study defined a methodology for automatic test generation with respect to two structural-based criteria, all-node and all-edge, for each individual SDL process. In this paper, we present new coverage criteria such as n-step-message-transfer and sender-receiver-round-trip, aiming at the communication between different SDL processes. Results: A test generator using innovative backward tracking and forward validation has been implemented to support these criteria, guiding test generation to detect bugs which could not be revealed by test cases generated only with respect to the all-node and all-edge criteria. Conclusions: The results of our case study support the feasibility of using our test generator to create test cases satisfying the proposed communication-based criteria. © 2012 Elsevier B.V. All rights reserved.",Architectural design models | Coverage testing and criteria | SDL (Specification and Description Language) | White-box testing,Information and Software Technology,2012-12-01,Conference Paper,"Restrepo, Andy;Eric Wong, W.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-80055108416,10.1016/j.infsof.2011.07.003,Towards an ontology-based retrieval of UML class diagrams,"Context: Software Reuse has always been an important area amongst software companies in order to increase their productivity and the quality of their products, but code reuse is not the only answer for this. Nowadays, reuse techniques proposals include software designs or even software specifications. Therefore, this research focuses on software design, specifically on UML Class Diagrams. A semantic technology has been applied to facilitate the retrieval process for an effective reuse. Objective: This research proposes an ontology-based retrieval technique by semantic similarity in order to support effective retrieval process for UML Class Diagrams. Since UML Class Diagrams are a de facto standard in the design stages of a Software Development Process, a good technique is needed to reuse them, i.e. reusing during the design stage instead of just the coding stages. Method: An application ontology modeled using UML specifications was designed to compare UML Class Diagram element types. To measure their similarity, a survey was conducted amongst UML experts. Query expansion was improved by a domain ontology supporting the retrieval phase. The calculus of minimal distances in ontologies was solved using a shortest path algorithm. Results: The case study shows the domain ontology importance in the UML Class Diagram retrieval process as well as the importance of an element type expansion method, such as an application ontology. A correlation between the query complexity and retrieved elements has been identified, by analyzing results. Finally, a positive Return of Investment (ROI) was estimated using Poulin's Model. Conclusion: Because Software Reuse has not to be limited to the coding stage, approaches to reuse design stage must be developed, i.e. UML Class Diagrams reuse. This approach proposes a technique for UML Class Diagrams retrieval, which is one important step towards reuse. Semantic technology combined with information retrieval improves the retrieval results. © 2011 Elsevier B.V. All rights reserved.",Information retrieval | Ontologies | Software Engineering | Software Reuse | UML Class Diagrams,Information and Software Technology,2012-01-01,Article,"Robles, Karina;Fraga, Anabel;Morato, Jorge;Llorens, Juan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865777310,10.1016/j.infsof.2012.06.008,Comprehensive two-level analysis of role-based delegation and revocation policies with UML and OCL,"Context: Role-based access control (RBAC) has become the de facto standard for access management in various large-scale organizations. Often role-based policies must implement organizational rules to satisfy compliance or authorization requirements, e.g., the principle of separation of duty (SoD). To provide business continuity, organizations should also support the delegation of access rights and roles, respectively. This, however, makes access control more complex and error-prone, in particular, when delegation concepts interplay with SoD rules. Objective: A systematic way to specify and validate access control policies consisting of organizational rules such as SoD as well as delegation and revocation rules shall be developed. A domain-specific language for RBAC as well as delegation concepts shall be made available. Method: In this paper, we present an approach to the precise specification and validation of role-based policies based on UML and OCL. We significantly extend our earlier work, which proposed a UML-based domain-specific language for RBAC, by supporting delegation and revocation concepts. Result: We show the appropriateness of our approach by applying it to a banking application. In particular, we give three scenarios for validating the interplay between SoD rules and delegation/revocation. Conclusion: To the best of our knowledge, this is the first attempt to formalize advanced RBAC concepts, such as history-based SoD as well as various delegation and revocation schemes, with UML and OCL. With the rich tool support of UML, we believe our work can be employed to validate and implement real-world role-based policies. © 2012 Elsevier B.V. All rights reserved.",Delegation | OCL | RBAC | Revocation | UML,Information and Software Technology,2012-12-01,Conference Paper,"Sohr, Karsten;Kuhlmann, Mirco;Gogolla, Martin;Hu, Hongxin;Ahn, Gail Joon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84858077284,10.1016/j.infsof.2012.01.001,Compliance in service-oriented architectures: A model-driven and view-based approach,"Context: Ensuring software systems conforming to multiple sources of relevant policies, laws, and regulations is significant because the consequences of infringement can be serious. Unfortunately, this goal is hardly achievable due to the divergence and frequent changes of compliance sources and the differences in perception and expertise of the involved stakeholders. In the long run, these issues lead to problems regarding complexity, understandability, maintainability, and reusability of compliance concerns. Objective: In this article, we present a model-driven and view-based approach for addressing problems related to compliance concerns. Method: Compliance concerns are represented using separate view models. This is achieved using domain-specific languages (DSLs) that enable non-technical and technical experts to formulate only the excerpts of the system according to their expertise and domain knowledge. The compliance implementations, reports, and documentation can be automatically generated from the models. The applicability of our approach has been validated using an industrial case study. Results: Our approach supports stakeholders in dealing with the divergence of multiple compliance sources. The compliance controls and relevant reports and documentation are generated from the models and hence become traceable, understandable, and reusable. Because the generated artifacts are associated with the models, the compliance information won't be lost as the system evolves. DSLs and view models convey compliance concerns to each stakeholder in a view that is most appropriate for his/her current work task. Conclusions: Our approach lays a solid foundation for ensuring conformance to relevant laws and regulations. This approach, on the one hand, aims at addressing the variety of expertise and domain knowledge of stakeholders. On the other hand, it also aims at ensuring the explicit links between compliance sources and the corresponding implementations, reports, and documents for conducting many important tasks such as root cause analysis, auditing, and governance. © 2012 Elsevier B.V. All rights reserved.",Compliance | Domain-specific languages | Model-driven | Process-driven SOAs | Service-oriented architectures | View-based,Information and Software Technology,2012-06-01,Article,"Tran, Huy;Zdun, Uwe;Holmes, Ta'Id;Oberortner, Ernst;Mulo, Emmanuel;Dustdar, Schahram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865720227,10.1016/j.infsof.2012.07.007,The maturity of maturity model research: A systematic mapping study,"Context: Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research. Objective: The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps. Method: A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications. Results: The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given. Conclusion: The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist. © 2012 Elsevier B.V. All rights reserved.",Design-oriented research | Maturity models | Software management | Systematic mapping study,Information and Software Technology,2012-12-01,Conference Paper,"Wendler, Roy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874631216,10.1109/APSEC.2012.122,An exploratory study of API usage examples on the web,"Usage examples are helpful for programmers learning to use APIs from third-party frameworks or libraries. There are lots of usage examples scattered in web pages on the Web, such as tutorials, blogs, and forums. A few researches have proposed approaches to leveraging these usage examples to improve programming. However, due to the lack of comprehensive understanding on the current situation of usage examples on the web, the work is still at the very beginning. Many concerns are reserved, for instance, how many usage examples can be found on the Web? how well do such examples support programmers on earth? what factors have impact on these examples' usability? In this paper, we conducted an exploratory study of usage examples on the web, including their distribution, characteristics like content style, correctness, and complexity, as well as their correlations. Through the study, we obtain some insight of how to facilitate utilization of usage examples on the web and what mechanisms could be provided. Possible research directions and problems are proposed at the end. © 2012 IEEE.",API | empirical study | usage examples | web search,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2012-01-01,Conference Paper,"Wang, Lijie;Zou, Yanzhen;Fang, Lu;Xie, Bing;Yang, Fuqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874251189,10.1109/ICOS.2012.6417654,A framework for extending usability engineering: API usability essentials: Extending usability via component-based platform,"Application Programming Interface (API) in software development acts as an important milestone for software productions. It is believed that API usability impacts upon ease-in-use, operationability and acceptability among its audience. Likewise, an ever increasing need for extending and integrating Usability Engineering (UE) has become vital for the success of software products. Earlier researches within this domain do not address API's usability via a component-based framework approach. The proposed framework emphasizes on consolidated formulation of various usability and quality models to derive chunks of dimensional variables. Further the paper highlights API usability practices and heuristics applied in API development process and discusses API product's artifacts component to be used in deriving further product-related components to support enhancing usability. © 2012 IEEE.",API Usability | Component-Aided Usability | Extending Usability | Model Consolidation | Usability Engineering (UE) and Software Engineering (SE) Integration,"2012 IEEE Conference on Open Systems, ICOS 2012",2012-12-01,Conference Paper,"Munir, Muhammad Bilal;Mushtaq, Arif",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84873961277,10.1109/IPCC.2012.6408606,Developing best practices for API reference documentation: Creating a platform to study how programmers learn new APIs,"Software developers use application-programming interface (API) documentation to learn how to use the features of software libraries. How quickly software developers learn to use a library's features determines how quickly they can apply those features in a software application. Recent studies have shown that API documentation is, unfortunately, not always as helpful to software developers as they need it to be. This paper studies the prototype of a tool and a method that are being developed to help technical writers identify the elements of API reference documentation that help software developers complete programming tasks. The tool and method described in this paper use a remote user-assessment platform, which enables researchers and technical writers to study the effect that document design variations have on a large and diverse audience. Such an approach can help technical writers identify new best practices for writing effective API documentation. © 2012 IEEE.",API documentation | best practices | remote user assessment | technical writing,IEEE International Professional Communication Conference,2012-12-01,Conference Paper,"Watson, Robert",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84870895497,10.1109/VLHCC.2012.6344511,Usable results from the field of API usability: A systematic mapping and further analysis,"Modern software development often involves the use of complex, reusable components called Application Programming Interfaces (APIs). Developers use APIs to complete tasks they could not otherwise accomplish in a reasonable time. These components are now vital to mainstream software development. But as APIs have become more important, understanding how to make them more usable is becoming a significant research question. To assess the current state of research in the field, we conducted a systematic mapping. A total of 28 papers were reviewed and categorized based on their research type and on the evaluation method employed by its authors. We extended the analysis of a subset of the papers we reviewed beyond the usual limits of a systematic map in order to more closely examine details of their evaluations - such as their structure and validity - and to summarize their recommendations. Based on these results, common problems in the field are discussed and future research directions are suggested. © 2012 IEEE.",API usability | application programming interface | meta-analysis | systematic map | systematic review,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2012-12-17,Conference Paper,"Burns, Chris;Ferreira, Jennifer;Hellmann, Theodore D.;Maurer, Frank",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84872348165,10.1145/2414536.2414638,Comparing the usability of grid-bag and constraint-based layouts,"While the usability of GUI design methods has been studied in general, the usability of layout specification methods is largely unexplored. In this paper we provide an empirical comparison of two popular GUI layout models, grid-bag layout and constraint-based layout. While the grid-bag layout is a powerful layout model, the constraint-based layout is able to generate even more general and flexible layout configurations. We performed a controlled experiment with postgraduate students of Computer Science and Software Engineering, measuring efficiency, accuracy and preference for typical layout specification and editing tasks. The results show significant differences between both layout models: the initial specification of GUIs is faster with a grid-bag layout whereas editing of existing complex layouts is faster and more accurate with a constraint-based layout. The study shows that constraint-based layout, although it may seem more complicated at first glance, can compete with and in some cases even outperform more conventional techniques in terms of their usability. © 2012 ACM.",API | empiric evaluation | layout | usability,"Proceedings of the 24th Australian Computer-Human Interaction Conference, OzCHI 2012",2012-12-01,Conference Paper,"Zeidler, Clemens;Müller, Johannes;Lutteroth, Christof;Weber, Gerald",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84868280460,10.1109/Agile.2012.7,Documentation work in agile teams: The role of documentation formalism in achieving a sustainable practice,"As its second guiding principle, agile software development promotes working software over comprehensive documentation. In this paper we investigate alignment between two different documentation practices and agile development. We report upon an experiment conducted to explore the impact of formalism and media type on various dimensions of documentation practice in agile teams. 28 students in 8 teams were divided into two groups: SAD and UML. Group SAD was to update and deliver their high-level software architecture in form of a textual description defined by RUP templates. Group UML was instructed to update and deliver their low-level software design in form of UML models. Our results show that iterative documentation practices led to more extensive and more detailed textual documentation. We found that writing documentation was perceived as a intrusive task leading to task specialization and allocation of documentation to less qualified team members. Consequently, this hampered collaboration within the team. Based in our findings, we suggest that if documentation is to be delivered with the project, producing documentation should be communicated and accepted by the team as a proper product. Furthermore, we argue that codification of internal development knowledge should be a non-intrusive task. © 2012 IEEE.",agile teams | knowledge sharing | organizational management and coordination | process improvement | project management | software development,"Proceedings - 2012 Agile Conference, Agile 2012",2012-11-07,Conference Paper,"Stettina, Christoph Johann;Heijstek, Werner;Fægri, Tor Erlend",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.19153/cleiej.15.2.6,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84867632830,10.1007/978-3-642-33666-9_13,Do professional developers benefit from design pattern documentation? A replication in the context of source code comprehension,"We present the results of a differentiated replication conducted with professional developers to assess whether the presence and the kind of documentation for the solutions or instances of design patterns affect source code comprehension. The participants were divided into three groups and asked to comprehend a chunk of the JHot- Draw source code. Depending on the group, each participant was or not provided with the graphical and textual representations of the design pattern instances implemented within that source code. In the case of graphically documented instances, we used UML class diagrams, while textually documented instances are reported as comment in the source code. The results revealed that participants provided with the documentation of the instances achieved a significantly better comprehension than the participants with source code alone. The effect of the kind of documentation is not statistically significant. © 2012 Springer-Verlag.",Controlled Experiment | Design Patterns | Maintenance | Replications | Software Models | Source Code Comprehension,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-10-24,Conference Paper,"Gravino, Carmine;Risi, Michele;Scanniello, Giuseppe;Tortora, Genoveffa",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1145/2088883.2088892,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.5072/PRISM/24760,,,,,,,,Exclude,Not a peer reviewed paper or is a pre-print
10.1016/j.jss.2022.111515,,10.11575/PRISM/24787,,,,,,,,Exclude,Not a peer reviewed paper or is a pre-print
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1504/IJAESD.2012.048308,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893595786,10.1109/ASE.2013.6693066,Improving efficiency of dynamic analysis with dynamic dependence summaries,"Modern applications make heavy use of third-party libraries and components, which poses new challenges for efficient dynamic analysis. To perform such analyses, transitive dependent components at all layers of the call stack must be monitored and analyzed, and as such may be prohibitively expensive for systems with large libraries and components. As an approach to address such expenses, we record, summarize, and reuse dynamic dataflows between inputs and outputs of components, based on dynamic control and data traces. These summarized dataflows are computed at a fine-grained instruction level; the result of which, we call 'dynamic dependence summaries.' Although static summaries have been proposed, to the best of our knowledge, this work presents the first technique for dynamic dependence summaries. The benefits to efficiency of such summarization may be afforded with losses of accuracy. As such, we evaluate the degree of accuracy loss and the degree of efficiency gain when using dynamic dependence summaries of library methods. On five large programs from the DaCapo benchmark (for which no existing whole-program dynamic dependence analyses have been shown to scale) and 21 versions of NANOXML, the summarized dependence analysis provided 90% accuracy and a speed-up of 100% (i.e., ×2), on average, when compared to traditional exhaustive dynamic dependence analysis. © 2013 IEEE.",Data-flow | Dynamic analysis | Dynamic Slicing | Program analysis | Summarization,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Palepu, Vijay Krishna;Xu, Guoqing;Jones, James A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893590344,10.1109/ASE.2013.6693067,Efficient parametric runtime verification with deterministic string rewriting,"Early efforts in runtime verification show that parametric regular and temporal logic specifications can be monitored efficiently. These approaches, however, have limited expressiveness: their specifications always reduce to monitors with finite state. More recent developments showed that parametric context-free properties can be efficiently monitored with overheads generally lower than 12-15%. While context-free grammars are more expressive than finite-state languages, they still do not allow every computable safety property. This paper presents a monitor synthesis algorithm for string rewriting systems (SRS). SRSs are well known to be Turing complete, allowing for the formal specification of any computable safety property. Earlier attempts at Turing complete monitoring have been relatively inefficient. This paper demonstrates that monitoring parametric SRSs is practical. The presented algorithm uses a modified version of Aho-Corasick string searching for quick pattern matching with an incremental rewriting approach that avoids reexamining parts of the string known to contain no redexes. © 2013 IEEE.",Monitoring | Runtime Verification | String Rewriting,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Meredith, Patrick;Rosu, Grigore",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893634411,10.1109/ASE.2013.6693101,TzuYu: Learning stateful typestates,"Behavioral models are useful for various software engineering tasks. They are, however, often missing in practice. Thus, specification mining was proposed to tackle this problem. Existing work either focuses on learning simple behavioral models such as finite-state automata, or relies on techniques (e.g., symbolic execution) to infer finite-state machines equipped with data states, referred to as stateful typestates. The former is often inadequate as finite-state automata lack expressiveness in capturing behaviors of data-rich programs, whereas the latter is often not scalable. In this work, we propose a fully automated approach to learn stateful typestates by extending the classic active learning process to generate transition guards (i.e., propositions on data states). The proposed approach has been implemented in a tool called TzuYu and evaluated against a number of Java classes. The evaluation results show that TzuYu is capable of learning correct stateful typestates more efficiently. © 2013 IEEE.",,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Xiao, Hao;Sun, Jun;Liu, Yang;Lin, Shang Wei;Sun, Chengnian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893588900,10.1109/ASE.2013.6693113,AutoComment: Mining question and answer sites for automatic comment generation,"Code comments improve software maintainability. To address the comment scarcity issue, we propose a new automatic comment generation approach, which mines comments from a large programming Question and Answer (Q&A) site. Q&A sites allow programmers to post questions and receive solutions, which contain code segments together with their descriptions, referred to as code-description mappings.We develop AutoComment to extract such mappings, and leverage them to generate description comments automatically for similar code segments matched in open-source projects. We apply AutoComment to analyze Java and Android tagged Q&A posts to extract 132,767 code-description mappings, which help AutoComment to generate 102 comments automatically for 23 Java and Android projects. The user study results show that the majority of the participants consider the generated comments accurate, adequate, concise, and useful in helping them understand the code. © 2013 IEEE.",automated comment generation | documentation | natural language processing for software engineering | program comprehension,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Wong, Edmund;Yang, Jinqiu;Tan, Lin",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84893586666,10.1109/ASE.2013.6693129,A pattern-based approach to parametric specification mining,"This paper presents a technique for using execution traces to mine parametric temporal specifications in the form of quantified event automata (QEA) - previously introduced as an expressive and efficient formalism for runtime verification. We consider a pattern-based mining approach that uses a pattern library to generate and check potential properties over given traces, and then combines successful patterns. By using predefined models to measure the tool's precision and recall we demonstrate that our approach can effectively and efficiently extract specifications in realistic scenarios. © 2013 IEEE.",,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Reger, Giles;Barringer, Howard;Rydeheard, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893556079,10.1109/ASE.2013.6693136,Smart Cloud Broker: Finding your home in the clouds,"As the rate of cloud computing adoption grows, so does the need for consumption assistance. Enterprises looking to migrate their IT systems to the cloud require assistance in identifying providers that offer resources with the most appropriate pricing and performance levels to match their specific business needs. In this paper, we present Smart Cloud Broker - a suite of software tools that allows cloud infrastructure consumers to evaluate and compare the performance of different Infrastructure as a Service (IaaS) offerings from competing cloud service providers, and consequently supports selection of the cloud configuration and provider with the specifications that best meet the user's requirements. Using Smart Cloud Broker, prospective cloud users can estimate the performance of the different cloud platforms by running live tests against representative benchmark applications under representative load conditions. © 2013 IEEE.",,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Baruwal Chhetri, Mohan;Chichin, Sergei;Bao Vo, Quoc;Kowalczyk, Ryszard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893533243,10.1109/ASE.2013.6693137,OCRA: A tool for checking the refinement of temporal contracts,"Contract-based design enriches a component model with properties structured in pairs of assumptions and guarantees. These properties are expressed in term of the variables at the interface of the components, and specify how a component interacts with its environment: the assumption is a property that must be satisfied by the environment of the component, while the guarantee is a property that the component must satisfy in response. Contract-based design has been recently proposed in many methodologies for taming the complexity of embedded systems. In fact, contract-based design enables stepwise refinement, compositional verification, and reuse of components. However, only few tools exist to support the formal verification underlying these methods. OCRA (Othello Contracts Refinement Analysis) is a new tool that provides means for checking the refinement of contracts specified in a linear-time temporal logic. The specification language allows to express discrete as well as metric real-time constraints. The underlying reasoning engine allows checking if the contract refinement is correct. OCRA has been used in different projects and integrated in CASE tools. © 2013 IEEE.",,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Cimatti, Alessandro;Dorigatti, Michele;Tonetta, Stefano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893613188,10.1109/ASE.2013.6693146,TRAM: A tool for transforming textual requirements into analysis models,"Tool support for automatically constructing analysis models from the natural language specification of requirements (NLR) is critical to model driven development (MDD), as it can bring forward the use of precise formal languages from the coding to the specification phase in the MDD lifecycle. TRAM provides such a support through a novel approach. By using a set of conceptual patterns to facilitate the transformation of an NLR to its target software model, TRAM has shown its potential as an automated tool to support the earliest phase of MDD. This paper describes TRAM and evaluates the tool against three benchmark approaches. © 2013 IEEE.",analysis models | conceptual patterns | Model transformation | natural language processing | semantic object models,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Letsholo, Keletso J.;Zhao, Liping;Chioasca, Erol Valeriu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893572451,10.1109/ASE.2013.6693149,Synthesizing fault-tolerant programs from deontic logic specifications,"We study the problem of synthesizing fault-tolerant components from specifications, i.e., the problem of automatically constructing a fault-tolerant component implementation from a logical specification of the component, and the system's required level of fault-tolerance. In our approach, the logical specification of the component is given in dCTL, a branching time temporal logic with deontic operators, especially designed for fault-tolerant component specification. The synthesis algorithm takes the component specification, and a user-defined level of fault-tolerance (masking, nonmasking, failsafe), and automatically determines whether a component with the required fault-tolerance is realizable. Moreover, if the answer is positive, then the algorithm produces such a fault-tolerant implementation. Our technique for synthesis is based on the use of (bi)simulation algorithms for capturing different fault-tolerance classes, and the extension of a synthesis algorithm for CTL to cope with dCTL specifications. © 2013 IEEE.",Correctness by construction | Deontic logics | Fault-tolerance | Formal specification | Program Synthesis | Temporal Logics,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Demasi, Ramiro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893266860,10.1109/ESEM.2013.14,An empirical study of API usability,"Modern software development extensively involves reusing library components accessed through their Application Programming Interfaces (APIs). Usability is therefore a fundamental goal of API design, but rigorous empirical studies of API usability are still relatively uncommon. In this paper, we present the design of an API usability study which combines interview questions based on the cognitive dimensions framework, with systematic observations of programmer behavior while solving programming tasks based on ''tokens''. We also discuss the implementation of the study to assess the usability of a persistence library API (offering functionalities such as storing objects into relational databases). The study involved 25 programmers (including students, researchers, and professionals), and provided additional evidence to some critical features evidenced by related studies, such as the difficulty of finding good names for API features and of discovering relations between API types. It also discovered new issues relevant to API design, such as the impact of flexibility, and confirmed the crucial importance of accurate documentation for usability. © 2013 IEEE.",application programming interfaces,International Symposium on Empirical Software Engineering and Measurement,2013-12-01,Conference Paper,"Piccioni, Marco;Furia, Carlo A.;Meyer, Bertrand",Include,
10.1016/j.jss.2022.111515,2-s2.0-84893249389,10.1109/ESEM.2013.17,Expectations and achievements: A longitudinal study on an offshoring strategy,"Offshore software development has gained momentum and most of software companies today have implemented offshore strategies of some sort. Many of these strategies are enforced by corporate top management and driven by assumptions that lower development wages guarantee cheaper and better software development. In practice, offshore software development is associated with many risks, and achievement of the expected benefits is not as straightforward as the rumor has it. In this paper we explore an implementation of an offshore strategy in a Swedish software company that opened its offshore branch in Russia. Based on extensive documentation analysis we create an overview of the initially expected benefits and obstacles that prevailed among onshore product and development unit managers. Years after implementation of the offshore in sourcing strategy we asked these managers about the achievement of their expectations. We observed that the company documented various expected benefits when implementing an off shoring strategy and also concerns that some of these benefits might not be achieved. Seven years after its implementation, the off shoring strategy was overall considered working, however the expected benefits were not fully achieved. More importantly, several gaps were identified, that suggest that the enforced strategy has resulted in a stable but not beneficial collaboration from the onshore perspective. © 2013 IEEE.",Concerns | Empirical case study | Expected benefits | Global software development | Insourcing | Longitudinal study | Offshoring | Perceived benefits,International Symposium on Empirical Software Engineering and Measurement,2013-12-01,Conference Paper,"Šmite, Darja;Cruzes, Daniela S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893288710,10.1109/ESEM.2013.24,DevNet: Exploring developer collaboration in heterogeneous networks of bug repositories,"During open source software development and maintenance, bug fixing is a result of developer collaboration. Understanding the structure of developer collaboration could be helpful for effective and efficient bug fixing. Most prior work on exploring developer collaboration in bug repositories only considers a particular form of developer collaboration. However, in real software bug repositories, developers collaborate with each other via multiple ways, e.g., commenting bugs, tossing bugs, and assigning bugs. In this paper, we present DevNet, a framework for representing and analyzing developer collaboration in bug repositories based on heterogeneous developer networks. Moreover, we illustrate that such developer collaboration can assist bug triage through a case study on the bug repositories of Eclipse and Mozilla involving over 800,000 bug reports. Experiment results show that our approach can improve the state-of-the-art bug triage methods by 5-15% in accuracy. We believe that the proposed approach provides new insights for analyzing software repositories through heterogeneous networks. © 2013 IEEE.",bug triage | developer collaboration | heterogeneous developer network | software bug repositories,International Symposium on Empirical Software Engineering and Measurement,2013-12-01,Conference Paper,"Wang, Song;Zhang, Wen;Yang, Ye;Wang, Qing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893231167,10.1109/ESEM.2013.53,The impact of agile principles and practices on large-scale software development projects: A multiple-case study of two projects at Ericsson,"BACKGROUND: Agile software development methods have a number of reported benefits on productivity, project visibility, software quality and other areas. There are also negative effects reported. However, the base of empirical evidence to the claimed effects needs more empirical studies. AIM: The purpose of the research was to contribute with empirical evidence on the impact of using agile principles and practices in large-scale, industrial software development. Research was focused on impacts within seven areas: Internal software documentation, Knowledge sharing, Project visibility, Pressure and stress, Coordination effectiveness, and Productivity. METHOD: Research was carried out as a multiple-case study on two contemporary, large-scale software development projects with different levels of agile adoption at Ericsson. Empirical data was collected through a survey of project members. RESULTS AND CONCLUSIONS: Intentional implementation of agile principles and practices were found to: correlate with a more balanced use of internal software documentation, contribute to knowledge sharing, correlate with increased project visibility and coordination effectiveness, reduce the need for other types of coordination mechanisms, and possibly increase productivity. No correlation with increase in pressure and stress were found. © 2013 IEEE.",Agile software development | empirical software engineering | large-scale software development | multiple-case study | survey,International Symposium on Empirical Software Engineering and Measurement,2013-12-01,Conference Paper,"Lagerberg, Lina;Skude, Tor;Emanuelsson, Par;Sandahl, Kristian;Stahl, Daniel",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84883669902,10.1145/2491411.2491434,Automated oracles: An empirical study on cost and effectiveness,"Software testing is an effective, yet expensive, method to improve software quality. Test automation, a potential way to reduce testing cost, has received enormous research attention recently, but the so-called ""oracle problem"" (how to decide the PASS/FAIL outcome of a test execution) is still a major obstacle to such cost reduction. We have extensively investigated state-of-the-art works that contribute to address this problem, from areas such as specification mining and model inference. In this paper, we compare three types of automated oracles: Data invariants, Temporal invariants, and Finite State Automata. More specifically, we study the training cost and the false positive rate; we evaluate also their fault detection capability. Seven medium to large, industrial application subjects and real faults have been used in our empirical investigation. Copyright 2013 ACM.",Automated testing oracles | Empirical study | Specification mining,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Nguyen, Cu D.;Marchetto, Alessandro;Tonella, Paolo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883682476,10.1145/2491411.2491435,Finding incorrect compositions of atomicity,"In object-oriented code, atomicity is ideally isolated in a library which encapsulates shared program state and provides atomic APIs for access. The library provides a convenient way for programmers to reason about the needed synchronization. However, as the library exports a limited set of APIs, it cannot satisfy every unplanned atomicity demand; therefore, clients may have to compose invocations of the library APIs to obtain new atomic functionality. This process is error-prone due to the complexity of reasoning required, hence tool support for uncovering incorrect compositions (i.e., atomic compositions that are implemented incorrectly) would be very helpful. A key difficulty is how to determine the intended atomic compositions, which are rarely documented. Existing inference techniques cannot be used to infer the atomic compositions because they cannot recognize the library and the client, which requires understanding the related program state. Even if extended to support the library/client, they lead to many false positives or false negatives because they miss the key program logic which reflects programmers' coding paradigms for atomic compositions. We define a new inference technique which identifies intended atomic compositions using two key symptoms based on program dependence. We then check dynamically whether these atomic compositions are implemented incorrectly as non-atomic. Evaluation on thirteen applications shows that our approach finds around 50 previously unknown incorrect compositions. Further study on Tomcat shows that almost half (5 out of 12) of discovered incorrect compositions are confirmed as bugs by the developers. Given that Tomcat is heavily used in 250, 000 sites including Linkedin.com and Ebay.com, we believe finding multiple new bugs in it automatically with relatively few false positives supports our heuristics for determining intended atomicity. Copyright 2013 ACM.",Atomic compositions | Concurrent programming | Predictive analysis | Program dependence | Static analysis,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-01-01,Conference Paper,"Liu, Peng;Dolby, Julian;Zhang, Charles",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883692170,10.1145/2491411.2491439,Effective dynamic detection of alias analysis errors,"Alias analysis is perhaps one of the most crucial and widely used analyses, and has attracted tremendous research efforts over the years. Yet, advanced alias analyses are extremely difficult to get right, and the bugs in these analyses are one key reason that they have not been adopted to production compilers. This paper presents NEONGOBY, a system for effectively detecting errors in alias analysis implementations, improving their correctness and hopefully widening their adoption. NEONGOBY detects the worst type of bugs where the alias analysis claims that two pointers never alias, but they actually alias at runtime. NEONGOBY works by dynamically observing pointer addresses during the execution of a test program and then checking these addresses against an alias analysis for errors. It is explicitly designed to (1) be agnostic to the alias analysis it checks for maximum applicability and ease of use and (2) detect alias analysis errors that manifest on real-world programs and workloads. It emits no false positives as long as test programs do not have undefined behavior per ANSI C specification or call external functions that interfere with our detection algorithm. It reduces performance overhead using a practical selection of techniques. Evaluation on three popular alias analyses and real-world programs Apache and MySQL shows that NEON-GOBY effectively finds 29 alias analysis bugs with zero false positives and reasonable overhead; the most serious four bugs have been patched by the developers. To enable alias analysis builders to start using NEONGOBY today, we have released it open-source at https://github.com/columbia/neongoby, along with our error detection results and proposed patches.",Alias analysis | Dynamic analysis | Error detection,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Wu, Jingyue;Hu, Gang;Tang, Yang;Yang, Junfeng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883681764,10.1145/2491411.2491455,Feature model extraction from large collections of informal product descriptions,"Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.",Domain analysis | Feature models | Product lines,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Davril, Jean Marc;Delfosse, Edouard;Hariri, Negar;Acher, Mathieu;Cleland-Huang, Jane;Heymans, Patrick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883708526,10.1145/2491411.2491452,Differential assertion checking,"Previous version of a program can be a powerful enabler for program analysis by defining new relative specifications and making the results of current program analysis more relevant. In this paper, we describe the approach of differential assertion checking (DAC) for comparing different versions of a program with respect to a set of assertions. DAC provides a natural way to write relative specifications over two programs. We introduce a novel modular approach to DAC by reducing it to safety checking of a composed program, which can be accomplished by standard program verifiers. In particular, we leverage automatic invariant generation to synthesize relative specifications for pairs of loops and procedures. We provide a preliminary evaluation of a prototype implementation within the SYMDIFF tool along two directions: (a) soundly verifying bug fixes in the presence of loops and (b) providing a knob for suppressing alarms when checking a new version of a program. Copyright 2013 ACM.",Differential analysis | Regressions | Verification,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Lahiri, Shuvendu K.;McMillan, Kenneth L.;Sharma, Rahul;Hawblitzel, Chris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883658994,10.1145/2491411.2491429,Precision reuse for efficient regression verification,"Continuous testing during development is a well-established technique for software-quality assurance. Continuous model checking from revision to revision is not yet established as a standard practice, because the enormous resource consumption makes its application impractical. Model checkers compute a large number of verification facts that are necessary for verifying if a given specification holds. We have identified a category of such intermediate results that are easy to store and efficient to reuse: abstraction precisions. The precision of an abstract domain specifies the level of abstraction that the analysis works on. Precisions are thus a precious result of the verification effort and it is a waste of resources to throw them away after each verification run. In particular, precisions are reasonably small and thus easy to store; they are easy to process and have a large impact on resource consumption. We experimentally show the impact of precision reuse on industrial verification problems created from 62 Linux kernel device drivers with 1 119 revisions. Copyright 2013 ACM.",Formal verification | Regression checking,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Beyer, Dirk;Löwe, Stefan;Novikov, Evgeny;Stahlbauer, Andreas;Wendler, Philipp",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883725267,10.1145/2491411.2491414,Synthesis of component and connector models from crosscutting structural views,"We present component and connector (C&C) views, which specify structural properties of component and connector models in an expressive and intuitive way. C&C views provide means to abstract away direct hierarchy, direct connectivity, port names and types, and thus can crosscut the traditional boundaries of the implementation-oriented hierarchical decomposition of systems and sub-systems, and reflect the partial knowledge available to different stakeholders involved in a system's design. As a primary application for C&C views we investigate the synthesis problem: given a C&C views specification, consisting of mandatory, alternative, and negative views, construct a concrete satisfying C&C model, if one exists. We show that the problem is NP-hard and solve it, in a bounded scope, using a reduction to SAT, via Alloy. We further extend the basic problem with support for library components, specification patterns, and architectural styles. The result of synthesis can be used for further exploration, simulation, and refinement of the C&C model or, as the complete, final model itself, for direct code generation. A prototype tool and an evaluation over four example systems with multiple specifications show promising results and suggest interesting future research directions towards a comprehensive development environment for the structure of component and connector designs. Copyright 2013 ACM.",Component and connector models | Synthesis,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Maoz, Shahar;Ringert, Jan Oliver;Rumpe, Bernhard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883698103,10.1145/2491411.2491428,API change and fault proneness: A threat to the success of android apps,"During the recent years, the market of mobile software applications (apps) has maintained an impressive upward trajectory. Many small and large software development companies invest considerable resources to target available opportunities. As of today, the markets for such devices feature over 850K+ apps for Android and 900K+ for iOS. Availability, cost, functionality, and usability are just some factors that determine the success or lack of success for a given app. Among the other factors, reliability is an important criteria: users easily get frustrated by repeated failures, crashes, and other bugs; hence, abandoning some apps in favor of others. This paper reports a study analyzing how the fault- and change-proneness of APIs used by 7, 097 (free) Android apps relates to applications' lack of success, estimated from user ratings. Results of this study provide important insights into a crucial issue: making heavy use of fault- and change-prone APIs can negatively impact the success of these apps. Copyright 2013 ACM.",Android | API changes | Empirical studies | Mining software repositories,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Linares-Vásquez, Mario;Bavota, Gabriele;Bernal-Cárdenas, Carlos;Di Penta, Massimiliano;Oliveto, Rocco;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883720690,10.1145/2491411.2491417,Practical static analysis of JavaScript applications in the presence of frameworks and libraries,"JavaScript is a language that is widely-used for both web-based and standalone applications such as those in the Windows 8 operating system. Analysis of JavaScript has long been known to be challenging due to the language's dynamic nature. On top of that, most JavaScript applications rely on large and complex libraries and frameworks, often written in a combination of JavaScript and native code such as C and C++. Stubs have been commonly employed as a partial specification mechanism to address the library problem; alas, they are tedious and error-prone. However, the manner in which library code is used within applications often sheds light on what library APIs return or pass into callbacks declared within the application. In this paper, we propose a technique which combines pointer analysis with a novel use analysis to handle many challenges posed by large JavaScript libraries. Our techniques have been implemented and empirically validated on a set of 25 Windows 8 JavaScript applications, averaging 1, 587 lines of code, together with about 30, 000 lines of library code, demonstrating a combination of scalability and precision. Copyright 2013 ACM.",JavaScript | Libraries | Points-to analysis | Use analysis,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Madsen, Magnus;Livshits, Benjamin;Fanning, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883660231,10.1145/2491411.2501854,Scalable and incremental software bug detection,"An important, but often neglected, goal of static analysis for detecting bugs is the ability to show defects to the programmer quickly. Unfortunately, existing static analysis tools scale very poorly, or are shallow and cannot find complex interprocedural defects. Previous attempts at reducing the analysis time by adding more resources (CPU, memory) or by splitting the analysis into multiple sub-analyses based on defect detection capabilities resulted in limited/negligible improvements. We present a technique for parallel and incremental static analysis using top-down, bottom-up and global specification inference based around the concept of a work unit, a self-contained atom of analysis input, that deterministically maps to its output. A work unit contains both abstract and concrete syntax to analyze, a supporting fragment of the class hierarchy, summarized interprocedural behavior, and defect reporting information, factored to ensure a high level of reuse when analyzing successive versions incrementally. Work units are created and consumed by an analysis master process that coordinates the multiple analysis passes, the flow of information among them, and incrementalizes the computation. Meanwhile, multiple analysis worker processes use abstract interpretation to compute work unit results. Process management and interprocess communication is done by a general-purpose computation distributor layer. We have implemented our approach and our experimental results show that using eight processor cores, we can perform complete analysis of code bases with millions of lines of code in a few hours, and even faster after incremental changes to that code. The analysis is thorough and accurate: it usually reports about one crash-causing defect per thousand lines of code, with a false positive rate of 10-20%. Copyright 2013 ACM.",Bug detection | Incremental | Parallel | Static analysis,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"McPeak, Scott;Gros, Charles Henri;Ramanathan, Murali Krishna",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883706244,10.1145/2491411.2494578,Towards emotional awareness in software development teams,"Emotions play an important role in determining work results and how team members collaborate within a project. When working in large, distributed teams, members can lose awareness of the emotional state of the project. We propose an approach to improve emotional awareness in software development teams by means of quantitative emotion summaries. Our approach automatically extracts and summarizes emotions expressed in collaboration artifacts by combining probabilistic topic modeling with lexical sentiment analysis techniques. We applied the approach to 1000 collaboration artifacts produced by three development teams in a three month period. Interviews with the teams' project leaders suggest that the proposed emotion summaries have a good correlation with the emotional state of the project, and could be useful for improving emotional awareness. However, the interviews also indicate that the current state of the summaries is not detailed enough and further improvements are needed. Copyright 2013 ACM.",Collaborative software engineering | Sentiment analysis | Topic extraction,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"Guzman, Emitza;Bruegge, Bernd",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84883737101,10.1145/2491411.2492401,Automatically describing software faults,"A developers ability to successfully debug a fault is directly related to their ability to comprehend the fault. Notwithstanding improvements in software-maintenance automation, this fault comprehension task remains largely manual and time consuming. I propose an automated approach to describe software faults, thus ameliorating comprehension and reducing manual effort. My approach leverages dynamic analysis, fault localization, and source-code mining to produce a succinct, natural-language fault summary. Copyright 2013 ACM.",Automated testing | Debugging | Fault comprehension | Latent semantic analysis,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings",2013-09-16,Conference Paper,"DiGiuseppe, Nicholas",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84887185032,10.1145/2500365.2500593,Hoare-style reasoning with (Algebraic) continuations,"Continuations are programming abstractions that allow for manipulating the ""future"" of a computation. Amongst their many applications, they enable implementing unstructured program flow through higher-order control operators such as callcc. In this paper we develop a Hoare-style logic for the verification of programs with higher-order control, in the presence of dynamic state. This is done by designing a dependent type theory with first class callcc and abort operators, where pre- and postconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin and Power, and Jaskelioff, to reduce the annotation burden and enable verification by symbolic evaluation. We illustrate working with the logic by verifying a number of characteristic examples.",Callcc | Continuations | Dependent types | Hoare logic,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2013-11-12,Conference Paper,"Delbianco, Germán Andrés;Nanevski, Aleksandar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886023176,10.1109/ICPC.2013.6613829,Evaluating source code summarization techniques: Replication and expansion,"During software evolution a developer must investigate source code to locate then understand the entities that must be modified to complete a change task. To help developers in this task, Haiduc et al. proposed text summarization based approaches to the automatic generation of class and method summaries, and via a study of four developers, they evaluated source code summaries generated using their techniques. In this paper we propose a new topic modeling based approach to source code summarization, and via a study of 14 developers, we evaluate source code summaries generated using the proposed technique. Our study partially replicates the original study by Haiduc et al. in that it uses the objects, the instruments, and a subset of the summaries from the original study, but it also expands the original study in that it includes more subjects and new summaries. The results of our study both support the findings of the original and provide new insights into the processes and criteria that developers use to evaluate source code summaries. Based on our results, we suggest future directions for research on source code summarization. © 2013 IEEE.",Program comprehension | replication study | source code summarization | text summarization | topic modeling,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"Eddy, Brian P.;Robinson, Jeffrey A.;Kraft, Nicholas A.;Carver, Jeffrey C.",Include,
10.1016/j.jss.2022.111515,2-s2.0-84886000599,10.1109/ICPC.2013.6613830,Automatic generation of natural language summaries for Java classes,"Most software engineering tasks require developers to understand parts of the source code. When faced with unfamiliar code, developers often rely on (internal or external) documentation to gain an overall understanding of the code and determine whether it is relevant for the current task. Unfortunately, the documentation is often absent or outdated. © 2013 IEEE.",documentation generation | program comprehension | Source code summarization,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"Moreno, Laura;Aponte, Jairo;Sridhara, Giriprasad;Marcus, Andrian;Pollock, Lori;Vijay-Shanker, K.",Include,
10.1016/j.jss.2022.111515,2-s2.0-84885983807,10.1109/ICPC.2013.6613836,Quality analysis of source code comments,"A significant amount of source code in software systems consists of comments, i. e., parts of the code which are ignored by the compiler. Comments in code represent a main source for system documentation and are hence key for source code understanding with respect to development and maintenance. Although many software developers consider comments to be crucial for program understanding, existing approaches for software quality analysis ignore system commenting or make only quantitative claims. Hence, current quality analyzes do not take a significant part of the software into account. In this work, we present a first detailed approach for quality analysis and assessment of code comments. The approach provides a model for comment quality which is based on different comment categories. To categorize comments, we use machine learning on Java and C/C++ programs. The model comprises different quality aspects: by providing metrics tailored to suit specific categories, we show how quality aspects of the model can be assessed. The validity of the metrics is evaluated with a survey among 16 experienced software developers, a case study demonstrates the relevance of the metrics in practice. © 2013 IEEE.",,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"Steidl, Daniela;Hummel, Benjamin;Juergens, Elmar",Include,
10.1016/j.jss.2022.111515,2-s2.0-84886061522,10.1109/ICPC.2013.6613842,Extracting problematic API features from forum discussions,"Software engineering activities often produce large amounts of unstructured data. Useful information can be extracted from such data to facilitate software development activities, such as bug reports management and documentation provision. Online forums, in particular, contain extensive valuable information that can aid in software development. However, no work has been done to extract problematic API features from online forums. In this paper, we investigate ways to extract problematic API features that are discussed as a source of difficulty in each thread, using natural language processing and sentiment analysis techniques. Based on a preliminary manual analysis of the content of a discussion thread and a categorization of the role of each sentence therein, we decide to focus on a negative sentiment sentence and its close neighbors as a unit for extracting API features. We evaluate a set of candidate solutions by comparing tool-extracted problematic API design features with manually produced golden test data. Our best solution yields a precision of 89%. We have also investigated three potential applications for our feature extraction solution: (i) highlighting the negative sentence and its neighbors to help illustrate the main API feature; (ii) searching helpful online information using the extracted API feature as a query; (iii) summarizing the problematic features to reveal the 'hot topics' in a forum. © 2013 IEEE.",APIs | AWT/Swing | Design Feedback | Information Extraction | Online Forums,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"Zhang, Yingying;Hou, Daqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886005716,10.1109/ICPC.2013.6613843,Multi-dimensional exploration of API usage,"This paper is concerned with understanding API usage in a systematic, explorative manner for the benefit of both API developers and API users. There exist complementary, less explorative methods, e.g., based on code search, code completion, or API documentation. In contrast, our approach is highly interactive and can be seen as an extension of what IDEs readily provide today. Exploration is based on multiple dimensions: i) the hierarchically organized scopes of projects and APIs; ii) metrics of API usage (e.g., number of project classes extending API classes); iii) metadata for APIs; iv) project- versus API-centric views. We also provide the QUAATLAS corpus of Java projects which enhances the existing QUALITAS corpus to enable API-usage analysis. We implemented the exploration approach in an open-source, IDE-like, Web-enabled tool EXAPUS. © 2013 IEEE.",API usage | code exploration | EXAPUS | metadata | program comprehension | QUAATLAS | QUALITAS | reverse engineering,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"De Roover, Coen;Lammel, Ralf;Pek, Ekaterina",Include,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886016562,10.1109/ICPC.2013.6613846,Building extensions for applications: Towards the understanding of extension possibilities,"Software extensions enable developers to introduce new features to a software system for supporting new requirements. In order for a developer to build an extension for a certain software system, the developer has to understand what extension possibilities exist, which software artifacts provide these possibilities, the constraints and dependencies between the extensible software artifacts, and how to correctly implement an extension. Building extensions for multilayered applications can be very challenging. For example, a simple user interface extension in a business application can require a developer to consider extensible artifacts from underlying user interfaces, business processes, databases, and code. In commercial applications, extension developers can depend on classical means like APIs, frameworks, documentation, tutorials, and example code provided by the software provider to understand the extension possibilities and how to successfully implement, run, and deploy an extension. For complex multilayered applications, relying on such classical means can be very hard and time-consuming for the extension developers. In IDEs, various program comprehension tools and approaches have helped developers in carrying out development tasks. However, most of the tools focus on the code level, lack the support for multilayered applications, and do not particularly focus on extensibility. In this paper we investigate the resources and methods that extension developers currently depend on for implementing extensions and we evaluate their effectiveness in a study of extension developers performing extension development tasks for a complex business application. Based on the results of our study, we identify the problems and challenges that face extension developers and we propose requirements that program comprehension tools should support to aid extension developers. © 2013 IEEE.",,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"Aly, Mohamed;Charfi, Anis;Mezini, Mira",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84886082527,10.1109/ICPC.2013.6613850,"Blogging developer knowledge: Motivations, challenges, and future directions","Why do software developers place so much effort into writing public blog posts about their knowledge, experiences, and opinions on software development? What are the benefits, problems, and tools needed - what can the research community do to help? In this paper, we describe a research agenda aimed at understanding the motivations and issues of software development blogging. We interviewed developers as well as mined and analyzed their blog posts. For this initial study, we selected developers from various backgrounds: IDE plugin development, mobile development, and web development. We found that developers used blogging for a variety of functions such as documentation, technology discussion, and announcing progress. They were motivated by a variety of reasons such as personal branding, knowledge retention, and feedback. Among the challenges for blog authors identified in our initial study, we found primitive tool support, difficulty recreating and recalling recent development experiences, and management of blog comments. Finally, many developers expressed that the motivations and benefits they received for blogging in public did not directly translate to corporate settings. © 2013 IEEE.",,IEEE International Conference on Program Comprehension,2013-10-28,Conference Paper,"Parnin, Chris;Treude, Christoph;Storey, Margaret Anne",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886378035,10.1109/ICSE.2013.6606569,Aluminum: Principled scenario exploration through minimality,"Scenario-finding tools such as Alloy are widely used to understand the consequences of specifications, with applications to software modeling, security analysis, and verification. This paper focuses on the exploration of scenarios: which scenarios are presented first, and how to traverse them in a well-defined way. We present Aluminum, a modification of Alloy that presents only minimal scenarios: those that contain no more than is necessary. Aluminum lets users explore the scenario space by adding to scenarios and backtracking. It also provides the ability to find what can consistently be used to extend each scenario. We describe the semantic basis of Aluminum in terms of minimal models of first-order logic formulas. We show how this theory can be implemented atop existing SAT-solvers and quantify both the benefits of minimality and its small computational overhead. Finally, we offer some qualitative observations about scenario exploration in Aluminum. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Nelson, Tim;Saghafi, Salman;Dougherty, Daniel J.;Fisler, Kathi;Krishnamurthi, Shriram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883727000,10.1109/ICSE.2013.6606570,Counter play-out: Executing unrealizable scenario-based specifications,"The scenario-based approach to the specification and simulation of reactive systems has attracted much research efforts in recent years. While the problem of synthesizing a controller or a transition system from a scenario-based specification has been studied extensively, no work has yet effectively addressed the case where the specification is unrealizable and a controller cannot be synthesized. This has limited the effectiveness of using scenario-based specifications in requirements analysis and simulation. In this paper we present counter play-out, an interactive debugging method for unrealizable scenario-based specifications. When we identify an unrealizable specification, we generate a controller that plays the role of the environment and lets the engineer play the role of the system. During execution, the former chooses environment's moves such that the latter is forced to eventually fail in satisfying the system's requirements. This results in an interactive, guided execution, leading to the root causes of unrealizability. The generated controller constitutes a proof that the specification is conflicting and cannot be realized. Counter play-out is based on a counter strategy, which we compute by solving a Rabin game using a symbolic, BDD-based algorithm. The work is implemented and integrated with PlayGo, an IDE for scenario-based programming developed at the Weizmann Institute of Science. Case studies show the contribution of our work to the state-of-the-art in the scenario-based approach to specification and simulation. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Maoz, Shahar;Sa'Ar, Yaniv",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886388235,10.1109/ICSE.2013.6606571,Unifying FSM-inference algorithms through declarative specification,"Logging system behavior is a staple development practice. Numerous powerful model inference algorithms have been proposed to aid developers in log analysis and system understanding. Unfortunately, existing algorithms are difficult to understand, extend, and compare. This paper presents InvariMint, an approach to specify model inference algorithms declaratively. We applied InvariMint to two model inference algorithms and present evaluation results to illustrate that InvariMint (1) leads to new fundamental insights and better understanding of existing algorithms, (2) simplifies creation of new algorithms, including hybrids that extend existing algorithms, and (3) makes it easy to compare and contrast previously published algorithms. Finally, algorithms specified with InvariMint can outperform their procedural versions. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Beschastnikh, Ivan;Brun, Yuriy;Abrahamson, Jenny;Ernst, Michael D.;Krishnamurthy, Arvind",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886421524,10.1109/ICSE.2013.6606572,What good are strong specifications?,"Experience with lightweight formal methods suggests that programmers are willing to write specification if it brings tangible benefits to their usual development activities. This paper considers stronger specifications and studies whether they can be deployed as an incremental practice that brings additional benefits without being unacceptably expensive. We introduce a methodology that extends Design by Contract to write strong specifications of functional properties in the form of preconditions, postconditions, and invariants. The methodology aims at being palatable to developers who are not fluent in formal techniques but are comfortable with writing simple specifications. We evaluate the cost and the benefits of using strong specifications by applying the methodology to testing data structure implementations written in Eiffel and C#. In our extensive experiments, testing against strong specifications detects twice as many bugs as standard contracts, with a reasonable overhead in terms of annotation burden and run-time performance while testing. In the wide spectrum of formal techniques for software quality, testing against strong specifications lies in a 'sweet spot' with a favorable benefit to effort ratio. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Polikarpova, Nadia;Furia, Carlo A.;Pei, Yu;Wei, Yi;Meyer, Bertrand",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886400702,10.1109/ICSE.2013.6606578,Detecting deadlock in programs with data-centric synchronization,"Previously, we developed a data-centric approach to concurrency control in which programmers specify synchronization constraints declaratively, by grouping shared locations into atomic sets. We implemented our ideas in a Java extension called AJ, using Java locks to implement synchronization. We proved that atomicity violations are prevented by construction, and demonstrated that realistic Java programs can be refactored into AJ without significant loss of performance. This paper presents an algorithm for detecting possible deadlock in AJ programs by ordering the locks associated with atomic sets. In our approach, a type-based static analysis is extended to handle recursive data structures by considering programmer-supplied, compiler-verified lock ordering annotations. In an evaluation of the algorithm, all 10 AJ programs under consideration were shown to be deadlock-free. One program needed 4 ordering annotations and 2 others required minor refactorings. For the remaining 7 programs, no programmer intervention of any kind was required. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Marino, Daniel;Hammer, Christian;Dolby, Julian;Vaziri, Mandana;Tip, Frank;Vitek, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886412891,10.1109/ICSE.2013.6606593,Beyond Boolean product-line model checking: Dealing with feature attributes and multi-features,"Model checking techniques for software product lines (SPL) are actively researched. A major limitation they currently have is the inability to deal efficiently with non-Boolean features and multi-features. An example of a non-Boolean feature is a numeric attribute such as maximum number of users which can take different numeric values across the range of SPL products. Multi-features are features that can appear several times in the same product, such as processing units which number is variable from one product to another and which can be configured independently. Both constructs are extensively used in practice but currently not supported by existing SPL model checking techniques. To overcome this limitation, we formally define a language that integrates these constructs with SPL behavioural specifications. We generalize SPL model checking algorithms correspondingly and evaluate their applicability. Our results show that the algorithms remain efficient despite the generalization. © 2013 IEEE.",Feature Cardinalities | Model Checking | Numeric Features | Semantics | Software Product Lines | Tools,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Cordy, Maxime;Schobbens, Pierre Yves;Heymans, Patrick;Legay, Axel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886411392,10.1109/ICSE.2013.6606604,Analysis of user comments: An approach for software requirements evolution,"User feedback is imperative in improving software quality. In this paper, we explore the rich set of user feedback available for third party mobile applications as a way to extract new/changed requirements for next versions. A potential problem using this data is its volume and the time commitment involved in extracting new/changed requirements. Our goal is to alleviate part of the process through automatic topic extraction. We process user comments to extract the main topics mentioned as well as some sentences representative of those topics. This information can be useful for requirements engineers to revise the requirements for next releases. Our approach relies on adapting information retrieval techniques including topic modeling and evaluating them on different publicly available data sets. Results show that the automatically extracted topics match the manually extracted ones, while also significantly decreasing the manual effort. © 2013 IEEE.",Information Retrieval | Requirements | Software Evolution | Topic Modeling | User Comments | User feedback,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Carreno, Laura V.Galvis;Winbladh, Kristina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886399799,10.1109/ICSE.2013.6606612,Automated software architecture security risk analysis using formalized signatures,"Reviewing software system architecture to pinpoint potential security flaws before proceeding with system development is a critical milestone in secure software development lifecycles. This includes identifying possible attacks or threat scenarios that target the system and may result in breaching of system security. Additionally we may also assess the strength of the system and its security architecture using well-known security metrics such as system attack surface, Compartmentalization, least-privilege, etc. However, existing efforts are limited to specific, predefined security properties or scenarios that are checked either manually or using limited toolsets. We introduce a new approach to support architecture security analysis using security scenarios and metrics. Our approach is based on formalizing attack scenarios and security metrics signature specification using the Object Constraint Language (OCL). Using formal signatures we analyse a target system to locate signature matches (for attack scenarios), or to take measurements (for security metrics). New scenarios and metrics can be incorporated and calculated provided that a formal signature can be specified. Our approach supports defining security metrics and scenarios at architecture, design, and code levels. We have developed a prototype software system architecture security analysis tool. To the best of our knowledge this is the first extensible architecture security risk analysis tool that supports both metric-based and scenario-based architecture security analysis. We have validated our approach by using it to capture and evaluate signatures from the NIST security principals and attack scenarios defined in the CAPEC database. © 2013 IEEE.",Architecture Security Risk analysis | Common attack patterns enumeration and classification (CAPEC) | Formal attack patterns specification | Software security,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Almorsy, Mohamed;Grundy, John;Ibrahim, Amani S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886394450,10.1109/ICSE.2013.6606617,"Expectations, outcomes, and challenges of modern code review","Code review is a common software engineering practice employed both in open source and industrial contexts. Review today is less formal and more 'lightweight' than the code inspections performed and studied in the 70s and 80s. We empirically explore the motivations, challenges, and outcomes of tool-based code reviews. We observed, interviewed, and surveyed developers and managers and manually classified hundreds of review comments across diverse teams at Microsoft. Our study reveals that while finding defects remains the main motivation for review, reviews are less about defects than expected and instead provide additional benefits such as knowledge transfer, increased team awareness, and creation of alternative solutions to problems. Moreover, we find that code and change understanding is the key aspect of code reviewing and that developers employ a wide range of mechanisms to meet their understanding needs, most of which are not met by current tools. We provide recommendations for practitioners and researchers. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Bacchelli, Alberto;Bird, Christian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886395063,10.1109/ICSE.2013.6606627,Reverb: Recommending code-related web pages,"The web is an important source of development-related resources, such as code examples, tutorials, and API documentation. Yet existing development environments are largely disconnected from these resources. In this work, we explore how to provide useful web page recommendations to developers by focusing on the problem of refinding web pages that a developer has previously used. We present the results of a study about developer browsing activity in which we found that 13.7% of developers visits to code-related pages are revisits and that only a small fraction (7.4%) of these were initiated through a low-cost mechanism, such as a bookmark. To assist with code-related revisits, we introduce Reverb, a tool which recommends previously visited web pages that pertain to the code visible in the developer's editor. Through a field study, we found that, on average, Reverb can recommend a useful web page in 51% of revisitation cases. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Sawadsky, Nicholas;Murphy, Gail C.;Jiresal, Rahul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886395205,10.1109/ICSE.2013.6606629,Discovering essential code elements in informal documentation,"To access the knowledge contained in developer communication, such as forum posts, it is useful to determine automatically the code elements referred to in the discussions. We propose a novel traceability recovery approach to extract the code elements contained in various documents. As opposed to previous work, our approach does not require an index of code elements to find links, which makes it particularly well-suited for the analysis of informal documentation. When evaluated on 188 StackOverflow answer posts containing 993 code elements, the technique performs with average 0.92 precision and 0.90 recall. As a major refinement on traditional traceability approaches, we also propose to detect which of the code elements in a document are salient, or germane, to the topic of the post. To this end we developed a three-feature decision tree classifier that performs with a precision of 0.65-0.74 and recall of 0.30-0.65, depending on the subject of the document. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Rigby, Peter C.;Robillard, Martin P.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84886395834,10.1109/ICSE.2013.6606648,A study of enabling factors for rapid fielding combined practices to balance speed and stability,"Agile projects are showing greater promise in rapid fielding as compared to waterfall projects. However, there is a lack of clarity regarding what really constitutes and contributes to success. We interviewed project teams with incremental development lifecycles, from five government and commercial organizations, to gain a better understanding of success and failure factors for rapid fielding on their projects. A key area we explored involves how Agile projects deal with the pressure to rapidly deliver high-value capability, while maintaining project speed (delivering functionality to the users quickly) and product stability (providing reliable and flexible product architecture). For example, due to schedule pressure we often see a pattern of high initial velocity for weeks or months, followed by a slowing of velocity due to stability issues. Business stakeholders find this to be disruptive as the rate of capability delivery slows while the team addresses stability problems. We found that experienced practitioners, when faced with these challenges, do not apply Agile practices alone. Instead they combine practices - Agile, architecture, or other - in creative ways to respond quickly to unanticipated stability problems. In this paper, we summarize the practices practitioners we interviewed from Agile projects found most valuable and provide an overarching scenario that provides insight into how and why these practices emerge. © 2013 IEEE.",agile software development | architecture | rapid fielding | software development practices | speed | stability,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Bellomo, Stephany;Nord, Robert L.;Ozkaya, Ipek",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886384968,10.1109/ICSE.2013.6606661,Teaching developer skills in the first software engineering course,"Both employers and graduate schools expect computer science graduates to be able to work as developers on software projects. Software engineering courses present the opportunity in the curriculum to learn the relevant skills. This paper presents our experience from Wayne State University and reviews challenges and constraints that we faced while trying to teach these skills. In our first software engineering course, we teach the iterative software development that includes practices of software change, summarized in the phased model of software change. The required resources for our software engineering course are comparable to the other computer science courses. The students - while working in teams - are graded based on their individual contribution to the team effort rather than on the work of the other team members, which improves the fairness of the grading and considerably lessens the stress for the best students in the course. Our students have expressed a high level of satisfaction, and in a survey, they indicated that the skills that they learned in the course are highly applicable to their careers. © 2013 IEEE.",actualization | concept location | developer role | evolutionary-iterative-agile development | First software engineering course | impact analysis | open source | phased model of software change | project technologies | realistic code | refactoring,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Rajlich, Vaclav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886410551,10.1109/ICSE.2013.6606681,Supporting application development with structured queries in the cloud,"To facilitate software development for multiple, federated cloud systems, abstraction layers have been introduced to mask the differences in the offerings, APIs, and terminology of various cloud providers. Such layers rely on a common ontology, which a) is difficult to create, and b) requires developers to understand both the common ontology and how various providers deviate from it. In this paper we propose and describe a structured query language for the cloud, Cloud SQL, along with a system and methodology for acquiring and organizing information from cloud providers and other entities in the cloud ecosystem such that it can be queried. It allows developers to run queries on data organized based on their semantic understanding of the cloud. Like the original SQL, we believe the use of a declarative query language will reduce development costs and make the multi-cloud accessible to a broader set of developers. © 2013 IEEE.",adaptive systems | cloud computing | query languages | software development,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Smit, Michael;Simmons, Bradley;Shtern, Mark;Litoiu, Marin",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886387055,10.1109/ICSE.2013.6606690,Computational alignment of goals and scenarios for complex systems,"The purpose of requirements validation is to determine whether a large requirements set will lead to the achievement of system-related goals under different conditions - a task that needs automation if it is to be performed quickly and accurately. One reason for the current lack of software tools to undertake such validation is the absence of the computational mechanisms needed to associate scenario, system specification and goal analysis tools. Therefore, in this paper, we report first research experiments in developing these new capabilities, and demonstrate them with a non-trivial example associated with a Rolls Royce aircraft engine software component. © 2013 IEEE.",goal achievement | operational requirements | Requirements validation | scenarios,Proceedings - International Conference on Software Engineering,2013-01-01,Conference Paper,"Alrajeh, Dalai;Russo, Alessandra;Lockerbie, James;Maiden, Neil;Mavin, Alistair;Novak, Mark",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886417033,10.1109/ICSE.2013.6606692,Formal specifications better than function points for code sizing,"Size and effort estimation is a significant challenge for the management of large-scale formal verification projects. We report on an initial study of relationships between the sizes of artefacts from the development of seL4, a formally-verified embedded systems microkernel. For each API function we first determined its COSMIC Function Point (CFP) count (based on the seL4 user manual), then sliced the formal specifications and source code, and performed a normalised line count on these artefact slices. We found strong and significant relationships between the sizes of the artefact slices, but no significant relationships between them and the CFP counts. Our finding that CFP is poorly correlated with lines of code is based on just one system, but is largely consistent with prior literature. We find CFP is also poorly correlated with the size of formal specifications. Nonetheless, lines of formal specification correlate with lines of source code, and this may provide a basis for size prediction in future formal verification projects. In future work we will investigate proof sizing. © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Staples, Mark;Kolanski, Rafal;Klein, Gerwin;Lewis, Corey;Andronick, June;Murray, Toby;Jeffery, Ross;Bass, Len",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886394243,10.1109/ICSE.2013.6606718,Analyzing the change-proneness of service-oriented systems from an industrial perspective,"Antipatterns and code smells have been widely proved to affect the change-proneness of software components. However, there is a lack of studies that propose indicators of changes for service-oriented systems. Like any other software systems, such systems evolve to address functional and non functional requirements. In this research, we investigate the change-proneness of service-oriented systems from the perspective of software engineers. Based on the feedback from our industrial partners we investigate which indicators can be used to highlight change-prone application programming interfaces (APIs) and service interfaces in order to improve their reusability and response time. The output of this PhD research will assist software engineers in designing stable APIs and reusable services with adequate response time. © 2013 IEEE.",Antipatterns | change-proneness | metrics | Service-oriented systems | web services | WSDL,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Romano, Daniele",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886433702,10.1109/ICSE.2013.6606720,An approach to documenting and evolving architectural design decisions,"Software architecture is considered as a set of architectural design decisions (ADDs). Capturing and representing ADDs during the architecting process is necessary for reducing architectural knowledge evaporation. Moreover, managing the evolution of ADDs helps to maintain consistency between requirements and the deployed system. In this work, we create the Triple View Model (TVM) as a general architecture framework for documenting ADDs. The TVM clarifies the notion of ADDs in three different views and covers key features of the architecting process. Based on the TVM, we propose a scenario-based method (SceMethod) to manage the documentation and the evolution of ADDs. Furthermore, we also develop a UML metamodel that incorporates evolution-centered characteristics to manage evolutionary architectural knowledge. We conduct a case study to validate the applicability and the effectiveness of our model and method. In our future work, we plan to investigate how to support ADD documentation and evolution in geographically separated software development (GSD). © 2013 IEEE.",,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Che, Meiru",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886391532,10.1109/ICSE.2013.6606723,Normalizing source code vocabulary to support program comprehension and software quality,"The literature reports that source code lexicon plays a paramount role in program comprehension, especially when software documentation is scarce, outdated or simply not available. In source code, a significant proportion of vocabulary can be either acronyms and-or abbreviations or concatenation of terms that can not be identified using consistent mechanisms such as naming conventions. It is, therefore, essential to disambiguate concepts conveyed by identifiers to support program comprehension and reap the full benefit of Information Retrieval-based techniques (e.g., feature location and traceability) whose linguistic information (i.e., source code identifiers and comments) used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. To this aim, we propose source code vocabulary normalization approaches that exploit contextual information to align the vocabulary found in the source code with that found in other software artifacts. We were inspired in the choice of context levels by prior works and by our findings. Normalization consists of two tasks: splitting and expansion of source code identifiers. We also investigate the effect of source code vocabulary normalization approaches on software maintenance tasks. Results of our evaluation show that our contextual-aware techniques are accurate and efficient in terms of computation time than state of the art alternatives. In addition, our findings reveal that feature location techniques can benefit from vocabulary normalization when no dynamic information is available. © 2013 IEEE.",information retrieval | program comprehension | software quality | Source code linguistic analysis,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Guerrouj, Latifa",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886398352,10.1109/ICSE.2013.6606742,A roadmap for software maintainability measurement,"Object-Oriented Programming (OOP) is one of the most used programming paradigms. Thus, researches dedicated in improvement of software quality that adhere to this paradigm are demanded. Complementarily, maintainability is considered a software attribute that plays an important role in its quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years and several researchers proposed a high number of metrics to measure it. Nevertheless, there is no standardization or a catalogue to summarize all the information about these metrics, helping the researchers to make decision about which metrics can be adopted to perform their experiments in OOSM. Actually, distinct areas in both academic and industrial environment, such as Software Development, Project Management, and Software Research can adopt them to support decision-making processes. Thus, this work researched about the usage of OOSM metrics in academia and industry in order to help researchers in making decision about the metrics suite to be adopted. We found 570 OOSM metrics. Additionally, as a preliminary result we proposed a catalog with 36 metrics that were most used in academic works/experiments, trying to guide researchers with their decision-make about which metrics are more indicated to be adopted in their experiments. © 2013 IEEE.",Catalog | Experimental Software Engineering | Metrics | Object-Oriented Development | Software Maintainability,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Saraiva, Juliana",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886400676,10.1109/ICSE.2013.6606758,Software requirement patterns,"Software requirements reuse becomes a fundamental activity for those IT organizations that conduct requirements engineering processes in similar settings. One strategy to implement this reuse is by exploiting a catalogue of software requirement patterns (SRPs). In this tutorial, we provide an introduction to the concept of SRP, summarise several existing approaches, and reflect on the consequences on several requirements engineering processes and activities. We take one of these approaches, the PABRE framework, as exemplar for the tutorial and analyse in more depth the catalogue of SRP that is proposed. We apply the concepts given on a practical exercise. © 2013 IEEE.",Patterns | Requirements Engineering | Software Requirements Patterns | Software Reuse,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Franch, Xavier",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886425863,10.1109/ICSE.2013.6606761,1<sup>st</sup> International workshop on conducting empirical studies in industry (CESI 2013),"The quality of empirical studies is critical for the success of the Software Engineering (SE) discipline. More and more SE researchers are conducting empirical studies involving the software industry. While there are established empirical procedures, relatively little is known about the dynamics of conducting empirical studies in the complex industrial environments. What are the impediments and how to best handle them? This was the primary driver for organising CESI 2013. The goals of this workshop include having a dialogue amongst the participating practitioners and academics on the theme of this workshop with the aim to produce tangible output that will be summarised in a post-workshop report. © 2013 IEEE.",Empirical studies | software industry,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Franch, Xavier;Madhavji, Nazim H.;Curtis, Bill;Votta, Larry",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84886414712,10.1109/ICSE.2013.6606775,1<sup>st</sup> International workshop on natural language analysis in software engineering (NaturaLiSE 2013),"Software engineers produce code that has formal syntax and semantics, which establishes its formal meaning. However, the code also includes significant natural language found primarily in identifier names and comments. Furthermore, the code is surrounded by non-source artifacts, predominantly written in natural language. The NaturaLiSE workshop focuses on natural language analysis of software. The workshop brings together researchers and practitioners interested in exploiting natural language information to create improved software engineering tools. Participants will explore natural language analysis applied to software artifacts, combining natural language and traditional program analysis, integration of natural language analyses into client tools, mining natural language data, and empirical studies focused on evaluating the usefulness of natural language analysis. © 2013 IEEE.",Natural language analysis of software artifacts | software engineering tools | software evolution | textual analysis,Proceedings - International Conference on Software Engineering,2013-10-30,Conference Paper,"Pollock, Lori;Binkley, David;Lawrie, Dawn;Hill, Emily;Oliveto, Rocco;Bavota, Gabriele;Bacchelli, Alberto",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891680358,10.1109/ICSM.2013.17,Content categorization of API discussions,"Text categorization, automatically labeling natural language text with pre-defined semantic categories, is an essential task for managing the abundant online data. An example of such data in Software Engineering is the large, ever-growing volume of forum discussions on how to use particular APIs. We have conducted a study to explore the question as to how well machine learning algorithms can be applied to categorize API discussions based on their content. Our goal is two-fold: (1) Can a relatively straightforward algorithm such as Naive Bayes work sufficiently well for this task? (2) If yes, how can we control its performance? We have achieved the best test accuracy mean (TAM) of 94.1% with our largest training data set for the AWT/Swing API, which consists of 833 forum discussions distributed over eight categories/topics. We have also investigated factors that impact classification accuracy, with the most important two being the size of the training set and multi-label documents (the phenomenon that some discussions involve more than one category). © 2013 IEEE.",APIs | AWT/Swing | MALLET | Naive Bayes | Online Forums | Text Categorization,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Hou, Daqing;Mo, Lingfeng",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84891682835,10.1109/ICSM.2013.18,An empirical study of API stability and adoption in the android ecosystem,"When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28% of API references in client applications are outdated with a median lagging time of 16 months. 22% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability. © 2013 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"McDonnell, Tyler;Ray, Baishakhi;Kim, Miryung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891673616,10.1109/ICSM.2013.32,An empirical investigation on documentation usage patterns in maintenance tasks,"When developers perform a software maintenance task, they need to identify artifacts-e.g., classes or more specifically methods-that need to be modified. To this aim, they can browse various kind of artifacts, for example use case descriptions, UML diagrams, or source code. This paper reports the results of a study-conducted with 33 participants-aimed at investigating (i) to what extent developers use different kinds of documentation when identifying artifacts to be changed, and (ii) whether they follow specific navigation patterns among different kinds of artifacts. Results indicate that, although participants spent a conspicuous proportion of the available time by focusing on source code, they browse back and forth between source code and either static (class) or dynamic (sequence) diagrams. Less frequently, participants-especially more experienced ones-follow an 'integrated' approach by using different kinds of artifacts. © 2013 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Bavota, Gabriele;Canfora, Gerardo;Penta, Massimiliano Di;Oliveto, Rocco;Panichella, Sebastiano",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84891668678,10.1109/ICSM.2013.39,The evolution of project inter-dependencies in a software ecosystem: The case of apache,"Software ecosystems consist of multiple software projects, often interrelated each other by means of dependency relations. When one project undergoes changes, other projects may decide to upgrade the dependency. For example, a project could use a new version of another project because the latter has been enhanced or subject to some bug-fixing activities. This paper reports an exploratory study aimed at observing the evolution of the Java subset of the Apache ecosystem, consisting of 147 projects, for a period of 14 years, and resulting in 1,964 releases. Specifically, we analyze (i) how dependencies change over time, (ii) whether a dependency upgrade is due to different kinds of factors, such as different kinds of API changes or licensing issues, and (iii) how an upgrade impacts on a related project. Results of this study help to comprehend the phenomenon of library/component upgrade, and provides the basis for a new family of recommenders aimed at supporting developers in the complex (and risky) activity of managing library/component upgrade within their software projects. © 2013 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Bavota, Gabriele;Canfora, Gerardo;Penta, Massimiliano Di;Oliveto, Rocco;Panichella, Sebastiano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891720029,10.1109/ICSM.2013.43,Enhancing software traceability by automatically expanding corpora with relevant documentation,"Software trace ability is the ability to describe and follow the life of a requirement in both a forward and backward direction by defining relationships to related development artifacts. A plethora of different trace ability recovery approaches use information retrieval techniques, which depend on the quality of the textual information in requirements and software artifacts. Not only is it important that stakeholders use meaningful names in these artifacts, but also it is crucial that the same names are used to specify the same concepts in different artifacts. Unfortunately, the latter is difficult to enforce and as a result, software trace ability approaches are not as efficient and effective as they could be - to the point where it is questionable whether the anticipated economic and quality benefits were indeed achieved. We propose a novel and automatic approach for expanding corpora with relevant documentation that is obtained using external function call documentation and sets of relevant words, which we implemented in Trace Lab. We experimented with three Java applications and we show that using our approach the precision of recovering trace ability links was increased by up to 31% in the best case and by approximately 9% on average. © 2013 IEEE.",API call | Machine learning | Software traceability,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Dasgupta, Tathagata;Grechanik, Mark;Moritz, Evan;Dit, Bogdan;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891672554,10.1109/ICSM.2013.61,Towards understanding large-scale adaptive changes from version histories,"A case study of three open source systems undergoing large adaptive maintenance tasks is presented. The adaptive maintenance task involves migrating each system to a new version of a third party API. The changes to support the migration were spread out over multiple years for each system. The first two systems are both part of KDE, namely KOffice and Extra gear/graphics. The adaptive maintenance task, for both systems, involves migrating to a new version of Qt. The third system is OpenSceneGraph that underwent a migration to a new version of OpenGL. The case study involves sifting through tens of thousands of commits to identify only those commits involved in the specific adaptive maintenance task. The object is to develop a data set that will be used for developing automated methods to identify/characterize adaptive maintenance commits. © 2013 IEEE.",Adaptive maintenance | Commit types | Maintenance classification,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Meqdadi, Omar;Alhindawi, Nouh;Collard, Michael L.;Maletic, Jonathan I.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891666910,10.1109/ICSM.2013.70,On the relationship between the vocabulary of bug reports and source code,"Text retrieval (TR) techniques have been widely used to support concept and bug location. When locating bugs, developers often formulate queries based on the bug descriptions. More than that, a large body of research uses bug descriptions to evaluate bug location techniques using TR. The implicit assumption is that the bug descriptions and the relevant source code files share important words. In this paper, we present an empirical study that explores this conjecture. We found that bug reports share more terms with the patched classes than with the other classes in the system. Furthermore, we found that the class names are more likely to share terms with the bug descriptions than other code locations, while more verbose parts of the code (e.g., comments) will share more words. We also found that the shared terms may be better predictors for bug location than some TR techniques. © 2013 IEEE.",Bug location | Source code vocabulary | Text retrieval,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Moreno, Laura;Bandara, Wathsala;Haiduc, Sonia;Marcus, Andrian",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891684636,10.1109/ICSM.2013.80,ECITY: A tool to track software structural changes using an evolving city,"One of the main challenges in the maintenance of large-scale software systems is to ascertain the underlying software structure and to analyze its evolution. In this paper we present a tool to assist software architects and developers in not only understanding the software structure of their system but more importantly to track the insertion, removal, or modification of components over time. The tool is based on the idea that the above-mentioned stakeholders should have an intuitive, efficient, and effective means to detect when, where, and what structural changes took place. The main components include an interactive visualization that provides an overview of these changes. The usefulness of this approach is highlighted through a summary of a user study we conducted. © 2013 IEEE.",Software architecture visualization | Software evolution | Software maintenance,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Khan, Taimur;Barthel, Henning;Ebert, Achim;Liggesmeyer, Peter",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891675311,10.1109/ICSM.2013.87,E-xplore: Enterprise API explorer,Plenty of open source libraries and frameworks are available for developers these days for reuse in their projects. However the difficulty in finding and reusing the correct API among the hundreds of available APIs far outweighs the advantage of saving time. The problem is acute in enterprise code base. Online forums like Stack Overflow are no help for enterprise source code as they are closed in nature. We have developed a tool called E-Xplore that addresses this issue by letting the programmers search in large source code base and browse them effectively. The tool also provides related artifacts in the form of result clustering. We evaluated E-Xplore with other tools via user study with developers working on an enterprise banking system with more than 10 million lines of code. A set of common tasks was given to the developers with and without the tool. We observed that the tool offered appreciable time and effort benefits in large scale software system development and maintenance. In this paper we describe the tool and its features which help develop and maintain source code effectively. © 2013 IEEE.,API | API Exploration | E-Xplore | Search | Semantic search | Source code,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Asadullah, Allahbaksh M.;Basavaraju, M.;Jain, Nikita",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891709165,10.1109/ICSM.2013.93,Large-scale automated refactoring using ClangMR,"Maintaining large code bases can be a challenging endeavour. As new libraries, APIs and standards are introduced, old code is migrated to use them. To provide as clean and succinct an interface as possible for developers, old APIs are ideally removed as new ones are introduced. In practice, this becomes difficult as automatically finding and transforming code in a semantically correct way can be challenging, particularly as the size of a code base increases. In this paper, we present a real-world implementation of a system to refactor large C++ code bases efficiently. A combination of the Clang compiler framework and the MapReduce parallel processor, ClangMR enables code maintainers to easily and correctly transform large collections of code. We describe the motivation behind such a tool, its implementation and then present our experiences using it in a recent API update with Google's C++ code base. © 2013 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Wright, Hyrum K.;Jasper, Daniel;Klimek, Manuel;Carruth, Chandler;Wan, Zhanyong",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891692517,10.1109/ICSM.2013.95,The adventure of developing a software application on a pre-release platform: Features and learned lessons,"From September 2011 to May 2012, Microsoft delivered three pre-releases of Windows 8. Its principal goal was to present the features of this operating system (OS) and motivate the development of new applications to this platform. However, as any pre-release version, several parts of the code and related documentations were incomplete. Furthermore, its APIs were very prone to be changed until the final release. Even considering such risks, the company X decided to face these challenges and create an application to be embedded into Windows 8 tablets as soon as this OS was released. This paper discusses interesting aspects of this adventure and some lessons that may be important to development teams that intend to trail this kind of project. Metrics are also used to better characterize some of these aspects. © 2013 IEEE.",Metrics | Risks management | Software development,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Siebra, Clauirton;Mascaro, Angelica;Silva, Fabio Q.B.;Santos, Andre L.M.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891714422,10.1109/ICSM.2013.96,Analysis of multi-dimensional code couplings,"Software systems consist of hundreds or thousands of files, which are usually not independent of each other but coupled. While it is obvious that structural dependencies like method calls or data accesses create couplings, there also exist other, more indirect forms of coupling that should be considered when modifying, extending, or debugging a system. In contrast to most previous research, in this work, code coupling is considered as a multi-dimensional construct: several forms of structural couplings are contrasted to couplings based on the history and the semantics of the source code entities. The work proposes two novel visualization techniques, which allow for exploring and visually comparing different concepts of coupling. Based on an empirical study on open source systems, the work further provides insights into the relationship between concepts of coupling and the modularization of software, first evidence on the usage of modularization principles can be derived thereof. Finally, a new application for adapting the modularization of a software system-component extraction-is introduced and tested with varying coupling data. This work summarizes the doctoral thesis of the author, suggests directions for future research, and reports lessons learned. © 2013 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Beck, Fabian",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891720206,10.1109/ICSM.2013.97,How good are code smells for evaluating software maintainability? Results from a comparative case study,"An advantage of code smells over traditional software measures is that the former are associated with an explicit set of refactorings to improve the existing design. Past research on code smells has emphasized the formalization and automated detection of code smells, but much less has been done to empirically investigate how good are code smells for evaluating software maintainability. This paper presents a summary of the findings in the thesis by Yamashita, which aimed at investigating the strengths and limitations of code smells for evaluating software maintainability. The study conducted comprised an outsourced maintenance project involving four Java web systems with equivalent functionality but dissimilar implementation, six software professionals, and two software companies. A main result from the study is that the usefulness of code smells differs according to the granularity level (e.g., whether the assessment is done at file or system level) and the particular operationalization of maintainability (e.g., maintainability can be measured via maintenance effort, or problems encountered during maintenance, etc). This paper summarises the most relevant findings from the thesis, discusses a series of lessons learned from conducting this study, and discusses avenues for new research in the area of code smells. © 2013 IEEE.",Bad smells | Code smells | Comparative Case Study | Empirical study | Software maintenance | Software quality,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Yamashita, Aiko",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891692973,10.1109/ICSM.2013.98,"Refactoring planning for design smell correction: Summary, opportunities and lessons learned","Complex refactoring processes, such as applying big refactorings or removing design smells are difficult to perform in practice. The complexity of these processes is partly due to their heuristic nature and to the constraints imposed by preconditions on the applicability of the individual refactorings. Developers have to find out manually how to apply a complex refactoring ""recipe"", from a refactoring book, for each particular situation. In a PhD thesis, we developed an approach for tackling this problem. We described how to better write refactoring ""recipes"" (Refactoring Strategies) and how to compute, from them, the precise refactoring sequences for each particular situation (Refactoring Plans). Our proposal introduced, for the first time, the use of automated planning for this kind of software engineering problems. This paper presents a short summary of that PhD thesis and discuss the future work, open questions, new research opportunities arisen and the lessons learned from it. © 2013 IEEE.",Automated planning | Design smells | Refactoring | Refactoring planning | Refactoring strategies,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Pérez, Javier",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84893308759,10.1109/ISSRE.2013.6698904,Composing hierarchical stochastic model from SysML for system availability analysis,"Comprehensive analytic model for system availability analysis often confronts the largeness issue where a system designer cannot easily handle the model and the solution is not given in a feasible solution time. Hierarchical decomposition of a large state-space model gives a promising solution to the largeness issue when the model is decomposable. However, the decomposability of analytic model is not always manually tractable especially when the model is generated in an automated manner. In this paper, we propose an automated model composition technique from a system design to a hierarchical stochastic model which is the judicious combination of combinatorial and state-space models. In particular, from SysML-based system specifications, a top-level fault tree and associated stochastic reward nets are automatically generated in hierarchical manner. The obtained hierarchical stochastic model can be solved analytically considerably faster than monolithic state-space models. Through an illustrative example of three-tier web application system on a virtualized infrastructure, the accuracy and efficiency of the solution are evaluated in comparison to a monolithic state space model and a static fault tree. © 2013 IEEE.",automated model composition | availability analysis | model decomposition | stochastic model | web application system,"2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013",2013-12-01,Conference Paper,"Machida, Fumio;Xiang, Jianwen;Tadano, Kumiko;Maeno, Yoshiharu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893225868,10.1109/ISSRE.2013.6698913,Filtering noise in mixed-purpose fixing commits to improve defect prediction and localization,"In open-source software projects, during fixing software faults, developers sometimes also perform other types of non-fixing code changes such as functionality enhancement, code restructuring/improving, or documentation. They commit non-fixing changes together with the fixing ones in the same transaction. We call them mixed-purpose fixing commits (MFCs). We have conducted an empirical study on MFCs in several popular open-source projects. Our results showed that MFCs are about 11%-39% of total fixing commits. In 3%-41% of MFCs, developers performed other change types without indicating them in the commit logs. Our study also showed that mining software repositories (MSR) approaches that rely on the recovery of the history of fixed/buggy files are affected by the noisy data where non-fixing changes in MFCs are considered as fixing ones. The results of our study motivated us to develop Cardo, a tool to identify MFCs and filter non-fixing changed files in the change sets of the fixing commits. It uses natural language processing to analyze the sentences in commit logs and program analysis to cluster the changes in the change sets to determine if a changed file is for non-fixing. Our empirical evaluation on several open-source projects showed that Cardo achieves on average 93% precision, and existing MSR approaches can be relatively improved up to 32% with data filtered by Cardo. © 2013 IEEE.",,"2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013",2013-12-01,Conference Paper,"Nguyen, Hoan Anh;Nguyen, Anh Tuan;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893250508,10.1109/ISSRE.2013.6698925,Sinfer: Inferring information flow lattices for checking self-stabilization,Self-stabilizing programs are guaranteed to recover from state corruption caused by software bugs or other events and eventually reach the correct state. Many real-world applications including embedded controllers and multimedia applications can be designed to make key components self-stabilizing. Type systems and static analyses can automatically check whether a program is self stabilizing. The existing approach for checking self-stabilization requires developers to manually annotate code. We present an annotation inference algorithm that automatically derives an initial set of annotations and therefore lowers the effort to build self-stabilizing systems. Our experience with the algorithm indicates that it effectively inferred annotations for our benchmarks. © 2013 IEEE.,,"2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013",2013-12-01,Conference Paper,"Hun Eom, Yong;Demsky, Brian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84889034226,10.1109/MSR.2013.6623998,Which work-item updates need your response?,"Work-item notifications alert the team collaborating on a work-item about any update to the work-item (e.g., addition of comments, change in status). However, as software professionals get involved with multiple tasks in project(s), they are inundated by too many notifications from the work-item tool. Users are upset that they often miss the notifications that solicit their response in the crowd of mostly useless ones. We investigate the severity of this problem by studying the work-item repositories of two large collaborative projects and conducting a user study with one of the project teams. We find that, on an average, only 1 out of every 5 notifications that are received by the users require a response from them. We propose TWINY - a machine learning based approach to predict whether a notification will prompt any action from its recipient. Such a prediction can help to suitably mark up notifications and to decide whether a notification needs to be sent out immediately or be bundled in a message digest. We conduct empirical studies to evaluate the efficacy of different classification techniques in this setting. We find that incremental learning algorithms are ideally suited, and ensemble methods appear to give the best results in terms of prediction accuracy. © 2013 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Mukherjee, Debdoot;Garg, Malika",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84889028624,10.1109/MSR.2013.6624001,Retrieving and analyzing mobile apps feature requests from online reviews,"Mobile app reviews are valuable repositories of ideas coming directly from app users. Such ideas span various topics, and in this paper we show that 23.3% of them represent feature requests, i.e. comments through which users either suggest new features for an app or express preferences for the re-design of already existing features of an app. One of the challenges app developers face when trying to make use of such feedback is the massive amount of available reviews. This makes it difficult to identify specific topics and recurring trends across reviews. Through this work, we aim to support such processes by designing MARA (Mobile App Review Analyzer), a prototype for automatic retrieval of mobile app feature requests from online reviews. The design of the prototype is a) informed by an investigation of the ways users express feature requests through reviews, b) developed around a set of pre-defined linguistic rules, and c) evaluated on a large sample of online reviews. The results of the evaluation were further analyzed using Latent Dirichlet Allocation for identifying common topics across feature requests, and the results of this analysis are reported in this paper. © 2013 IEEE.",Feature requests | Mobile apps | Online reviews,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Iacob, Claudia;Harrison, Rachel",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84889012812,10.1109/MSR.2013.6624005,Deficient documentation detection: A methodology to locate deficient project documentation using topic analysis,"A project's documentation is the primary source of information for developers using that project. With hundreds of thousands of programming-related questions posted on programming Q&A websites, such as Stack Overflow, we question whether the developer-written documentation provides enough guidance for programmers. In this study, we wanted to know if there are any topics which are inadequately covered by the project documentation. We combined questions from Stack Overflow and documentation from the PHP and Python projects. Then, we applied topic analysis to this data using latent Dirichlet allocation (LDA), and found topics in Stack Overflow that did not overlap the project documentation. We successfully located topics that had deficient project documentation. We also found topics in need of tutorial documentation that were outside of the scope of the PHP or Python projects, such as MySQL and HTML. © 2013 IEEE.",Documentation | LDA | Stack overflow | Topic analysis,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Campbell, Joshua Charles;Zhang, Chenlei;Xu, Zhen;Hindle, Abram;Miller, James",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84889021604,10.1109/MSR.2013.6624006,Detecting API usage obstacles: A study of iOS and android developer questions,"Software frameworks provide sets of generic functionalities that can be later customized for a specific task. When developers invoke API methods in a framework, they often encounter obstacles in finding the correct usage of the API, let alone to employ best practices. Previous research addresses this line of questions by mining API usage patterns to induce API usage templates, by conducting and compiling interviews of developers, and by inferring correlations among APIs. In this paper, we analyze API-related posts regarding iOS and Android development from a Q&A website, stackoverflow.com. Assuming that API-related posts are primarily about API usage obstacles, we find several iOS and Android API classes that appear to be particularly likely to challenge developers, even after we factor out API usage hotspots, inferred by modelling API usage of open source iOS and Android applications. For each API with usage obstacles, we further apply a topic mining tool to posts that are tagged with the API, and we discover several repetitive scenarios in which API usage obstacles occur. We consider our work as a stepping stone towards understanding API usage challenges based on forum-based input from a multitude of developers, input that is prohibitively expensive to collect through interviews. Our method helps to motivate future research in API usage, and can allow designers of platforms - such as iOS and Android - to better understand the problems developers have in using their platforms, and to make corresponding improvements. © 2013 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Wang, Wei;Godfrey, Michael W.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84888997247,10.1109/MSR.2013.6624012,Making sense of online code snippets,"Stack Overflow contains a large number of high-quality source code snippets. The quality of these snippets has been verified by users marking them as solving a specific problem. Stack Overflow treats source code snippets as plain text and searches surface snippets as they would any other text. Unfortunately, plain text does not capture the structural qualities of these snippets; for example, snippets frequently refer to specific API (e.g., Android), but by treating the snippets as text, linkage to the Android API is not always apparent. We perform snippet analysis to extract structural information from short plain-text snippets that are often found in Stack Overflow. This analysis is able to identify 253,137 method calls and type references from 21,250 Stack Overflow code snippets. We show how identifying these structural relationships from snippets could perform better than lexical search over code blocks in practice. © 2013 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Subramanian, Siddharth;Holmes, Reid",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84889051028,10.1109/MSR.2013.6624020,Apache commits: Social network dataset,"Building non-trivial software is a social endeavor. Therefore, understanding the social network of developers is key to the study of software development organizations. We present a graph representation of the commit behavior of developers within the Apache Software Foundation for 2010 and 2011. Relationships between developers in the network represent collaborative commit behavior. Several similarity and summary metrics have been pre-calculated. The data, along with the tools that were used to create it and some further discussion, can be found at: http://sequoia.cs.byu.edu/lab/?page=artifacts/ apacheGraphs. © 2013 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"MacLean, Alexander C.;Knutson, Charles D.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84889073565,10.1109/MSR.2013.6624027,Bug resolution catalysts: Identifying essential non-committers from bug repositories,"Bugs are inevitable in software projects. Resolving bugs is the primary activity in software maintenance. Developers, who fix bugs through code changes, are naturally important participants in bug resolution. However, there are other participants in these projects who do not perform any code commits. They can be reporters reporting bugs; people having a deep technical know-how of the software and providing valuable insights on how to solve the bug; bug-tossers who re-assign the bugs to the right set of developers. Even though all of them act on the bugs by tossing and commenting, not all of them may be crucial for bug resolution. In this paper, we formally define essential non-committers and try to identify these bug resolution catalysts. We empirically study 98304 bug reports across 11 open source and 5 commercial software projects for validating the existence of such catalysts. We propose a network analysis based approach to construct a Minimal Essential Graph that identifies such people in a project. Finally, we suggest ways of leveraging this information for bug triaging and bug report summarization. © 2013 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Mani, Senthil;Nagar, Seema;Mukherjee, Debdoot;Narayanam, Ramasuri;Sinha, Vibha Singhal;Nanavati, Amit A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84889043511,10.1109/MSR.2013.6624033,A network of rails: A graph dataset of ruby on rails and associated projects,"Software projects, whether open source, proprietary, or a combination thereof, rarely exist in isolation. Rather, most projects build on a network of people and ideas from dozens, hundreds, or even thousands of other projects. Using the GitHub APIs it is possible to extract these relationships for millions of users and projects. In this paper we present a dataset of a large network of open source projects centered around Ruby on Rails. This dataset provides insight into the relationships between Ruby on Rails and an ecosystem involving 1116 projects. To facilitate understanding of this data in the context of relationships between projects, users, and their activities, it is provided as a graph database suitable for assessing network properties of the community and individuals within those communities and can be found at https://github.com/ pridkett/gitminer-data-rails. © 2013 IEEE.",Data | GitHub | Graph databases | Ruby on rails,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Wagstrom, Patrick;Jergensen, Corey;Sarma, Anita",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84889011289,10.1109/MSR.2013.6624034,The GHTorent dataset and tool suite,"During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it. © 2013 IEEE.",Dataset | GitHub | Repository,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Gousios, Georgios",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84889004632,10.1109/MSR.2013.6624045,Mining succinct and high-coverage API usage patterns from source code,"During software development, a developer often needs to discover specific usage patterns of Application Programming Interface (API) methods. However, these usage patterns are often not well documented. To help developers to get such usage patterns, there are approaches proposed to mine client code of the API methods. However, they lack metrics to measure the quality of the mined usage patterns, and the API usage patterns mined by the existing approaches tend to be many and redundant, posing significant barriers for being practical adoption. To address these issues, in this paper, we propose two quality metrics (succinctness and coverage) for mined usage patterns, and further propose a novel approach called Usage Pattern Miner (UP-Miner) that mines succinct and high-coverage usage patterns of API methods from source code. We have evaluated our approach on a large-scale Microsoft codebase. The results show that our approach is effective and outperforms an existing representative approach MAPO. The user studies conducted with Microsoft developers confirm the usefulness of the proposed approach in practice. © 2013 IEEE.",API usage | Mining software repositories | Sequence mining | Software reuse | Usage pattern,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Wang, Jue;Dang, Yingnong;Zhang, Hongyu;Chen, Kai;Xie, Tao;Zhang, Dongmei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84889005790,10.1109/MSR.2013.6624048,The MSR cookbook: Mining a decade of research,"The Mining Software Repositories (MSR) research community has grown significantly since the first MSR workshop was held in 2004. As the community continues to broaden its scope and deepens its expertise, it is worthwhile to reflect on the best practices that our community has developed over the past decade of research. We identify these best practices by surveying past MSR conferences and workshops. To that end, we review all 117 full papers published in the MSR proceedings between 2004 and 2012. We extract 268 comments from these papers, and categorize them using a grounded theory methodology. From this evaluation, four high-level themes were identified: data acquisition and preparation, synthesis, analysis, and sharing/replication. Within each theme we identify several common recommendations, and also examine how these recommendations have evolved over the past decade. In an effort to make this survey a living artifact, we also provide a public forum that contains the extracted recommendations in the hopes that the MSR community can engage in a continuing discussion on our evolving best practices. © 2013 IEEE.",,IEEE International Working Conference on Mining Software Repositories,2013-12-09,Conference Paper,"Hemmati, Hadi;Nadi, Sarah;Baysal, Olga;Kononenko, Oleksii;Wang, Wei;Holmes, Reid;Godfrey, Michael W.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84888167548,10.1145/2509136.2509546,Verifying quantitative reliability for programs that execute on unreliable hardware,"Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and masking of soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. We present Rely, a programming language that enables developers to reason about the quantitative reliability of an application - namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces. We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification that characterizes the reliability of the underlying hardware components and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely. Copyright © 2013 ACM.",Approximate computing,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2013-11-28,Conference Paper,"Carbin, Michael;Misailovic, Sasa;Rinard, Martin C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84888159501,10.1145/2509136.2509555,Synthesis modulo recursive functions,"We describe techniques for synthesis and verification of recursive functional programs over unbounded domains. Our techniques build on top of an algorithm for satisfiability modulo recursive functions, a framework for deductive synthesis, and complete synthesis procedures for algebraic data types. We present new counterexample-guided algorithms for constructing verified programs. We have implemented these algorithms in an integrated environment for interactive verification and synthesis from relational specifications. Our system was able to synthesize a number of useful recursive functions that manipulate unbounded numbers and data structures. Copyright © 2013 ACM.",Inductive learning | Satisfiability modulo theories | Software synthesis,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2013-11-28,Conference Paper,"Kneuss, Etienne;Kuncak, Viktor;Kuraj, Ivan;Suter, Philippe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84888194843,10.1145/2509136.2509523,Detecting API documentation errors,"When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on de- velopment forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively main- tained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because tra- ditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not under- stand NL sentences. In this paper, we propose the first approach, DOCREF, specifically designed and developed to detect API doc- umentation errors. We formulate a class of inconsistencies to in- dicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DOCREF and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DOCREF has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them. Copyright © 2013 ACM.",API documentation error | Outdated documentation,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",2013-11-28,Conference Paper,"Zhong, Hao;Su, Zhendong",Include,Does not assess comments
10.1016/j.jss.2022.111515,,10.1145/2491956.2462169,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2491956.2462174,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883094727,10.1145/2491956.2491978,Verifying higher-order programs with the Dijkstra monad,"Modern programming languages, ranging from Haskell and ML, to JavaScript, C# and Java, all make extensive use of higher-order state. This paper advocates a new verification methodology for higher-order stateful programs, based on a new monad of predicate transformers called the Dijkstra monad. Using the Dijkstra monad has a number of benefits. First, the monad naturally yields a weakest pre-condition calculus. Second, the computed specifications are structurally simpler in several ways, e.g., single-state post-conditions are sufficient (rather than the more complex two-state post-conditions). Finally, the monad can easily be varied to handle features like exceptions and heap invariants, while retaining the same type inference algorithm. We implement the Dijkstra monad and its type inference algorithm for the F* programming language. Our most extensive case study evaluates the Dijkstra monad and its F* implementation by using it to verify JavaScript programs. Specifically, we describe a tool chain that translates programs in a subset of JavaScript decorated with assertions and loop invariants to F *. Once in F*, our type inference algorithm computes verification conditions and automatically discharges their proofs using an SMT solver. We use our tools to prove that a core model of the JavaScript runtime in F* respects various invariants and that a suite of JavaScript source programs are free of runtime errors. Copyright © 2013 ACM.",Dynamic languages | Hoare monad | Predicate transformer | Refinement types,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2013-09-02,Conference Paper,"Swamy, Nikhil;Weinberger, Joel;Schlesinger, Cole;Chen, Juan;Livshits, Benjamin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874161636,10.1145/2429069.2429083,Static and dynamic semantics of NoSQL languages,"We present a calculus for processing semistructured data that spans differences of application area among several novel query languages, broadly categorized as ""NoSQL"". This calculus lets users define their own operators, capturing a wider range of data processing capabilities, whilst providing a typing precision so far typical only of primitive hard-coded operators. The type inference algorithm is based on semantic type checking, resulting in type information that is both precise, and flexible enough to handle structured and semistructured data. We illustrate the use of this calculus by encoding a large fragment of Jaql, including operations and iterators over JSON, embedded SQL expressions, and co-grouping, and show how the encoding directly yields a typing discipline for Jaql as it is, namely without the addition of any type definition or type annotation in the code. © 2013 ACM.",bigdata analytics | cloud computing | jaql | NoSQL | type inference,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Benzaken, Véronique;Castagna, Giuseppe;Nguyên, Kim;Siméon, Jérôme",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874176762,10.1145/2429069.2429101,Deadlock-freedom-by-design: Multiparty asynchronous global programming,"Over the last decade, global descriptions have been successfully employed for the verification and implementation of communicating systems, respectively as protocol specifications and choreographies. In this work, we bring these two practices together by proposing a purely-global programming model. We show a novel interpretation of asynchrony and parallelism in a global setting and develop a typing discipline that verifies choreographies against protocol specifications, based on multiparty sessions. Exploiting the nature of global descriptions, our type system defines a new class of deadlock-free concurrent systems (deadlock-freedom-by-design), provides type inference, and supports session mobility. We give a notion of Endpoint Projection (EPP) which generates correct entity code (as pi-calculus terms) from a choreography. Finally, we evaluate our approach by providing a prototype implementation for a concrete programming language and by applying it to some examples from multicore and service-oriented programming. © 2013 ACM.",choreography | concurrency | sessions | types,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Carbone, Marco;Montesi, Fabrizio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874141408,10.1145/2429069.2429105,High-level separation logic for low-level code,"Separation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lower-level languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machine-code programs. The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuation-passing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higher-order frame connective, a novel read-only frame connective, and a 'later' modality, support the definition of derived forms to support structured-programming-style reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assembly-language labels lets us give definitions and proof rules for powerful assembly-language 'macros' such as while loops, conditionals and procedures. We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection. © 2013 ACM.",machine code | proof assistants | separation logic,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Jensen, Jonas B.;Benton, Nick;Kennedy, Andrew",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874146393,10.1145/2429069.2429109,Quantitative relaxation of concurrent data structures,"There is a trade-off between performance and correctness in implementing concurrent data structures. Better performance may be achieved at the expense of relaxing correctness, by redefining the semantics of data structures. We address such a redefinition of data structure semantics and present a systematic and formal framework for obtaining new data structures by quantitatively relaxing existing ones. We view a data structure as a sequential specification S containing all ""legal"" sequences over an alphabet of method calls. Relaxing the data structure corresponds to defining a distance from any sequence over the alphabet to the sequential specification: the k-relaxed sequential specification contains all sequences over the alphabet within distance k from the original specification. In contrast to other existing work, our relaxations are semantic (distance in terms of data structure states). As an instantiation of our framework, we present two simple yet generic relaxation schemes, called out-of-order and stuttering relaxation, along with several ways of computing distances. We show that the out-of-order relaxation, when further instantiated to stacks, queues, and priority queues, amounts to tolerating bounded out-of-order behavior, which cannot be captured by a purely syntactic relaxation (distance in terms of sequence manipulation, e.g. edit distance). We give concurrent implementations of relaxed data structures and demonstrate that bounded relaxations provide the means for trading correctness for performance in a controlled way. The relaxations are monotonic which further highlights the trade-off: increasing k increases the number of permitted sequences, which as we demonstrate can lead to better performance. Finally, since a relaxed stack or queue also implements a pool, we actually have new concurrent pool implementations that outperform the state-of-the-art ones. © 2013 ACM.",(concurrent) data structures | costs | quantitative models | relaxed semantics,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Henzinger, Thomas A.;Kirsch, Christoph M.;Payer, Hannes;Sezgin, Ali;Sokolova, Ana",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874161285,10.1145/2429069.2429110,Plan B: A buffered memory model for Java,"Recent advances in verification have made it possible to envision trusted implementations of real-world languages. Java with its type-safety and fully specified semantics would appear to be an ideal candidate; yet, the complexity of the translation steps used in production virtual machines have made it a challenging target for verifying compiler technology. One of Java's key innovations, its memory model, poses significant obstacles to such an endeavor. The Java Memory Model is an ambitious attempt at specifying the behavior of multithreaded programs in a portable, hardware agnostic, way. While experts have an intuitive grasp of the properties that the model should enjoy, the specification is complex and not well-suited for integration within a verifying compiler infrastructure. Moreover, the specification is given in an axiomatic style that is distant from the intuitive reordering-based reasonings traditionally used to justify or rule out behaviors, and ill suited to the kind of operational reasoning one would expect to employ in a compiler. This paper takes a step back, and introduces a Buffered Memory Model (BMM) for Java. We choose a pragmatic point in the design space sacrificing generality in favor of a model that is fully characterized in terms of the reorderings it allows, amenable to formal reasoning, and which can be efficiently applied to a specific hardware family, namely x86 multiprocessors. Although the BMM restricts the reorderings compilers are allowed to perform, it serves as the key enabling device to achieving a verification pathway from bytecode to machine instructions. Despite its restrictions, we show that it is backwards compatible with the Java Memory Model and that it does not cripple performance on TSO architectures. © 2013 ACM.",concurrency | java | memory model | verified compilation,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Demange, Delphine;Laporte, Vincent;Zhao, Lei;Jagannathan, Suresh;Pichardie, David;Vitek, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874131685,10.1145/2429069.2429111,Logical relations for fine-grained concurrency,"Fine-grained concurrent data structures (or FCDs) reduce the granularity of critical sections in both time and space, thus making it possible for clients to access different parts of a mutable data structure in parallel. However, the tradeoff is that the implementations of FCDs are very subtle and tricky to reason about directly. Consequently, they are carefully designed to be contextual refinements of their coarse-grained counterparts, meaning that their clients can reason about them as if all access to them were sequentialized. In this paper, we propose a new semantic model, based on Kripke logical relations, that supports direct proofs of contextual refinement in the setting of a type-safe high-level language. The key idea behind our model is to provide a simple way of expressing the ""local life stories"" of individual pieces of an FCD's hidden state by means of protocols that the threads concurrently accessing that state must follow. By endowing these protocols with a simple yet powerful transition structure, as well as the ability to assert invariants on both heap states and specification code, we are able to support clean and intuitive refinement proofs for the most sophisticated types of FCDs, such as conditional compare-and-set (CCAS). © 2013 ACM.",data abstraction | fine-grained concurrency | linearizability | local state | logical relations | refinement | separation logic,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Turon, Aaron J.;Thamsborg, Jacob;Ahmed, Amal;Birkedal, Lars;Dreyer, Derek",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874183838,10.1145/2429069.2429123,Sigma*: Symbolic learning of input-output specifications,"We present Sigma*, a novel technique for learning symbolic models of software behavior. Sigma* addresses the challenge of synthesizing models of software by using symbolic conjectures and abstraction. By combining dynamic symbolic execution to discover symbolic input-output steps of the programs and counterexample guided abstraction refinement to over-approximate program behavior, Sigma* transforms arbitrary source representation of programs into faithful input-output models. We define a class of stream filters - programs that process streams of data items - for which Sigma* converges to a complete model if abstraction refinement eventually builds up a sufficiently strong abstraction. In other words, Sigma* is complete relative to abstraction. To represent inferred symbolic models, we use a variant of symbolic transducers that can be effectively composed and equivalence checked. Thus, Sigma* enables fully automatic analysis of behavioral properties such as commutativity, reversibility and idempotence, which is useful for web sanitizer verification and stream programs compiler optimizations, as we show experimentally. We also show how models inferred by Sigma* can boost performance of stream programs by parallelized code generation. © 2013 ACM.","behavioral properties | compiler optimization, | equivalence checking | inductive learning | parallelization | specification synthesis | stream programs",Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Botinčan, Matko;Babić, Domagoj",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874151469,10.1145/2429069.2429125,Synthesis of biological models from mutation experiments,"Executable biology presents new challenges to formal methods. This paper addresses two problems that cell biologists face when developing formally analyzable models. First, we show how to automatically synthesize a concurrent in-silico model for cell development given in-vivo experiments of how particular mutations influence the experiment outcome. The problem of synthesis under mutations is unique because mutations may produce non-deterministic outcomes (presumably by introducing races between competing signaling pathways in the cells) and the synthesized model must be able to replay all these outcomes in order to faithfully describe the modeled cellular processes. In contrast, a ""regular"" concurrent program is correct if it picks any outcome allowed by the non-deterministic specification. We developed synthesis algorithms and synthesized a model of cell fate determination of the earthworm C. elegans. A version of this model previously took systems biologists months to develop. Second, we address the problem of under-constrained specifications that arise due to incomplete sets of mutation experiments. Under-constrained specifications give rise to distinct models, each explaining the same phenomenon differently. Addressing the ambiguity of specifications corresponds to analyzing the space of plausible models. We develop algorithms for detecting ambiguity in specifications, i.e., whether there exist alternative models that would produce different fates on some unperformed experiment, and for removing redundancy from specifications, i.e., computing minimal non-ambiguous specifications. Additionally, we develop a modeling language and embed it into Scala. We describe how this language design and embedding allows us to build an efficient synthesizer. For our C. elegans case study, we infer two observationally equivalent models expressing different biological hypotheses through different protein interactions. One of these hypotheses was previously unknown to biologists. © 2013 ACM.",executable biology | program synthesis | specification ambiguity analysis,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2013-02-26,Conference Paper,"Köksal, Ali Sinan;Pu, Yewen;Srivastava, Saurabh;Bodík, Rastislav;Fisher, Jasmin;Piterman, Nir",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891048525,10.1109/SCAM.2013.6648183,Empirical evidence of large-scale diversity in API usage of object-oriented software,"In this paper, we study how object-oriented classes are used across thousands of software packages. We concentrate on ""usage diversity"", defined as the different statically observable combinations of methods called on the same object. We present empirical evidence that there is a significant usage diversity for many classes. For instance, we observe in our dataset that Java's String is used in 2460 manners. We discuss the reasons of this observed diversity and the consequences on software engineering knowledge and research. © 2013 IEEE.",,"IEEE 13th International Working Conference on Source Code Analysis and Manipulation, SCAM 2013",2013-01-01,Conference Paper,"Mendez, Diego;Baudry, Benoit;Monperrus, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891057650,10.1109/SCAM.2013.6648185,Driving a sound static software analyzer with branch-and-bound,"During the last decade, static analyzers of source code have improved greatly. Today, precise analyzers that propagate values for the program's variables, for instance with interval arithmetic, are used in the industry. The simultaneous propagation of sets of values, while computationally efficient, is a source of approximations, and ultimately of false positives. When the loss of precision is detrimental to the user's goals, a user needs to provide some kind of manual guidance. Frama-C, a framework for the static analysis of C programs, provides a sound value analyzer. This analyzer can optionally be guided by skillfully placed user annotations. This article describes SPALTER, a Frama-C plug-in that uses a variation of the Skelboe-Moore algorithm from the field of interval arithmetic to guide Frama-C's value analyzer towards a high-level objective set by the user. SPALTER reproduces the results of a case study that used Frama-C's value analysis and required extensive manual guidance. In difference, our approach with SPALTER required no guidance, except preparation of the analyzed program by slicing. © 2013 IEEE.",,"IEEE 13th International Working Conference on Source Code Analysis and Manipulation, SCAM 2013",2013-01-01,Conference Paper,"Mattsen, Sven;Cuoq, Pascal;Schupp, Sibylle",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2465529.2465540,,,,,,,,Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891543964,10.1109/VISSOFT.2013.6650541,Lightweight software reverse engineering using augmented matrix visualizations,"Empirical studies show that understanding existing software is a cost driver in maintenance activities and throughout the software development lifecycle. Presenting large software systems visually to support reverse engineering requires having the right summary to convey the important details. An experienced engineer knows the important details and can customize the view accordingly. In a reverse engineering context without this upfront knowledge and experience, any derived insights are strongly dependent on the (perhaps incorrectly) chosen view. We propose a lightweight visual approach to reverse engineering that supports users in interactively extracting high-level information from software at large scales with little or no upfront system knowledge. To support this we provide a matrix-based visualization method capable of displaying full systems in space-constrained views. The view visually emphasizes high level structural properties and employs a novel dependency display method to summarize and aggregate dependencies as the view changes, eliminating the need for user-directed aggregation decisions. We describe the visualization concept and a prototype tool that supports it, provide an example from the Apache HttpComponents Client open source project, and offer some areas of future work. © 2013 IEEE.",dependencies | dependency clustering | design structure matrix | reverse engineering | software visualization,2013 1st IEEE Working Conference on Software Visualization - Proceedings of VISSOFT 2013,2013-12-01,Conference Paper,"Wehrwein, Bradley",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84891535834,10.1109/VISSOFT.2013.6650526,DEVis: A tool for visualizing software document evolution,"During the software development process many technical documents are produced. Such documents and their evolution history contain rich information about the development process. Analyzing these document changes may reveal regularities and anomalies which are helpful to understand the software system and its development process. In this paper, we propose DEVis, an interactive visualization tool to visualize the software documentation evolution and aid the tasks of analyzing software development process. We initially evaluated our tool using the knowledge task-based framework and discuss the challenging aspects and lessons learned during the development of DEVis. © 2013 IEEE.",document evolution | software documentation | Visualization of software documentations,2013 1st IEEE Working Conference on Software Visualization - Proceedings of VISSOFT 2013,2013-12-01,Conference Paper,"Zhi, Junji;Ruhe, Günther",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84887500972,10.1109/ICGSE.2013.23,Experience in managing requirements between distributed parties in a research project context,"This paper describes the experience of managing a requirements process between distributed parties with diverse interests in a research project context. We present some key 'lessons learned' from a new case study, the DESTECS project, and summarise lessons learned from previous experience reports. Key risks include obstacles imposed by the geographic distance; the different domain knowledge and working contexts of partners; and a risk that autonomous partners' goals do not always coincide. Our observations on a new case study broadly support a previous study, but we also propose some new lessons to learn, including the creation of a small, representative 'requirements authority' (RA); investing time in studying common concepts early in the project; and ensuring that expectations for requirements and for deliveries are made explicit. © 2013 IEEE.",Distributed software development | Global software development | Requirements | Research consortium,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",2013-01-01,Conference Paper,"Pierce, Ken;Ingram, Claire;Bos, Bert;Ribeiro, Augusto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887446894,10.1109/ICGSE.2013.28,Managing product delivery quality in distributed software development,"Distributed software development is a globally accepted practice by large organizations for achieving significant cost savings in research and development. However when it comes to product quality, defect prevention and early detection has always been a challenge. Factors such as differences in language, coding standards and documentation styles across distributed teams can aggravate the situation. These issues, if not addressed, will manifest themselves as product defects leading to enormous costs in fixing at later phases of development. This in turn reduces the business partner or product owner's confidence. Thus, along with defining effective processes to address these issues, it becomes very important to ensure that the product quality expectations of various stakeholders are understood and mutually agreed upon. This paper summarizes our approach and practice in managing these issues while also delivering the expected product quality in our recent project. © 2013 IEEE.",Defect prevention and early detection | Global software development | Requirement elicitation | Software quality management,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",2013-01-01,Conference Paper,"Bhattacharjee, Samrat;Bhadauria, Abhinandan;Kumar Ik, Girish",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84887474159,10.1109/ICGSE.2013.29,Towards integrated variant management in global software engineering: An experience report,"In the automotive domain, customer demands and market constraints are progressively realized by electric/ electronic components and corresponding software. Variant traceability in SPL is crucial in the context of different tasks, like change impact analysis, especially in complex global software projects. In addition, traceability concepts must be extended by partly automated variant configuration mechanisms to handle restrictions and dependencies between variants. Such variant configuration mechanism helps to reduce complexity when configuring a valid variant and to establish an explicit documentation of dependencies between components. However, integrated variant management has not been sufficiently addressed so far. Especially, the increasing number of software variants requires an examination of traceable and configurable software variants over the software lifecycle. This paper emphasizes variant traceability achievements in a large global software engineering project, elaborates existing challenges, and evaluates an industrial usage of an integrated variant management based on experiences. © 2013 IEEE.",Integrated feature modeling | Software product line | Traceability,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",2013-01-01,Conference Paper,"Manz, Christian;Stupperich, Michael;Reichert, Manfred",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881129097,10.1007/978-3-642-39799-8_13,SVA and PSL local variables - A practical approach,"SystemVerilog Assertions (SVA), as well as Property Specification Language (PSL) are linear temporal logics based on LTL [14], extended with regular expressions and local variables. In [6] Bustan and Havlicek show that the local variable extensions, as well as regular expressions with intersection, render the verification problem of SVA and PSL formulae EXPSPACE-complete. In this paper we show a practical approach for the verification problem of SVA and PSL with local variables. We show that in practice, for a significant and meaningful subsets of those languages, which we denote PSLpract, local variables do not increase the complexity of their verification problem, keeping it in PSPACE. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-08-12,Conference Paper,"Armoni, Roy;Fisman, Dana;Jin, Naiyong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881144373,10.1007/978-3-642-39799-8_32,Automatic generation of quality specifications,"The logic LTL∇ extends LTL by quality operators. The satisfaction value of an LTL∇ formula in a computation refines the 0/1 value of formulas to a real value in [0,1]. The higher the value is, the better is the quality of the computation. The quality operator ∇λ, for a quality constant λ ∈ [0,1], enables the designer to prioritize different satisfaction possibilities. Formally, the satisfaction value of a sub-formula ∇ λφ is λ times the satisfaction value of φ. For example, the LTL∇ formula G(req → (X grant ∨ ∇1/2 F grant)) has value 1 in computations in which every request is immediately followed by a grant, value 1/2 if grants to some requests involve a delay, and value 0 if some request is not followed by a grant. The design of an LTL∇ formula typically starts with an formula on top of which the designer adds the parameterized ∇ operators. In the Boolean setting, the problem of automatic generation of specifications from binary-tagged computations is of great importance and is a very challenging one. Here we consider the quantitative counterpart: an LTL∇ query is an LTL∇ formula in which some of the quality constants are replaced by variables. Given an LTL∇ query and a set of computations tagged by satisfaction values, the goal is to find an assignment to the variables in the query so that the obtained LTL∇ formula has the given satisfaction values, or, if this is impossible, best approximates them. The motivation to solving LTL∇ queries is that in practice it is easier for a designer to provide desired satisfaction values in representative computations than to come up with quality constants that capture his intuition of good and bad quality. We study the problem of solving LTL ∇ queries and show that while the problem is NP-hard, interesting fragments can be solved in polynomial time. One such fragment is the case of a single tagged computation, which we use for introducing a heuristic for the general case. The polynomial solution is based on an analysis of the search space, showing that reasoning about the infinitely many possible assignments can proceed by reasoning about their partition into finitely many classes. Our experimental results show the effectiveness and favorable outcome of the heuristic. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-08-12,Conference Paper,"Almagor, Shaull;Avni, Guy;Kupferman, Orna",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881183094,10.1007/978-3-642-39799-8_53,Effectively-propositional reasoning about reachability in linked data structures,"This paper proposes a novel method of harnessing existing SAT solvers to verify reachability properties of programs that manipulate linked-list data structures. Such properties are essential for proving program termination, correctness of data structure invariants, and other safety properties. Our solution is complete, i.e., a SAT solver produces a counterexample whenever a program does not satisfy its specification. This result is surprising since even first-order theorem provers usually cannot deal with reachability in a complete way, because doing so requires reasoning about transitive closure. Our result is based on the following ideas: (1) Programmers must write assertions in a restricted logic without quantifier alternation or function symbols. (2) The correctness of many programs can be expressed in such restricted logics, although we explain the tradeoffs. (3) Recent results in descriptive complexity can be utilized to show that every program that manipulates potentially cyclic, singly- and doubly-linked lists and that is annotated with assertions written in this restricted logic, can be verified with a SAT solver. We implemented a tool atop Z3 and used it to show the correctness of several linked list programs. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-08-12,Conference Paper,"Itzhaky, Shachar;Banerjee, Anindya;Immerman, Neil;Nanevski, Aleksandar;Sagiv, Mooly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881132177,10.1007/978-3-642-39799-8_58,Towards distributed software model-checking using decision diagrams,"Symbolic data structures such as Decision Diagrams have proved successful for model-checking. For high-level specifications such as those used in programming languages, especially when manipulating pointers or arrays, building and evaluating the transition is a challenging problem that limits wider applicability of symbolic methods. We propose a new symbolic algorithm, EquivSplit, allowing an efficient and fully symbolic manipulation of transition relations on Data Decision Diagrams. It allows to work with equivalence classes of states rather than individual states. Experimental evidence on the concurrent software oriented benchmark BEEM shows that this approach is competitive. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-08-12,Conference Paper,"Colange, Maximilien;Baarir, Souheib;Kordon, Fabrice;Thierry-Mieg, Yann",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881174128,10.1007/978-3-642-39799-8_64,Program repair without regret,"We present a new and flexible approach to repair reactive programs with respect to a specification. The specification is given in linear-temporal logic. Like in previous approaches, we aim for a repaired program that satisfies the specification and is syntactically close to the faulty program. The novelty of our approach is that it produces a program that is also semantically close to the original program by enforcing that a subset of the original traces is preserved. Intuitively, the faulty program is considered to be a part of the specification, which enables us to synthesize meaningful repairs, even for incomplete specifications. Our approach is based on synthesizing a program with a set of behaviors that stay within a lower and an upper bound. We provide an algorithm to decide if a program is repairable with respect to our new notion, and synthesize a repair if one exists. We analyze several ways to choose the set of traces to leave intact and show the boundaries they impose on repairability. We have evaluated the approach on several examples. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-08-12,Conference Paper,"Von Essen, Christian;Jobstmann, Barbara",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877291573,10.1145/2460999.2461003,Evaluating usage and quality of technical software documentation: An empirical study,"Context: Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the lack of usage and quality of documentation in practice. Unfortunately, in many projects, practitioners find that software documentation artifacts are outdated, incomplete and sometimes not beneficial. Objective: Motivated by the needs of NovAtel Inc. (NovAtel), a world-leading company of GPS software systems, we propose in this paper an approach to analyze the usage and quality of software documentation in development and maintenance phases. Method: The approach incorporates inputs from automated analysis (e.g., mining of project's data) and also experts' opinion extracted from survey-based questionnaire. The approach has been designed based on the ""action-research"" approach and in close collaboration between industry and academia. Results: To evaluate the feasibility and usefulness of the proposed approach, we have applied it in an industrial setting and results are presented in this paper. One of the results is that, in the context of our case-study, usage of documentation for an implementation purpose is higher than the usage for maintenance purposes. Conclusion: It is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the task to be completed (e.g. development and maintenance). In addition, we identify the most important and relevant quality attributes which are critical to improving documentation quality. Copyright 2013 ACM.",Action research | Case study | Empirical software engineering | Maintenance | Software development | Software documentation,ACM International Conference Proceeding Series,2013-05-13,Conference Paper,"Garousi, Golara;Garousi, Vahid;Moussavi, Mahmoud;Ruhe, Guenther;Smith, Brian",Include,
10.1016/j.jss.2022.111515,2-s2.0-84877261010,10.1145/2460999.2461015,A systematic review of design diversity-based solutions for fault-tolerant SOAs,"Context: Over recent years, software developers have been evaluating the benefits of both Service-Oriented Architecture and software fault tolerance techniques based on design diversity by creating fault-tolerant composite services that leverage functionally equivalent services, or variant services. Three major design issues need to be considered while building software fault-tolerant architectures based on design diversity namely, selection and execution of variants and selection of an adjudication algorithm to determine the correct or adjudicated result from the variants. Each design issue, in turn, can be realized by a set of alternative design solutions, which present different degrees of quality requirements (e.g. memory consumption and reliability). Objective: To investigate whether existing approaches for fault-tolerant composite services support the above mentioned design issues and to provide a detailed classification of the analysed approaches. Method: A systematic literature review of diversity-based approaches for fault-tolerant composite services, which compose our primary studies. Results: We found 17 primary studies providing direct evidence about the research question. Our findings reveal that the primary studies support a wide variety of design decisions. For example, (i) variant services may be chosen at different points during the software lifecycle; (ii) both parallel and sequential execution schemes have been addressed; and (iii) a variety of adjudication mechanisms were found amongst the target papers. Conclusion: We build up a broad picture of what design issues have been addressed by existing diversity-based approaches for fault-tolerant composite services. Finally, practical issues and difficulties are summarized and directions for future work are suggested. Copyright 2013 ACM.",Composite services | Fault tolerance | SLR | SOA,ACM International Conference Proceeding Series,2013-05-13,Conference Paper,"Nascimento, Amanda S.;Rubira, Cecília M.F.;Burrows, Rachel;Castor, Fernando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877268215,10.1145/2460999.2461019,Using CBR and CART to predict maintainability of relational database-driven software applications,"Background: Relational database-driven software applications have gained significant importance in modern software development. Given that software maintainability is an important quality attribute, predicting these applications' maintainability can provide various benefits to software organizations, such as adopting a defensive design and more informed resource management. Aims: The aim of this paper is to present the results from employing two well-known prediction techniques to estimate the maintainability of relational database-driven applications. Method: Case-based reasoning (CBR) and classification and regression trees (CART) were applied to data gathered on 56 software projects from software companies. The projects concerned development and/or maintenance of relational database-driven applications. Unlike previous studies, all variables (28 independent and 1 dependent) were measured on a 5-point bi-polar scale. Results: Results showed that CBR performed slightly better (at 76.8% correct predictions) in terms of prediction accuracy when compared to CART (67.8%). In addition, the two important predictors identified were documentation quality and understandability of the applications. Conclusions: The results show that CBR can be used by software companies to formalize and improve their process of maintainability prediction. Future work involves gathering more data and also employing other prediction techniques. Copyright 2013 ACM.",Case-based reasoning | Classification trees | Maintainability | Prediction | Relational database-driven software applications,ACM International Conference Proceeding Series,2013-05-13,Conference Paper,"Riaz, Mehwish;Mendes, Emilia;Tempero, Ewan;Sulayman, Muhammad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887185032,10.1145/2500365.2500593,Hoare-style reasoning with (Algebraic) continuations,"Continuations are programming abstractions that allow for manipulating the ""future"" of a computation. Amongst their many applications, they enable implementing unstructured program flow through higher-order control operators such as callcc. In this paper we develop a Hoare-style logic for the verification of programs with higher-order control, in the presence of dynamic state. This is done by designing a dependent type theory with first class callcc and abort operators, where pre- and postconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin and Power, and Jaskelioff, to reduce the annotation burden and enable verification by symbolic evaluation. We illustrate working with the logic by verifying a number of characteristic examples.",Callcc | Continuations | Dependent types | Hoare logic,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2013-11-12,Conference Paper,"Delbianco, Germán Andrés;Nanevski, Aleksandar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887500972,10.1109/ICGSE.2013.23,Experience in managing requirements between distributed parties in a research project context,"This paper describes the experience of managing a requirements process between distributed parties with diverse interests in a research project context. We present some key 'lessons learned' from a new case study, the DESTECS project, and summarise lessons learned from previous experience reports. Key risks include obstacles imposed by the geographic distance; the different domain knowledge and working contexts of partners; and a risk that autonomous partners' goals do not always coincide. Our observations on a new case study broadly support a previous study, but we also propose some new lessons to learn, including the creation of a small, representative 'requirements authority' (RA); investing time in studying common concepts early in the project; and ensuring that expectations for requirements and for deliveries are made explicit. © 2013 IEEE.",Distributed software development | Global software development | Requirements | Research consortium,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",2013-01-01,Conference Paper,"Pierce, Ken;Ingram, Claire;Bos, Bert;Ribeiro, Augusto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887446894,10.1109/ICGSE.2013.28,Managing product delivery quality in distributed software development,"Distributed software development is a globally accepted practice by large organizations for achieving significant cost savings in research and development. However when it comes to product quality, defect prevention and early detection has always been a challenge. Factors such as differences in language, coding standards and documentation styles across distributed teams can aggravate the situation. These issues, if not addressed, will manifest themselves as product defects leading to enormous costs in fixing at later phases of development. This in turn reduces the business partner or product owner's confidence. Thus, along with defining effective processes to address these issues, it becomes very important to ensure that the product quality expectations of various stakeholders are understood and mutually agreed upon. This paper summarizes our approach and practice in managing these issues while also delivering the expected product quality in our recent project. © 2013 IEEE.",Defect prevention and early detection | Global software development | Requirement elicitation | Software quality management,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",2013-01-01,Conference Paper,"Bhattacharjee, Samrat;Bhadauria, Abhinandan;Kumar Ik, Girish",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84887474159,10.1109/ICGSE.2013.29,Towards integrated variant management in global software engineering: An experience report,"In the automotive domain, customer demands and market constraints are progressively realized by electric/ electronic components and corresponding software. Variant traceability in SPL is crucial in the context of different tasks, like change impact analysis, especially in complex global software projects. In addition, traceability concepts must be extended by partly automated variant configuration mechanisms to handle restrictions and dependencies between variants. Such variant configuration mechanism helps to reduce complexity when configuring a valid variant and to establish an explicit documentation of dependencies between components. However, integrated variant management has not been sufficiently addressed so far. Especially, the increasing number of software variants requires an examination of traceable and configurable software variants over the software lifecycle. This paper emphasizes variant traceability achievements in a large global software engineering project, elaborates existing challenges, and evaluates an industrial usage of an integrated variant management based on experiences. © 2013 IEEE.",Integrated feature modeling | Software product line | Traceability,"Proceedings - IEEE 8th International Conference on Global Software Engineering, ICGSE 2013",2013-01-01,Conference Paper,"Manz, Christian;Stupperich, Michael;Reichert, Manfred",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874828402,10.1145/2430536.2430539,Facilitating the transition from use case models to analysis models: Approach and experiments,"Use case modeling, including use case diagrams and use case specifications (UCSs), is commonly applied to structure and document requirements. UCSs are usually structured but unrestricted textual documents complying with a certain use case template. However, because Use Case Models (UCMods) remain essentially textual, ambiguity is inevitably introduced. In this article, we propose a use case modeling approach, called Restricted Use Case Modeling (RUCM), which is composed of a set of well-defined restriction rules and a modified use case template. The goal is two-fold: (1) restrict the way users can document UCSs in order to reduce ambiguity and (2) facilitate the manual derivation of initial analysis models which, when using the Unified Modeling Language (UML), are typically composed of class diagrams, sequence diagrams, and possibly other types of diagrams. Though the proposed restriction rules and template are based on a clear rationale, two main questions need to be investigated. First, do users find them too restrictive or impractical in certain situations? In other words, can users express the same requirements with RUCM as with unrestricted use cases? Second, do the rules and template have a positive, significant impact on the quality of the constructed analysis models? To investigate these questions, we performed and report on two controlled experiments, which evaluate the restriction rules and use case template in terms of (1) whether they are easy to apply while developing UCMods and facilitate the understanding of UCSs, and (2) whether they help users manually derive higher quality analysis models than what can be generated when they are not used, in terms of correctness, completeness, and redundancy. This article reports on the first controlled experiments that evaluate the applicability of restriction rules on use case modeling and their impact on the quality of analysis models. The measures we have defined to characterize restriction rules and the quality of analysis class and sequence diagrams can be reused to perform similar experiments in the future, either with RUCM or other approaches. Results show that the restriction rules are overall easy to apply and that RUCM results into significant improvements over traditional approaches (i.e., with standard templates, without restrictions) in terms of class correctness and class diagram completeness, message correctness and sequence diagram completeness, and understandability of UCSs. © 2013 ACM.",Design | Documentation | Experimentation | Measurement,ACM Transactions on Software Engineering and Methodology,2013-02-01,Article,"Yue, Tao;Briand, Lionel C.;Labiche, Yvan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874898771,10.1145/2430536.2430541,Detecting missing method calls as violations of the majority rule,"When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this article, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system that searches for missing method calls in software based on the other method calls that are observable. Our key insight is that the voting theory concept of majority rule holds for method calls: a call is likely to be missing if there is a majority of similar pieces of code where this call is present. The evaluation shows that the system predictions go further missing method calls and often reveal different kinds of code smells (e.g., violations of API best practices). © 2013 ACM.",Verification,ACM Transactions on Software Engineering and Methodology,2013-02-01,Article,"Monperrus, Martin;Mezini, Mira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84894588881,10.1145/2491509.2491512,Marple: Detecting faults in path segments using automatically generated analyses,"Generally, a fault is a property violation at a program point along some execution path. To obtain the path where a fault occurs, we can either run the program or manually identify the execution paths through code inspection. In both of the cases, only a very limited number of execution paths can be examined for a program. This article presents a static framework, Marple, that automatically detects path segments where a fault occurs at a whole program scale. An important contribution of the work is the design of a demand-driven analysis that effectively addresses scalability challenges faced by traditional path-sensitive fault detection. The techniques are made general via a specification language and an algorithm that automatically generates path-based analyses from specifications. The generality is achieved in handling both data- and controlcentric faults as well as both liveness and safety properties, enabling the exploitation of fault interactions for diagnosis and efficiency. Our experimental results demonstrate the effectiveness of our techniques in detecting path segments of buffer overflows, integer violations, null-pointer dereferences, and memory leaks. Because we applied an interprocedural, path-sensitive analysis, our static fault detectors generally report better precision than the tools available for comparison. Our demand-driven analyses are shown scalable to deployed applications such as apache, putty, and ffmpeg. © 2013 ACM.",Demand-driven | Faults | Path segments | Specification,ACM Transactions on Software Engineering and Methodology,2013-01-01,Article,"Le, Wei;Soffa, Mary Lou",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84894548893,10.1145/2491509.2491515,The value of design rationale information,"A complete and detailed (full) Design Rationale Documentation (DRD) could support many software development activities, such as an impact analysis or a major redesign. However, this is typically too onerous for systematic industrial use as it is not cost effective to write, maintain, or read. The key idea investigated in this article is that DRD should be developed only to the extent required to support activities particularly difficult to execute or in need of significant improvement in a particular context. The aim of this article is to empirically investigate the customization of the DRD by documenting only the information items that will probably be required for executing an activity. This customization strategy relies on the hypothesis that the value of a specific DRD information item depends on its category (e.g., assumptions, related requirements, etc.) and on the activity it is meant to support. We investigate this hypothesis through two controlled experiments involving a total of 75 master students as experimental subjects. Results show that the value of a DRD information item significantly depends on its category and, within a given category, on the activity it supports. Furthermore, on average among activities, documenting only the information items that have been required at least half of the time (i.e., the information that will probably be required in the future) leads to a customized DRD containing about half the information items of a full documentation. We expect that such a significant reduction in DRD information should mitigate the effects of some inhibitors that currently prevent practitioners from documenting design decision rationale. © 2013 ACM.",Design decisions | Empirical software engineering | Software architecture | Software maintenance | Value-based software engineering,ACM Transactions on Software Engineering and Methodology,2013-01-01,Article,"Falessi, Davide;Briand, Lionel C.;Cantone, Giovanni;Capilla, Rafael;Kruchten, Philippe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84894587693,10.1145/2491509.2491517,An algebra of design patterns,"In a pattern-oriented software design process, design decisions are made by selecting and instantiating appropriate patterns, and composing them together. In our previous work, we enabled these decisions to be formalized by defining a set of operators on patterns with which instantiations and compositions can be represented. In this article, we investigate the algebraic properties of these operators.We provide and prove a complete set of algebraic laws so that equivalence between pattern expressions can be proven. Furthermore, we define an always-terminating normalization of pattern expression to a canonical form which is unique modulo equivalence in first-order logic. By a case study, the pattern-oriented design of an extensible request-handling framework, we demonstrate two practical applications of the algebraic framework. First, we can prove the correctness of a finished design with respect to the design decisions made and the formal specification of the patterns. Second, we can even derive the design from these components. © 2013 ACM.",Algebra | Design patterns | Equational reasoning | Formal method | Pattern composition | Software design methodology,ACM Transactions on Software Engineering and Methodology,2013-07-01,Conference Paper,"Zhu, Hong;Bayley, Ian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84894596998,10.1145/2491509.2491521,Use case and task models: An integrated development methodology and its formal foundation,"User Interface (UI) development methods are poorly integrated with standard software engineering practice. The differences in terms of artifacts involved, development philosophies, and lifecycles can often result in inconsistent system and UI specifications leading to duplication of effort and increased maintenance costs. To address such shortcomings, we propose an integrated development methodology for use case and task models. Use cases are generally used to capture functional requirements whereas task models specify the detailed user interactions with the UI. Our methodology can assist practitioners in developing software processes which allow these two kinds of artifacts to be developed in a codependent and integrated manner.We present our methodology, describe its semantic foundations along with a set of formal conformance relations, and introduce an automated verification tool. © 2013 ACM.",Conformance | Integrated development methodology | Task models | Use case models | Verification,ACM Transactions on Software Engineering and Methodology,2013-01-01,Article,"Sinnig, Daniel;Chalin, Patrice;Khendek, Ferhat",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886561770,10.1145/2522920.2522921,Test-and-Adapt: An approach for improving service interchangeability,"Service-oriented applications do not fully benefit from standard APIs yet, and many applications fail to use interchangeably all the services that implement a standard service API. This article presents an approach to develop adaptation strategies that improve service interchangeability for service-oriented applications based on standard APIs. In our approach, an adaptation strategy consists of sets of parametric adaptation plans (called test-and-adapt plans), which execute test cases to reveal the occurrence of interchangeability problems, and activate runtime adaptors according to the test results. Throughout this article, we formalize the structure of the parametric test-and-adapt plans and of their execution semantics, present an algorithm for identifying correct execution orders through sets of test-and-adapt plans, provide empirical evidence of the occurrence of interchangeability problems for sample applications and services, and discuss the effectiveness of the approach in terms of avoided failures, runtime overheads and development costs. © 2013 ACM.",Dynamic adaption of serviceoriented applications based on standard apis | Reliability of service-oriented architectures,ACM Transactions on Software Engineering and Methodology,2013-10-01,Article,"Denaro, Giovanni;Pezz'e, Mauro;Tosi, Davide",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886477286,10.1145/2522920.2522922,A Methodology for testing CPU emulators,"A CPU emulator is a software system that simulates a hardware CPU. Emulators are widely used by computer scientists for various kind of activities (e.g., debugging, profiling, and malware analysis). Although no theoretical limitation prevents developing an emulator that faithfully emulates a physical CPU, writing a fully featured emulator is a very challenging and error prone task. Modern CISC architectures have a very rich instruction set, some instructions lack proper specifications, and others may have undefined effects in corner cases. This article presents a testing methodology specific for CPU emulators, based on fuzzing. The emulator is ""stressed"" with specially crafted test cases, to verify whether the CPU is properly emulated or not. Improper behaviors of the emulator are detected by running the same test case concurrently on the emulated and on the physical CPUs and by comparing the state of the two after the execution. Differences in the final state testify defects in the code of the emulator. We implemented this methodology in a prototype (named as EmuFuzzer), analyzed five state-of-the-art IA-32 emulators (QEMU, Valgrind, Pin, BOCHS, and JPC), and found several defects in each of them, some of which can prevent proper execution of programs. © 2013 ACM.",Automatic test generation | Emulation | Fuzzing | Software testing,ACM Transactions on Software Engineering and Methodology,2013-10-01,Conference Paper,"Martignoni, Lorenzo;Paleari, Roberto;Reina, Alessandro;Roglia, Giampaolo Fresi;Bruschi, Danilo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84886461926,10.1145/2522920.2522926,Path-and index-sensitive string analysis based on monadic second-order logic,"We propose a novel technique for statically verifying the strings generated by a program. The verification is conducted by encoding the program in Monadic Second-order Logic (M2L). We use M2L to describe constraints among program variables and to abstract built-in string operations. Once we encode a program in M2L, a theorem prover for M2L, such as MONA, can automatically check if a string generated by the program satisfies a given specification, and if not, exhibit a counterexample. With this approach, we can naturally encode relationships among strings, accounting also for cases in which a program manipulates strings using indices. In addition, our string analysis is path sensitive in that it accounts for the effects of string and Boolean comparisons, as well as regular-expression matches. We have implemented our string analysis algorithm, and used it to augment an industrial security analysis for Web applications by automatically detecting and verifying sanitizers-methods that eliminate malicious patterns from untrusted strings, making these strings safe to use in security-sensitive operations. On the 8 benchmarks we analyzed, our string analyzer discovered 128 previously unknown sanitizers, compared to 71 sanitizers detected by a previously presented string analysis. © 2013 ACM.",Static program analysis | String analysis | Web security,ACM Transactions on Software Engineering and Methodology,2013-10-01,Article,"Tateishi, Takaaki;Pistoia, Marco;Tripp, Omer",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84872035936,10.1109/TSE.2012.16,Identifying and summarizing systematic code changes via rule inference,"Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company. © 2013 IEEE.",Logic-based program representation | Program differencing | Rule learning | Software evolution,IEEE Transactions on Software Engineering,2013-01-01,Article,"Kim, Miryung;Notkin, David;Grossman, Dan;Wilson, Gary",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84872050299,10.1109/TSE.2012.1,Performance specification and evaluation with unified stochastic probes and fluid analysis,"Rapid and accessible performance evaluation of complex software systems requires two critical features: the ability to specify useful performance metrics easily and the capability to analyze massively distributed architectures, without recourse to large compute clusters. We present the unified stochastic probe, a performance specification mechanism for process algebra models that combines many existing ideas: state and action-based activation, location-based specification, many-probe specification, and immediate signaling. These features, between them, allow the precise and compositional construction of complex performance measurements. The paper shows how a subset of the stochastic probe language can be used to specify common response-time measures in massive process algebra models. The second contribution of the paper is to show how these response-time measures can be analyzed using so-called fluid techniques to produce rapid results. In doing this, we extend the fluid approach to incorporate immediate activities and a new type of response-time measure. Finally, we calculate various response-time measurements on a complex distributed wireless network of O(10^{129}) states in size. © 2013 IEEE.",Fluid approximation | Measurement probes | Passage-time analysis | Performance evaluation tools | Performance modeling | Stochastic process algebra,IEEE Transactions on Software Engineering,2013-01-11,Article,"Hayden, Richard A.;Bradley, Jeremy T.;Clark, Allan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84865244647,10.1109/TSE.2012.14,Whole test suite generation,"Not all bugs lead to program crashes, and not always is there a formal specification to check the correctness of a software test's outcome. A common scenario in software testing is therefore that test data are generated, and a tester manually adds test oracles. As this is a difficult task, it is important to produce small yet representative test sets, and this representativeness is typically measured using code coverage. There is, however, a fundamental problem with the common approach of targeting one coverage goal at a time: Coverage goals are not independent, not equally difficult, and sometimes infeasible-the result of test generation is therefore dependent on the order of coverage goals and how many of them are feasible. To overcome this problem, we propose a novel paradigm in which whole test suites are evolved with the aim of covering all coverage goals at the same time while keeping the total size as small as possible. This approach has several advantages, as for example, its effectiveness is not affected by the number of infeasible targets in the code. We have implemented this novel approach in the EvoSuite tool, and compared it to the common approach of addressing one goal at a time. Evaluated on open source libraries and an industrial case study for a total of 1,741 classes, we show that EvoSuite achieved up to 188 times the branch coverage of a traditional approach targeting single branches, with up to 62 percent smaller test suites. © 1976-2012 IEEE.",branch coverage | collateral coverage | genetic algorithm | infeasible goal | length | Search-based software engineering,IEEE Transactions on Software Engineering,2013-02-08,Article,"Fraser, Gordon;Arcuri, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84875695205,10.1109/TSE.2012.40,Locating need-to-externalize constant strings for software internationalization with generalized string-taint analysis,"Nowadays, a software product usually faces a global market. To meet the requirements of different local users, the software product must be internationalized. In an internationalized software product, user-visible hard-coded constant strings are externalized to resource files so that local versions can be generated by translating the resource files. In many cases, a software product is not internationalized at the beginning of the software development process. To internationalize an existing product, the developers must locate the user-visible constant strings that should be externalized. This locating process is tedious and error-prone due to 1) the large number of both user-visible and non-user-visible constant strings and 2) the complex data flows from constant strings to the Graphical User Interface (GUI). In this paper, we propose an automatic approach to locating need-to-externalize constant strings in the source code of a software product. Given a list of precollected API methods that output values of their string argument variables to the GUI and the source code of the software product under analysis, our approach traces from the invocation sites (within the source code) of these methods back to the need-to-externalize constant strings using generalized string-taint analysis. In our empirical evaluation, we used our approach to locate need-to-externalize constant strings in the uninternationalized versions of seven real-world open source software products. The results of our evaluation demonstrate that our approach is able to effectively locate need-to-externalize constant strings in uninternationalized software products. Furthermore, to help developers understand why a constant string requires translation and properly translate the need-to-externalize strings, we provide visual representation of the string dependencies related to the need-to-externalize strings. © 1976-2012 IEEE.",need-to-externalize constant strings | Software internationalization | string-taint analysis,IEEE Transactions on Software Engineering,2013-04-08,Article,"Wang, Xiaoyin;Zhang, Lu;Xie, Tao;Mei, Hong;Sun, Jiasu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877271808,10.1109/TSE.2012.63,Automated API property inference techniques,"Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information. © 1976-2012 IEEE.",API evolution | API property | API usage pattern | data mining | interface | pattern mining | programming rules | protocols | specifications,IEEE Transactions on Software Engineering,2013-05-13,Article,"Robillard, Martin P.;Bodden, Eric;Kawrykow, David;Mezini, Mira;Ratchford, Tristan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877272739,10.1109/TSE.2012.54,Compositional verification for hierarchical scheduling of real-time systems,"Hierarchical Scheduling (HS) techniques achieve resource partitioning among a set of real-time applications, providing reduction of complexity, confinement of failure modes, and temporal isolation among system applications. This facilitates compositional analysis for architectural verification and plays a crucial role in all industrial areas where high-performance microprocessors allow growing integration of multiple applications on a single platform. We propose a compositional approach to formal specification and schedulability analysis of real-time applications running under a Time Division Multiplexing (TDM) global scheduler and preemptive Fixed Priority (FP) local schedulers, according to the ARINC-653 standard. As a characterizing trait, each application is made of periodic, sporadic, and jittering tasks with offsets, jitters, and nondeterministic execution times, encompassing intra-application synchronizations through semaphores and mailboxes and interapplication communications among periodic tasks through message passing. The approach leverages the assumption of a TDM partitioning to enable compositional design and analysis based on the model of preemptive Time Petri Nets (pTPNs), which is expressly extended with a concept of Required Interface (RI) that specifies the embedding environment of an application through sequencing and timing constraints. This enables exact verification of intra-application constraints and approximate but safe verification of interapplication constraints. Experimentation illustrates results and validates their applicability on two challenging workloads in the field of safety-critical avionic systems. © 1976-2012 IEEE.",ARINC-653 | compositional verification | hierarchical scheduling | preemptive fixed priority | preemptive time Petri nets | Real-time systems | symbolic state-space analysis | time division multiplexing,IEEE Transactions on Software Engineering,2013-05-13,Article,"Carnevali, Laura;Pinzuti, Alessandro;Vicario, Enrico",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84878387692,10.1109/TSE.2012.69,Abstracting runtime heaps for program understanding,"Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, and computing changes between abstract models. The abstract model is designed to emphasize high-level concepts about heap-based data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We use this tool to identify, pinpoint, and correct sources of memory bloat for programs from DaCapo. © 1976-2012 IEEE.",Heap structure | memory profiling | program understanding | runtime analysis,IEEE Transactions on Software Engineering,2013-06-05,Article,"Marron, Mark;Sanchez, Cesar;Su, Zhendong;Fahndrich, Manuel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84880204955,10.1109/TSE.2012.62,Synthesizing modal transition systems from triggered scenarios,"Synthesis of operational behavior models from scenario-based specifications has been extensively studied. The focus has been mainly on either existential or universal interpretations. One noteworthy exception is Live Sequence Charts (LSCs), which provides expressive constructs for conditional universal scenarios and some limited support for nonconditional existential scenarios. In this paper, we propose a scenario-based language that supports both existential and universal interpretations for conditional scenarios. Existing model synthesis techniques use traditional two-valued behavior models, such as Labeled Transition Systems. These are not sufficiently expressive to accommodate specification languages with both existential and universal scenarios. We therefore shift the target of synthesis to Modal Transition Systems (MTS), an extension of labeled Transition Systems that can distinguish between required, unknown, and proscribed behavior to capture the semantics of existential and universal scenarios. Modal Transition Systems support elaboration of behavior models through refinement, which complements an incremental elicitation process suitable for specifying behavior with scenario-based notations. The synthesis algorithm that we define constructs a Modal Transition System that uses refinement to characterize all the Labeled Transition Systems models that satisfy a mixed, conditional existential and universal scenario-based specification. We show how this combination of scenario language, synthesis, and Modal Transition Systems supports behavior model elaboration. © 1976-2012 IEEE.",MTS | partial behavior models | Scenarios | synthesis,IEEE Transactions on Software Engineering,2013-07-22,Article,"Sibay, German Emir;Braberman, Victor;Uchitel, Sebastian;Kramer, Jeff",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883433355,10.1109/TSE.2013.10,Model-based test oracle generation for automated unit testing of agent systems,"Software testing remains the most widely used approach to verification in industry today, consuming between 30-50 percent of the entire development cost. Test input selection for intelligent agents presents a problem due to the very fact that the agents are intended to operate robustly under conditions which developers did not consider and would therefore be unlikely to test. Using methods to automatically generate and execute tests is one way to provide coverage of many conditions without significantly increasing cost. However, one problem using automatic generation and execution of tests is the oracle problem: How can we automatically decide if observed program behavior is correct with respect to its specification? In this paper, we present a model-based oracle generation method for unit testing belief-desire-intention agents. We develop a fault model based on the features of the core units to capture the types of faults that may be encountered and define how to automatically generate a partial, passive oracle from the agent design models. We evaluate both the fault model and the oracle generation by testing 14 agent systems. Over 400 issues were raised, and these were analyzed to ascertain whether they represented genuine faults or were false positives. We found that over 70 percent of issues raised were indicative of problems in either the design or the code. Of the 19 checks performed by our oracle, faults were found by all but 5 of these checks. We also found that 8 out the 11 fault types identified in our fault model exhibited at least one fault. The evaluation indicates that the fault model is a productive conceptualization of the problems to be expected in agent unit testing and that the oracle is able to find a substantial number of such faults with relatively small overhead in terms of false positives. © 1976-2012 IEEE.",BDI agents | Test oracles | unit testing,IEEE Transactions on Software Engineering,2013-09-09,Article,"Padgham, Lin;Zhang, Zhiyong;Thangarajah, John;Miller, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883437866,10.1109/TSE.2013.12,Patterns of knowledge in API reference documentation,"Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and.NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs. © 1976-2012 IEEE.",.NET | API documentation | content analysis | data mining | empirical study | grounded method | Java | pattern mining | software documentation,IEEE Transactions on Software Engineering,2013-09-09,Article,"Maalej, Walid;Robillard, Martin P.",Include,
10.1016/j.jss.2022.111515,2-s2.0-84883351715,10.1109/TSE.2013.15,TACO: Efficient SAT-based bounded verification using symmetry breaking and tight bounds,"SAT-based bounded verification of annotated code consists of translating the code together with the annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the failure is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this paper, we present Translation of Annotated COde (TACO), a prototype tool which implements a novel, general, and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We instrument code analysis with a symmetry-breaking predicate which, on one hand, reduces the size of the search space by ignoring certain classes of isomorphic models and, on the other hand, allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading to an improvement of the efficiency of the analysis of orders of magnitude, compared to the noninstrumented SAT-based analysis. We show that in some cases our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking, or SMT-solving. © 1976-2012 IEEE.",Alloy | DynAlloy | KodKod | SAT-based code analysis | Static analysis,IEEE Transactions on Software Engineering,2013-09-09,Article,"Galeotti, Juan P.;Rosner, Nicolas;Lopez Pombo, Carlos G.;Frias, Marcelo F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84884899241,10.1109/TSE.2013.18,Monitoring data usage in distributed systems,"IT systems manage increasing amounts of sensitive data and there is a growing concern that they comply with policies that regulate data usage. In this paper, we use temporal logic to express policies and runtime monitoring to check system compliance. While well-established methods for monitoring linearly ordered system behavior exist, a major challenge is monitoring distributed and concurrent systems where actions are locally observed in the different system parts. These observations can only be partially ordered, while policy compliance may depend on the actions' actual order of appearance. Technically speaking, it is in general intractable to check compliance of partially ordered traces. We identify fragments of our policy specification language for which compliance can be checked efficiently, namely, by monitoring a single representative trace in which the observed actions are totally ordered. Through a case study we show that the fragments are capable of expressing nontrivial policies and that monitoring representative traces is feasible on real-world data. © 2013 IEEE.",distributed systems | Monitors | regulation | temporal logic | verification,IEEE Transactions on Software Engineering,2013-10-07,Article,"Basin, David;Harvan, Matus;Klaedtke, Felix;Zalinescu, Eugen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84890085180,10.1109/TSE.2013.43,Learning project management decisions: A case study with case-based reasoning versus data farming,"Background: Given information on just a few prior projects, how do we learn the best and fewest changes for current projects? Aim: To conduct a case study comparing two ways to recommend project changes. 1) Data farmers use Monte Carlo sampling to survey and summarize the space of possible outcomes. 2) Case-based reasoners (CBR) explore the neighborhood around test instances. Method: We applied a state-of-the data farmer (SEESAW) and a CBR tool (({\cal W}2)) to software project data. Results: CBR with ({\cal W}2) was more effective than SEESAW's data farming for learning best and recommended project changes, effectively reducing runtime, effort, and defects. Further, CBR with ({\cal W}2) was comparably easier to build, maintain, and apply in novel domains, especially on noisy data sets. Conclusion: Use CBR tools like ({\cal W}2) when data are scarce or noisy or when project data cannot be expressed in the required form of a data farmer. Future Work: This study applied our own CBR tool to several small data sets. Future work could apply other CBR tools and data farmers to other data (perhaps to explore other goals such as, say, minimizing maintenance effort). © 1976-2012 IEEE.",case-based reasoning | COCOMO | data farming | Search-based software engineering,IEEE Transactions on Software Engineering,2013-12-01,Article,"Menzies, Tim;Brady, Adam;Keung, Jacky;Hihn, Jairus;Williams, Steven;El-Rawas, Oussama;Green, Phillip;Boehm, Barry",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887278859,10.1145/2507065.2507073,Visual research methods and communication design,"Visual research methods include a variety of empirical approaches to studying social life and social processes, including communication and documentation. Developed largely in anthropology and sociology, visual methods typically involve the use of photography, videography, and drawing in qualitative studies of lived experience. Despite the use of visual methods in related fields such as CSCW, HCI, and computer science education, such approaches are underdeveloped in studies of communication design. In this paper, the author provides a historical and theoretical overview of visual research methods before detailing three interrelated approaches that may be productively applied to work in communication design. The author then illustrates how these approaches were adapted to communication design studies in industry and academe before describing implications for future work in this area. © 2013 ACM.",photo-elicitation | visual ethnography | visual research methods | wagr,SIGDOC 2013 - Proceedings of the 31st ACM International Conference on Design of Communication,2013-11-14,Conference Paper,"McNely, Brian J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887279630,10.1145/2507065.2507070,On signifying the complexity of inter-agent relations in agentsheets games and simulations,"This paper reports the results of an empirical study about the semiotic engineering of signs of complexity for live documentation of games and simulations built with a visual programming learning environment. The study highlights the essence of the semiotic engineering process and shows how its outcome has been received by a group of users who can speak for a large portion of the live documentation system's user population. It also shows how the communication of complexity is, in and of itself, a major design challenge, especially when mastering complexity is one of the prime purposes of the documented object. Because the study was carried out in the context of a live documentation system, its conclusions can also illustrate how to conduct semiotically-inspired interaction design. © 2013 ACM.",agentsheets. | computational thinking acquisition | live documentation | meaning of program representations | semiotic engineering in practice,SIGDOC 2013 - Proceedings of the 31st ACM International Conference on Design of Communication,2013-11-14,Conference Paper,"Mota, Marcelle P.;Monteiro, Ingrid T.;Ferreira, Juliana J.;Slaviero, Cleyton;De Souza, Clarisse Sieckenius",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887295940,10.1145/2507065.2507076,API documentation and software community values: A survey of open-source API documentation,"Studies of what software developers need from API documentation have reported consistent findings over the years; however, these studies all used similar methods - usually a form of observation or survey. Our study looks at API documentation as artifacts of the open-source software communities who produce them to study how documentation produced by the communities who use the software compares to past studies of what software developers want and need from API documentation. We reviewed API documentation from 33 of the most popular open-source software projects, assessed their documentation elements, and evaluated the quality of their visual design and writing. We found that the documentation we studied included most or all the documentation elements reported as desirable in earlier studies and in the process, we found that the design and writing quality of many documentation sets received considerable attention. Our findings reinforce the API requirements identified in the literature and suggest that the design and writing quality of the documentation are also critical API documentation requirements that warrant further study. © 2013 ACM.",api | api reference documentation | application programming interface | software documentation | software libraries,SIGDOC 2013 - Proceedings of the 31st ACM International Conference on Design of Communication,2013-11-14,Conference Paper,"Watson, Robert;Mark Stamnes, Mark;Jeannot-Schroeder, Jacob;Spyridakis, Jan H.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84887279091,10.1145/2507065.2507092,Participatory documentation: A case study & rationale,"This poster focuses on how consumers learn to use marketing automation software, what types of documentation they prefer, and why. Interviews with U.S. and U.K. marketers demonstrate that implementing marketing automation software requires users to re-learn their jobs, and requires companies to reconfigure organizational structures and workflows. Accordingly, users are interested in knowing how counterparts apply the software, despite raising concerns about privacy. Based on these findings, the poster illustrates the advantages that software companies - especially those operating on a Waterfall development model - can gain by allowing users to participate in creating and refining documentation. In addition to reducing the learning curve for users, participatory documentation enables companies to gather feedback that is critical to making documentation and software more usable. © 2013 Author.",documentation | instructional design | marketing automation software | participatory culture | technical communication,SIGDOC 2013 - Proceedings of the 31st ACM International Conference on Design of Communication,2013-11-14,Conference Paper,"Pitts, Elizabeth A.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,,10.1145/2379776.2379782,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84874284297,10.1145/2480741.2480742,Game theory meets network security and privacy,"This survey provides a structured and comprehensive overview of research on security and privacy in computer and communication networks that use game-theoretic approaches. We present a selected set of works to highlight the application of game theory in addressing different forms of security and privacy problems in computer networks and mobile applications. We organize the presented works in six main categories: security of the physical and MAC layers, security of self-organizing networks, intrusion detection systems, anonymity and privacy, economics of network security, and cryptography. In each category, we identify security problems, players, and game models. We summarize the main results of selected works, such as equilibrium analysis and security mechanism designs. In addition, we provide a discussion on the advantages, drawbacks, and future direction of using game theory in this field. In this survey, our goal is to instill in the reader an enhanced understanding of different research approaches in applying gametheoretic methods to network security. This survey can also help researchers from various fields develop game-theoretic solutions to current and emerging security problems in computer networking. © 2013 ACM.",Cryptography | Game theory | Intrusion detection system | Location privacy | Multiparty computation | Network security and privacy | Revocation | Wireless security,ACM Computing Surveys,2013-06-01,Review,"Manshaei, Mohammad Hossein;Zhu, Quanyan;Alpcan, Tansu;Basar, Tamer;Hubaux, Jean Pierre",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84880099657,10.1145/2480741.2480745,Recovery within long-running transactions,"As computer systems continue to grow in complexity, the possibility of failure increases. At the same time, the increase in computer system pervasiveness in day-to-day activities bring along increased expectations on their reliability. This has led to the need for effective and automatic error-recovery techniques to resolve failures. Transactions enable the handling of failure propagation over concurrent systems due to dependencies, restoring the system to the point before the failure occurred. However, in various settings, especially when interacting with the real world, reversal is not possible. The notion of compensations has been long advocated as a way of addressing this issue, through the specification of activities which can be executed to undo partial transactions. Still, there is no accepted standard theory; the literature offers a plethora of distinct formalisms and approaches. In this survey, we review the compensations from a theoretical point of view by (i) giving a historic account of the evolution of compensating transactions; (ii) delineating and describing a number of design options involved; (iii) presenting a number of formalisms found in the literature, exposing similarities and differences; (iv) comparing formal notions of compensation correctness; (v) giving insights regarding the application of compensations in practice; and (vi) discussing current and future research trends in the area. © 2013 ACM.",Compensations,ACM Computing Surveys,2013-06-01,Review,"Colombo, Christian;Pace, Gordon J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84878609247,10.1145/2501654.2501658,Near-duplicate video retrieval: Current research and future trends,"The exponential growth of online videos, along with increasing user involvement in video-related activities, has been observed as a constant phenomenon during the last decade. User's time spent on video capturing, editing, uploading, searching, and viewing has boosted to an unprecedented level. The massive publishing and sharing of videos has given rise to the existence of an already large amount of near-duplicate content. This imposes urgent demands on near-duplicate video retrieval as a key role in novel tasks such as video search, video copyright protection, video recommendation, and many more. Driven by its significance, nearduplicate video retrieval has recently attracted a lot of attention. As discovered in recent works, latest improvements and progress in near-duplicate video retrieval, as well as related topics including low-level feature extraction, signature generation, and high-dimensional indexing, are employed to assist the process. As we survey the works in near-duplicate video retrieval, we comparatively investigate existing variants of the definition of near-duplicate video, describe a generic framework, summarize state-of-the-art practices, and explore the emerging trends in this research topic. © 2013 ACM.",Indexing | Near-duplicate video | Video retrieval,ACM Computing Surveys,2013-10-01,Article,"Liu, Jiajun;Huang, Zi;Cai, Hongyun;Shen, Heng Tao;Ngo, Chong Wah;Wang, Wei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881184013,10.1007/s10664-012-9233-9,Integrating conceptual and logical couplings for change impact analysis in software,"The paper presents an approach that combines conceptual and evolutionary techniques to support change impact analysis in source code. Conceptual couplings capture the extent to which domain concepts and software artifacts are related to each other. This information is derived using Information Retrieval based analysis of textual software artifacts that are found in a single version of software (e.g., comments and identifiers in a single snapshot of source code). Evolutionary couplings capture the extent to which software artifacts were co-changed. This information is derived from analyzing patterns, relationships, and relevant information of source code changes mined from multiple versions in software repositories. The premise is that such combined methods provide improvements to the accuracy of impact sets compared to the two individual approaches. A rigorous empirical assessment on the changes of the open source systems Apache httpd, ArgoUML, iBatis, KOffice, and jEdit is also reported. The impact sets are evaluated at the file and method levels of granularity for all the software systems considered in the empirical evaluation. The results show that a combination of conceptual and evolutionary techniques, across several cut-off points and periods of history, provides statistically significant improvements in accuracy over either of the two techniques used independently. Improvements in F-measure values of up to 14% (from 3% to 17%) over the conceptual technique in ArgoUML at the method granularity, and up to 21% over the evolutionary technique in iBatis (from 9% to 30%) at the file granularity were reported. © 2012 Springer Science+Business Media New York.",Change impact analysis | Conceptual and logical coupling | Information Retrieval | Mining software repositories | Open-source software | Software evolution and maintenance,Empirical Software Engineering,2013-10-01,Article,"Kagdi, Huzefa;Gethers, Malcom;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881192641,10.1007/s10664-012-9228-6,Studying re-opened bugs in open source software,"Bug fixing accounts for a large amount of the software maintenance resources. Generally, bugs are reported, fixed, verified and closed. However, in some cases bugs have to be re-opened. Re-opened bugs increase maintenance costs, degrade the overall user-perceived quality of the software and lead to unnecessary rework by busy practitioners. In this paper, we study and predict re-opened bugs through a case study on three large open source projects - namely Eclipse, Apache and OpenOffice. We structure our study along four dimensions: (1) the work habits dimension (e.g., the weekday on which the bug was initially closed), (2) the bug report dimension (e.g., the component in which the bug was found) (3) the bug fix dimension (e.g., the amount of time it took to perform the initial fix) and (4) the team dimension (e.g., the experience of the bug fixer). We build decision trees using the aforementioned factors that aim to predict re-opened bugs. We perform top node analysis to determine which factors are the most important indicators of whether or not a bug will be re-opened. Our study shows that the comment text and last status of the bug when it is initially closed are the most important factors related to whether or not a bug will be re-opened. Using a combination of these dimensions, we can build explainable prediction models that can achieve a precision between 52.1-78.6 % and a recall in the range of 70.5-94.1 % when predicting whether a bug will be re-opened. We find that the factors that best indicate which bugs might be re-opened vary based on the project. The comment text is the most important factor for the Eclipse and OpenOffice projects, while the last status is the most important one for Apache. These factors should be closely examined in order to reduce maintenance cost due to re-opened bugs. © 2012 Springer Science+Business Media, LLC.",Bug reports | Open source software | Re-opened bugs,Empirical Software Engineering,2013-10-01,Article,"Shihab, Emad;Ihara, Akinori;Kamei, Yasutaka;Ibrahim, Walid M.;Ohira, Masao;Adams, Bram;Hassan, Ahmed E.;Matsumoto, Ken Ichi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84884674509,10.1007/s10664-012-9209-9,Automated topic naming: Supporting cross-project analysis of software maintenance activities,"Software repositories provide a deluge of software artifacts to analyze. Researchers have attempted to summarize, categorize, and relate these artifacts by using semi-unsupervised machine-learning algorithms, such as Latent Dirichlet Allocation (LDA). LDA is used for concept and topic analysis to suggest candidate word-lists or topics that describe and relate software artifacts. However, these word-lists and topics are difficult to interpret in the absence of meaningful summary labels. Current attempts to interpret topics assume manual labelling and do not use domain-specific knowledge to improve, contextualize, or describe results for the developers. We propose a solution: automated labelled topic extraction. Topics are extracted using LDA from commit-log comments recovered from source control systems. These topics are given labels from a generalizable cross-project taxonomy, consisting of non-functional requirements. Our approach was evaluated with experiments and case studies on three large-scale Relational Database Management System (RDBMS) projects: MySQL, PostgreSQL and MaxDB. The case studies show that labelled topic extraction can produce appropriate, context-sensitive labels that are relevant to these projects, and provide fresh insight into their evolving software development activities. © 2012 Springer Science+Business Media, LLC.",Latent Dirichlet allocation | Repository mining | Software maintenance | Topic models,Empirical Software Engineering,2013-12-01,Article,"Hindle, Abram;Ernst, Neil A.;Godfrey, Michael W.;Mylopoulos, John",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871927010,10.1016/j.jss.2012.09.006,Constraint-based specification of model transformations,"Model transformations are a central element of model-driven development (MDD) approaches. The correctness, modularity and flexibility of model transformations is critical to their effective use in practical software development. In this paper we describe an approach for the automated derivation of correct-by-construction transformation implementations from high-level specifications. We illustrate this approach on a range of model transformation case studies of different kinds (re-expression, refinement, quality improvement and abstraction transformations) and describe ways in which transformations can be composed and evolved using this approach. © 2012 Elsevier Inc. All rights reserved.",Model transformations | Model-driven development | Software synthesis,Journal of Systems and Software,2013-02-01,Article,"Lano, K.;Kolahdouz-Rahimi, S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871925484,10.1016/j.jss.2012.09.036,Design of component-based real-time applications,"This paper presents the key aspects of a model-based methodology that is proposed for the design of component-based applications with hard real-time requirements. The methodology relies on RT-CCM (Real-time Container Component Model), a component technology aimed to make the timing behaviour of the applications predictable and inspired in the Lightweight CCM specification of the OMG. Some new mechanisms have been introduced in the underlying framework that make it possible to schedule the execution of code and the transmission of messages of an application while guaranteeing that the application will meet its timing requirements when executed. The added mechanisms also enable the application designer to configure this scheduling without interfering with the opacity typically required in component management. Moreover, the methodology includes a process for generating the real-time model of a component-based application as a composition of the reusable real-time models of the components that form it. From the analysis of this model the application designer obtains the configuration values that must be applied to the component instances and the elements of the framework in order to make the application fulfil its timing requirements. © 2012 Elsevier Inc.",Component-based applications | Reactive model | Real-time systems | Schedulability | Software component | Software reusability,Journal of Systems and Software,2013-02-01,Article,"López Martínez, Patricia;Barros, Laura;Drake, José M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84871927997,10.1016/j.jss.2012.09.037,A posteriori operation detection in evolving software models,"As every software artifact, also software models are subject to continuous evolution. The operations applied between two successive versions of a model are crucial for understanding its evolution. Generic approaches for detecting operations a posteriori identify atomic operations, but neglect composite operations, such as refactorings, which leads to cluttered difference reports. To tackle this limitation, we present an orthogonal extension of existing atomic operation detection approaches for detecting also composite operations. Our approach searches for occurrences of composite operations within a set of detected atomic operations in a post-processing manner. One major benefit is the reuse of specifications available for executing composite operations also for detecting applications of them. We evaluate the accuracy of the approach in a real-world case study and investigate the scalability of our implementation in an experiment. © 2012 Elsevier Inc. All rights reserved.",Model comparison | Model evolution | Model refactoring,Journal of Systems and Software,2013-02-01,Article,"Langer, Philip;Wimmer, Manuel;Brosch, Petra;Herrmannsdörfer, Markus;Seidl, Martina;Wieland, Konrad;Kappel, Gerti",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84875269874,10.1016/j.jss.2012.10.041,25 years of software engineering in Brazil: Beyond an insider's view,"The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 SBES editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian SE scenario. Our findings also suggest that greater attention should be given to the SE area, by improving researchers' interaction with industry and increasing collaboration between researchers, especially internationally. © 2012 Elsevier Inc.",Expert opinion survey | Mapping study | Software engineering,Journal of Systems and Software,2013-04-01,Conference Paper,"Silveira Neto, Paulo Anselmo Da Mota;Gomes, Joás Sousa;De Almeida, Eduardo Santana;Leite, Jair Cavalcanti;Batista, Thais Vasconcelos;Leite, Larissa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84875245673,10.1016/j.jss.2012.11.044,Evidence of software inspection on feature specification for software product lines,"In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.© 2012 Elsevier Inc. All rights reserved.",Empirical study | Software inspection | Software product lines | Software quality control,Journal of Systems and Software,2013-05-01,Article,"Souza, Iuri Santos;Da Silva Gomes, Gecynalda Soares;Da Mota Silveira Neto, Paulo Anselmo;Do Carmo Machado, Ivan;De Almeida, Eduardo Santana;De Lemos Meira, Silvio Romero",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84879951544,10.1016/j.jss.2013.03.063,Research state of the art on GoF design patterns: A mapping study,"Design patterns are used in software development to provide reusable and documented solutions to common design problems. Although many studies have explored various aspects of design patterns, no research summarizing the state of research related to design patterns existed up to now. This paper presents the results of a mapping study of about 120 primary studies, to provide an overview of the research efforts on Gang of Four (GoF) design patterns. The research questions of this study deal with (a) if design pattern research can be further categorized in research subtopics, (b) which of the above subtopics are the most active ones and (c) what is the reported effect of GoF patterns on software quality attributes. The results suggest that design pattern research can be further categorized to research on GoF patterns formalization, detection and application and on the effect of GoF patterns on software quality attributes. Concerning the intensity of research activity of the abovementioned subtopics, research on pattern detection and on the effect of GoF patterns on software quality attributes appear to be the most active ones. Finally, the reported research to date on the effect of GoF patterns on software quality attributes are controversial; because some studies identify one pattern's effect as beneficial whereas others report the same pattern's effect as harmful. © 2013 Elsevier Inc. All rights reserved.",Design patterns | Mapping study | Software quality attributes,Journal of Systems and Software,2013-07-01,Article,"Ampatzoglou, Apostolos;Charalampidou, Sofia;Stamelos, Ioannis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84880176279,10.1016/j.jss.2012.08.024,Improving logic-based testing,"Logic-based testers design tests from logical expressions that appear in software artifacts such as source code, design models, and requirements specifications. This paper presents three improvements to logic-based test design. First, in the context of mutation testing, we present fault hierarchies for the six relational operators. Applying the ROR mutation operator causes each relational operator to generate seven mutants per clause. The fault hierarchies show that only three of these seven mutants are needed. Second, we show how to bring the power of the ROR operator to logic-based test criteria such as the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. Third, we present theoretical results supported by empirical data that show that the more recent coverage criterion of minimal-MUMCUT can find significantly more faults than MCDC. The paper has three specific recommendations: (1) Change the way the ROR mutation operator is defined in existing and future mutation systems. (2) Augment logic-based test criteria to incorporate relational operator replacement from mutation. (3) Replace the use of MCDC with minimal-MUMCUT, both in practice and in standards documents like FAA-DO178B. © 2013 Elsevier Inc. All rights reserved.",Logic-based testing | MCDC | Mutation analysis | Software testing,Journal of Systems and Software,2013-08-01,Article,"Kaminski, Gary;Ammann, Paul;Offutt, Jeff",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84880144133,10.1016/j.jss.2013.03.084,"Relevance, benefits, and problems of software modelling and model driven techniques - A survey in the Italian industry","Context Claimed benefits of software modelling and model driven techniques are improvements in productivity, portability, maintainability and interoperability. However, little effort has been devoted at collecting evidence to evaluate their actual relevance, benefits and usage complications. Goal The main goals of this paper are: (1) assess the diffusion and relevance of software modelling and MD techniques in the Italian industry, (2) understand the expected and achieved benefits, and (3) identify which problems limit/prevent their diffusion. Method We conducted an exploratory personal opinion survey with a sample of 155 Italian software professionals by means of a Web-based questionnaire on-line from February to April 2011. Results Software modelling and MD techniques are very relevant in the Italian industry. The adoption of simple modelling brings common benefits (better design support, documentation improvement, better maintenance, and higher software quality), while MD techniques make it easier to achieve: improved standardization, higher productivity, and platform independence. We identified problems, some hindering adoption (too much effort required and limited usefulness) others preventing it (lack of competencies and supporting tools). Conclusions The relevance represents an important objective motivation for researchers in this area. The relationship between techniques and attainable benefits represents an instrument for practitioners planning the adoption of such techniques. In addition the findings may provide hints for companies and universities. © 2013 Elsevier Inc.",Model driven techniques | Personal opinion survey | Software modelling,Journal of Systems and Software,2013-08-01,Article,"Torchiano, Marco;Tomassetti, Federico;Ricca, Filippo;Tiso, Alessandro;Reggio, Gianna",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881476524,10.1016/j.jss.2013.03.104,A design rule language for aspect-oriented programming,"Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches. © 2013 Elsevier Inc.",Aspect-oriented programming | Design rules | Modularity,Journal of Systems and Software,2013-09-01,Article,"Costa Neto, Alberto;Bonifácio, Rodrigo;Ribeiro, Márcio;Pontual, Carlos Eduardo;Borba, Paulo;Castor, Fernando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84882637923,10.1016/j.jss.2013.03.078,Efficient optimization of large probabilistic models,"The development of safety critical systems often requires design decisions which influence not only dependability, but also other properties which are often even antagonistic to dependability, e.g.; cost. Finding good compromises considering different goals while at the same time guaranteeing sufficiently high safety of a system is a very difficult task. We propose an integrated approach for modeling, analysis and optimization of safety critical systems. It is fully automated with an implementation based on the Eclipse platform. The approach is tool-independent, different analysis tools can be used and there exists an API for the integration of different optimization and estimation algorithms. For safety critical systems, a very important criterion is the hazard occurrence probability, whose computation can be quite costly. Therefore we also provide means to speed up optimization by devising different combinations of stochastic estimators and illustrate how they can be integrated into the approach. We illustrate the approach on relevant case-studies and provide experimental details to validate its effectiveness and applicability. © 2013 Elsevier Inc.",Formal methods | Multi-objective optimization | Safety analysis | Safety optimization,Journal of Systems and Software,2013-10-01,Article,"Struck, Simon;Güdemann, Matthias;Ortmeier, Frank",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84884152279,10.1016/j.jss.2013.06.064,A systematic review on the functional testing of semantic web services,"Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services. © 2013 Elsevier Inc.",Functional testing | Semantic web services | Systematic literature review | Testing approach,Journal of Systems and Software,2013-01-01,Article,"Tahir, Abbas;Tosi, Davide;Morasca, Sandro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887024943,10.1016/j.jss.2013.07.060,Introduction to the JSS special issue of Web 2.0 engineering: New practices and emerging Challenges,,,Journal of Systems and Software,2013-12-01,Editorial,"Brambilla, Marco;Cappiello, Cinzia;Garrigós, Irene;Mazón, Jose Norberto;Meliá, Santiago",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84887015555,10.1016/j.jss.2013.05.045,Detecting Web requirements conflicts and inconsistencies under a model-based perspective,"Web requirements engineering is an essential phase in the software project life cycle for the project results. This phase covers different activities and tasks that in many situations, depending on the analyst's experience or intuition, help getting accurate specifications. One of these tasks is the conciliation of requirements in projects with different groups of users. This article presents an approach for the systematic conciliation of requirements in big projects dealing with a model-based approach. The article presents a possible implementation of the approach in the context of the NDT (Navigational Development Techniques) Methodology and shows the empirical evaluation in a real project by analysing the improvements obtained with our approach. The paper presents interesting results that demonstrate that we can get a reduction in the time required to find conflicts between requirements, which implies a reduction in the global development costs. © 2013 Elsevier Inc. All rights reserved.",Consistency | Contradiction | Web requirements,Journal of Systems and Software,2013-12-01,Article,"Escalona, M. J.;Urbieta, M.;Rossi, G.;Garcia-Garcia, J. A.;Luna, E. Robles",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84887016915,10.1016/j.jss.2013.07.021,A language-independent approach to black-box testing using Erlang as test specification language,"Integration of reused, well-designed components and subsystems is a common practice in software development. Hence, testing integration interfaces is a key activity, and a whole range of technical challenges arise from the complexity and versatility of such components. In this paper, we present a methodology to fully test different implementations of a software component integration API. More precisely, we propose a black-box testing approach, based on the use of QuickCheck and inspired by the TTCN-3 test architecture, to specify and test the expected behavior of a component. We have used a real-world multimedia content management system as case study. This system offers the same integration API for different technologies: Java, Erlang and HTTP/XML. Using our method, we have tested all integration API implementations using the same test specification, increasing the confidence in its interoperability and reusability. © 2013 Elsevier Inc.",Black-box testing | Functional testing | Test automation,Journal of Systems and Software,2013-12-01,Article,"Castro, Laura M.;Francisco, Miguel A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867403345,10.1016/j.infsof.2012.06.013,Reasoning with contextual requirements: Detecting inconsistency and conflicts,"Context: The environment in which the system operates, its context, is variable. The autonomous ability of a software to adapt to context has to be planned since the requirements analysis stage as a strong mutual influence between requirements and context does exist. On the one hand, context is a main factor to decide whether to activate a requirement, the applicable alternatives to meet an activated requirement as well as their qualities. On the other hand, the system actions to reach requirements could cause changes in the context. Objectives: Modelling the relationship between requirements and context is a complex task and developing error-free models is hard to achieve without an automated support. The main objective of this paper is to develop a set of automated analysis mechanisms to support the requirements engineers to detect and analyze modelling errors in contextual requirements models. Method: We study the analysis of the contextual goal model which is a requirements model that weaves together the variability of both context and requirements. Goal models are used during the early stages of software development and, thus, our analysis detects errors early in the development process. We develop two analysis mechanisms to detect two kinds of modelling errors. The first mechanism concerns the detection of inconsistent specification of contexts in a goal model. The second concerns the detection of conflicting context changes that arise as a consequence of the actions performed by the system to meet different requirements simultaneously. We support our analysis with a CASE tool and provide a systematic process that guides the construction and analysis of contextual goal models. We illustrate and evaluate our framework via a case study on a smart-home system for supporting the life of people having dementia problems. Results: The evaluation showed a significant ability of our analysis mechanisms to detect errors which were not notable by requirements engineers. Moreover, the evaluation showed acceptable performance of these mechanisms when processing up to medium-sized contextual goal models. The modelling constructs which we proposed as an input to enable the analysis were found easy to understand and capture. Conclusions: Our developed analysis for the detection of inconsistency and conflicts in contextual goal models is an essential step for the entire system correctness. It avoids us developing unusable and unwanted functionalities and functionalities which lead to conflicts when they operate together. Further research to improve our analysis to scale with large-sized models and to consider other kinds of errors is still needed. © 2012 Elsevier B.V. All rights reserved.",Adaptive systems engineering | Consistency and conflicts analysis | Contextual requirements | Goal modelling | Requirements engineering,Information and Software Technology,2013-01-01,Conference Paper,"Ali, Raian;Dalpiaz, Fabiano;Giorgini, Paolo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877923702,10.1016/j.infsof.2013.01.006,An investigation of how quality requirements are specified in industrial practice,"Context: This paper analyses a sub-contractor specification in the mobile handset domain. Objective: The objective is to understand how quality requirements are specified and which types of requirements exist in a requirements specification from industry. Method: The case study is performed in the mobile handset domain, where a requirements specification was analyzed by categorizing and characterizing the pertaining requirements. Results: The requirements specification is written in structured natural language with unique identifiers for the requirements. Of the 2178 requirements, 827 (38%) are quality requirements. Of the quality requirements, 56% are quantified, i.e., having a direct metric in the requirement. The variation across the different sub-domains within the requirements specification is large. Conclusion: The findings from this study suggest that methods for quality requirements need to encompass many aspects to comprehensively support working with quality requirements. Solely focusing on, for example, quantification of quality requirements might overlook important requirements since there are many quality requirements in the studied specification where quantification is not appropriate. © 2013 Elsevier B.V. All rights reserved.",Case study | Document analysis | Metrics | Qualitative research | Quality requirements,Information and Software Technology,2013-07-01,Article,"Berntsson Svensson, Richard;Olsson, Thomas;Regnell, Björn",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869881906,10.1016/j.infsof.2012.08.005,Guest Editorial for Special Section from Component-based Software Engineering (CBSE) 2011,,,Information and Software Technology,2013-02-01,Editorial,"Bertolino, Antonia;Cooper, Kendra",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84893681664,10.1016/j.infsof.2013.02.011,ODEP-DPS: Ontology-driven engineering process for the collaborative development of semantic data providing services,"Context: Data services are services that handle operations involving the management of data. A problem with data services is that their interfaces are defined by their syntax alone. Consequently, Data Providing Services (DPSs) have been proposed to explicitly define semantics using ontologies for services that only retrieve data. However, the semantic annotations of DPSs are developed as afterthoughts to deployed data services. Objective: The objective of this work is to present a DPS development process that considers all of a DPS's dimensions including its data acquisition logic, syntax and semantics thus addressing the issue of semantic annotations developed as afterthoughts. This shall decrease the cost of deploying and maintaining DPSs. Method: This paper contributes a holistic and collaborative process - ODEP-DPS - for the development of DPSs. It is holistic as it considers both semantics and syntax from requirements to implementation. And it is collaborative as it separates responsibilities between the roles of those who require the data and those who own them. The process is to be ontology-driven as an ontological model shall be utilized through each phase of the process; it shall formalize the requirements domain, be used as a basis for the syntactic data model, and serve as the domain ontology for annotating the deployed DPSs. Results: This paper proposes the ODEP-DPS development process, in addition to defining three artefacts used throughout the process. In particular, a message descriptor is defined that binds semantics and syntax into a single reusable unit. A comprehensive definition of a DPS is also provided. ODEP-DPS is evaluated using a real-life case study from a mental health institution. Conclusion: This study contributes a holistic and collaborative development process that provides an end-to-end solution for the development of semantic data providing services. It addresses semantics being developed as afterthoughts by tightly coupling semantics and syntax. © 2013 Elsevier B.V. All rights reserved.",Ontologies | Ontology-driven development | Semantic data providing services | Semantic web | Semantic web service development | Service oriented architecture,Information and Software Technology,2013-01-01,Article,"Brown, Kevin P.;Capretz, Miriam A.M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877909370,10.1016/j.infsof.2012.12.006,Empirical studies concerning the maintenance of UML diagrams and their use in the maintenance of code: A systematic mapping study,"Context: The Unified Modelling Language (UML) has, after ten years, become established as the de facto standard for the modelling of object-oriented software systems. It is therefore relevant to investigate whether its use is important as regards the costs involved in its implantation in industry being worthwhile. Method: We have carried out a systematic mapping study to collect the empirical studies published in order to discover ""What is the current existing empirical evidence with regard to the use of UML diagrams in source code maintenance and the maintenance of the UML diagrams themselves? Results: We found 38 papers, which contained 63 experiments and 3 case studies. Conclusion: Although there is common belief that the use of UML is beneficial for source code maintenance, since the quality of the modifications is greater when UML diagrams are available, only 3 papers concerning this issue have been published. Most research (60 empirical studies) concerns the maintainability and comprehensibility of the UML diagrams themselves which form part of the system's documentation, since it is assumed that they may influence source code maintainability, although this has not been empirically validated. Moreover, the generalizability of the majority of the experiments is questionable given the material, tasks and subjects used. There is thus a need for more experiments and case studies to be performed in industrial contexts, i.e., with real systems and using maintenance tasks conducted by practitioners under real conditions that truly show the utility of UML diagrams in maintaining code, and that the fact that a diagram is more comprehensible or modifiable influences the maintainability of the code itself. This utility should also be studied from the viewpoint of cost and productivity, and the consistent and simultaneous maintenance of diagrams and code must also be considered in future empirical studies. © 2013 Elsevier B.V. All rights reserved.",Empirical studies | Software maintenance | Systematic literature review | Systematic mapping study | UML,Information and Software Technology,2013-01-01,Review,"Fernández-Sáez, Ana M.;Genero, Marcela;Chaudron, Michel R.V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84885181961,10.1016/j.infsof.2013.07.013,A formal approach for run-time verification of web applications using scope-extended LTL,"Context: In the past decade, the World Wide Web has been subject to rapid changes. Web sites have evolved from static information pages to dynamic and service-oriented applications that are used for a broad range of activities on a daily basis. For this reason, thorough analysis and verification of Web Applications help assure the deployment of high quality applications. Objectives: In this paper, an approach is presented to the formal verification and validation of existing web applications. The approach consists of using execution traces of a web application to automatically generate a communicating automata model. The obtained model is used to model checking the application against predefined properties, to perform regression testing, and for documentation. Methods: Traces used in the proposed approach are collected by monitoring a web application while it is explored by a user or a program. An automata-based model is derived from the collected traces by mapping the pages of the application under test into states and the links and forms used to browse the application into transitions between the states. Properties, meanwhile, express correctness and quality requirements on web applications and might concern all states of the model; in many cases, these properties concern only a proper subset of the states, in which case the model is refined to designate the subset of the global states of interest. A related problem of property specification in Linear Temporal Logic (LTL) over only a subset of states of a system is solved by means of specialized operators that facilitate specifying properties over propositional scopes in a concise and intuitive way. Each scope constitutes a subset of states that satisfy a propositional logic formula. Results: An implementation of the verification approach that uses the model checker Spin is presented where an integrated toolset is developed and empirical results are shown. Also, Linear Temporal Logic is extended with propositional scopes. Conclusion: a formal approach is developed to build a finite automata model tuned to features of web applications that have to be validated, while delegating the task of property verification to an existing model checker. Also, the problem of property specification in LTL over a subset of the states of a given system is addressed, and a generic and practical solution is proposed which does not require any changes in the system model by defining specialized operators in LTL using scopes. © 2013 Elsevier B.V. All rights reserved.",Communicating automata model | Dynamic analysis | Linear Temporal Logic | Model checking | Web applications | Web engineering,Information and Software Technology,2013-01-01,Article,"Haydar, May;Petrenko, Alexandre;Boroday, Sergiy;Sahraoui, Houari",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84876293295,10.1016/j.infsof.2012.12.007,Translation of Z specifications to executable code: Application to the database domain,"Context: It is well-known that the use of formal methods in the software development process results in high-quality software products. Having specified the software requirements in a formal notation, the question is how they can be transformed into an implementation. There is typically a mismatch between the specification and the implementation, known as the specification-implementation gap. Objective: This paper introduces a set of translation functions to fill the specification-implementation gap in the domain of database applications. We only present the formal definition, not the implementation, of the translation functions. Method: We chose Z, SQL and Delphi languages to illustrate our methodology. Because the mathematical foundation of Z has many properties in common with SQL, the translation functions from Z to SQL are derived easily. For the translation of Z to Delphi, we extend Delphi libraries to support Z mathematical structures such as sets and tuples. Then, based on these libraries, we derive the translation functions from Z to Delphi. Therefore, we establish a formal relationship between Z specifications and Delphi/SQL code. To prove the soundness of the translation from a Z abstract schema to the Delphi/SQL code, we define a Z design-level schema. We investigate the consistency of the Z abstract schema with the Z design-level schema by using Z refinement rules. Then, by the use of the laws of Morgan refinement calculus, we prove that the Delphi/SQL code refines the Z design-level schema. Results: The proposed approach can be used to build the correct prototype of a database application from its specification. This prototype can be evolved, or may be used to validate the software requirements specification against user requirements. Conclusion: Therefore, the work presented in this paper reduces the overall cost of the development of database applications because early validation reveals requirement errors sooner in the software development cycle. © 2012 Elsevier B.V. All rights reserved.",Database | Formal methods | Graphical user interface | Prototyping | Software development,Information and Software Technology,2013-06-01,Article,"Khalafinejad, Saeed;Mirian-Hosseinabadi, Seyed Hassan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84880850545,10.1016/j.infsof.2013.03.008,The state of the art in automated requirements elicitation,"Context In large software development projects a huge number of unstructured text documents from various stakeholders becomes available and needs to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Consequently, substantial research has been done to automate the process through a plethora of tools and technologies. Objective This paper aims to capture the current state of automated requirements elicitation and derive future research directions by identifying gaps in the existing body of knowledge and through relating existing works to each other. More specifically, we are investigating the following research question: What is the state of the art in research covering tool support for automated requirements elicitation from natural language documents? Method A systematic review of the literature in automated requirements elicitation is performed. Identified works are categorized using an analysis framework comprising tool categories, technological concepts and evaluation approaches. Furthermore, the identified papers are related to each other through citation analysis to trace the development of the research field. Results We identified, categorized and related 36 relevant publications. Summarizing the observations we made, we propose future research to (1) investigate alternative elicitation paradigms going beyond a pure automation approach (2) compare the effects of different types of knowledge on elicitation results (3) apply comparative evaluation methods and multi-dimensional evaluation measures and (4) strive for a closer integration of research activities across the sub-fields of automatic requirements elicitation. Conclusion Through the results of our paper, we intend to contribute to the Requirements Engineering body of knowledge by (1) conceptualizing an analysis framework for works in the area of automated requirements elicitation, going beyond former classifications (2) providing an extensive overview and categorization of existing works in this area (3) formulating concise directions for future research. © 2013 Elsevier B.V. All rights reserved.",Automation | Requirements Elicitation | Requirements Engineering | Requirements Reuse | Systematic Review,Information and Software Technology,2013-10-01,Review,"Meth, Hendrik;Brhel, Manuel;Maedche, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84884143407,10.1016/j.infsof.2013.05.006,Combining service-orientation and software product line engineering: A systematic mapping study,"Context Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas. © 2013 Elsevier B.V. All rights reserved.",Service-oriented architecture | Software product lines | Systematic mapping,Information and Software Technology,2013-01-01,Review,"Mohabbati, Bardia;Asadi, Mohsen;Gašević, Dragan;Hatala, Marek;Müller, Hausi A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869878047,10.1016/j.infsof.2012.08.015,System integration by developing adapters using a database abstraction,"Context: Large software systems are usually developed by integrating several smaller systems, which may have been developed independently. The integration of such systems often requires the development of a custom adapter (sometimes called mediator or glue logic) for bridging any technical incompatibilities between the systems. Adapter development often focuses on how to respond to events from the external interfaces, e.g., by applying data conversions and performing events on (other) external interfaces. Such an operational focus is closely related to an implementation of the adapter, but it makes it complicated to reason about complex adapters. For example, it requires a lot of effort to understand the relation that the adapter establishes between the systems to be integrated, and to avoid any internal inconsistencies. Objective: This article investigates a way to develop adapters in terms of a more abstract, model-based specification. Experts from the application domain should be able to reason about the specification, and the specification should contain enough details to generate an implementation. Method: Based on a few industrial adapters from the domain of Maritime Safety and Security, we study ways to specify them conveniently, and ways to generate them efficiently. On this basis, we identify an approach for developing adapters. In turn, this approach is validated using an additional set of adapters. Results: After abstracting from the details of the interface technologies, the studied adapters could be generated using techniques for incremental view maintenance. This requires an adapter specification in terms of database views to relate the main semantic concepts in the application domain. Conclusion: For developing adapters, it can be useful to model all interface technologies as database operations. Thus adapters can be specified using database views, which improve the conceptual understanding of the adapters. Publish/subscribe-style adapters can then be generated using incremental view maintenance. © 2012 Elsevier B.V. All rights reserved.",Adapter generation | Distributed systems | Incremental view maintenance | Model-driven development,Information and Software Technology,2013-02-01,Conference Paper,"Mooij, Arjan J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84876283324,10.1016/j.infsof.2012.12.003,Transforming and tracing reused requirements models to home automation models,"Context: Model-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a project's success. Despite the importance of RE, MDSD approaches commonly leave textual requirements specifications to one side. Objective: Our aim is to integrate textual requirements specifications into the MDSD approach by using the MDSD techniques themselves, including metamodelling and model transformations. The proposal is based on the assumption that a reuse-based Model-Driven Requirements Engineering (MDRE) approach will improve the requirements engineering stage, the quality of the development models generated from requirements models, and will enable the traces from requirements to other development concepts (such as analysis or design) to be maintained. Method: The approach revolves around the Requirements Engineering Metamodel, denominated as REMM, which supports the definition of the boilerplate based textual requirements specification languages needed for the definition of model transformation from application requirements models to platform-specific application models and code. Results: The approach has been evaluated through its application to Home Automation (HA) systems. The HA Requirement Specification Language denominated as HAREL is used to define application requirements models which will be automatically transformed and traced to the application model conforming to the HA Domain Specific Language. Conclusions: An anonymous online survey has been conducted to evaluate the degree of acceptance by both HA application developers and MDSD practitioners. The main conclusion is that 66.7% of the HA experts polled strongly agree that the automatic transformation of the requirements models to HA models improves the quality of the HA models. Moreover, 58.3% of the HA participants strongly agree with the usefulness of the traceability matrix which links requirements to HA functional units in order to discover which devices are related to a specific requirement. We can conclude that the experts we have consulted agree with the proposal we are presenting here, since the average mark given is 4 out of 5. © 2012 Elsevier B.V. All rights reserved.",Home automation models | Model driven software development | Models transformation | Requirements metamodel | Requirements reuse | Requirements traceability,Information and Software Technology,2013-06-01,Article,"Moros, Begoña;Toval, Ambrosio;Rosique, Francisca;Sánchez, Pedro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869861862,10.1016/j.infsof.2012.09.004,Interpretative case studies on agile team productivity and management,"Context: The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as ""light"" approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods. Objective: Our objective is to provide a better understanding of the factors and mediators that impact agile team productivity. Method: We have conducted a multiple-case study for 6 months in three large Brazilian companies that have been using agile methods for over 2 years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation. Results: We developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams. Conclusion: Teams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies. © 2012 Elsevier B.V. All rights reserved.",Agile software development | Industrial case studies | Team management | Team productivity factors | Thematic analysis,Information and Software Technology,2013-02-01,Conference Paper,"De Melo, Claudia O.;S. Cruzes, Daniela;Kon, Fabio;Conradi, Reidar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84869861894,10.1016/j.infsof.2012.08.007,Efficient and deterministic application deployment in component-based enterprise distributed real-time and embedded systems,"Context: Component-based middleware, such as the Lightweight CORBA Component Model, is increasingly used to implement enterprise distributed real-time and embedded (DRE) systems. In addition to supporting the quality-of-service (QoS) requirements of individual DRE systems, component technologies must also support bounded latencies when effecting deployment changes to DRE systems in response to changing environmental conditions and operational requirements. Objective: The goals of this paper are to (1) study sources of inefficiencies and non-deterministic performance in deployment capabilities for DRE systems and (2) devise solutions to overcome these performance problems. Method: The paper makes two contributions to the study of the deployment and configuration of distributed component based applications. First, we analyze how conventional implementations of the OMG's Deployment and Configuration (D&C) specification for component-based systems can significantly degrade deployment latencies. Second, we describe architectural changes and performance optimizations implemented within the Locality-Enhanced Deployment and Configuration Engine (LE-DAnCE) implementation of the D&C specification to obtain efficient and deterministic deployment latencies. Results: We analyze the performance of LE-DAnCE in the context of component deployments on 10 nodes for a representative DRE system consisting of 1000 components and in a cluster environment with up to 100 nodes. Our results show LE-DAnCE's optimizations provide a bounded deployment latency of less than 2 s for the 1000 component scenario with just a 4 percent jitter. Conclusion: The improvements contained in the LE-DAnCE infrastructure provide an efficient and scaleable standards-based deployment system for component-based enterprise DRE systems. In particular, deployment time parallelism can improve deployment latency significantly, both during pre-deployment analysis of the deployment plan and during the process of installing and activating components. © 2012 Elsevier B.V. All rights reserved.",Component-based real-time systems | Deployment and configuration | Deployment latency | Deterministic deployment | Efficient deployment | Parallel deployment,Information and Software Technology,2013-02-01,Conference Paper,"Otte, William R.;Gokhale, Aniruddha;Schmidt, Douglas C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867401202,10.1016/j.infsof.2012.06.002,A systematic review of software robustness,"Context: With the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view. Objective: To address this issue, we have conducted a literature review in the field of robustness. Method: This review has been conducted by following guidelines for systematic literature reviews. Systematic reviews are used to find and classify all existing and available literature in a certain field. Results: From 9193 initial papers found in three well-known research databases, the 144 relevant papers were extracted through a multi-step filtering process with independent validation in each step. These papers were then further analyzed and categorized based on their development phase, domain, research, contribution and evaluation type. The results indicate that most existing results on software robustness focus on verification and validation of Commercial of the shelf (COTS) or operating systems or propose design solutions for robustness while there is a lack of results on how to elicit and specify robustness requirements. The research is typically solution proposals with little to no evaluation and when there is some evaluation it is primarily done with small, toy/academic example systems. Conclusion: We conclude that there is a need for more software robustness research on real-world, industrial systems and on software development phases other than testing and design, in particular on requirements engineering. © 2012 Elsevier B.V. All rights reserved.",Robustness | Software robustness | Systematic review,Information and Software Technology,2013-01-01,Review,"Shahrokni, Ali;Feldt, Robert",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84867395838,10.1016/j.infsof.2012.07.009,Model-driven performance analysis of rule-based domain specific visual models,"Context: Domain-Specific Visual Languages (DSVLs) play a crucial role in Model-Driven Engineering (MDE). Most DSVLs already allow the specification of the structure and behavior of systems. However, there is also an increasing need to model, simulate and reason about their non-functional properties. In particular, QoS usage and management constraints (performance, reliability, etc.) are essential characteristics of any non-trivial system. Objective: Very few DSVLs currently offer support for modeling these kinds of properties. And those which do, tend to require skilled knowledge of specialized notations, which clashes with the intuitive nature of DSVLs. In this paper we present an alternative approach to specify QoS properties in a high-level and platform-independent manner. Method: We propose the use of special objects (observers) that can be added to the graphical specification of a system for describing and monitoring some of its non-functional properties. Results: Observers allow extending the global state of the system with the variables that the designer wants to analyze, being able to capture the performance properties of interest. A performance evaluation tool has also been developed as a proof of concept for the proposal. Conclusion: The results show how non-functional properties can be specified in DSVLs using observers, and how the performance of systems specified in this way can be evaluated in a flexible and effective way. © 2012 Elsevier B.V. All rights reserved.",Domain specific visual languages | Model-driven engineering | Performance analysis | Simulation,Information and Software Technology,2013-01-01,Article,"Troya, Javier;Vallecillo, Antonio;Durán, Francisco;Zschaler, Steffen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893381315,10.1109/WCRE.2013.6671315,Documenting APIs with examples: Lessons learned with the APIMiner platform,"Software development increasingly relies on Application Programming Interfaces (APIs) to increase productivity. However, learning how to use new APIs in many cases is a nontrivial task given their ever-increasing complexity. To help developers during the API learning process, we describe in this paper a platform - called APIMiner - that instruments the standard Java-based API documentation format with concrete examples of usage. The examples are extracted from a private source code repository - composed by real systems - and summarized using a static slicing algorithm. We also describe a particular instantiation of our platform for the Android API. To evaluate the proposed solution, we performed a field study, when professional Android developers used the platform by four months. © 2013 IEEE.",API documentation | field study | JavaDoc | source code examples,"Proceedings - Working Conference on Reverse Engineering, WCRE",2013-12-01,Conference Paper,"Montandon, Joao Eduardo;Borges, Hudson;Felix, Daniel;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905862744,10.4230/OASIcs.ICCSW.2013.57,Improving the quality of APIs through the analysis of software crash reports,"Modern programs depend on apis to implement a significant part of their functionality. Apart from the way developers use apis to build their software, the stability of these programs relies on the apis design and implementation. In this work, we evaluate the reliability of apis, by examining software telemetry data, in the form of stack traces, coming from Android application crashes. We got 4.9 GB worth of crash data that thousands of applications send to a centralized crash report management service. We processed that data to extract approximately a million stack traces, stitching together parts of chained exceptions, and established heuristic rules to draw the border between applications and api calls. We examined 80% of the stack traces to map the space of the most common application failure reasons. Our findings show that the top ones can be attributed to memory exhaustion, race conditions or deadlocks, and missing or corrupt resources. At the same time, a significant number of our stack traces (over 10%) remains unclassified due to generic unchecked exceptions, which do not highlight the problems that lead to crashes. Finally, given the classes of crash causes we found, we argue that api design and implementation improvements, such as specific exceptions, non-blocking algorithms, and default resources, can eliminate common failures.",Application programming interfaces | Crash reports | Mobile applications | Stack traces,OpenAccess Series in Informatics,2013-01-01,Conference Paper,"Kechagia, Maria;Mitropoulos, Dimitris;Spinellis, Diomidis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84884877429,10.1007/978-3-642-38977-1_12,Configurable software product lines - Supporting heterogeneous configuration knowledge,"Although different types of enterprise information systems have been built as configurable software product lines, the growing heterogeneity and diversity in system development approaches makes it difficult to specify the configuration knowledge. In this paper we examine the deficiencies of current approaches to the specification of configuration knowledge, and as a solution propose the notion of Domain Knowledge Modeling Languages (DKMLs). We also present GenArch+, an extensible tool that supports the creation and composition of DKMLs. We illustrate and evaluate the use of DKMLs in four different product lines. Our quantitative and qualitative assessment suggests that the use of DMKLs brings improvements for heterogeneous configuration knowledge specification. © 2013 Springer-Verlag Berlin Heidelberg.",Configuration Knowledge | Object-Oriented Frameworks | Product Derivation | Software Product Lines,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-10-07,Conference Paper,"Cirilo, Elder;Kulesza, Uirá;Garcia, Alessandro;Cowan, Don;Alencar, Paulo;Lucena, Carlos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84881372394,10.1109/TE.2013.2238543,Using intelligent tutors to teach students how APIs are used for software engineering in practice,"Computer science courses typically incorporate integrated training in software engineering, which includes learning how to reuse existing code libraries in new programs. This commonly presents a need to use the libraries' application programming interfaces (APIs) during project-based learning activities. The students learn these APIs with little support from the instructor. Instead, they primarily learn from whatever code examples they find on the Web-a scattershot approach that is slow, frustrating, and sometimes fruitless. Preliminary work investigated an improved approach that provides students with intelligent API tutors. This interactive educational content walks students through code examples selectively harvested from the Web. This approach increased students' knowledge of API method names, but a key pedagogical challenge remained: Students still lacked an understanding of common problems and surprises that they might encounter when using APIs in practice. Perhaps even more crucially, the prior statistical study provided no information about how students would perceive and accept the intelligent tutor approach, when given a choice. Therefore, this paper presents an enhanced approach that augments intelligent tutors with selected information from online FAQs and online open-source code, thereby providing more explanation and context about how to use APIs in practice. A qualitative study assessed student perceptions of the system's usability as well as suggestions for improvement, revealing that 100% of students considered the approach to be useful. A key implication is that providing information from online FAQs, related articles, and open-source code could facilitate computer science students' learning how to use APIs. © 1963-2012 IEEE.",Application programming interface (API) | computer science education | electronic learning | software engineering | software libraries,IEEE Transactions on Education,2013-02-01,Article,"Krishnamoorthy, Vasanth;Appasamy, Bharatwaj;Scaffidi, Christopher",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891298933,10.1007/978-3-319-02654-1_14,Method and tool support for classifying software languages with Wikipedia,"Wikipedia provides useful input for efforts on mining taxonomies or ontologies in specific domains. In particular, Wikipedia's categories serve classification. In this paper, we describe a method and a corresponding tool, WikiTax, for exploring Wikipedia's category graph with the objective of supporting the development of a classification of software languages. The category graph is extracted level by level. The extracted graph is visualized in a tree-like manner. Category attributes (i.e., metrics) such as depth are visualized. Irrelevant edges and nodes may be excluded. These exclusions are documented while using a manageable and well-defined set of 'exclusion types' as comments. © 2013 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-12-01,Conference Paper,"Lämmel, Ralf;Mosen, Dominik;Varanovich, Andrei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893127753,10.1145/2488388.2488512,Inferring dependency constraints on parameters for web services,"Recently many popular websites such as Twitter and Flickr expose their data through web service APIs, enabling third-party organizations to develop client applications that provide functionalities beyond what the original websites offer. These client applications should follow certain constraints in order to correctly interact with the web services. One common type of such constraints is Dependency Constraints on Parameters. Given a web service operation O and its parameters Pi, Pj,..., these constraints describe the requirement on one parameter Pi that is dependent on the conditions of some other parameter(s) Pj. For example, when requesting the Twitter operation ""GET statuses/user-timeline"", a user-id parameter must be provided if a screen-name parameter is not provided. Violations of such constraints can cause fatal errors or incorrect results in the client applications. However, these constraints are often not formally specified and thus not available for automatic verification of client applications. To address this issue, we propose a novel approach, called INDICATOR, to automatically infer dependency constraints on parameters for web services, via a hybrid analysis of heterogeneous web service artifacts, including the service documentation, the service SDKs, and the web services themselves. To evaluate our approach, we applied INDICATOR to infer dependency constraints for four popular web services. The results showed that INDICATOR effectively infers constraints with an average precision of 94.4% and recall of 95.5%. Copyright is held by the International World Wide Web Conference Committee (IW3C2).",Constraints | Documentation analysis | Parameters | Service SDK | Testing | Web service,WWW 2013 - Proceedings of the 22nd International Conference on World Wide Web,2013-01-01,Conference Paper,"Wu, Qian;Wu, Ling;Liang, Guangtai;Wang, Qianxiang;Xie, Tao;Mei, Hong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.11575/PRISM/25474,,,,,,,,Exclude,A survey paper
10.1016/j.jss.2022.111515,,10.7282/T3GX498K,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84877297166,10.1109/CSMR.2013.28,A new family of software anti-patterns: Linguistic anti-patterns,"Recent and past studies have shown that poor source code lexicon negatively affects software understand ability, maintainability, and, overall, quality. Besides a poor usage of lexicon and documentation, sometimes a software artifact description is misleading with respect to its implementation. Consequently, developers will spend more time and effort when understanding these software artifacts, or even make wrong assumptions when they use them. This paper introduces the definition of software linguistic antipatterns, and defines a family of them, i.e., those related to inconsistencies (i) between method signatures, documentation, and behavior and (ii) between attribute names, types, and comments. Whereas ""design"" antipatterns represent recurring, poor design choices, linguistic antipatterns represent recurring, poor naming and commenting choices. The paper provides a first catalogue of one family of linguistic antipatterns, showing real examples of such antipatterns and explaining what kind of misunderstanding they can cause. Also, the paper proposes a dectector prototype for Java programs called LAPD (Linguistic Anti-Pattern Detector), and reports a study investigating the presence of linguistic antipatterns in four Java software projects. © 2013 IEEE.",Software antipatterns | Source code lexicon | Textual analysis of software artifacts,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",2013-05-13,Conference Paper,"Arnaoudova, Venera;Di Penta, Massimiliano;Antoniol, Giuliano;Guéhéneuc, Yann Gaël",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84893381315,10.1109/WCRE.2013.6671315,Documenting APIs with examples: Lessons learned with the APIMiner platform,"Software development increasingly relies on Application Programming Interfaces (APIs) to increase productivity. However, learning how to use new APIs in many cases is a nontrivial task given their ever-increasing complexity. To help developers during the API learning process, we describe in this paper a platform - called APIMiner - that instruments the standard Java-based API documentation format with concrete examples of usage. The examples are extracted from a private source code repository - composed by real systems - and summarized using a static slicing algorithm. We also describe a particular instantiation of our platform for the Android API. To evaluate the proposed solution, we performed a field study, when professional Android developers used the platform by four months. © 2013 IEEE.",API documentation | field study | JavaDoc | source code examples,"Proceedings - Working Conference on Reverse Engineering, WCRE",2013-12-01,Conference Paper,"Montandon, Joao Eduardo;Borges, Hudson;Felix, Daniel;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893395105,10.1109/WCRE.2013.6671308,Mining system specific rules from change patterns,"A significant percentage of warnings reported by tools to detect coding standard violations are false positives. Thus, there are some works dedicated to provide better rules by mining them from source code history, analyzing bug-fixes or changes between system releases. However, software evolves over time, and during development not only bugs are fixed, but also features are added, and code is refactored. In such cases, changes must be consistently applied in source code to avoid maintenance problems. In this paper, we propose to extract system specific rules by mining systematic changes over source code history, i.e., not just from bug-fixes or system releases, to ensure that changes are consistently applied over source code. We focus on structural changes done to support API modification or evolution with the goal of providing better rules to developers. Also, rules are mined from predefined rule patterns that ensure their quality. In order to assess the precision of such specific rules to detect real violations, we compare them with generic rules provided by tools to detect coding standard violations on four real world systems covering two programming languages. The results show that specific rules are more precise in identifying real violations in source code than generic ones, and thus can complement them. © 2013 IEEE.",,"Proceedings - Working Conference on Reverse Engineering, WCRE",2013-12-01,Conference Paper,"Hora, Andre;Anquetil, Nicolas;Ducasse, Stephane;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84886486483,10.1007/978-3-642-40994-3_29,Area under the precision-recall curve: Point estimates and confidence intervals,"The area under the precision-recall curve (AUCPR) is a single number summary of the information in the precision-recall (PR) curve. Similar to the receiver operating characteristic curve, the PR curve has its own unique properties that make estimating its enclosed area challenging. Besides a point estimate of the area, an interval estimate is often required to express magnitude and uncertainty. In this paper we perform a computational analysis of common AUCPR estimators and their confidence intervals. We find both satisfactory estimates and invalid procedures and we recommend two simple intervals that are robust to a variety of assumptions. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-10-31,Conference Paper,"Boyd, Kendrick;Eng, Kevin H.;Page, C. David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/978-1-4614-6596-6_15,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84880756195,10.1007/978-3-642-39643-4_28,A Qualitative and quantitative analysis on metadata-based frameworks usage,"The usage of metadata-based frameworks is becoming popular for some kinds of software, such as web and enterprise applications. They use domain-specific metadata, usually defined as annotations or in XML documents, to adapt its behavior to each application class. Despite of their increasingly usage, there are not a study that evaluated the consequences of their usage to the application. The present work presents the result of an experiment that aimed to compare the development of similar applications created: (a) without frameworks; (b) with a traditional framework; (c) with a metadata-based framework. As a result, it uses metrics and a qualitative evaluation to assess the benefits and drawbacks in the use of this kind of framework. © 2013 Springer-Verlag Berlin Heidelberg.",experiment | framework | metadata | metric | software architecture | software design,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-08-01,Conference Paper,"Guerra, Eduardo;Fernandes, Clovis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883237103,10.1109/TPC.2013.2263649,User manuals for a primary care electronic medical record system: A mixed-methods study of user-and vendor-generated documents,"Research problem: Tutorials and user manuals are important forms of impersonal support for using software applications, including electronic medical records (EMRs). Differences between user-and vendor-generated documentation may indicate support needs, which are not sufficiently addressed by the official documentation, and reveal new elements that may inform the design of tutorials and user manuals. Research question: What are the differences between user-generated tutorials and manuals for an EMR and the official user manual from the software vendor? Literature review: Effective design of tutorials and user manuals requires careful packaging of information, balance between declarative and procedural texts, an action and task-oriented approach, support for error recognition and recovery, and effective use of visual elements. No previous research compared these elements between formal and informal documents. Methodology: We conducted a mixed-methods study. Seven tutorials and two manuals for an EMR were collected from three family health teams and compared with the official user manual from the software vendor. Documents were qualitatively analyzed using a framework analysis approach in relation to the principles of technical documentation described before. Subsets of the data were quantitatively analyzed using cross-tabulation to compare the types of error information and visual cues in screen captures between user-and vendor-generated manuals. Results and discussion: The user-developed tutorials and manuals differed from the vendor-developed manual in that they contained mostly procedural and not declarative information; were customized to the specific workflow, user roles, and patient characteristics; contained more error information related to work processes than software usage; and used explicit visual cues on screen captures to help users identify window elements. These findings imply that to support EMR implementation, tutorials and manuals need to be customized and adapted to specific organizational contexts and workflows. The main limitation of the study is its generalizability. Future research should address this limitation and may explore alternative approaches to software documentation, such as modular manuals or participatory design. © 1988-2012 IEEE.",Electronic medical record (EMR) | minimalism | tutorial | user manual | user-generated documentation | workflow,IEEE Transactions on Professional Communication,2013-01-01,Article,"Shachak, Aviv;Dow, Rustam;Barnsley, Jan;Tu, Karen;Domb, Sharon;Jadad, Alejandro R.;Lemieux-Charles, Louise",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84883299138,10.1007/978-3-642-40477-1_52,Existing but not explicit - The user perspective in scrum projects in practice,"Agile software development processes are becoming more common, but this does not mean that the user perspective in the development is catered for. It has its challenges to integrate the users' aspects in Scrum projects in practice. In order to better understand these challenges we have interviewed IT professionals using Scrum focusing on four different areas: responsibility for the user perspective, emphasis on usability and user experience through documentation, usability activities with users and the organisational and contextual settings for emphasizing the user perspective. Results show that the responsibility for the user perspective is unclear in Scrum projects, and that often the user perspective is neither discussed nor described in the projects. However, the user perspective is often present through informal feedback used to understand the context of use and inform design for example. Finally the paper presents implications for working with the user perspective in Scrum projects. © 2013 Springer-Verlag.",agile software development | responsibility | Scrum | Usability | user experience | user perspective,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-09-05,Conference Paper,"Cajander, Åsa;Larusdottir, Marta;Gulliksen, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84892160034,10.1007/978-3-319-03260-3_35,Using and asking: APIs used in the Android market and asked about in StackOverflow,"Programming is knowledge intensive. While it is well understood that programmers spend lots of time looking for information, with few exceptions, there is a significant lack of data on what information they seek, and why. Modern platforms, like Android, comprise complex APIs that often perplex programmers. We ask: which elements are confusing, and why? Increasingly, when programmers need answers, they turn to StackOverflow. This provides a novel opportunity. There are a vast number of applications for Android devices, which can be readily analyzed, and many traces of interactions on StackOverflow. These provide a complementary perspective on using and asking, and allow the two phenomena to be studied together. How does the market demand for the USE of an API drive the market for knowledge about it? Here, we analyze data from Android applications and StackOverflow together, to find out what it is that programmers want to know and why. © 2013 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-12-01,Conference Paper,"Kavaler, David;Posnett, Daryl;Gibler, Clint;Chen, Hao;Devanbu, Premkumar;Filkov, Vladimir",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84876005946,10.1145/2451592.2451594,Understanding multilayered applications for building extensions,"Modern software applications typically consist of several logical layers (for example user interface, databases, business process, code, etc.). Software is usually delivered by a software provider to support a certain application domain through a set of predefined functionalities. A user acquiring the software can obtain extra functionalities through extensions. Extensions can be developed by the software provider, by the customer, or a third-party and then integrated with the core software. In order to enable core software for accommodating extensions, software providers must give the right means for enabling developers to build extensions. Based on the new functionalities required, developers building extensions usually consider different layers of the core software when developing extensions. For example, a simple user interface extension in a business application would need a developer to consider extensible artifacts from underlying user interfaces, business processes, databases, and code. Usually a developer relies on code based artifacts, documentation, examples, and /or training in order to be able to understand the extensibility model of a core application. Various program comprehension tools have helped developers in carrying out general development tasks. However, most of the tools focus on the code level, lack the support for multilayered applications, and do not support extensibility. In this paper we discuss the challenges and problems of the developers when building extensions for multilayered applications. We also assess the feasibility of several program comprehension tools in addressing these challenges. Copyright © 2013 ACM.",Comprehension | Extensibility | IDE | Multilayer,CoCoS 2013 - Proceedings of the 1st Workshop on Comprehension of Complex Systems,2013-04-15,Conference Paper,"Aly, Mohamed;Charfi, Anis;Wu, Di;Mezini, Mira",Exclude,is out of considered timeline
10.1016/j.jss.2022.111515,2-s2.0-84897756129,10.1145/2541940.2541977,Prototyping symbolic execution engines for interpreted languages,"Symbolic execution is being successfully used to automatically test statically compiled code [4, 7, 9, 15]. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways. We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called CHEF) for turning a vanilla interpreter into a sound and complete symbolic execution engine. CHEF symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure. Using CHEF, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. CHEF-produced engines are up to 1000× more performant than if directly executing the interpreter symbolically without CHEF. Copyright is held by the owner/author(s).",Interpreter instrumentation | Software analysis optimizations | State selection strategies,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2014-03-14,Conference Paper,"Bucur, Stefan;Kinder, Johannes;Candea, George",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84897755932,10.1145/2541940.2541960,Transactionalizing legacy code: An experience report using GCC and memcached,"The addition of transactional memory (TM) support to existing languages provides the opportunity to create new software from scratch using transactions, and also to simplify or extend legacy code by replacing existing synchronization with language-level transactions. In this paper, we describe our experiences transactionalizing the memcached application through the use of the GCC implementation of the Draft C++ TM Specification. We present experiences and recommendations that we hope will guide the effort to integrate TM into languages, and that may also contribute to the growing collective knowledge about how programmers can begin to exploit TM in existing production-quality software. Copyright © 2014 ACM.",C++ | GCC | Memcached | Transactional memory,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2014-03-14,Conference Paper,"Ruan, Wenjia;Vyas, Trilok;Liu, Yujie;Spear, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907821879,10.1145/2652524.2652534,Engineering of quality requirements as perceived by near-shore development centers' architects in eastern Europe: The hole in the whole,"Context: To software architects (SAs), the quality requirements (QRs) to a software system are key to designing the software architecture. However, understanding SAs' roles in the QRs engineering activities only recently became a topic in empirical requirements engineering research and very little is still known about QRs engineering from SAs' in large and distributed projects. Goal: This exploratory study aims at explicating how SAs are involved in engineering QRs in a specific distributed development setting, namely in organizations that distribute software development activities to closely located business units, known as near-shore development centres (NDCs), and in a specific geographic zone, namely Eastern Europe. Method: Based on interviews with 16 practitioners working on large projects in NDCs, we explicate the participation and involvement of NDCs' architects in QRs tasks. Results: We found that SAs from NDCs (i) are actively involved in QRs documentation and validation, (ii) are relatively passive participants in QRs elicitation and prioritization, and (iii) are not at all involved in QRs negotiation. Perhaps, our most surprising finding is that NDCs may often have economic incentives to misalign with onshore QRs practices. Conclusions: We explicated QRs practices, compared them to previously published ones, and found implications for both researchers and practitioners. Though, our results are preliminary, as they are from an exploratory study.",distributed software development | exploratory case study | interview-based research method | near-shore development centres | quality requirements | software architecture,International Symposium on Empirical Software Engineering and Measurement,2014-09-18,Conference Paper,"Daneva, Maya;Marczak, Sabrina;Herrmann, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907859292,10.1145/2652524.2652554,On the impact of passive voice requirements on domain modelling,"Context: The requirements specification is a central artefact in the software engineering (SE) process, and its quality (might) influence downstream activities like implementation or testing. One quality defect that is often mentioned in standards is the use of passive voice. However, the consequences of this defect are still unclear. Goal: We need to understand whether the use of passive voice in requirements has an influence on other activities in SE. In this work we focus on domain modelling. Method: We designed an experiment, in which we ask students to draw a domain model from a given set of requirements written in active or passive voice. We compared the completeness of the resulting domain model by counting the number of missing actors, domain objects and their associations with respect to a specified solution. Results: While we could not see a difference in the number of missing actors and objects, participants which received passive sentences missed almost twice the associations. Conclusion: Our experiment indicates that, against common knowledge, actors and objects in a requirement can often be understood from the context. However, the study also shows that passive sentences complicate understanding how certain domain concepts are interconnected.",analytical quality assurance | experimentation | natural language | quality assurance | requirements engineering,International Symposium on Empirical Software Engineering and Measurement,2014-09-18,Conference Paper,"Femmer, Henning;Kuera, Jan;Vetrò, Antonio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907858877,10.1145/2652524.2652541,What's in a bug report?,"Context: Bug reports are the primary means by which users of a system are able to communicate a problem to the developers, and their contents are important - not only to support developers in maintaining the system, but also as the basis of automated tools to assist in the challenging tasks of finding and fixing bugs. Goal: This paper aims to investigate how users report bugs in systems: what information is provided, how frequently, and the consequences of this. Method: The study examined the quality and quantity of information provided in 1600 bugs reports drawn from four open-source projects (Eclipse, Firefox, Apache HTTP, and Facebook API), recorded what information users actually provide, how and when users provide the information, and how this affects the outcome of the bug. Results: Of the recorded sources of information, only observed behaviour and expected results appeared in more than 50% of reports. Those sources deemed highly useful by developers and tools such as stack traces and test cases appeared very infrequently. However, no strong relationship was observed between the provided information and the outcome of the bug. Conclusions: The paper demonstrates a clear mismatch between the information that developers would wish to appear in a bug report, and the information that actually appears. Furthermore, the quality of bug reports has an important impact on research which might rely on extracting this information automatically.",bug report | bug repository | software maintenance,International Symposium on Empirical Software Engineering and Measurement,2014-09-18,Conference Paper,"Davies, Steven;Roper, Marc",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907820538,10.1145/2652524.2652562,Using qualitative metasummary to synthesize empirical findings in literature reviews,"Context- A common problem in Systematic Reviews in software engineering is that they provide very limited syntheses. Goal- In the search for alternatives of effective methods for synthesizing empirical evidence, in this paper, we explore the use of the Qualitative Metasummary method, which is a quantitatively oriented aggregation of mixed research findings. Method - We describe the use of qualitative metasummary through an example using 15 studies addressing antecedents of performance of software development teams. Qualitative metasummary includes extraction and grouping of findings, and calculation of frequency and intensity effect sizes. Results - The instance described in this paper produced a 10-factor model that effectively summarizes the current empirical knowledge on performance of software development teams. Then, we assessed the method in terms of ease of use, usefulness and reliability of results. Conclusion - The Qualitative Metasummary method offers rich indexes of experiences and events under investigation, focusing on the effects of a variable over other, which is consistent with the central interest of systematic reviews. However, its main limitations are (i) challenging comparability/integratability between primary studies, (ii) loss of detailed contextual information, (iii) and the great deal of effort demanded to synthesize larger sets of papers.",qualitative metasummary | research synthesis | systematic review | team performance,International Symposium on Empirical Software Engineering and Measurement,2014-09-18,Conference Paper,"Ribeiro, Danilo Monteiro;Cardoso, Marcos;Da Silva, Fabio Q.B.;França, César",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907831107,10.1145/2652524.2652537,An empirical simulation-based study of real-time speech translation for multilingual global project teams,"Context: Real-time speech translation technology is today available but still lacks a complete understanding of how such technology may affect communication in global software projects. Goal: To investigate the adoption of combining speech recognition and machine translation in order to overcome language barriers among stakeholders who are remotely negotiating software requirements. Method: We performed an empirical simulation-based study including: Google Web Speech API and Google Translate service, two groups of four subjects, speaking Italian and Brazilian Portuguese, and a test set of 60 technical and non-technical utterances. Results: Our findings revealed that, overall: (i) a satisfactory accuracy in terms of speech recognition was achieved, although significantly affected by speaker and utterance differences; (ii) adequate translations tend to follow accurate transcripts, meaning that speech recognition is the most critical part for speech translation technology. Conclusions: Results provide a positive albeit initial evidence towards the possibility to use speech translation technologies to help globally distributed team members to communicate in their native languages.",controlled experiment | global software engineering | machine translation | requirements meetings,International Symposium on Empirical Software Engineering and Measurement,2014-09-18,Conference Paper,"Calefato, Fabio;Lanubile, Filippo;Prikladnicki, Rafael;Pinto, João Henrique S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907808593,10.1145/2652524.2652583,Process mining can be applied to software too!,"Modern information systems produce tremendous amounts of event data. The area of process mining deals with extracting knowledge from this data. Real-life processes can be effectively discovered, analyzed and optimized with the help of mature process mining techniques. There is a variety of process mining case studies and experience reports from such business areas as healthcare, public, transportation and education. Although nowadays, these techniques are mostly used for discovering business processes. The goal of this industrial paper is to show that process mining can be applied to software too. Here we present and analyze our experiences on applying process mining in different productive software systems used in the touristic domain. Process models and user interface workflows underlie the functional specifications of the systems we experiment with. When the systems are utilized, user interaction is recorded in event logs. After applying process mining methods to these logs, process and user interface flow models are automatically derived. These resulting models provide insight regarding the real usage of the software, motivate the changes in the functional specifications, enable usability improvements and software redesign. Thus, with the help of our examples we demonstrate that process mining facilitates new forms of software analysis. The user interaction with almost every software system can be mined in order to improve the software and to monitor and measure its real usage.",client technology | process mining | software process mining | user interface design,International Symposium on Empirical Software Engineering and Measurement,2014-09-18,Conference Paper,"Rubin, Vladimir A.;Mitsyuk, Alexey A.;Lomazova, Irina A.;Van Der Aalst, Wil M.P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2652524.2652592,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2635868.2684428,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84986917830,10.1145/2635868.2635912,Feedback generation for performance problems in introductory programming assignments,"Providing feedback on programming assignments manually is a tedious, error prone, and time-consuming task. In this paper, we motivate and address the problem of generating feedback on performance aspects in introductory programming assignments. We studied a large number of functionally correct student solutions to introductory programming assignments and observed: (1) There are different algorithmic strategies, with varying levels of efficiency, for solving a given problem. These different strategies merit different feedback. (2) The same algorithmic strategy can be implemented in countless different ways, which are not relevant for reporting feedback on the student program. We propose a light-weight programming language extension that allows a teacher to define an algorithmic strategy by specifying certain key values that should occur during the execution of an implementation. We describe a dynamic analysis based approach to test whether a student's program matches a teacher's specification. Our experimental results illustrate the effectiveness of both our specification language and our dynamic analysis. On one of our benchmarks consisting of 2316 functionally correct implementations to 3 programming problems, we identified 16 strategies that we were able to describe using our specification language (in 95 minutes after inspecting 66, i.e., around 3%, implementations). Our dynamic analysis correctly matched each implementation with its corresponding specification, thereby automatically producing the intended feedback.",Dynamic analysis | Education | Moocs | Performance analysis | Trace specification,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Gulwani, Sumit;Radiček, Ivan;Zuleger, Florian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986922016,10.1145/2635868.2635878,FlowTwist: Efficient context-sensitive inside-out taint analysis for large codebases,"Over the past years, widely used platforms such as the Java Class Library have been under constant attack through vulnerabilities that involve a combination of two taint-analysis problems: an integrity problem allowing attackers to trigger sensitive operations within the platform, and a confidentiality problem allowing the attacker to retrieve sensitive information or pointers from the results of those operations. While existing static taint analyses are good at solving either of those problems, we show that they scale prohibitively badly when being applied to situations that require the exploitation of both an integrity and confidentiality problem in combination. The main problem is the huge attack surface of libraries such as the Java Class Library, which exposes thousands of methods potentially controllable by an attacker. In this work we thus present FlowTwist, a novel taintanalysis approach that works inside-out, i.e., tracks data ows from potentially vulnerable calls to the outer level of the API which the attacker might control. This inside-out analysis requires a careful, context-sensitive coordination of both a backward and a forward taint analysis. In this work, we expose a design of the analysis approach based on the IFDS algorithm, and explain several extensions to IFDS that enable not only this coordination but also a helpful reporting of error situations to security analysts. Experiments with the Java Class Library show that, while a simple forward taint-analysis approach does not scale even with much machine power, FlowTwist's algorithm is able to fully analyze the library within 10 minutes.",Confused deputy | IFDS | Taint analysis,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Lerch, Johannes;Hermann, Ben;Bodden, Eric;Mezini, Mira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986910158,10.1145/2635868.2635924,Mining preconditions of APIs in large-scale code corpus,"Modern software relies on existing application programming interfaces (APIs) from libraries. Formal specifications for the APIs enable many software engineering tasks as well as help developers correctly use them. In this work, we mine large-scale repositories of existing open-source software to derive potential preconditions for API methods. Our key idea is that APIs' preconditions would appear frequently in an ultra-large code corpus with a large number of API usages, while project-specific conditions will occur less frequently. First, we find all client methods invoking APIs. We then compute a control dependence relation from each call site and mine the potential conditions used to reach those call sites. We use these guard conditions as a starting point to automatically infer the preconditions for each API. We analyzed almost 120 million lines of code from SourceForge and Apache projects to infer preconditions for the standard Java Development Kit (JDK) library. The results show that our technique can achieve high accuracy with recall from 75-80% and precision from 82-84%. We also found 5 preconditions missing from human written specifications. They were all confirmed by a specification expert. In a user study, participants found 82% of the mined preconditions as a good starting point for writing specifications. Using our mining result, we also built a benchmark of more than 4,000 precondition-related bugs.",Big code mining | JML | Preconditions | Specification mining,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Nguyen, Hoan Anh;Dyer, Robert;Nguyen, Tien N.;Rajan, Hridesh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986904834,10.1145/2635868.2635890,Automatic mining of specifications from invocation traces and method invariants,"Software library documentation often describes individual methods' APIs, but not the intended protocols and method interactions. This can lead to library misuse, and restrict runtime detection of protocol violations and automated verification of software that uses the library. Specification mining, if accurate, can help mitigate these issues, which has led to significant research into new modelinference techniques that produce FSM-based models from program invariants and execution traces. However, there is currently a lack of empirical studies that, in a principled way, measure the impact of the inference strategies on model quality. To this end, we identify four such strategies and systematically study the quality of the models they produce for nine off-the-shelf libraries. We find that (1) using invariants to infer an initial model significantly improves model quality, increasing precision by 4% and recall by 41%, on average; (2) effective invariant filtering is crucial for quality and scalability of strategies that use invariants; and (3) using traces in combination with invariants greatly improves robustness to input noise. We present our empirical evaluation, implement new and extend existing model-inference techniques, and make public our implementations, ground-truth models, and experimental data. Our work can lead to higher-quality model inference, and directly improve the techniques and tools that rely on model inference.",Execution traces | Log analysis | Model inference,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Krka, Ivo;Brun, Yuriy;Medvidovic, Nenad",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84986922232,10.1145/2635868.2635880,Identifying the characteristics of vulnerable code changes: An empirical study,"To focus the efforts of security experts, the goals of this empirical study are to analyze which security vulnerabilities can be discovered by code review, identify characteristics of vulnerable code changes, and identify characteristics of developers likely to introduce vulnerabilities. Using a three-stage manual and automated process, we analyzed 267,046 code review requests from 10 open source projects and identified 413 Vulnerable Code Changes (VCC). Some key results include: (1) code review can identify common types of vulnerabilities; (2) while more experienced contributors authored the majority of the VCCs, the less experienced contributors' changes were 1.8 to 24 times more likely to be vulnerable; (3) the likelihood of a vulnerability increases with the number of lines changed, and (4) modified files are more likely to contain vulnerabilities than new files. Knowing which code changes are more prone to contain vulnerabilities may allow a security expert to concentrate on a smaller subset of submitted code changes. Moreover, we recommend that projects should: (a) create or adapt secure coding guidelines, (b) create a dedicated security review team, (c) ensure detailed comments during review to help knowledge dissemination, and (d) encourage developers to make small, incremental changes rather than large changes.",Code review | Inspection | Open source | Security defects | Vulnerability,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Bosu, Amiangshu;Carver, Jeffrey C.;Hafiz, Munawar;Hilley, Patrick;Janni, Derek",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986893030,10.1145/2635868.2635877,Selection and presentation practices for code example summarization,"Code examples are an important source for answering questions about software libraries and applications. Many usage contexts for code examples require them to be distilled to their essence: e.g., when serving as cues to longer documents, or for reminding developers of a previously known idiom. We conducted a study to discover how code can be summarized and why. As part of the study, we collected 156 pairs of code examples and their summaries from 16 participants, along with over 26 hours of think-aloud verbalizations detailing the decisions of the participants during their summarization activities. Based on a qualitative analysis of this data we elicited a list of practices followed by the participants to summarize code examples and propose empirically-supported hypotheses justifying the use of specific practices. One main finding was that none of the participants exclusively extracted code verbatim for the summaries, motivating abstractive summarization. The results provide a grounded basis for the development of code example summarization and presentation technology.",Code examples | Summarization,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Ying, Annie T.T.;Robillard, Martin P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986879770,10.1145/2635868.2635870,Automatic generation of release notes,"This paper introduces ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. It was designed based on the manual analysis of 1,000 existing release notes. To evaluate the quality of the ARENA release notes, we performed three empirical studies involving a total of 53 participants (45 professional developers and 8 students). The results indicate that the ARENA release notes are very good approximations of those produced by developers and often include important information that is missing in the manually produced release notes.",Release notes | Software documentation | Software evolution,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Moreno, Laura;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;Marcus, Andrian;Canfora, Gerardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986915810,10.1145/2635868.2635916,SAFE<inf>WAPI</inf>: Web API misuse detector for web applications,"The evolution of Web 2.0 technologies makes web applications prevalent in various platforms including mobile devices and smart TVs. While one of the driving technologies of web applications is JavaScript, the extremely dynamic features of JavaScript make it very difficult to define and detect errors in JavaScript applications. The problem becomes more important and complicated for JavaScript web applications which may lead to severe security vulnerabilities. To help developers write safe JavaScript web applications using vendor-specific Web APIs, vendors specify their APIs often in Web IDL, which enables both API writers and users to communicate better by understanding the expected behaviors of the Web APIs. In this paper, we present SAFEWAPI, a tool to analyzeWeb APIs and JavaScript web applications that use the Web APIs and to detect possible misuses of Web APIs by the web applications. Even though the JavaScript language semantics allows to call a function defined with some parameters without any arguments, platform developers may require application writers to provide the exact number of arguments. Because the library functions in Web APIs expose their intended semantics clearly to web application developers unlike pure JavaScript functions, we can detect wrong uses of Web APIs precisely. For representative misuses of Web APIs defined by software quality assurance engineers, SAFEWAPI detected such misuses in real-world JavaScript web applications. It also found inconsistencies in the Web API specifications and some real bugs in sample examples in the specifications.",Bug detection | JavaScript | Static analysis | Web application,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Bae, Sunggyeong;Cho, Hyunghun;Lim, Inho;Ryu, Sukyoung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986906008,10.1145/2635868.2635887,No issue left behind: Reducing information overload in issue tracking,"Modern software development tools such as issue trackers are often complex and multi-purpose tools that provide access to an immense amount of raw information. Unfortunately, developers sometimes feel frustrated when they cannot easily obtain the particular information they need for a given task; furthermore, the constant influx of new data - the vast majority of which is irrelevant to their task at hand - may result in issues being ""dropped on the floor"". In this paper, we present a developer-centric approach to issue tracking that aims to reduce information overload and improve developers' situational awareness. Our approach is motivated by a grounded theory study of developer comments, which suggests that customized views of a project's repositories that are tailored to developer-specific tasks can help developers better track their progress and understand the surrounding technical context. From the qualitative study, we uncovered a model of the kinds of information elements that are essential for developers in completing their daily tasks, and from this model we built a tool organized around customized issue-tracking dashboards. Further quantitative and qualitative evaluation demonstrated that this dashboard-like approach to issue tracking can reduce the volume of irrelevant emails by over 99% and also improve support for specific issue-tracking tasks.",Developer dashboards | Information needs | Issue tracking | Personalization | Situational awareness,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Baysal, Olga;Holmes, Reid;Godfrey, Michael W.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986889517,10.1145/2635868.2635874,Learning to rank relevant files for bug reports using domain knowledge,"When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files of a project with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and potentially could lead to a substantial increase in productivity. This paper introduces an adaptive ranking approach that leverages domain knowledge through functional decompositions of source code files into methods, API descriptions of library components used in the code, the bug-fixing history, and the code change history. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluated our system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the newly introduced learning-to-rank approach significantly outperforms two recent state-of-the-art methods in recommending relevant files for bug reports. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70% of the bug reports in the Eclipse Platform and Tomcat projects.",Bug reports | Learning to rank | Software maintenance,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Ye, Xin;Bunescu, Razvan;Liu, Chang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986881489,10.1145/2635868.2661678,BugLocalizer: Integrated tool support for bug localization,"To manage bugs that appear in a software, developers often make use of a bug tracking system such as Bugzilla. Users can report bugs that they encounter in such a system. Whenever a user reports a new bug report, developers need to read the summary and description of the bug report and manually locate the buggy files based on this information. This manual process is often time consuming and tedious. Thus, a number of past studies have proposed bug localization techniques to automatically recover potentially buggy files from bug reports. Unfortunately, none of these techniques are integrated to bug tracking systems and thus it hinders their adoption by practitioners. To help disseminate research in bug localization to practitioners, we develop a tool named BugLocalizer, which is implemented as a Bugzilla extension and builds upon a recently proposed bug localization technique. Our tool extracts texts from summary and description fields of a bug report and source code files. It then computes similarities of the bug report with source code files to find the buggy files. Developers can use our tool online from a Bugzilla web interface by providing a link to a git source code repository and specifying the version of the repository to be analyzed. We have released our tool publicly in GitHub, which is available at: https://github.com/smagsmu/buglocalizer. We have also provided a demo video, which can be accessed at: http://youtu.be/iWHaLNCUjBY.",Bug localization | Bugzilla | Git,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2014-11-16,Conference Paper,"Thung, Ferdian;Le, Tien Duy B.;Kochhar, Pavneet Singh;Lo, David",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2628136.2628155,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907012604,10.1145/2628136.2628160,Coeffects: A calculus of context-dependent computation,"The notion of context in functional languages no longer refers just to variables in scope. Context can capture additional properties of variables (usage patterns in linear logics; caching requirements in dataflow languages) as well as additional resources or properties of the execution environment (rebindable resources; platform version in a cross-platform application). The recently introduced notion of coeffects captures the latter, whole-context properties, but it failed to capture fine-grained per-variable properties. We remedy this by developing a generalized coeffect system with annotations indexed by a coeffect shape. By instantiating a concrete shape, our system captures previously studied flat (whole-context) coeffects, but also structural (per-variable) coeffects, making coeffect analyses more useful. We show that the structural system enjoys desirable syntactic properties and we give a categorical semantics using extended notions of indexed comonad. The examples presented in this paper are based on analysis of established language features (liveness, linear logics, dataflow, dynamic scoping) and we argue that such context-aware properties will also be useful for future development of languages for increasingly heterogeneous and distributed platforms. © 2014 ACM.",coeffects | context | indexed comonads | types,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2014-01-01,Conference Paper,"Petricek, Tomas;Orchard, Dominic;Mycroft, Alan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2628136.2628143,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2628136.2628161,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907015826,10.1145/2628136.2628159,A relational framework for higher-order shape analysis,"We propose the integration of a relational specification framework within a dependent type system capable of verifying complex invariants over the shapes of algebraic datatypes. Our approach is based on the observation that structural properties of such datatypes can often be naturally expressed as inductively-defined relations over the recursive structure evident in their definitions. By interpreting constructor applications (abstractly) in a relational domain, we can define expressive relational abstractions for a variety of complex data structures, whose structural and shape invariants can be automatically verified. Our specification language also allows for definitions of parametricrelations for polymorphic data types that enable highly composable specifications and naturally generalizes to higher-order polymorphic functions. We describe an algorithm that translates relational specifications into a decidable fragment of first-order logic that can be efficiently discharged by an SMT solver. We have implemented these ideas in a type checker called CATALYST that is incorporated within the MLton SML compiler. Experimental results and case studies indicate that our verification strategy is both practical and effective. © 2014 ACM.",decidability | dependent types | inductive relations | parametric relations | relational specifications | standard ml,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2014-01-01,Conference Paper,"Kaki, Gowtham;Jagannathan, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931072943,10.1109/ICSME.2014.31,An exploratory study on self-admitted technical debt,"Throughout a software development life cycle, developers knowingly commit code that is either incomplete, requires rework, produces errors, or is a temporary workaround. Such incomplete or temporary workarounds are commonly referred to as 'technical debt'. Our experience indicates that self-admitted technical debt is common in software projects and may negatively impact software maintenance, however, to date very little is known about them. Therefore, in this paper, we use source-code comments in four large open source software projects-Eclipse, Chromium OS, Apache HTTP Server, and ArgoUML to identify self-admitted technical debt. Using the identified technical debt, we study 1) the amount of self-admitted technical debt found in these projects, 2) why this self-admitted technical debt was introduced into the software projects and 3) how likely is the self-admitted technical debt to be removed after their introduction. We find that the amount of self-admitted technical debt exists in 2.4% - 31% of the files. Furthermore, we find that developers with higher experience tend to introduce most of the self-admitted technical debt and that time pressures and complexity of the code do not correlate with the amount of self-admitted technical debt. Lastly, although self-admitted technical debt is meant to be addressed or removed in the future, only between 26.3% - 63.5% of self-admitted technical debt gets removed from projects after introduction.",Software comments | Technical debt,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Potdar, Aniket;Shihab, Emad",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84931056175,10.1109/ICSME.2014.34,An empirical study of the energy consumption of android applications,"Energy is a critical resource for smartphones. However, developers who create apps for these platforms lack quantitative and objective information about the behavior of apps with respect to energy consumption. In this paper, we describe the results of our source-line level energy consumption study of 405 real-world market applications. Based on our study, we discover several interesting observations. For example, we find on average apps spend 61% of their energy in idle states, network is the most energy consuming component, and only a few APIs dominate non-idle energy consumption. The results of this study provide developers with objective information about how energy is consumed by a broad sample of mobile applications and can guide them in their efforts of improving the energy efficiency of their applications.",Empirical study | Energy | Mobile applications,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Li, Ding;Hao, Shuai;Gui, Jiaping;Halfond, William G.J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931040481,10.1109/ICSME.2014.49,Writing acceptable patches: An empirical study of open source project patches,"Software developers submit patches to handle tens or even hundreds of bugs reported daily. However, not all submitted patches can be directly integrated into the code base, since they might not pass patch review that is adopted in most software projects. As the result of patch review, incoming patches can be rejected or asked for resubmission after improvement. Both scenarios interrupt the workflow of patch writers and reviewers, increase their workload, and potentially delay the general development process. In this paper, we aim to help developers write acceptable patches to avoid patch rejection and resubmission. To this end, we derive a comprehensive list of patch rejection reasons from a manual inspection of 300 rejected Eclipse and Mozilla patches, a large-scale online survey of Eclipse and Mozilla developers, and the literature. We also investigate which patch-rejection reasons are more decisive and which are difficult to judge from the perspective of patch reviewers. Our findings include 1) suboptimal solution and incomplete fix are the most frequent patch-rejection reasons 2) whether a patch introduces new bugs is very important yet very difficult to judge 3) reviewers reject a large patch not solely because of its size, but mainly because of the underlying reasons that induce its large size, such as the involvement of unnecessary changes 4) reviewers consider certain problems to be much more destructive than patch writers expect, such as the inconsistency of documentation in a patch and 5) bad timing of patch submission and a lack of communication with team members can also result in a negative patch review.",empirical study | patch,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Tao, Yida;Han, Donggyun;Kim, Sunghun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931031612,10.1109/ICSME.2014.53,Combining text mining and data mining for bug report classification,"Misclassification of bug reports inevitably sacrifices the performance of bug prediction models. Manual examinations can help reduce the noise but bring a heavy burden for developers instead. In this paper, we propose a hybrid approach by combining both text mining and data mining techniques of bug report data to automate the prediction process. The first stage leverages text mining techniques to analyze the summary parts of bug reports and classifies them into three levels of probability. The extracted features and some other structured features of bug reports are then fed into the machine learner in the second stage. Data grafting techniques are employed to bridge the two stages. Comparative experiments with previous studies on the same data - three large-scale open source projects - consistently achieve a reasonable enhancement (from 77.4% to 81.7%, 73.9% to 80.2% and 87.4% to 93.7%, respectively) over their best results in terms of overall performance. Additional comparative empirical experiments on other two popular open source repositories confirm the findings and demonstrate the benefits of our approach.",,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Zhou, Yu;Tong, Yanxiang;Gu, Ruihang;Gall, Harald",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931084209,10.1109/ICSME.2014.56,Clonepedia: Summarizing code clones by common syntactic context for software maintenance,"Code clones have to be made explicit and be managed in software maintenance. Researchers have developed many clone detection tools to detect and analyze code clones in software systems. These tools report code clones as similar code fragments in source files. However, clone-related maintenance tasks (e.g., refactorings) often involve a group of code clones appearing in larger syntactic context (e.g., code clones in sibling classes or code clones calling similar methods). Given a list of low-level code-fragment clones, developers have to manually summarize from bottom up low-level code clones that are relevant to the syntactic context of a maintenance task. In this paper, we present a clone summarization technique to summarize code clones with respect to their common syntactic context. The clone summarization allows developers to locate and maintain code clones in a top-down manner by type hierarchy and usage dependencies. We have implemented our approach in the Clonepedia tool and conducted a user study on JHotDraw with 16 developers. Our results show that Clonepedia users can better locate and refactor code clones, compared with developers using the CloneDetective tool.",Clone Summarization | Code Clone | Syntactic Pattern,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Lin, Yun;Xing, Zhenchang;Peng, Xin;Liu, Yang;Sun, Jun;Zhao, Wenyun;Dong, Jinsong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931045780,10.1109/ICSME.2014.57,Refactoring fat interfaces using a genetic algorithm,"Recent studies have shown that the violation of the Interface Segregation Principle (ISP) is critical for maintaining and evolving software systems. Fat interfaces (i.e., interfaces violating the ISP) change more frequently and degrade the quality of the components coupled to them. According to the ISP the interfaces' design should force no client to depend on methods it does not invoke. Fat interfaces should be split into smaller interfaces exposing only the methods invoked by groups of clients. However, applying the ISP is a challenging task when fat interfaces are invoked differently by many clients. In this paper, we formulate the problem of applying the ISP as a multi-objective clustering problem and we propose a genetic algorithm to solve it. We evaluate the capability of the proposed genetic algorithm with 42,318 public Java APIs whose clients' usage has been mined from the Maven repository. The results of this study show that the genetic algorithm outperforms other search based approaches (i.e., random and simulated annealing approaches) in splitting the APIs according to the ISP.",APIs | genetic algorithms | Interface Segregation Principle | refactoring | search-based software engineering,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Romano, Daniele;Raemaekers, Steven;Pinzger, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931056205,10.1109/ICSME.2014.64,Code tagging as a social game,"Keywords or tags summarize documents on an abstract level and can also be used for describing code fragments. They might be leveraged for retrieving features of a software system, understanding program functionality, or providing additional context. While automatic approaches at best are only able to retrieve information that is already contained in the source code, manual tagging could add valuable extra information from qualified expertise of the developers. However, tagging code is tedious. To make code tagging more fun, we introduce a social gasification approach: developers independently tag code fragments and are rewarded if their solutions conform to the solution of other developers. We implemented the game as a Facebook plug-in. A pilot user study suggests that the game mechanics are motivating and promote the proposition of reasonable tags.",code tagging | crowdsourcing | documentation | gamification | social game,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Biegel, Benjamin;Beck, Fabian;Lesch, Benedikt;Diehl, Stephan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931037905,10.1109/ICSME.2014.66,Interactive visualization of bug reports using topic evolution and extractive summaries,"Software bug reports are important project artifacts that evolve throughout the life of a software project. Software bugs are issues that are reported by users when these issues hinder their work. Software projects evolve over time as bugs are addressed and new features are added. Managing bugs can be a significant challenge as a project manager generally needs to be aware of all the bug reports for the current version, and this can be even more challenging when the number of bug reports becomes large. It is preferable that a developer new to a project improves her knowledge with the project along with the bug reports during working on it, which is likely to help her avoid or handle the reported issues. In this paper, we propose a prototype that assists developers review a project's bug reports by interactively visualizing insightful information regarding the bug reports using topic analysis. In addition, in order to reduce developers' time and efforts when studying a bug report, the proposed prototype also provides an extractive summary visualization of each bug report. In this research, it is shown that our proposed prototype performs better in terms of precision, recall, and F-measure than a baseline approach that uses time-sensitive keyword extraction.",Bug report | Interactive visualization | Summarization | Topic evolution,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Yeasmin, Shamima;Roy, Chanchal K.;Schneider, Kevin A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924886892,10.1109/ICSME.2014.84,Overthrowing the tyranny of alphabetical ordering in documentation systems,"Software developers are often unsure of the exact name of the API method they need to use to invoke the desired behavior. Most state-of-the-art documentation browsers present API artefacts in alphabetical order. Albeit easy to implement, alphabetical order does not help much: if the developer knew the name of the required method, he could have just searched for it in the first place. In a context where multiple projects use the same API, and their source code is available, we can improve the API presentation by organizing the elements in the order in which they are more likely to be used by the developer. Usage frequency data for methods is gathered by analyzing other projects from the same ecosystem and this data is used then to improve tools. We present a preliminary study on the potential of this approach to improve the API presentation by reducing the time it takes to find the method that implements a given feature. We also briefly present our experience with two proof-of-concept tools implemented for Smalltalk and Java.",documentation | ecosystem analysis,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Spasojević, Boris;Lungu, Mircea;Nierstrasz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931067475,10.1109/ICSME.2014.88,A manual categorization of android app development issues on stack overflow,"While many tutorials, code examples, and documentation about Android APIs exist, developers still face various problems with the implementation of Android Apps. Many of these issues are discussed on Q&A-sites, such as Stack Overflow. In this paper we present a manual categorization of 450 Android related posts of Stack Overflow concerning their question and problem types. The idea is to find dependencies between certain problems and question types to get better insights into issues of Android App development. The categorization is developed using card sorting with three experienced Android App developers. An initial approach to automate the classification of Stack Overflow posts using Lucene is also presented. The study highlights that the most common question types are 'How to⋯?' and 'What is the problem⋯?'. The problems that are discussed most often are related to 'User Interface' and 'Core Elements'. In particular, the problem category 'Layout' is often related to 'What is the problem⋯?' and 'Frameworks' issues often come with 'Is it possible⋯?' questions.",,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Beyer, Stefanie;Pinzger, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931071190,10.1109/ICSME.2014.99,Prompter: A self-confident recommender system,"Developers often consult different sources of information like Application Programming Interfaces (API) documentation, forums, Q&A websites, etc. With the aim of gathering additional knowledge for the programming task at hand. The process of searching and identifying valuable pieces of information requires developers to spend time and energy in formulating the right queries, assessing the returned results, and integrating the obtained knowledge into the code base. All of this is often done manually. We present Prompter, a plug-in for the Eclipse IDE which automatically searches and identifies relevant Stack Overflow discussions, evaluates their relevance given the code context in the IDE, and notifies the developer if and only if a user-defined confidence threshold is surpassed.",,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Ponzanelli, Luca;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;Lanza, Michele",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84931035392,10.1109/ICSME.2014.110,Context-sensitive code completion tool for better API usability,"Developers depend on APIs of frameworks and libraries to support the development process. Due to the large number of existing APIs, it is difficult to learn, remember, and use them during the development of a software. To mitigate the problem, modern integrated development environments provide code completion facilities that free developers from remembering every detail. In this paper, we introduce CSCC, a simple, efficient context-sensitive code completion tool that leverages previous code examples to support method completion. Compared to other existing code completion tools, CSCC uses new sources of contextual information together with lightweight source code analysis to better recommend API method calls.",API methods | Code Completion | Eclipse plugin,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Asaduzzaman, Muhammad;Roy, Chanchal K.;Schneider, Kevin A.;Hou, Daqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931095956,10.1109/ICSME.2014.125,Specification and detection of SOA antipatterns,"Service-oriented architecture (SOA) provides a collection of principles and methodologies for designing and developing service-based systems (SBSs). SBSs are composed of loosely-coupled, platform independent, and reusable functional units, i.e., services. Alternative technologies to implement SBSs are REST-style (Representational State Transfer), Service Component Architecture (SCA), SOAP-based Web service, and so on. However, SBSs cannot overcome some common software engineering challenges, e.g., evolution, to fit new user requirements or changes in execution contexts. All these changes may degrade the quality of design and quality of service of SBSs and may cause the presence of common bad practiced solutions - antipatterns.",,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",2014-12-04,Conference Paper,"Palma, Francis",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84928673057,10.1109/ISSRE.2014.13,Zen-CC: An automated and incremental conformance checking solution to support interactive product configuration,"In the context of product line engineering (PLE), providing immediate feedback on the correctness of a manual configuration step to users has a practical impact on whether a configuration process with tool support can be successfully adopted in practice. Model-based PLE has brought opportunities to enable automated product configuration and derivation for large-scale systems/software, in which models are used as the abstract specification of commonalities and variabilities of products of a product line. In our previous work, we have proposed a UML-based variability modeling methodology and an interactive configuration process. Based on these work, in this paper, we propose an automated and incremental conformance checking approach to ensure that the manual configuration to each variation point conforms to a set of pre-defined conformance rules specified in OCL. The proposed approach, called Zen-CC is implemented as a component of our product configuration and derivation tool, named as Zen-Configurator. The proposed approach is evaluated with two real-world case studies and results showed that the performance of Zen-CC is significantly better than a baseline algorithm checking all the conformance rules at each configuration step. Moreover, the performance of Zen-CC rarely varies during the configuration process, suggesting that our approach is scalable for configuring products with a large number of configuration points.",Conformance Checking | Product Configuration | Product Line Engineering | Variation Point,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2014-12-11,Conference Paper,"Lu, Hong;Yue, Tao;Ali, Shaukat;Nie, Kunming;Zhang, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928671282,10.1109/ISSRE.2014.12,A systematic approach for software interference analysis,"Interferences are a common challenge in integrated systems. An interference is a failure propagation scenario in which a failure of one software component propagates to another software component via the platform's shared computational resources. To account for this, safety standards demand freedom from interference in order to control failure propagation between mixed-critical software components. However, the analysis of potential interferences for a given system is often performed ad-hoc, for example using lists of known issues. Arguing the sufficiency of the interference analysis is difficult using such an approach, especially when dealing with new technologies for which established lists do not exist yet. To this end, this paper presents an interference analysis method that allows for the systematic identification and specification of interferences.",Fault-Tolerance | Mixed-Criticality | Partitioning | Segregation | Software and System Safety | Software Interference,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2014-12-11,Conference Paper,"Zimmer, Bastian;Dropmann, Christoph;Hanger, Jochen Ulrich",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928665497,10.1109/ISSRE.2014.16,Runtime verification of web services for interconnected medical devices,"This paper presents a framework to ensure the correctness of service-oriented architectures based on runtime verification techniques. Traditionally, the reliability of safety critical systems is ensured by testing the complete system including all subsystems. When those systems are designed as service-oriented architectures, and independently developed subsystems are composed to new systems at runtime, this approach is no longer viable. Instead, the presented framework uses runtime monitors synthesised from high-level specifications to ensure safety constraints. The framework has been designed for the interconnection of medical devices in the operating room. As a case study, the framework is applied to the interconnection of an ultrasound dissector and a microscope. Benchmarks show that the monitoring overhead is negligible in this setting.",Automata | LTL | Medical Devices | Runtime Verification | SMT | Web Services,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2014-12-11,Conference Paper,"Decker, Normann;Kuhn, Franziska;Thoma, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84909955581,10.1145/2593770.2593780,Legal Goal-Oriented Requirement Language (Legal GRL) for modeling regulations,"Every year, governments introduce new or revised regulations that are imposing new types of requirements on software development. Analyzing and modeling these legal requirements is time consuming, challenging and cumbersome for software and requirements engineers. Having regulation models can help understand regulations and converge toward better compliance levels for software and systems. This paper introduces a systematic method to extract legal requirements from regulations by mapping the latter to the Legal Profile for Goal-oriented Requirements Language (GRL) (Legal GRL). This profile provides a conceptual meta-model for the anatomy of regulations and maps its elements to standard GRL with specialized annotations and links, with analysis techniques that exploit this additional information. The paper also illustrates examples of Legal GRL models for The Privacy and Electronic Communications Regulations. Existing tool support (jUCMNav) is also extended to support Legal GRL modeling.",Goal modeling | Legal requirements | Software compliance,"6th International Workshop on Modeling in Software Engineering, MiSE 2014 - Proceedings",2014-06-02,Conference Paper,"Ghanavati, Sepideh;Amyot, Daniel;Rifaut, André",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924358180,10.1145/2593770.2593775,Modeling business processes to generate artifacts for software development: A methodology,"When business processes are to be automated by an information system, business process knowledge is required in many ways throughout the software development life cycle (SDLC). Frequently, analysis is repeated to recapture this knowledge resulting in duplicate effort and inconsistent artifacts. We present our unified business process modeling methodology, UPROM that is used to generate various artifacts from business process models developed in conformance with its notation and approach. Together with business process models, the artifacts are user requirements document, software size estimation, process metrics list, process definition document and business glossary. These artifacts are easily maintainable as they originate from a single source and generated automatically by a tool. The methodology is applied in three case studies and summary of results is provided.",Business glossary | Business process modeling | COSMIC | Process definition document | Process metrics | Requirements analysis | Software size estimation,"6th International Workshop on Modeling in Software Engineering, MiSE 2014 - Proceedings",2014-06-02,Conference Paper,"Aysolmaz, Banu;Demirors, Onur",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942465552,10.1145/2593770.2593771,Toward tractable instantiation of conceptual data models using non-semantics-preserving model transformations,"As a bridge from informal business requirements to precise specifications, conceptual models serve a critical role in the development of enterprise systems. Instantiating conceptual models with test data can help stakeholders validate the model and provide developers with a test database to validate their code. ORM is a popular conceptual modeling language due in part to its expressive constraint language. Due to that expressiveness, instantiating an arbitrary ORM model is NP-hard. Smaragdakis et al. identified a subset of ORM called ORM- that can be instantiated in polynomial time. However, ORM- excludes several constraints commonly used in commercial models. Recent research has extended ORM- through semantics-preserving transformations. We extend the set of ORM models that can be transformed to ORM- models by using a class of non-semantics-preserving transformations called constraint strengthening. We formalize our approach as a special case of Stevens' model transformation framework. We discuss an example transformation and its limitations, and we conclude with a proposal for future research.",Databases | Model transformation | ORM | Test data generation,"6th International Workshop on Modeling in Software Engineering, MiSE 2014 - Proceedings",2014-06-02,Conference Paper,"Nizol, Matthew;Dillon, Laura K.;Stirewalt, R. E.K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928110030,10.1145/2597073.2597085,Mining energy-greedy API usage patterns in android apps: An empirical study,"Energy consumption of mobile applications is nowadays a hot topic, given the widespread use of mobile devices. The high demand for features and improved user experience, given the available powerful hardware, tend to increase the apps' energy consumption. However, excessive energy consumption in mobile apps could also be a consequence of energy greedy hardware, bad programming practices, or particular API usage patterns. We present the largest to date quantitative and qualitative empirical investigation into the categories of API calls and usage patterns that-in the context of the Android development framework-exhibit particularly high energy consumption profiles. By using a hardware power monitor, we measure energy consumption of method calls when executing typical usage scenarios in 55 mobile apps from different domains. Based on the collected data, we mine and analyze energy-greedy APIs and usage patterns. We zoom in and discuss the cases where either the anomalous energy consumption is unavoidable or where it is due to suboptimal usage or choice of APIs. Finally, we synthesize our findings into actionable knowledge and recipes for developers on how to reduce energy consumption while using certain categories of Android APIs and patterns.",Empirical study | Energy consumption | Mobile applications,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Linares-Vásquez, Mario;Bavota, Gabriele;Bernal-Cárdenas, Carlos;Oliveto, Rocco;Di Penta, Massimiliano;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907357885,10.1145/2597073.2597110,Mining questions about software energy consumption,"A growing number of software solutions have been proposed to address application-level energy consumption problems in the last few years. However, little is known about how much software developers are concerned about energy consumption, what aspects of energy consumption they consider important, and what solutions they have in mind for improving energy efficiency. In this paper we present the first empirical study on understanding the views of application programmers on software energy consumption problems. Using STACKOVERFLOW as our primary data source, we analyze a carefully curated sample of more than 300 questions and 550 answers from more than 800 users. With this data, we observed a number of interesting findings. Our study shows that practitioners are aware of the energy consumption problems: the questions they ask are not only diverse - we found 5 main themes of questions - but also often more interesting and challenging when compared to the control question set. Even though energy consumption-related questions are popular when considering a number of different popularity measures, the same cannot be said about the quality of their answers. In addition, we observed that some of these answers are often flawed or vague. We contrast the advice provided by these answers with the state-of-the-art research on energy consumption. Our summary of software energy consumption problems may help researchers focus on what matters the most to software developers and end users.",Practitioners | Q&A | Software energy consumption,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Pinto, Gustavo;Castor, Fernando;Liu, Yu David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84938783489,10.1145/2597073.2597099,Characterizing and predicting blocking bugs in open source projects,"As software becomes increasingly important, its quality becomes an increasingly important issue. Therefore, prior work focused on software quality and proposed many prediction models to identify the location of software bugs, to estimate their fixing-time, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These blocking bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems. In this paper, we study blocking-bugs in six open source projects and propose a model to predict them. Our goal is to help developers identify these blocking bugs early on. We collect the bug reports from the bug tracking systems of the projects, then we obtain 14 different factors related to, for example, the textual description of the bug, the location the bug is found in and the people involved with the bug. Based on these factors we build decision trees for each project to predict whether a bug will be a blocking bug or not. Then, we analyze these decision trees in order to determine which factors best indicate these blocking bugs. Our results show that our prediction models achieve F-measures of 15-42%, which is a twoto four-fold improvement over the baseline random predictors. We also find that the most important factors in determining blocking bugs are the comment text, comment size, the number of developers in the CC list of the bug report and the reporter's experience. Our analysis shows that our models reduce the median time to identify a blocking bug by 3-18 days. Copyright is held by the author/owner(s). Publication rights licensed to ACM.",Code metrics | Post-release defects | Process metrics,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Garcia, Harold Valdivia;Shihab, Emad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84938774788,10.1145/2597073.2597101,Unsupervised discovery of intentional process models from event logs,"Research on guidance and method engineering has highlighted that many method engineering issues, such as lack of flexibility or adaptation, are solved more effectively when intentions are explicitly specified. However, software engineering process models are most often described in terms of sequences of activities. This paper presents a novel approach, so-called Map Miner Method (MMM), designed to automate the construction of intentional process models from process logs. To do so, MMM uses Hidden Markov Models to model users' activities logs in terms of users' strategies. MMM also infers users' intentions and constructs fine-grained and coarsegrained intentional process models with respect to the Map metamodel syntax (i.e., metamodel that specifies intentions and strategies of process actors). These models are obtained by optimizing a new precision-fitness metric. The result is a software engineering method process specification aligned with state of the art of method engineering approaches. As a case study, the MMM is used to mine the intentional process associated to the Eclipse platform usage. Observations show that the obtained intentional process model offers a new understanding of software processes, and could readily be used for recommender systems.",Event logs mining | Intentional process modeling | Software development process,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Khodabandelou, Ghazaleh;Hug, Charlotte;Deneckère, Rebecca;Salinesi, Camille",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931042038,10.1145/2597073.2597105,"It's not a bug, it's a feature: Does misclassification affect bug localization?","Bug localization refers to the task of automatically processing bug reports to locate source code files that are responsible for the bugs. Many bug localization techniques have been proposed in the literature. These techniques are often evaluated on issue reports that are marked as bugs by their reporters in issue tracking systems. However, recent findings by Herzig et al. find that a substantial number of issue reports marked as bugs, are not bugs but other kinds of issues like refactorings, request for enhancement, documentation changes, test case creation, and so on. Herzig et al. report that these misclassifications affect bug prediction, namely the task of predicting which files are likely to be buggy in the future. In this work, we investigate whether these misclas-sifications also affect bug localization. To do so, we analyze issue reports that have been manually categorized by Herzig et al. and apply a bug localization technique to recover a ranked list of candidate buggy files for each issue report. We then evaluate whether the quality of ranked lists of reports reported as bugs is the same as that of real bug reports. Our findings shed light that there is a need for additional cleaning steps to be performed on issue reports before they are used to evaluate bug localization techniques.",Bug localization | Misclassification,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Kochhar, Pavneet Singh;Le, Tien Duy B.;Lo, David",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84908635692,10.1145/2597073.2597089,Undocumented and unchecked: Exceptions that spell trouble,"Modern programs rely on large application programming interfaces (apis). The Android framework comprises 231 core apis, and is used by countless developers. We examine a sample of 4,900 distinct crash stack traces from 1,800 different Android applications, looking for Android api methods with undocumented exceptions that are part of application crashes. For the purposes of this study, we take as a reference the version 15 of the Android api, which matches our stack traces. Our results show that a significant number of crashes (19%) might have been avoided if these methods had the corresponding exceptions documented as part of their interface.",APIs | Exceptions | Mobile applications | Stack traces,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Kechagia, Maria;Spinellis, Diomidis",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84920701994,10.1145/2597073.2597118,Sentiment analysis of commit comments in GitHub: An empirical study,"Emotions have a high impact in productivity, task quality, creativity, group rapport and job satisfaction. In this work we use lexical sentiment analysis to study emotions expressed in commit comments of different open source projects and analyze their relationship with different factors such as used programming language, time and day of the week in which the commit was made, team distribution and project approval. Our results show that projects developed in Java tend to have more negative commit comments, and that projects that have more distributed teams tend to have a higher positive polarity in their emotional content. Additionally, we found that commit comments written on Mondays tend to a more negative emotion. While our results need to be confirmed by a more representative sample they are an initial step into the study of emotions and related factors in open source projects.",Human factors in software engineering | Sentiment analysis,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Guzman, Emitza;Azócar, David;Li, Yang",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84931053770,10.1145/2597073.2597126,Lean ghtorrent: GitHub data on demand,"In recent years, GitHub∗ has become the largest code host in the world, with more than 5M developers collaborating across 10M repositories. Numerous popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap, Django or jQuery) have chosen GitHub as their host and have migrated their code base to it. GitHub offers a tremendous research potential. For instance, it is a flagship for current open source development, a place for developers to showcase their expertise to peers or potential recruiters, and the platform where social coding features or pull requests emerged. However, GitHub data is, to date, largely underexplored. To facilitate studies of GitHub, we have created GHTorrent, a scalable, queriable, offline mirror of the data offered through the GitHub REST API. In this paper we present a novel feature of GHTorrent designed to offer customisable data dumps on demand. The new GHTorrent data-on-demand service offers users the possibility to request via a web form up-to-date GHTorrent data dumps for any collection of GitHub repositories. We hope that by offering cus-tomisable GHTorrent data dumps we will not only lower the ""barrier for entry"" even further for researchers interested in mining GitHub data (thus encourage researchers to intensify their mining efforts), but also enhance the replicability of GitHub studies (since a snapshot of the data on which the results were obtained can now easily accompany each study).",Data on demand | Dataset | GitHub,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Gousios, Georgios;Vasilescu, Bogdan;Serebrenik, Alexander;Zaidman, Andy",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84924179321,10.1145/2597073.2597136,Understanding software evolution: The Maisqual Ant data set,"Software engineering is a maturing discipline which has seen many drastic advances in the last years. However, some studies still point to the lack of rigorous and mathematically grounded methods to raise the field to a new emerging science, with proper and reproducible foundations to build upon. Indeed, mathematicians and statisticians do not necessarily have software engineering knowledge, while software engineers and practitioners do not necessarily have a mathematical background. The Maisqual research project intends to fill the gap between both fields by proposing a controlled and peer-reviewed data set series ready to use and study. These data sets feature metrics from different repositories, from source code to mail activity and configuration management meta data. Metrics are described and commented, and all the steps followed for their extraction and treatment are described with contextual information about the data and its meaning. This article introduces the Apache Ant weekly data set, featuring 636 extracts of the project over 12 years at different levels of artefacts - application, files, functions. By associating community and process related information to code extracts, this data set unveils interesting perspectives on the evolution of one of the great success stories of open source.",Data mining | Software engineering | Software metrics,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Baldassari, Boris;Preux, Philippe",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2660193.2660203,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84920817349,10.1145/2660193.2660231,Chisel: Reliability- and accuracy-aware optimization of approximate computational kernels,"The accuracy of an approximate computation is the distance between the result that the computation produces and the corresponding fully accurate result. The reliability of the computation is the probability that it will produce an acceptably accurate result. Emerging approximate hardware platforms provide approximate operations that, inreturn for reduced energy consumption and/or increased performance, exhibit reduced reliability and/or accuracy. We present Chisel, asystem for reliability- and accuracy-aware optimization of approximate computational kernels that run on approximate hardware platforms. Given a combined reliability and/or accuracy specification, Chisel automatically selects approximate kernel operations to synthesize an approximate computation that minimizes energy consumption while satisfying its reliability and accuracy specification. We evaluate Chisel on five applications from the image processing, scientific computing, and financial analysis domains. The experimental results show that our implemented optimization algorithm enables Chisel to optimize our set of benchmark kernels to obtain energy savings from 8.7% to 19.8% compared to the original (exact) kernel implementations while preserving important reliability guarantees.",Approximate computing,ACM SIGPLAN Notices,2014-12-31,Article,"Misailovic, Sasa;Carbin, Michael;Achour, Sara;Qi, Zichao;Rinard, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2660193.2660237,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2660193.2660195,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2660193.2660241,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2660193.2660213,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2594291.2594344,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2594291.2594323,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2594291.2594328,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2594291.2594322,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2594291.2594321,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2594291.2594325,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893492360,10.1145/2535838.2537850,A Galois connection calculus for abstract interpretation,"We introduce a Galois connection calculus for language independent specification of abstract interpretations used in programming language semantics, formal verification, and static analysis. This Galois connection calculus and its type system are typed by abstract interpretation. © 2014 Authors.",abstract interpretation | galois connection | static analysis | verification,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Cousot, Patrick;Cousot, Radhia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893464815,10.1145/2535838.2535840,"Polymorphic functions with set-theoretic types - Part 1: Syntax, semantics, and evaluation","This article is the first part of a two articles series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed λ-calculus with intersection types and an efficient evaluation model for it. In the second part, presented in a companion paper, we define a local type inference system that allows the programmer to omit explicit instantiation annotations, and a type reconstruction system that allows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data. © 2014 ACM.",intersection types | polymorphism | types | xml,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Castagna, Giuseppe;Nguyen, Kim;Xu, Zhiwu;Im, Hyeonseung;Lenglet, Sergueï;Padovani, Luca",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893453611,10.1145/2535838.2535855,Fissile type analysis: Modular checking of almost everywhere invariants,"We present a generic analysis approach to the imperative relationship update problem, in which destructive updates temporarily violate a global invariant of interest. Such invariants can be conveniently and concisely specified with dependent refinement types, which are efficient to check flow-insensitively. Unfortunately, while traditional flow-insensitive type checking is fast, it is inapplicable when the desired invariants can be temporarily broken. To overcome this limitation, past works have directly ratcheted up the complexity of the type analysis and associated type invariants, leading to inefficient analysis and verbose specifications. In contrast, we propose a generic lifting of modular refinement type analyses with a symbolic analysis to efficiently and effectively check concise invariants that hold almost everywhere. The result is an efficient, highly modular flow-insensitive type analysis to optimistically check the preservation of global relationship invariants that can fall back to a precise, disjunctive symbolic analysis when the optimistic assumption is violated. This technique permits programmers to temporarily break and then re-establish relationship invariants - a flexibility that is crucial for checking relationships in real-world, imperative languages. A significant challenge is selectively violating the global type consistency invariant over heap locations, which we achieve via almost type-consistent heaps. To evaluate our approach, we have encoded the problem of verifying the safety of reflective method calls in dynamic languages as a refinement type checking problem. Our analysis is capable of validating reflective call safety at interactive speeds on commonly-used Objective-C libraries and applications. © 2014 ACM.",almost everywhere invariants | almost type-consistent heaps | dependent refinement types | objective-c | reflection,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Coughlin, Devin;Chang, Bor Yuh Evan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893478271,10.1145/2535838.2535876,A trusted mechanised JavaSript specification,"JavaScript is the most widely used web language for client-side applications. Whilst the development of JavaScript was initially just led by implementation, there is now increasing momentum behind the ECMA standardisation process. The time is ripe for a formal, mechanised specification of JavaScript, to clarify ambiguities in the ECMA standards, to serve as a trusted reference for high-level language compilation and JavaScript implementations, and to provide a platform for high-assurance proofs of language properties. We present JSCert, a formalisation of the current ECMA standard in the Coq proof assistant, and JSRef, a reference interpreter for JavaScript extracted from Coq to OCaml. We give a Coq proof that JSRef is correct with respect to JSCert and assess JSRef using test262, the ECMA conformance test suite. Our methodology ensures that JSCert is a comparatively accurate formulation of the English standard, which will only improve as time goes on. We have demonstrated that modern techniques of mechanised specification can handle the complexity of JavaScript. © 2014 ACM.",coq | javascript | mechanised semantics,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Bodin, Martin;Charguéraud, Arthur;Filaretti, Daniele;Gardner, Philippa;Maffeis, Sergio;Naudžiuniene, Daiva;Schmitt, Alan;Smith, Gareth",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893453765,10.1145/2535838.2535859,Bridging boolean and quantitative synthesis using smoothed proof search,"We present a new technique for parameter synthesis under boolean and quantitative objectives. The input to the technique is a ""sketch"" - a program with missing numerical parameters - and a probabilistic assumption about the program's inputs. The goal is to automatically synthesize values for the parameters such that the resulting program satisfies: (1) a {boolean specification}, which states that the program must meet certain assertions, and (2) a {quantitative specification}, which assigns a real valued rating to every program and which the synthesizer is expected to optimize. Our method - called smoothed proof search - reduces this task to a sequence of unconstrained smooth optimization problems that are then solved numerically. By iteratively solving these problems, we obtain parameter values that get closer and closer to meeting the boolean specification; at the limit, we obtain values that provably meet the specification. The approximations are computed using a new notion of smoothing for program abstractions, where an abstract transformer is approximated by a function that is continuous according to a metric over abstract states. We present a prototype implementation of our synthesis procedure, and experimental results on two benchmarks from the embedded control domain. The experiments demonstrate the benefits of smoothed proof search over an approach that does not meet the boolean and quantitative synthesis goals simultaneously. © 2014 ACM.",abstract interpretation | probabilistic programs | probabilistic verification | program smoothing | synthesis,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Chaudhuri, Swarat;Clochard, Martin;Solar-Lezama, Armando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2535838.2535887,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893495034,10.1145/2535838.2535854,Modular reasoning about heap paths via effectively propositional formulas,"First order logic with transitive closure, and separation logic enable elegant interactive verification of heap-manipulating programs. However, undecidabilty results and high asymptotic complexity of checking validity preclude complete automatic verification of such programs, even when loop invariants and procedure contracts are specified as formulas in these logics. This paper tackles the problem of procedure-modular verification of reachability properties of heap-manipulating programs using efficient decision procedures that are complete: that is, a SAT solver must generate a counterexample whenever a program does not satisfy its specification. By (a) requiring each procedure modifies a fixed set of heap partitions and creates a bounded amount of heap sharing, and (b) restricting program contracts and loop invariants to use only deterministic paths in the heap, we show that heap reachability updates can be described in a simple manner. The restrictions force program specifications and verification conditions to lie within a fragment of first-order logic with transitive closure that is reducible to effectively propositional logic, and hence facilitate sound, complete and efficient verification. We implemented a tool atop Z3 and report on preliminary experiments that establish the correctness of several programs that manipulate linked data structures. © 2014 ACM.",linked list | smt | verification,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Itzhaky, Shachar;Banerjee, Anindya;Immerman, Neil;Lahav, Ori;Nanevski, Aleksandar;Sagiv, Mooly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893498144,10.1145/2535838.2535857,Symbolic optimization with SMT solvers,"The rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities. In this paper, we present SYMBA, an efficient SMT-based optimization algorithm for objective functions in the theory of linear real arithmetic (LRA). Given a formula φ and an objective function t, SYMBA finds a satisfying assignment of φ that maximizes the value of t. SYMBA utilizes efficient SMT solvers as black boxes. As a result, it is easy to implement and it directly benefits from future advances in SMT solvers. Moreover, SYMBA can optimize a set of objective functions, reusing information between them to speed up the analysis. We have implemented SYMBA and evaluated it on a large number of optimization benchmarks drawn from program analysis tasks. Our results indicate the power and efficiency of SYMBA in comparison with competing approaches, and highlight the importance of its multi-objective-function feature. © 2014 ACM.",invariant generation | optimization | program analysis | satisfiability modulo theories | symbolic abstraction,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2014-02-11,Conference Paper,"Li, Yi;Albarghouthi, Aws;Kincaid, Zachary;Gurfinkel, Arie;Chechik, Marsha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924918964,10.1109/SCAM.2014.34,C/C++ thread safety analysis,"Writing multithreaded programs is hard. Static analysis tools can help developers by allowing threading policies to be formally specified and mechanically checked. They essentially provide a static type system for threads, and can detect potential race conditions and deadlocks. This paper describes Clang Thread Safety Analysis, a tool which uses annotations to declare and enforce thread safety policies in C and C++ programs. Clang is a production-quality C++ compiler which is available on most platforms, and the analysis can be enabled for any build with a simple warning flag:-Wthread-safety. The analysis is deployed on a large scale at Google, where it has provided sufficient value in practice to drive widespread voluntary adoption. Contrary to popular belief, the need for annotations has not been a liability, and even confers some benefits with respect to software evolution and maintenance.",C++ | calculus of capabilities | deadlock | linear logic | race conditions | static analysis | thread roles | thread safety | type systems,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"Hutchins, Delesley;Ballman, Aaron;Sutherland, Dean",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84924914254,10.1109/SCAM.2014.32,The ekeko/X program transformation tool,"Developers often need to perform repetitive changes to source code. For instance, to repair several instances of a bug or to update all clients of a library to a newer version. Manually performing such changes is laborious and error-prone. Program transformation tools enable automating changes, but specifying changes as a program transformation requires significant expertise. Code templates are often touted as a remedy, yet have never been endorsed wholeheartedly. Their use is mostly limited to expressing the syntactic characteristics of the intended change subjects. Less familiar means have to be resorted to for expressing their structural, control flow, and data flow characteristics. In this tool paper, we introduce a decidedly template-driven program transformation tool called Ekeko/X. Its specifications feature templates for specifying all of the aforementioned characteristics of its subjects. To this end, developers can associate different directives with individual components of a template. Each matching directive imposes particular constraints on the matches for the component it is associated with. Rewriting directives, on the other hand, determine how each match should be changed. We develop Ekeko/X from the ground up, starting from its applicative logic meta-programming foundation. We highlight the key choices in this implementation and demonstrate its use through two example program transformations.",code templates | program transformation | refactoring,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"De Roover, Coen;Inoue, Katsuro",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84924910334,10.1109/SCAM.2014.33,ACUA: API change and usage auditor,"Modern software uses frameworks through their Application Programming Interfaces (APIs). Framework APIs may change while frameworks evolve. Client programs have to upgrade to new releases of frameworks if security vulnerabilities are discovered in the used releases. Patching security vulnerabilities can be delayed by non-security-related API changes when the frameworks used by client programs are not up to date. Keeping frameworks updated can reduce the reaction time to patch security leaks. Client program upgrades are not cost free, developers need to understand the API usages in client programs and API changes between framework releases before conduct upgrading tasks. In this paper, we propose a tool ACUA to generate reports containing detailed API change and usage information by analyzing the binary code of both frameworks and clients programs written in Java. Developers can use the API change and usage reports generated by ACUA to estimate the work load and decide when to starting upgrading client programs based on the estimation.",API change and usage | framework API evolution | Software maintenance,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"Wu, Wei;Adams, Bram;Gueheneuc, Yann Gael;Antoniol, Giuliano",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84924868831,10.1109/SCAM.2014.30,Semantic versioning versus breaking changes: A study of the maven repository,"For users of software libraries or public programming interfaces (APIs), backward compatibility is a desirable trait. Without compatibility, library users will face increased risk and cost when upgrading their dependencies. In this study, we investigate semantic versioning, a versioning scheme which provides strict rules on major versus minor and patch releases. We analyze seven years of library release history in Maven Central, and contrast version identifiers with actual incompatibilities. We find that around one third of all releases introduce at least one breaking change, and that this figure is the same for minor and major releases, indicating that version numbers do not provide developers with information in stability of interfaces. Additionally, we find that the adherence to semantic versioning principles has only marginally increased over time. We also investigate the use of deprecation tags and find out that methods get deleted without applying deprecated tags, and methods with deprecated tags are never deleted. We conclude the paper by arguing that the adherence to semantic versioning principles should increase because it provides users of an interface with a way to determine the amount of rework that is expected when upgrading to a new version.",Semantic versioning | Software libraries,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"Raemaekers, Steven;Van Deursen, Arie;Visser, Joost",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924891686,10.1109/SCAM.2014.42,Are object graphs extracted using abstract interpretation significantly different from the code?,"To evolve object-oriented code, one must understand both the code structure in terms of classes, and the runtime structure in terms of abstractions of objects that are being created and relations between those objects. To help with this understanding, static program analysis can extract heap abstractions such as object graphs. But the extracted graphs can become too large if they do not sufficiently abstract objects, or too imprecise if they abstract objects excessively to the point of being similar to a class diagram, where one box for a class represents all the instances of that class. One previously proposed solution uses both annotations and abstract interpretation to extract a global, hierarchical, abstract object graph that conveys both abstraction and design intent, but can still be related to the code structure. In this paper, we define metrics that relate nodes and edges in the object graph to elements in the code structure, to measure how they differ, and if the differences are indicative of language or design features such as encapsulation, polymorphism and inheritance. We compute the metrics across eight systems totaling over 100 KLOC, and show a statistically significant difference between the code and the object graph. In several cases, the magnitude of this difference is large.",abstract interpretation | object graphs | object-oriented metrics | runtime structure,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"Abi-Antoun, Marwan;Chandrashekar, Sumukhi;Vanciu, Radu;Giang, Andrew",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924874076,10.1109/SCAM.2014.14,On automatically generating commit messages via summarization of source code changes,"Although version control systems allow developers to describe and explain the rationale behind code changes in commit messages, the state of practice indicates that most of the time such commit messages are either very short or even empty. In fact, in a recent study of 23K+ Java projects it has been found that only 10% of the messages are descriptive and over 66% of those messages contained fewer words as compared to a typical English sentence (i.e., 15-20 words). However, accurate and complete commit messages summarizing software changes are important to support a number of development and maintenance tasks. In this paper we present an approach, coined as Change Scribe, which is designed to generate commit messages automatically from change sets. Change Scribe generates natural language commit messages by taking into account commit stereotype, the type of changes (e.g., files rename, changes done only to property files), as well as the impact set of the underlying changes. We evaluated Change Scribe in a survey involving 23 developers in which the participants analyzed automatically generated commit messages from real changes and compared them with commit messages written by the original developers of six open source systems. The results demonstrate that automatically generated messages by Change Scribe are preferred in about 62% of the cases for large commits, and about 54% for small commits.",code changes | commit message | summarization,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"Cortes-Coy, Luis Fernando;Linares-Vasquez, Mario;Aponte, Jairo;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924859315,10.1109/SCAM.2014.15,On the use of context in recommending exception handling code examples,"Studies show that software developers often either misuse exception handling features or use them inefficiently, and such a practice may lead an undergoing software project to a fragile, insecure and non-robust application system. In this paper, we propose a context-aware code recommendation approach that recommends exception handling code examples from a number of popular open source code repositories hosted at GitHub. It collects the code examples exploiting GitHub code search API, and then analyzes, filters and ranks them against the code under development in the IDE by leveraging not only the structural (i.e., graph-based) and lexical features but also the heuristic quality measures of exception handlers in the examples. Experiments with 4,400 code examples and 65 exception handling scenarios as well as comparisons with four existing approaches show that the proposed approach is highly promising.",context-relevance | Exception handler | lexical similarity | structural similarity,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014",2014-12-04,Conference Paper,"Rahman, Mohammad Masudur;Roy, Chanchal K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904364222,10.1145/2591971.2591986,Movers and shakers: Kinetic energy harvesting for the internet of things,"Numerous energy harvesting wireless devices that will serve as building blocks for the Internet of Things (IoT) are currently under development. However, there is still only limited understanding of the properties of various energy sources and their impact on energy harvesting adaptive algorithms. Hence, we focus on characterizing the kinetic (motion) en-ergy that can be harvested by a wireless node with an IoT form factor and on developing energy allocation algorithms for such nodes. In this paper, we describe methods for estimating harvested energy from acceleration traces. To characterize the energy availability associated with specific human activities (e.g., relaxing, walking, cycling), we analyze a motion dataset with over 40 participants. Based on acceleration measurements that we collected for over 200 hours, we study energy generation processes associated with day-long human routines. We also briey summarize our experiments with moving objects. We develop energy allocation algorithms that take into account practical IoT node design considerations, and evaluate the algorithms using the collected measurements. Our observations provide insights into the design of motion energy harvesters, IoT nodes, and energy harvesting adaptive algorithms.. Copyright © 2014 ACM.",Algorithms | Energy harvesting | Internet of Things | Low-power networking | Measurements | Motion energy,SIGMETRICS 2014 - Proceedings of the 2014 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems,2014-01-01,Conference Paper,"Gorlatova, Maria;Sarik, John;Grebla, Guy;Cong, Mina;Kymissis, Ioannis;Zussman, Gil",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2591971.2591995,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84920662086,10.1109/VISSOFT.2014.24,How developers visualize compiler messages: A foundational approach to notification construction,"Self-explanation is one cognitive strategy through which developers comprehend error notifications. Self-explanation, when left solely to developers, can result in a significant loss of productivity because humans are imperfect and bounded in their cognitive abilities. We argue that modern IDEs offer limited visual affordances for aiding developers with self-explanation, because compilers do not reveal their reasoning about the causes of errors to the developer. The contribution of our paper is a foundational set of visual annotations that aid developers in better comprehending error messages when compilers expose their internal reasoning. We demonstrate through a user study of 28 undergraduate Software Engineering students that our annotations align with the way in which developers self-explain error notifications. We show that these annotations allow developers to give significantly better self-explanations when compared against today's dominant visualization paradigm, and that better self-explanations yield better mental models of notifications. The results of our work suggest that the diagrammatic techniques developers use to explain problems can serve as an effective foundation for how IDEs should visually communicate to developers.",,"Proceedings - 2nd IEEE Working Conference on Software Visualization, VISSOFT 2014",2014-12-09,Conference Paper,"Barik, Titus;Lubick, Kevin;Christie, Samuel;Murphy-Hill, Emerson",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84920653537,10.1109/VISSOFT.2014.33,FAVe: Visualizing user feedback for software evolution,"App users can submit feedback about downloaded apps by writing review comments and giving star ratings directly in the distribution platforms. Previous research has shown that this type of feedback contains important information for software evolution. However, in the case of the most popular apps, the amount of received feedback and its unstructured nature can produce difficulties in its analysis. We present an interactive user feedback visualization which displays app reviews from four different points of view: general, review based, feature based and topic-feature based. We conducted a study which visualized 2009 reviews from the Dropbox app available in the App Store. Participants considered the approach useful for software evolution tasks as they found it could aid developers and analysts get an overview of the most and least popular app features, and to prioritize their work. While using different strategies to find relevant information during the study, most participants came to the same conclusions regarding the user reviews and assigned tasks.",,"Proceedings - 2nd IEEE Working Conference on Software Visualization, VISSOFT 2014",2014-12-09,Conference Paper,"Guzman, Emitza;Bhuvanagiri, Padma;Bruegge, Bernd",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908627888,10.1145/2642937.2642981,Dompletion: DOM-aware JavaScript code completion,"JavaScript is a scripting language that plays a prominent role in modern web applications. It is dynamic in nature and interacts heavily with the Document Object Model (DOM) at runtime. These characteristics make providing code completion support to Java-Script programmers particularly challenging. We propose an automated technique that reasons about existing DOM structures, dynamically analyzes the JavaScript code, and provides code completion suggestions for JavaScript code that interacts with the DOM through its APIs. Our automated code completion scheme is implemented in an open source tool called Dompletion. The results of our empirical evaluation indicate that (1) DOM structures exhibit patterns, which can be extracted and reasoned about in the context of code completion suggestions; (2) Dompletion can provide code completion suggestions with a recall of 89%, precision of 90%, and an average time of 2.8 seconds.",Code completion | DOM | JavaScript | Web applications,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Bajaj, Kartik;Pattabiraman, Karthik;Mesbah, Ali",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908614183,10.1145/2642937.2642973,Symbolic state validation through runtime data,"Real world programs are typically built on top of many library functions. Symbolic analysis of these programs generally requires precise models of these functions' Application Programming Interfaces (APIs), which are mostly unavailable because these models are costly to construct. A variant approach of symbolic analysis is to over-approximate the return values of those APIs that have not been modeled. However, such approximation can induce many unreachable symbolic states, which are expensive to validate manually. In this paper, we propose a static approach to automatically validating the reported anomalous symbolic states. The validation makes use of the available runtime data of the unmodeled APIs collected from previous program executions. We show that the symbolic state validation problem can be cast as a MAX-SAT problem and solved by existing constraint solvers. Our approach is motivated by two observations. We may bind the symbolic parameters in un-modeled APIs based on observations made in former executions by other programs. The binding enables us to use the corresponding observed concrete return values of APIs to validate the symbolic states arising from the over-approximated return values of the un-modeled APIs. Second, some symbolic constraints can be accurately evaluated despite the imprecision of the over-approximated symbolic values. Our technique found 80 unreported bugs when it was applied to 10 popular programs with a total of 1.5 million lines of code. All of them can be confirmed by test cases. Our technique presents a promising way to apply the big data paradigm to software engineering. It provides a mechanism to validate the symbolic states of a project by leveraging the many concrete input-output values of APIs collected from other projects.",API modeling | Dynamic analysis | Symbolic analysis | Warning validation,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Li, Yueqi;Cheung, Shing Chi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908627892,10.1145/2642937.2642980,Automated synthesis and deployment of cloud applications,"Complex networked applications are assembled by connecting software components distributed across multiple machines. Building and deploying such systems is a challenging problem which requires a significant amount of expertise: the system architect must ensure that all component dependencies are satisfied, avoid conflicting components, and add the right amount of component replicas to account for quality of service and fault-tolerance. In a cloud environment, one also needs to minimize the virtual resources provisioned upfront, to reduce the cost of operation. Once the full architecture is designed, it is necessary to correctly orchestrate the deployment phase, to ensure all components are started and connected in the right order. We present a toolchain that automates the assembly and deployment of such complex distributed applications. Given as input a high-level specification of the desired system, the set of available components together with their requirements, and the maximal amount of virtual resources to be committed, it synthesizes the full architecture of the system, placing components in an optimal manner using the minimal number of available machines, and automatically deploys the complete system in a cloud environment.",,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Di Cosmo, Roberto;Lienhardt, Michael;Treinen, Ralf;Zacchiroli, Stefano;Zwolakowski, Jakub;Eiche, Antoine;Agahi, Alexis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908614376,10.1145/2642937.2643010,Statistical learning approach for mining API usage mappings for code migration,"The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C#. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy.",API mappings | API usages | Code migration | Statistical learning,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Nguyen, Anh Tuan;Nguyen, Hoan Anh;Nguyen, Tung Thanh;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908654378,10.1145/2642937.2642938,Automated domain-specific C verification with mbeddr,"When verifying C code, two major problems must be addressed. One is the specification of the verified systems properties, the other one is the construction of the verification environment. Neither C itself, nor existing C verification tools, offer the means to efficiently specify application domain-level properties and environments for verification. These two shortcomings hamper the usability of C verification, and limit its adoption in practice. In this paper we introduce an approach that addresses both problems and results in user-friendly and practically usable C verification. The novelty of the approach is the combination of domainspecific language engineering and C verification. We apply the approach in the domain of state-based software, using mbeddr and CBMC. Wevalidate the implementation with an example from the Pacemaker Challenge, developing a functionally verified, lightweight, and deployable cardiac pulse generator. The approach itself is domain-independent.",CBMC | Domain-specific languages | Mbeddr | Verification,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Molotnikov, Zaur;Völter, Markus;Ratiu, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908611339,10.1145/2642937.2643012,Derailer: Interactive security analysis for web applications,"Derailer is an interactive tool for finding security bugs in web applications. Using symbolic execution, it enumerates the ways in which application data might be exposed. The user is asked to examine these exposures and classify the conditions under which they occur as security-related or not; in so doing, the user effectively constructs a specification of the application's security policy. The tool then highlights exposures missing security checks, which tend to be security bugs. We have tested Derailer's scalability on several large opensource Ruby on Rails applications. We have also applied it to a large number of student projects (designed with different security policies in mind), exposing a variety of security bugs that eluded human reviewers.",Security | Static analysis | Web applications,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Near, Joseph P.;Jackson, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908626278,10.1145/2642937.2642968,Minimal strongly unsatisfiable subsets of reactive system specifications,"Verifying realizability in the specification phase is expected to reduce the development costs of safety-critical reactive systems. If a specification is not realizable, we must correct the specification. However, it is not always obvious what part of a specification should be modified. In this paper, we propose a method for obtaining the location of flaws. Rather than realizability, we use strong satisfiability, due to the fact that many practical unrealizable specifications are also strongly unsatisfiable. Using strong satisfiability, the process of analyzing realizability becomes less complex. We define minimal strongly unsatisfiable subsets (MSUSs) to locate flaws, and construct a procedure to compute them. We also show correctness properties of our method, and clarify the time complexity of our method. Furthermore, we implement the procedure, and confirm that MSUSs are computable for specifications of reactive systems at non-trivial scales.",Büchi automata | Flaw-location analysis | LTL | Minimal unsatisfiable subsets | Reactive systems | Realizability | Specifications | Strong satisfiability,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Hagihara, Shigeki;Egawa, Naoki;Shimakawa, Masaya;Yonezaki, Naoki",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908653194,10.1145/2642937.2642997,Potential biases in bug localization: Do they matter?,"Issue tracking systems are valuable resources during software maintenance activities and contain information about the issues faced during the development of a project as well as after its release. Many projects receive many reports of bugs and it is challenging for developers to manually debug and fix them. To mitigate this problem, past studies have proposed information retrieval (IR)-based bug localization techniques, which takes as input a textual description of a bug stored in an issue tracking system, and returns a list of potentially buggy source code files. These studies often evaluate their effectiveness on issue reports marked as bugs in issue tracking systems, using as ground truth the set of files that are modified in commits that fix each bug. However, there are a number of potential biases that can impact the validity of the results reported in these studies. First, issue reports marked as bugs might not be reports of bugs due to error in the reporting and classification process. Many issue reports are about documentation update, request for improvement, refactoring, code cleanups, etc. Second, bug reports might already explicitly specify the buggy program files and for these reports bug localization techniques are not needed. Third, files that get modified in commits that fix the bugs might not contain the bug. This study investigates the extent these potential biases affect the results of a bug localization technique and whether bug localization researchers need to consider these potential biases when evaluating their solutions. In this paper, we analyse issue reports from three different projects: HTTP-Client, Jackrabbit, and Lucene-Java to examine the impact of above three biases on bug localization. Our results show that one of these biases significantly and substantially impacts bug localization results, while the other two biases have negligible or minor impact.",Bias | Bug localization | Empirical study | Issue reports,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Kochhar, Pavneet Singh;Tian, Yuan;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908632081,10.1145/2642937.2648621,Sesame: Modeling and analyzing high-quality service compositions,"Today, software components are traded on markets in form of services. These services can be service compositions consisting of several services. If a software architect wants to provide such a service composition in the market for trade, she needs to perform several tasks: she needs to model the composition, to discover existing services to be part of that composition, and to analyze the composition's functional correctness as well as its quality, e.g., performance. Up to now, the architect needed to find and use different tools for these tasks. Typically, these tools are not interoperable with each other. We provide the tool SeSAME that supports a software architect in all of these tasks. SeSAME is an integrated Eclipse-based tool-suite providing a comprehensive service specification language to model service compositions and existing services. Furthermore, it includes modules for service matching, functional analysis, and non-functional analysis. SeSAME is the first tool that integrates all these tasks into one tool-suite and, thereby, provides holistic support for trading software services. Thus, it contributes to the acceptance and success of a service market.",Non-functional analysis | Service matching | Service oriented computing | Service specification | Verification,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Arifulina, Svetlana;Walther, Sven;Becker, Matthias;Platenius, Marie C.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84908618742,10.1145/2642937.2648627,DupFinder: Integrated tool support for duplicate bug report detection,"To track bugs that appear in a software, developers often make use of a bug tracking system. Users can report bugs that they encounter in such a system. Bug reporting is inherently an uncoordinated distributed process though and thus when a user submits a new bug report, there might be cases when another bug report describing exactly the same problem is already present in the system. Such bug reports are duplicate of each other and these duplicate bug reports need to be identified. A number of past studies have proposed a number of automated approaches to detect duplicate bug reports. However, these approaches are not integrated to existing bug tracking systems. In this paper, we propose a tool named DupFinder, which implements the state-of-theart unsupervised duplicate bug report approach by Runeson et al., as a Bugzilla extension. DupFinder does not require any training data and thus can easily be deployed to any project. DupFinder extracts texts from summary and description fields of a new bug report and recent bug reports present in a bug tracking system, uses vector space model to measure similarity of bug reports, and provides developers with a list of potential duplicate bug reports based on the similarity of these reports with the new bug report. We have released DupFinder as an open source tool in GitHub, which is available at: uhttps://github.com/smagsmu/dupfinder.",Bugzilla | Duplicate bug reports | Integrated tool support,ASE 2014 - Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering,2014-01-01,Conference Paper,"Thung, Ferdian;Kochhar, Pavneet Singh;Lo, David",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84994128013,10.1145/2568225.2568291,TradeMaker: Automated dynamic analysis of synthesized tradespaces,"System designers today are focusing less on point solutions for complex systems and more on design spaces, often with a focus on understanding tradeoffs among non-functional properties across such spaces. This shift places a premium on the efficient comparative evaluation of non-functional properties of designs in such spaces. While static analysis of designs will sometimes suffice, often one must run designs dynamically, under comparable loads, to determine properties and tradeoffs. Yet variant designs often present variant interfaces, requiring that common loads be specialized to many interfaces. The main contributions of this paper are a mathematical framework, architecture, and tool for specification-driven synthesis of design spaces and common loads specialized to individual designs for dynamic tradeoff analysis of non-functional properties in large design spaces. To test our approach we used it to run an experiment to test the validity of static metrics for object-relational database mappings, requiring design space and load synthesis for, and dynamic analysis of, hundreds of database designs.",Dynamic Analysis | ORM | Specication-driven Synthesis | Static Analysis | Tradeo Space,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Bagheri, Hamid;Tang, Chong;Sullivan, Kevin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994141774,10.1145/2568225.2568249,"Two's company, three's a crowd: A case study of crowdsourcing software development","Crowdsourcing is an emerging and promising approach which involves delegating a variety of tasks to an unknown workforce - the crowd. Crowdsourcing has been applied quite successfully in various contexts from basic tasks on Amazon Mechanical Turk to solving complex industry problems, e.g. InnoCentive. Companies are increasingly using crowdsourcing to accomplish specific software development tasks. However, very little research exists on this specific topic. This paper presents an in-depth industry case study of crowdsourcing software development at a multinational corporation. Our case study highlights a number of challenges that arise when crowdsourcing software development. For example, the crowdsourcing development process is essentially a waterfall model and this must eventually be integrated with the agile approach used by the company. Crowdsourcing works better for specific software development tasks that are less complex and stand-alone without interdependencies. The development cost was much greater than originally expected, overhead in terms of company effort to prepare specifications and answer crowdsourcing community queries was much greater, and the time-scale to complete contests, review submissions and resolve quality issues was significant. Finally, quality issues were pushed later in the lifecycle given the lengthy process necessary to identify and resolve quality issues. Given the emphasis in software engineering on identifying bugs as early as possible, this is quite problematic.",case study | challenges | Crowdsourcing | software development,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Stol, Klaas Jan;Fitzgerald, Brian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994177801,10.1145/2568225.2568258,MintHint: Automated synthesis of repair hints,"Being able to automatically repair programs is at the same time a very compelling vision and an extremely challenging task. In this paper, we present MintHint, a novel technique for program repair that is a departure from most of todays approaches. Instead of trying to fully automate program repair, which is often an unachievable goal, MintHint performs statistical correlation analysis to identify expressions that are likely to occur in the repaired code and generates, using pattern-matching based synthesis, repair hints from these expressions. Intuitively, these hints suggest how to rectify a faulty statement and help developers find a complete, actual repair. We also present an empirical evaluation of MintHint in two parts. The first part is a user study that shows that, when debugging, developers productivity improved manyfold with the use of repair hintsinstead of traditional fault localization information alone. The second part consists of applying MintHint to several faults in Unix utilities to further assess the effectiveness of the approach. Our results show that MintHint performs well even in common situations where (1) the repair space searched does not contain the exact repair, and (2) the operational specification obtained from the test cases for repair is incomplete or even imprecise, which can be challenging for approaches aiming at fully automated repair.",Program repair | program synthesis | repair hints | statistical correlations,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Kaleeswaran, Shalini;Tulsian, Varun;Kanade, Aditya;Orso, Alessandro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84993660318,10.1145/2568225.2568315,Influence of social and technical factors for evaluating contribution in GitHub,"Open source software is commonly portrayed as a meritocracy, where decisions are based solely on their technical merit. However, literature on open source suggests a complex social structure underlying the meritocracy. Social work environments such as GitHub make the relationships between users and between users and work artifacts transparent. This transparency enables developers to better use information such as technical value and social connections when making work decisions. We present a study on open source software contribution in GitHub that focuses on the task of evaluating pull requests, which are one of the primary methods for contributing code in GitHub. We analyzed the association of various technical and social measures with the likelihood of contribution acceptance. We found that project managers made use of information signaling both good technical contribution practices for a pull request and the strength of the social connection between the submitter and project manager when evaluating pull requests. Pull requests with many comments were much less likely to be accepted, moderated by the submitter's prior interaction in the project. Well-established projects were more conservative in accepting pull requests. These findings provide evidence that developers use both technical and social information when evaluating potential contributions to open source software projects.",contribution | GitHub | open source | signaling theory | social computing | social media | transparency,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Tsay, Jason;Dabbish, Laura;Herbsleb, James",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994140281,10.1145/2568225.2568247,Improving automated source code summarization via an eye-tracking study of programmers,"Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool and provide evidence to support the development of source code summarization systems.",program comprehension | source code summaries,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Rodeghero, Paige;McMillan, Collin;McBurney, Paul W.;Bosch, Nigel;D'Mello, Sidney",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84993661835,10.1145/2568225.2568246,Inferring models of concurrent systems from logs of their behavior with CSight,"Concurrent systems are notoriously difficult to debug and understand. A common way of gaining insight into system behavior is to inspect execution logs and documentation. Unfortunately, manual inspection of logs is an arduous process, and documentation is often incomplete and out of sync with the implementation. To provide developers with more insight into concurrent systems, we developed CSight. CSight mines logs of a system's executions to infer a concise and accurate model of that system's behavior, in the form of a communicating finite state machine (CFSM). Engineers can use the inferred CFSM model to understand complex behavior, detect anomalies, debug, and increase confidence in the correctness of their implementations. CSight's only requirement is that the logged events have vector timestamps. We provide a tool that automatically adds vector timestamps to system logs. Our tool prototypes are available at http://synoptic.googlecode.com/. This paper presents algorithms for inferring CFSM models from traces of concurrent systems, proves them correct, provides an implementation, and evaluates the implementation in two ways: by running it on logs from three different networked systems and via a user study that focused on bug finding. Our evaluation finds that CSight infers accurate models that can help developers find bugs.",concurrency | CSight | distributed systems | log analysis | Model inference,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Beschastnikh, Ivan;Brun, Yuriy;Ernst, Michael D.;Krishnamurthy, Arvind",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994112899,10.1145/2568225.2568288,APE: An annotation language and middleware for energy-efficient mobile application development,"Energy-efficiency is a key concern in continuously-running mobile applications, such as those for health and context monitoring. Unfortunately, developers must implement complex and customized power-management policies for each application. This involves the use of complex primitives and writing error-prone multithreaded code to monitor hardware state. To address this problem, we present APE, an annotation language and middleware service that eases the development of energy-efficient Android applications. APE annotations are used to demarcate a power-hungry code segment whose execution is deferred until the device enters a state that minimizes the cost of that operation. The execution of power-hungry operations is coordinated across applications by the APE middleware. Several examples show the expressive power of our approach. A case study of using APE annotations in a real mobile sensing application shows that annotations can cleanly specify a power management policy and reduce the complexity of its implementation. An empirical evaluation of the middleware shows that APE introduces negligible overhead and equals hand-tuned code in energy savings, in this case achieving 63.4% energy savings compared to the case when there is no coordination.",annotations | energy-efficiency | hardware monitoring | Mobile applications | programming,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Nikzad, Nima;Chipara, Octav;Griswold, William G.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994173009,10.1145/2568225.2568285,Case studies and tools for contract specifications,"Contracts are a popular tool for specifying the functional behavior of software. This paper characterizes the contracts that developers write, the contracts that developers could write, and how a developer reacts when shown the difference. This paper makes three research contributions based on an investigation of open-source projects' use of Code Contracts. First, we characterize Code Contract usage in practice. For example, approximately three-fourths of the Code Contracts are basic checks for the presence of data. We discuss similarities and differences in usage across the projects, and we identify annotation burden, tool support, and training as possible explanations based on developer interviews. Second, based on contracts automatically inferred for four of the projects, we find that developers underutilize contracts for expressing state updates, object state indicators, and conditional properties. Third, we performed user studies to learn how developers decide which contracts to enforce. The developers used contract suggestions to support their existing use cases with more expressive contracts. However, the suggestions did not lead them to experiment with other use cases for which contracts are better-suited. In support of the research contributions, the paper presents two engineering contributions: (1) Celeriac, a tool for generating traces of .NET programs compatible with the Daikon invariant detection tool, and (2) Contract Inserter, a Visual Studio add-in for discovering and inserting likely invariants as Code Contracts.",design by contract | invariant detection | Specifications,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Schiller, Todd W.;Donohue, Kellen;Coward, Forrest;Ernst, Michael D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84993660528,10.1145/2568225.2568299,How do API documentation and static typing affect API usability?,"When developers use Application Programming Interfaces (APIs), they often rely on documentation to assist their tasks. In previous studies, we reported evidence indicating that static type systems acted as a form of implicit documentation, benefiting developer productivity. Such implicit documentation is easier to maintain, given it is enforced by the compiler, but previous experiments tested users without any explicit documentation. In this paper, we report on a controlled experiment and an exploratory study comparing the impact of using documentation and a static or dynamic type system on a development task. Results of our study both confirm previous findings and show that the benefits of static typing are strengthened with explicit documentation, but that this was not as strongly felt with dynamically typed languages.",API Usability | Documentation | Static Type Systems,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Endrikat, Stefan;Hanenberg, Stefan;Robbes, Romain;Stefik, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84993660533,10.1145/2568225.2568295,Mining billions of AST nodes to study actual and potential usage of Java language features,"Programming languages evolve over time, adding additional language features to simplify common tasks and make the language easier to use. For example, the Java Language Specification has four editions and is currently drafting a fifth. While the addition of language features is driven by an assumed need by the community (often with direct requests for such features), there is little empirical evidence demonstrating how these new features are adopted by developers once released. In this paper, we analyze over 31k open-source Java projects representing over 9 million Java files, which when parsed contain over 18 billion AST nodes. We analyze this corpus to find uses of new Java language features over time. Our study gives interesting insights, such as: there are millions of places features could potentially be used but weren't; developers convert existing code to use new features; and we found thousands of instances of potential resource handling bugs.",empirical study | Java | language feature use | software mining,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Dyer, Robert;Rajan, Hridesh;Nguyen, Hoan Anh;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84993660584,10.1145/2568225.2568304,Patch verification via multiversion interprocedural control flow graphs,"Software development is inherently incremental; however, it is challenging to correctly introduce changes on top of existing code. Recent studies show that 15%-24% of the bug fixes are incorrect, and the most important yet hard-to-acquire information for programming changes is whether this change breaks any code elsewhere. This paper presents a framework, called Hydrogen, for patch verification. Hydrogen aims to automatically determine whether a patch correctly fixes a bug, a new bug is introduced in the change, a bug can impact multiple software releases, and the patch is applicable for all the impacted releases. Hydrogen consists of a novel program representation, namely multiversion interprocedural control flow graph (MVICFG), that integrates and compares control flow of multiple versions of programs, and a demand-driven, path-sensitive symbolic analysis that traverses the MVICFG for detecting bugs related to software changes and versions. In this paper, we present the definition, construction and applications of MVICFGs. Our experimental results show that Hydrogen correctly builds desired MVICFGs and is scalable to real-life programs such as libpng, tightvnc and putty. We experimentally demonstrate that MVICFGs can enable efficient patch verification. Using the results generated by Hydrogen, we have found a few documentation errors related to patches for a set of open-source programs.",Multiversion | Patch Verfication | Software Changes,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Le, Wei;Pattison, Shannon D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942515324,10.1145/2597008.2597156,Revealing the relationship between architectural elements and source code characteristics,"Understanding how a software system is structured, i.e. its architecture, is crucial for software comprehension. It allows developers to understand an implemented system and reason about how non-functional requirements are addressed. Yet, many systems lack any architectural documentation, or it is often outdated due to software evolution. In current practice, the process of recovering a system's architecture relies primarily on developer knowledge. Although existing architecture recovery approaches can help to identify architectural elements, these approaches require improvement to identify architectural concepts of a system automatically. Towards this goal, we analyze the usefulness of adopting different code-level characteristics to group elements into architectural modules. Our main contributions are an evaluation of the relationships between different sets of characteristics and their corresponding accuracies, and the evaluation results, which help us to understand which characteristics reveal information about the source code structure. Our experiment shows that an identified set of characteristics achieves an average accuracy of 80%, which indicates the usefulness of the considered characteristics for architecture recovery and thus to improving software comprehension.",Architecture reconstruction | Architecture recovery | Software architecture | Source code characteristics,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"Zapalowski, Vanius;Nunes, Ingrid;Nunes, Daltro José",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84910644399,10.1145/2597008.2597155,How do API changes trigger stack overflow discussions? A study on the android SDK,"The growing number of questions related to mobile development in StackOverow highlights an increasing interest of software developers in mobile programming. For the Android platform, 213,836 questions were tagged with Android-related labels in StackOverow between July 2008 and August 2012. This paper aims at investigating how changes occurring to Android APIs trigger questions and activity in StackOverflow, and whether this is particularly true for certain kinds of changes. Our findings suggest that Android developers usually have more questions when the behavior of APIs is modified. In addition, deleting public methods from APIs is a trigger for questions that are (i) more discussed and of major interest for the community, and (ii) posted by more experienced developers. In general, results of this paper provide important insights about the use of social media to learn about changes in software ecosystems, and establish solid foundations for building new recommenders for notifying developers/managers about important changes and recommending them relevant crowdsourced solutions.",Android | API Changes | Social Media | StackOverflow,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"Linares-Vásquez, Mario;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942016350,10.1145/2597008.2597142,Towards more accurate content categorization of API discussions,"Nowadays, software developers often discuss the usage of various APIs in online forums. Automatically assigning pre-defined semantic categorizes to API discussions in these forums could help manage the data in online forums, and assist developers to search for useful information. We refer to this process as content categorization of API discussions. To solve this problem, Hou and Mo proposed the usage of naive Bayes multinomial, which is an effective classification algorithm. In this paper, we propose a Cache-bAsed compoSitE algorithm, short formed as CASE, to automatically categorize API discussions. Considering that the content of an API discussion contains both textual description and source code, CASE has 3 components that analyze an API discussion in 3 different ways: Text, code, and original. In the text component, CASE only considers the textual description; in the code component, CASE only considers the source code; in the original component, CASE considers the original content of an API discussion which might include textual description and source code. Next, for each component, since different terms (i.e., words) have different affinities to different categories, CASE caches a subset of terms which have the highest affinity scores to each category, and builds a classifier based on the cached terms. Finally, CASE combines all the 3 classifiers to achieve a better accuracy score. We evaluate the performance of CASE on 3 datasets which contain a total of 1,035 API discussions. The experiment results show that CASE achieves accuracy scores of 0.69, 0.77, and 0.96 for the 3 datasets respectively, which outperforms the state-of-the-art method proposed by Hou and Mo by 11%, 10%, and 2%, respectively.",API Discussion | Cache-Based Method | Composite Method | Text Categorization,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"Zhou, Bo;Xia, Xin;Lo, David;Tian, Cong;Wang, Xinyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942509536,10.1145/2597008.2597799,CODES: Mining sourCe cOde descriptions from developErs diScussions,"Program comprehension is a crucial activity, preliminary to any software maintenance task. Such an activity can be difficult when the source code is not adequately documented, or the documentation is outdated. Differently from the many existing software re-documentation approaches, based on different kinds of code analysis, this paper describes CODES (mining sourCe cOde Descriptions from developErs diScussions), a tool which applies a ""social"" approach to software re-documentation. Specifically, CODES extracts candidate method documentation from StackOverow discussions, and creates Javadoc descriptions from it. We evaluated CODES to mine Lucene and Hibernate method descriptions. The results indicate that CODES is able to extract descriptions for 20% and 28% of the Lucene and Hibernate methods with a precision of 84% and 91% respectively. Demo URL: http://youtu.be/Rnc5ni1AAzc Demo Web Page: Www.ing.unisannio.it/spanichella/pages/tools/CODES.",Code re-documentation | Mining developer discussions | Program comprehension,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"Vassallo, Carmine;Panichella, Sebastiano;Di Penta, Massimiliano;Canfora, Gerardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84910643783,10.1145/2597008.2597152,An empirical comparison of static and dynamic type systems on API usage in the presence of an IDE: Java vs. Groovy with eclipse,"Several studies have concluded that static type systems offer an advantage over dynamic type systems for programming tasks involving the discovery of a new API. However, these studies did not take into account modern IDE features; the advanced navigation and code completion techniques available in modern IDEs could drastically alter their conclusions. This study describes an experiment that compares the usage of an unknown API using Java and Groovy using the IDE Eclipse. It turns out that the previous finding that static type systems improve the usability of an unknown API still holds, even in the presence of a modern IDE.",Empirical research | Programming languages | Type systems,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"Petersen, Pujan;Hanenberg, Stefan;Robbes, Romain",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84908627116,10.1145/2597008.2597788,Cross-Language bug localization,"Bug localization refers to the process of identifying source code files that contain defects from textual descriptions in bug reports. Existing bug localization techniques work on the assumption that bug reports, and identifiers and comments in source code files, are written in the same language (i.e., English). However, software users from non-English speaking countries (e.g., China) often use their native languages (e.g., Chinese) to write bug reports. For this setting, existing studies on bug localization would not work as the terms that appear in the bug reports do not appear in the source code. We refer to this problem as cross-language bug localization. In this paper, we propose a cross-language bug localization algorithm named CrosLocator, which is based on language translation. Since different online translators (e.g., Google and Microsoft translators) have different translation accuracies for various texts, CrosLocator uses multiple translators to convert a non-English textual description of a bug report into English { each bug report would then have multiple translated versions. For each translated version, CrosLocator applies a bug localization technique to rank source code files. Finally, CrosLocator combines the multiple ranked lists of source code files. Our preliminary experiment on Ruby-China shows that CrosLocator could achieve mean reciprocal rank (mrr) and mean average precision (map) scores of up to 0.146 and 0.116, which outperforms a baseline approach by an average of 10% and 12% respectively.",Bug localization | Cross-language | Rank | Translator,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"Xia, Xin;Lo, David;Wang, Xingen;Zhang, Chenyi;Wang, Xinyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84931071048,10.1145/2597008.2597149,Automatic documentation generation via source code summarization of method context,"A documentation generator is a programming tool that creates documentation for software by analyzing the statements and comments in the software's source code. While many of these tools are manual, in that they require speciallyformatted metadata written by programmers, new research has made inroads towards automatic generation of documentation. These approaches work by stitching together keywords from the source code into readable natural language sentences. These approaches have been shown to be effective, but carry a key limitation: The generated documents do not explain the source code's context. They can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a technique that includes this context by analyzing how the Java methods are invoked. In a user study, we found that programmers benefit from our generated documentation because it includes context information.",Source code summarization,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"McBurney, Paul W.;McMillan, Collin",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84942532192,10.1145/2597008.2597793,Improving topic model source code summarization,"In this paper, we present an emerging source code summarization technique that uses topic modeling to select keywords and topics as summaries for source code. Our approach organizes the topics in source code into a hierarchy, with more general topics near the top of the hierarchy. In this way, we present the software's highest-level functionality first, before lower-level details. This is an advantage over previous approaches based on topic models, that only present groups of related keywords without a hierarchy. We conducted a preliminary user study that found our approach selects keywords and topics that the participants found to be accurate in a majority of cases.",Software topic models | Source code summarization,"22nd International Conference on Program Comprehension, ICPC 2014 - Proceedings",2014-06-02,Conference Paper,"McBurney, Paul W.;Liu, Cheng;McMillan, Collin;Weninger, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905451678,10.1145/2601248.2601292,UML consistency rules: A systematic mapping study,"Context: The Unified Modeling Language (UML), with its 14 different diagram types, is the de-facto standard modeling language for object-oriented modeling and documentation. Since the various UML diagrams describe different aspects of one, and only one, software under development, they are not independent but strongly depend on each other in many ways. In other words, the UML diagrams describing a software product must be consistent. Inconsistencies between these diagrams may be a source of faults in software systems. It is therefore paramount that these inconsistencies be detected, analyzed and hopefully fixed. Objective: The aim of this article is to deliver a comprehensive summary of UML consistency rules as they are described in the literature to date to obtain an extensive and detailed overview of the current research in this area. Method: We performed a Systematic Mapping Study by following well-known guidelines. We selected 95 primary studies from a search with seven search engines performed in December 2012. Results: Different results are worth mentioning. First it appears that researchers tend to discuss very similar consistency rules, over and over again. Most rules are horizontal (98.10%) and syntactic (88.21%). The most used diagrams are the class diagram (71.58%), the sequence diagram (47.37%) and the state machine diagram (42.11%). Conclusion: The fact that many rules are duplicated in primary studies confirms the need for a well-accepted list of consistency rules. This paper is a first step in this direction. Results indicate that much more work is needed to develop consistency rules for all 14 UML diagrams, in all dimensions of consistency (e.g., semantic and syntactic on the one hand, horizontal, vertical and evolution on the other hand). Copyright 2014 ACM.",Systematic mapping study | UML consistency rules | Unified Modeling Language (UML),ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Torre, Damiano;Labiche, Yvan;Genero, Marcela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905498843,10.1145/2601248.2601266,Automatic extraction of developer expertise,"Context: Expert identification is becoming critical to ease the communication between developers in case of global software development or to better know members of large software communities. To quickly identify who are the experts that will best perform a given development task, both the assignment of skills to developers and the computation of their corresponding expertise level have to be automated. Since the real level of expertise is tedious to assess, our challenge is to identify developers having a significant level of experience with respect to a skill. Method: In this paper we propose XTic, an approach that takes up this challenge with the intent to be accurate and efficient. XTic provides a language to specify skills. It also provides an automatic process that extracts skills and experience levels from source code repositories. Our approach is based on the idea that an expert has a high level of experience with respect to a skill. Results: We have validated XTic both on open source and industrial projects to measure its accuracy and its efficiency. The results we obtained show that its accuracy is between moderate and strong and that it scales well with medium and large size software projects. Conclusion: XTic supports the specification of a diversity of developer skills and the extraction of the expertise of these developers under the form of level of experience. Copyright 2014 ACM.",Software maintenance | Software repositories | Team communication,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Teyton, Cédric;Palyart, Marc;Falleri, Jean Rémy;Morandat, Floréal;Blanc, Xavier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905457369,10.1145/2601248.2601293,Knowledge sharing for common understanding of technical specifications through artifactual culture,"Context: Software engineering is a knowledge intensive activity that is supported by documenting and sharing the required knowledge through a wide variety of artifacts. Global Software Development (GSD) teams heavily rely on artifacts as a vital means of knowledge sharing. However, there is little empirical knowledge about the key reasons and practices of using artifacts in GSD for knowledge sharing to support common understanding of technical specifications. Objective: This study aims at empirically studying the key motivators, practices, and drawbacks of artifact-based knowledge sharing for achieving common understanding of technical specifications in the context of GSD. Method: We conducted an exploratory case study in an organization that was involved in several GSD projects. Results: Our findings revealed the key challenges that necessitated the use of artifacts for sharing technical specification knowledge. We also present the practices that make up the artifact-based knowledge sharing system in the studied case. Finally, we shed some light on the caveats of knowledge sharing practices adopted by the studied company. The findings can provide useful insights into the artifact-based knowledge sharing practices and how it can be complemented by having certain level of social ties among distributed team members, even through asynchronous means. Copyright 2014 ACM.",Case study | GSD | Knowledge management,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Zahedi, Mansooreh;Babar, Muhammad Ali",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905435496,10.1145/2601248.2601274,Outcomes of a community workshop to identify and rank barriers to the systematic literature review process,"Systematic Literature Reviews (SLRs) are an important tool used by software engineering researchers to summarize the state of knowledge about a particular topic. Currently, SLR authors must perform the dificult, time-consuming task in largely manual fashion. To identify barriers faced by SLR authors, we conducted an interactive community workshop prior to ESEM'13. Workshop participants generated a total of 100 ideas that, through group discussions, formed 37 composite barriers to the SLR process. Further analysis reveals the barriers relate to latent themes regarding the SLR process, primary studies, the practitioner community, and tooling. This paper describes the barriers identified during the workshop along with a ranking of those barriers that is based on votes by workshop attendees. The paper concludes by describing the impact of these barriers on three important constituencies: SLR Methodology Researchers, SLR Authors and SLR consumers. Copyright 2014 ACM.",Empirical software engineering | Systematic Literature Review | Workshop,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Hassler, Edgar;Carver, Jeffrey C.;Kraft, Nicholas A.;Hale, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905496645,10.1145/2601248.2601282,Artifacts of software reference architectures: A case study,"Context: Software reference architectures (SRA) have emerged as an approach to systematically reuse architectural knowledge and software elements in the development of software systems. Over the last years, research has been conducted to uncover the artifacts that SRAs provide in order to build software systems. However, empirical studies have not focused on providing industrial evidence about such artifacts. Aim: This paper investigates which artifacts constitute an SRA, how SRAs are designed, the potential reuse of SRA's artifacts, and how they are used in practice. Method: The study consists of a case study made in collaboration with a multinational consulting company that designs SRAs for diverse client organizations. A total of nine European client organizations that use an SRA participated in the study. We analyzed available documentation and contacted 28 practitioners. Results: In the nine analyzed projects, we observed that the artifacts that constitute an SRA are mainly software elements, guidelines and documentation. The design and implementation of SRAs are influenced by the reuse of artifacts from previous software system development and experiences, and the reuse of an SRA across different business domains may be possible when they are platform-oriented. Regarding SRAs usage, we observed that conformance checking is seldom performed. Conclusions: This study reports artifacts of SRAs as stated by practitioners in order to help software architects and scientists in the inception, design, and application of SRAs. Copyright © 2014 ACM.",Case study | Empirical software engineering | Software reference architecture | Software reuse,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Martínez-Fernández, Silverio;Ayala, Claudia;Franch, Xavier;Marques, Helena Martins",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905460466,10.1145/2601248.2601295,Modelling software engineering research with RSML,"Background. In order to understand research on a particular computing topic, practitioners and researchers often need to obtain an overview of its research methods. Current research methods coding schemes either capture insufficient details to support a full critical assessment, or are specialised to a particular research type. Aim. This paper defines and demonstrates RSML, a Research Schema Modelling Language that captures a high level of detail and is applicable to most types of computing research. Method. RSML was designed using concepts from the research methods literature, then refined inductively. An RSML editor was created to assist coders and help reduce coding errors. To demonstrate the feasibility of modelling research with RSML, and to exemplify the summary information that can be derived from a database of RSML encodings, a trial review of 24 articles from one journal was conducted. Results. The review illustrates quantitatively the journal's focus on artifact construction and empiricism. It also reveals that observations are rarely used to inform artifact construction, and purely empirical studies are scarce. Conclusion. RSML can be used to model sophisticated, multifaceted research spanning a wide range of software engineering topics, yielding insights that are not easily captured by current coding schemes. Copyright is held by the owner/author(s).",Categorisation | Checklists | Modelling | Research methods,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Jordan, Howell;Beecham, Sarah;Botterweck, Goetz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905454889,10.1145/2601248.2601251,Dealing with identifiers and comments in source code comprehension and maintenance: Results from an ethnographically-informed study with students and professionals,"There are a number of empirical studies that assess the benefit deriving from the use of documentation and models in the execution of maintenance tasks. The greater part of these studies are quantitative and fail to analyze the values, beliefs, and assumptions that inform and shape source code comprehensibility and maintainability. We designed and conducted a qualitative study to understand the role of source code comments and identifiers in source code comprehensibility and maintainability. In particular, we sought to understand how novice and young professional developers perceive comments and identifier names after they have inspected the system behavior visible in its user interfaces. Novice developers were 18 third-year Bachelor students in Computer Science. The young professional developers were 12 and had work experience in between 3 months and 2 and half years. The used qualitative methodological approach is ethnographic. We asked the participants to inspect the behavior of a Java application visible in its user interfaces and then to comprehend and modify the source code of that application. We immersed ourselves and participated to the study, while collecting data by means of contemporaneous field notes, audio recordings, and copies of various artifacts. From the collected data, we have identified insights into comprehension and maintenance practices. The main insights can be summarized as follows: (i) with respect to novice developers, professional developers prefer to deal with identifier names rather than comments, (ii) all the participants indicate as essential the use of naming convention techniques for identifiers, and (iii) for all the participants the names of identifiers are important and should be properly chosen. Summarizing, independently from the kind of developer, it is advisable to use naming convention techniques and to properly choose identifiers. Copyright is held by the owner/author.",Ethnographically-informed study | Program comprehension | Qualitative study | Software maintenance,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Salviulo, Felice;Scanniello, Giuseppe",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84905497113,10.1145/2601248.2601277,Identifying performance issues based on method invocation patterns of an API,"Software systems use many third party libraries by invoking their APIs. A software system may potentially use an API in an inefficient manner, for example, by creating unnecessary or a large number of short-lived objects. This can cause performance degradation in terms of memory usage and latency in critical applications. In this paper we use an object invocation model based on object creation and their method invocations from different code locations. We use a framework to extract the model features from a running software system. The extracted features are then used in a clustering based mechanism to identify problematic code locations within the software system. We demonstrate our approach by analyzing Java Collection API objects in a Java-based open source editor JEdit. We have successfully identified interesting code locations and discussed their impact on software performance. Copyright is held by the owner/author(s).",API object invocation model | API usage analysis | Bytecode instrumentation | Dynamic program analysis,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Khan, Majid Ali;Muhammad, Shahabuddin;Muhammad, Tufail",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905506092,10.1145/2601248.2613080,Research proposal: Objective evaluation of object oriented design quality,"Given two designs that meet the same specification-to what extent can we use objective measures to assert that either of the designs is of superior quality? This research focuses on design quality at the class interaction-level and the extent to which we can compare the quality of software designs-with an aim of being able to identify exemplars of 'good design' and give practical advice on design problems in both industrial and educational settings . An initial pilot study examining ""program to an interface"" use in open source systems has identified that any general notion of design quality must be applicable across a wide range of systems and architectures. Copyright 2014 ACM.",Design quality | Non-functional requirements | Programming guidelines,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Stevenson, Jamie",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84922196368,10.1109/ICGSE.2014.16,Exploring the impact of API complexity on failure-proneness,"Interfaces, or APIs, are central to realizing the benefits of information hiding, but despite their widespread use, designing good interfaces is not a trivial activity. Particular design choices can have a significant detrimental effect on quality or development productivity, in particular, in large and geo-graphically distributed projects. In this paper, we take a two-step approach. We first examined the impact of API complexity on the failure proneness of source code files. By API complexity we mean how easy or difficult is to use a particular API based on the public functions and data of this API. Second, we examine the relationship of API complexity and geographical distribution of the development teams. In our study we used data from two large-scale systems from two different software companies and nine open source projects from the GNOME community. Our analyses showed that increases in the complexity of APIs are associated with increases in the failure proneness of source code files. API complexity is the second best predictor in our results. Furthermore, the organizational context and the patterns of geographical distribution of the development teams have a moderating effect on the role of API complexity. We discuss the research and practical implication of the results.",API complexity | corporate versus open source | glogbal software development | software failures,"Proceedings - 2014 IEEE 9th International Conference on Global Software Engineering, ICGSE 2014",2014-10-01,Conference Paper,"Cataldo, Marcelo;De Souza, Cleidson R.B.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84904800815,10.1007/978-3-319-08867-9_2,SMT-based model checking for recursive programs,"We present an SMT-based symbolic model checking algorithm for safety verification of recursive programs. The algorithm is modular and analyzes procedures individually. Unlike other SMT-based approaches, it maintains both over- and under-approximations of procedure summaries. Under-approximations are used to analyze procedure calls without inlining. Over-approximations are used to block infeasible counterexamples and detect convergence to a proof. We show that for programs and properties over a decidable theory, the algorithm is guaranteed to find a counterexample, if one exists. However, efficiency depends on an oracle for quantifier elimination (QE). For Boolean Programs, the algorithm is a polynomial decision procedure, matching the worst-case bounds of the best BDD-based algorithms. For Linear Arithmetic (integers and rationals), we give an efficient instantiation of the algorithm by applying QE lazily. We use existing interpolation techniques to over-approximate QE and introduce Model Based Projection to under-approximate QE. Empirical evaluation on SV-COMP benchmarks shows that our algorithm improves significantly on the state-of-the-art. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Komuravelli, Anvesh;Gurfinkel, Arie;Chaki, Sagar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904813404,10.1007/978-3-319-08867-9_4,Shape analysis via second-order bi-abduction,"We present a new modular shape analysis that can synthesize heap memory specification on a per method basis. We rely on a second-order biabduction mechanism that can give interpretations to unknown shape predicates. There are several novel features in our shape analysis. Firstly, it is grounded on second-order bi-abduction. Secondly, we distinguish unknown pre-predicates in pre-conditions, from unknown post-predicates in post-condition; since the former may be strengthened, while the latter may be weakened. Thirdly, we provide a new heap guard mechanism to support more precise preconditions for heap specification. Lastly, we formalise a set of derivation and normalization rules to give concise definitions for unknown predicates. Our approach has been proven sound and is implemented on top of an existing automated verification system.We show its versatility in synthesizing a wide range of intricate shape specifications. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Le, Quang Loc;Gherghina, Cristian;Qin, Shengchao;Chin, Wei Ngan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904795419,10.1007/978-3-319-08867-9_6,From invariant checking to invariant inference using randomized search,"We describe a general framework c2i for generating an invariant inference procedure from an invariant checking procedure. Given a checker and a language of possible invariants, c2i generates an inference procedure that iteratively invokes two phases. The search phase uses randomized search to discover candidate invariants and the validate phase uses the checker to either prove or refute that the candidate is an actual invariant. To demonstrate the applicability of c2i, we use it to generate inference procedures that prove safety properties of numerical programs, prove non-termination of numerical programs, prove functional specifications of array manipulating programs, prove safety properties of string manipulating programs, and prove functional specifications of heap manipulating programs that use linked list data structures. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Sharma, Rahul;Aiken, Alex",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904812672,10.1007/978-3-319-08867-9_18,Verifying relative error bounds using symbolic simulation,"In this paper we consider the problem of formally verifying hardware that is specified to compute reciprocal, reciprocal square root, and power-of-two functions on floating point numbers to within a given relative error. Such specifications differ from the common case in which any given input is specified to have exactly one correct output. Our approach is based on symbolic simulation with binary decision diagrams, and involves two distinct steps. First, we prove a lemma that reduces the relative error specification to several inequalities that involve reasoning about natural numbers only. The most complex of these inequalities asserts that the product of several naturals is less-than/greater-than another natural. Second, we invoke one of several customized algorithms that decides the inequality, without performing the expensive symbolic multiplications directly. We demonstrate the effectiveness of our approach on a next-generation Intel® processor design and report encouraging time and space metrics for these proofs. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Bingham, Jesse;Leslie-Hurd, Joe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904819933,10.1007/978-3-319-08867-9_21,Software verification in the Google app-engine cloud,"Software verification often requires a large amount of computing resources. In the last years, cloud services emerged as an inexpensive, flexible, and energy-efficient source of computing power. We have investigated if such cloud resources can be used effectively for verification. We chose the platform-as-a-service offer Google App Engine and ported the open-source verification framework CPAchecker to it. We provide our new verification service as a web front-end to users who wish to solve single verification tasks (tutorial usage), and an API for integrating the service into existing verification infrastructures (massively parallel bulk usage). We experimentally evaluate the effectiveness of this service and show that it can be successfully used to offload verification work to the cloud, considerably sparing local verification resources. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Beyer, Dirk;Dresler, Georg;Wendler, Philipp",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904812437,10.1007/978-3-319-08867-9_29,Safraless synthesis for epistemic temporal specifications,"In this paper we address the synthesis problem for specifications given in linear temporal single-agent epistemic logic, KLTL (or KL 1), over single-agent systems having imperfect information of the environment state. [17] have shown that this problem is 2Exptime complete. However, their procedure relies on complex automata constructions that are notoriously resistant to efficient implementations as they use Safra-like determinization. We propose a ""Safraless"" synthesis procedure for a large fragment of KLTL. The construction transforms first the synthesis problem into the problem of checking emptiness for universal co-Büchi tree automata using an information-set construction. Then we build a safety game that can be solved using an antichain-based symbolic technique exploiting the structure of the underlying automata. The technique is implemented and applied to a couple of case studies. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Bozianu, Rodica;Dima, Cǎtǎlin;Filiot, Emmanuel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904802955,10.1007/978-3-319-08867-9_36,G4LTL-ST: Automatic generation of PLC programs,"G4LTL-ST automatically synthesizes control code for industrial Programmable Logic Controls (PLC) from timed behavioral specifications of input-output signals. These specifications are expressed in a linear temporal logic (LTL) extended with non-linear arithmetic constraints and timing constraints on signals. G4LTL-ST generates code in IEC 61131-3-compatible Structured Text, which is compiled into executable code for a large number of industrial field-level devices. The synthesis algorithm of G4LTL-ST implements pseudo-Boolean abstraction of data constraints and the compilation of timing constraints into LTL, together with a counterstrategy-guided abstraction-refinement synthesis loop. Since temporal logic specifications are notoriously difficult to use in practice, G4LTL-ST supports engineers in specifying realizable control problems by suggesting suitable restrictions on the behavior of the control environment from failed synthesis attempts. © 2014 Springer International Publishing.",assumption generation | industrial automation | synthesis | theory combination,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Cheng, Chih Hong;Huang, Chung Hao;Ruess, Harald;Stattelmann, Stefan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84904802541,10.1007/978-3-319-08867-9_37,Automatic atomicity verification for clients of concurrent data structures,"Mainstream programming languages offer libraries of concurrent data structures. Each method call on a concurrent data structure appears to take effect atomically. However, clients of such data structures often require stronger guarantees. For instance, a histogram class that is implemented using a concurrent map may require a method to atomically increment a histogram bar, but its implementation requires multiple calls to the map and hence is not atomic by default. Indeed, prior work has shown that atomicity errors in clients of concurrent data structures occur frequently in production code. We present an automatic and modular verification technique for clients of concurrent data structures. We define a novel sufficient condition for atomicity of clients called condensability. We present a tool called Snowflake that generates proof obligations for condensability of Java client methods and discharges them using an off-the-shelf SMT solver. We applied Snowflake to an existing suite of client methods from several open-source applications. It successfully verified 76.9% of the atomic methods without any change and verified the rest of them with small code refactoring and/or annotations. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Lesani, Mohsen;Millstein, Todd;Palsberg, Jens",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2628136.2628155,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907012604,10.1145/2628136.2628160,Coeffects: A calculus of context-dependent computation,"The notion of context in functional languages no longer refers just to variables in scope. Context can capture additional properties of variables (usage patterns in linear logics; caching requirements in dataflow languages) as well as additional resources or properties of the execution environment (rebindable resources; platform version in a cross-platform application). The recently introduced notion of coeffects captures the latter, whole-context properties, but it failed to capture fine-grained per-variable properties. We remedy this by developing a generalized coeffect system with annotations indexed by a coeffect shape. By instantiating a concrete shape, our system captures previously studied flat (whole-context) coeffects, but also structural (per-variable) coeffects, making coeffect analyses more useful. We show that the structural system enjoys desirable syntactic properties and we give a categorical semantics using extended notions of indexed comonad. The examples presented in this paper are based on analysis of established language features (liveness, linear logics, dataflow, dynamic scoping) and we argue that such context-aware properties will also be useful for future development of languages for increasingly heterogeneous and distributed platforms. © 2014 ACM.",coeffects | context | indexed comonads | types,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2014-01-01,Conference Paper,"Petricek, Tomas;Orchard, Dominic;Mycroft, Alan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2628136.2628143,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2628136.2628161,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907015826,10.1145/2628136.2628159,A relational framework for higher-order shape analysis,"We propose the integration of a relational specification framework within a dependent type system capable of verifying complex invariants over the shapes of algebraic datatypes. Our approach is based on the observation that structural properties of such datatypes can often be naturally expressed as inductively-defined relations over the recursive structure evident in their definitions. By interpreting constructor applications (abstractly) in a relational domain, we can define expressive relational abstractions for a variety of complex data structures, whose structural and shape invariants can be automatically verified. Our specification language also allows for definitions of parametricrelations for polymorphic data types that enable highly composable specifications and naturally generalizes to higher-order polymorphic functions. We describe an algorithm that translates relational specifications into a decidable fragment of first-order logic that can be efficiently discharged by an SMT solver. We have implemented these ideas in a type checker called CATALYST that is incorporated within the MLton SML compiler. Experimental results and case studies indicate that our verification strategy is both practical and effective. © 2014 ACM.",decidability | dependent types | inductive relations | parametric relations | relational specifications | standard ml,"Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP",2014-01-01,Conference Paper,"Kaki, Gowtham;Jagannathan, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922196368,10.1109/ICGSE.2014.16,Exploring the impact of API complexity on failure-proneness,"Interfaces, or APIs, are central to realizing the benefits of information hiding, but despite their widespread use, designing good interfaces is not a trivial activity. Particular design choices can have a significant detrimental effect on quality or development productivity, in particular, in large and geo-graphically distributed projects. In this paper, we take a two-step approach. We first examined the impact of API complexity on the failure proneness of source code files. By API complexity we mean how easy or difficult is to use a particular API based on the public functions and data of this API. Second, we examine the relationship of API complexity and geographical distribution of the development teams. In our study we used data from two large-scale systems from two different software companies and nine open source projects from the GNOME community. Our analyses showed that increases in the complexity of APIs are associated with increases in the failure proneness of source code files. API complexity is the second best predictor in our results. Furthermore, the organizational context and the patterns of geographical distribution of the development teams have a moderating effect on the role of API complexity. We discuss the research and practical implication of the results.",API complexity | corporate versus open source | glogbal software development | software failures,"Proceedings - 2014 IEEE 9th International Conference on Global Software Engineering, ICGSE 2014",2014-10-01,Conference Paper,"Cataldo, Marcelo;De Souza, Cleidson R.B.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84897969341,10.1145/2544136,Dynamite: A tool for the verification of alloy models based on PVS,"Automatic analysis of Alloy models is supported by the Alloy Analyzer, a tool that translates an Alloy model to a propositional formula that is then analyzed using off-the-shelf SAT solvers. The translation requires user-provided bounds on the sizes of data domains. The analysis is limited by the bounds and is therefore partial. Thus, the Alloy Analyzer may not be appropriate for the analysis of critical applications where more conclusive results are necessary. Dynamite is an extension of PVS that embeds a complete calculus for Alloy. It also includes extensions to PVS that allow one to improve the proof effort by, for instance, automatically analyzing new hypotheses with the aid of the Alloy Analyzer. Since PVS sequents may get cluttered with unnecessary formulas, we use the Alloy unsat-core extraction feature in order to refine proof sequents. An internalization of Alloy's syntax as an Alloy specification allows us to use the Alloy Analyzer for producing witnesses for proving existentially quantified formulas. Dynamite complements the partial automatic analysis offered by the Alloy Analyzer with semi-automatic verification through theorem proving. It also improves the theorem proving experience by using the Alloy Analyzer for early error detection, sequent refinement, and witness generation. © 2014 ACM.",Alloy | Alloy calculus | PVS | Unsat-cores,ACM Transactions on Software Engineering and Methodology,2014-01-01,Article,"Moscato, Mariano M.;Lopez Pombo, Carlos G.;Frias, Marcelo F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84901610601,10.1145/2581376,Architecture-level configuration of large-scale embedded software systems,"Configuration in the domain of Integrated Control Systems (ICS) is largely manual, laborious, and error prone. In this article, we propose a model-based configuration approach that provides automation support for reducing configuration effort and the likelihood of configuration errors in the ICS domain. We ground our approach on component-based specifications of ICS families. We then develop a configuration algorithm using constraint satisfaction techniques over finite domains to generate products that are consistent with respect to their ICS family specifications. We reason about the termination and consistency of our configuration algorithm analytically. We evaluate the effectiveness of our configuration approach by applying it to a real subsea oil production system. Specifically, we have rebuilt a number of existing verified product configurations of our industry partner. Our experience shows that our approach can automatically infer up to 50% of the configuration decisions, and reduces the complexity of making configuration decisions. © 2014 ACM.",Consistent configuration | Constraint satisfaction techniques | Formal specification | Model-based product-line engineering | Product configuration | UML/OCL,ACM Transactions on Software Engineering and Methodology,2014-01-01,Article,"Behjati, Razieh;Nejati, Shiva;Briand, Lionel C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84901636097,10.1145/2581377,Solving the search for source code,"Programmers frequently search for source code to reuse using keyword searches. The search effectiveness in facilitating reuse, however, depends on the programmer's ability to specify a query that captures how the desired code may have been implemented. Further, the results often include many irrelevant matches that must be filtered manually. More semantic search approaches could address these limitations, yet existing approaches are either not flexible enough to find approximate matches or require the programmer to define complex specifications as queries. We propose a novel approach to semantic code search that addresses several of these limitations and is designed for queries that can be described using a concrete input/output example. In this approach, programmers write lightweight specifications as inputs and expected output examples. Unlike existing approaches to semantic search, we use an SMT solver to identify programs or program fragments in a repository, which have been automatically transformed into constraints using symbolic analysis, that match the programmer-provided specification. We instantiated and evaluated this approach in subsets of three languages, the Java String library, Yahoo! Pipes mashup language, and SQL select statements, exploring its generality, utility, and trade-offs. The results indicate that this approach is effective at finding relevant code, can be used on its own or to filter results from keyword searches to increase search precision, and is adaptable to find approximate matches and then guide modifications to match the user specifications when exact matches do not already exist. These gains in precision and flexibility come at the cost of performance, for which underlying factors and mitigation strategies are identified. © 2014 ACM.",Lightweight specification | Semantic code search | SMT solvers | Symbolic analysis,ACM Transactions on Software Engineering and Methodology,2014-01-01,Article,"Stolee, Kathryn T.;Elbaum, Sebastian;Dobos, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907033652,10.1145/2622669,On the comprehension of program comprehension,"Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension.We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge. We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments. Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support. © 2014 ACM.",Context-aware software engineering | Empirical software engineering | Information needs | Knowledge sharing | Program comprehension,ACM Transactions on Software Engineering and Methodology,2014-09-05,Conference Paper,"Maalej, Walid;Tiarks, Rebecca;Roehm, Tobias;Koschke, Rainer",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900565665,10.1109/TSE.2014.2312954,Analyzing critical decision-based processes,"Decision-based processes are composed of tasks whose application may depend on explicit decisions relying on the state of the process environment. In specific domains such as healthcare, decision-based processes are often complex and critical in terms of timing and resources. The paper presents a variety of tool-supported techniques for analyzing models of such processes. The analyses allow a variety of errors to be detected early and incrementally on partial models, notably: inadequate decisions resulting from inaccurate or outdated information about the environment state; incomplete decisions; non-deterministic task selections; unreachable tasks along process paths; and violations of non-functional process requirements involving time, resources or costs. The proposed techniques are based on different instantiations of the same generic algorithm that propagates decorations iteratively through the process model. This algorithm in particular allows event-based models to be automatically decorated with state-based invariants. A formal language supporting both event-based and state-based specifications is introduced as a process modeling language to enable such analyses. This language mimics the informal flowcharts commonly used by process stakeholders. It extends High-Level Message Sequence Charts with guards on task-related and environment-related variables. The language provides constructs for specifying task compositions, task refinements, decision trees, multi-agent communication scenarios, and time and resource constraints. The proposed techniques are demonstrated on the incremental building and analysis of a complex model of a real protocol for cancer therapy. © 2012 IEEE.",decision errors | domain-specific languages | Formal specification | model verification | non-functional requirements | process analysis | Process modeling | safety-critical workflows,IEEE Transactions on Software Engineering,2014-01-01,Article,"Damas, Christophe;Lambeau, Bernard;Van Lamsweerde, Axel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900556141,10.1109/TSE.2013.2297712,Automatic summarization of bug reports,"Software developers access bug reports in a project's bug repository to help with a number of different tasks, including understanding how previous changes have been made and understanding multiple aspects of particular defects. A developer's interaction with existing bug reports often requires perusing a substantial amount of text. In this article, we investigate whether it is possible to summarize bug reports automatically so that developers can perform their tasks by consulting shorter summaries instead of entire bug reports. We investigated whether existing conversation-based automated summarizers are applicable to bug reports and found that the quality of generated summaries is similar to summaries produced for e-mail threads and other conversations. We also trained a summarizer on a bug report corpus. This summarizer produces summaries that are statistically better than summaries produced by existing conversation-based generators. To determine if automatically produced bug report summaries can help a developer with their work, we conducted a task-based evaluation that considered the use of summaries for bug report duplicate detection tasks. We found that summaries helped the study participants save time, that there was no evidence that accuracy degraded when summaries were used and that most participants preferred working with summaries to working with original bug reports. © 2012 IEEE.",Bug report duplicate detection | Empirical software engineering | summarization of software artifacts,IEEE Transactions on Software Engineering,2014-01-01,Article,"Rastkar, Sarah;Murphy, Gail C.;Murray, Gabriel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84901010188,10.1109/TSE.2014.2312918,Automated fixing of programs with contracts,"This paper describes AutoFix, an automatic debugging technique that can fix faults in general-purpose software. To provide high-quality fix suggestions and to enable automation of the whole debugging process, AutoFix relies on the presence of simple specification elements in the form of contracts (such as pre- and postconditions). Using contracts enhances the precision of dynamic analysis techniques for fault detection and localization, and for validating fixes. The only required user input to the AutoFix supporting tool is then a faulty program annotated with contracts; the tool produces a collection of validated fixes for the fault ranked according to an estimate of their suitability. In an extensive experimental evaluation, we applied AutoFix to over 200 faults in four code bases of different maturity and quality (of implementation and of contracts). AutoFix successfully fixed 42 percent of the faults, producing, in the majority of cases, corrections of quality comparable to those competent programmers would write; the used computational resources were modest, with an average time per fix below 20 minutes on commodity hardware. These figures compare favorably to the state of the art in automated program fixing, and demonstrate that the AutoFix approach is successfully applicable to reduce the debugging burden in real-world scenarios. © 2013 IEEE.",Automatic program repair | contracts | dynamic analysis,IEEE Transactions on Software Engineering,2014-01-01,Article,"Pei, Yu;Furia, Carlo A.;Nordio, Martin;Wei, Yi;Meyer, Bertrand;Zeller, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84901065451,10.1109/TSE.2014.2312942,REPENT: Analyzing the nature of identifier renamings,"Source code lexicon plays a paramount role in software quality: poor lexicon can lead to poor comprehensibility and even increase software fault-proneness. For this reason, renaming a program entity, i.e., altering the entity identifier, is an important activity during software evolution. Developers rename when they feel that the name of an entity is not (anymore) consistent with its functionality, or when such a name may be misleading. A survey that we performed with 71 developers suggests that 39 percent perform renaming from a few times per week to almost every day and that 92 percent of the participants consider that renaming is not straightforward. However, despite the cost that is associated with renaming, renamings are seldom if ever documented - for example, less than 1 percent of the renamings in the five programs that we studied. This explains why participants largely agree on the usefulness of automatically documenting renamings. In this paper we propose REanaming Program ENTities (REPENT), an approach to automatically document - detect and classify - identifier renamings in source code. REPENT detects renamings based on a combination of source code differencing and data flow analyses. Using a set of natural language tools, REPENT classifies renamings into the different dimensions of a taxonomy that we defined. Using the documented renamings, developers will be able to, for example, look up methods that are part of the public API (as they impact client applications), or look for inconsistencies between the name and the implementation of an entity that underwent a high risk renaming (e.g., towards the opposite meaning). We evaluate the accuracy and completeness of REPENT on the evolution history of five open-source Java programs. The study indicates a precision of 88 percent and a recall of 92 percent. In addition, we report an exploratory study investigating and discussing how identifiers are renamed in the five programs, according to our taxonomy. © 2013 IEEE.",empirical study | Identifier renaming | mining software repositories | program comprehension | refactoring,IEEE Transactions on Software Engineering,2014-01-01,Article,"Arnaoudova, Venera;Eshkevari, Laleh M.;Penta, Massimiliano Di;Oliveto, Rocco;Antoniol, Giuliano;Guéhéneuc, Yann Gaël",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84903132737,10.1109/TSE.2014.2321179,Bayesian networks for evidence-based decision-making in software engineering,"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making. © 2014 IEEE.",Bayesian networks | Bayesian statistics | Evidence-based decision-making | post-release defects | software metrics | software reliability,IEEE Transactions on Software Engineering,2014-06-01,Article,"Misirli, Ayse Tosun;Bener, Ayse Basar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84903182414,10.1109/TSE.2014.2322867,Static analysis for extracting permission checks of a large scale framework: The challenges and solutions for analyzing android,"A common security architecture is based on the protection of certain resources by permission checks (used e.g., in Android and Blackberry). It has some limitations, for instance, when applications are granted more permissions than they actually need, which facilitates all kinds of malicious usage (e.g., through code injection). The analysis of permission-based framework requires a precise mapping between API methods of the framework and the permissions they require. In this paper, we show that naive static analysis fails miserably when applied with off-the-shelf components on the Android framework. We then present an advanced class-hierarchy and field-sensitive set of analyses to extract this mapping. Those static analyses are capable of analyzing the Android framework. They use novel domain specific optimizations dedicated to Android. © 2014 IEEE.",Android | call-graph | Java | Large scale framework | permissions | security | Soot | static analysis,IEEE Transactions on Software Engineering,2014-06-01,Article,"Bartel, Alexandre;Klein, Jacques;Monperrus, Martin;Le Traon, Yves",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84906256844,10.1109/TSE.2014.2322618,An empirical analysis of business process execution language usage,"The current state of executable business process languages allows for and demands optimization of design practices and specifications. In this paper, we present the first empirical study that analyses Web Services Business Process Execution Language (WS-BPEL or BPEL) usage and characteristics of real world executable business processes. We have analysed 1,145 BPEL processes by measuring activity usage and process complexity. In addition, we investigated the occurrence of activity usage patterns. The results revealed that the usage frequency of BPEL activities varies and that some activities have a strong co-occurrence. BPEL activities often appear in activity patterns that are repeated in multiple processes. Furthermore, the current process complexity metrics have proved to be inadequate for measuring BPEL process complexity. The empirical results provide fundamental knowledge on how BPEL specification and process design practices can be improved. We propose BPEL design guidelines and BPEL language improvements for the design of more understandable and less complex processes. The results are of interest to business process language designers, business process tool developers, business process designers and developers, and software engineering researchers, and contribute to the general understanding of BPEL and service-oriented architecture. © 2014 IEEE.",complexity measure | empirical study | process complexity | process comprehension | process patterns | service composition | WS-BPEL Analysis,IEEE Transactions on Software Engineering,2014-08-01,Article,"Hertis, Matej;Juric, Matjaz B.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84910606699,10.1109/TSE.2014.2347969,Using traceability links to recommend adaptive changes for documentation evolution,"Developer documentation helps developers learn frameworks and libraries, yet developing and maintaining accurate documentation requires considerable effort and resources. Contributors who work on developer documentation often need to manually track all changes in the code, determine which changes are significant enough to document, and then, adapt the documentation. We propose AdDoc, a technique that automatically discovers documentation patterns, i.e., coherent sets of code elements that are documented together, and that reports violations of these patterns as the code and the documentation evolves. We evaluated our approach in a retrospective analysis of four Java open source projects and found that at least 50 percent of all the changes in the documentation were related to existing documentation patterns. Our technique allows contributors to quickly adapt existing documentation, so that they can focus their documentation effort on the new features.",Documentation | frameworks | maintainability,IEEE Transactions on Software Engineering,2014-11-01,Article,"Dagenais, Barthélémy;Robillard, Martin P.",Include,
10.1016/j.jss.2022.111515,2-s2.0-84917680092,10.1109/TSE.2014.2354344,Model-transformation design patterns,"This paper defines a catalogue of patterns for the specification and design of model transformations, and provides a systematic scheme and classification of these patterns, together with pattern application examples in leading model transformation languages such as ATL, QVT, GrGen.NET, and others. We consider patterns for improving transformation modularization and efficiency and for reducing data storage requirements. We define a metamodel-based formalization of model transformation design patterns, and measurement-based techniques to guide the selection of patterns. We also provide an evaluation of the effectiveness of transformation patterns on a range of different case studies.",Design patterns | Model transformations | Model-driven development,IEEE Transactions on Software Engineering,2014-01-01,Article,"Lano, Kevin;Kolahdouz-Rahimi, Shekoufeh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921765967,10.1145/2666216.2666217,Business process modeling: Vocabulary problem and requirements specification,"Process models are composed of graphical elements and words. However, words used to name elements during process design have potentially ambiguous meanings, which might result in quality problems. We believe that ontologies might serve as a means to address this problem. This paper discusses aspects related to words used to represent concepts in labels and why ontologies can improve this representation. Also, we analyze how the requirements specifications can influence the terms used during modeling. The discussion regarding ontologies is conceptual. We performed an experiment to analyze empirically the vocabulary problem in the context of process models. In the experiment the selection of terms represented with different levels of explicitness in requirements specifications is evaluated. Our findings suggest that the vocabulary problem occurs in process models. Also, different levels of explicitness affect the labels but are not sufficient to solve the vocabulary problem.",Business process modeling | Ontology | Vocabulary problem,SIGDOC 2014 - Proceedings of the 32nd Annual International Conference on the Design of Communication,2014-09-27,Conference Paper,"Gassen, Jonas Bulegon;Mendling, Jan;Thom, Lucinéia Heloisa;De Oliveira, José Palazzo M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921817460,10.1145/2666216.2666220,Identifying how U.S. nonprofit organizations operate within the Information Process Maturity Model,"In this paper, the researcher investigates grant writers' perspectives on knowledge management practices within nonprofit organizations. The researcher uses survey data (n = 448) to assess where organizations fall within JoAnn Hackos's Information Process Maturity Model (IPMM) [1]. The paper asks, how do proposal writers perceive current documentation efforts within their organizations? Is there a relationship between an organization's annual operating budget and employees' satisfaction with documentation efforts? Results indicate that most nonprofit organizations fall within the Ad hoc or Rudimentary stage of the IPMM. However, this classification may be due to grant writers' preferences to create individual rather than organizational documentation systems. Organizational documentation practices have both short- and long-term economic implications for the nonprofit industry, particularly fundraising, where employee turnover is a problem.",Documentation | Institutional memory | Knowledge management | Nonprofit | Procedures | Proposal writers | Research,SIGDOC 2014 - Proceedings of the 32nd Annual International Conference on the Design of Communication,2014-09-27,Conference Paper,"Gunning, Sarah K.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84921777033,10.1145/2666216.2666224,"Women, religion, and professional communication: Communication design for the female relief society, 1842-1920","My archival research internship experience with a women's discourses project suggests that professional documentation is a vital part of building and maintaining organizations. The historical sources I examined from a religious institution's women's organization displayed a variety of professional communication genres, all of them working to make the larger organization successful and functional. Organizational communication worked to simultaneously promote women's industrial independence while tying women to the larger organization by promoting identities for participating women. These forms of communication ultimately united and disciplined the women of the organization, allowing them to share religious best practices, domestic techniques, and community values with one another. There is much work to be done on the history of professional communication in archives, including how it has been used to design religious organizations.",Archival research | Genre analysis | Women's studies,SIGDOC 2014 - Proceedings of the 32nd Annual International Conference on the Design of Communication,2014-09-27,Conference Paper,"Petersen, Emily January",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921816472,10.1145/2666216.2666225,Genre cycling: The infrastructural function of an operational assessment review and reporting process at a federal scientific supercomputing user facility,"This paper reports on the first phase of a study of technical documentation and reporting at a scientific supercomputing user facility. This paper proposes a new conceptualization of how multiple genres interrelate to coordinate and mediate the functioning of an organization. Based on the case of the operational assessment review and reporting process, this paper strives to differentiate the function of organizational genres to maintain the infrastructural operations of an organization from the function of genres to mediate the production of an organization's mission-based output. The theory-informed analytical tool proposed by this case study, the genre cycle, proposes parameters for further inquiry into the generalizability of the concept.",Assemblage | Coordination | Genre | Genre cycle | Infrastructure | Mediation | Organization | Supercomputing,SIGDOC 2014 - Proceedings of the 32nd Annual International Conference on the Design of Communication,2014-09-27,Conference Paper,"Read, Sarah;Papka, Michael E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2522968.2522969,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2522968.2522976,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84892642784,10.1145/2543581.2543590,The perception of egocentric distances in virtual environments - A review,"Over the last 20 years research has been done on the question of how egocentric distances, i.e., the subjectively reported distance from a human observer to an object, are perceived in virtual environments. This review surveys the existing literature on empirical user studies on this topic. In summary, there is amean estimation of egocentric distances in virtual environments of about 74% of the modeled distances. Many factors possibly influencing distance estimates were reported in the literature. We arranged these factors into four groups, namely measurement methods, technical factors, compositional factors, and human factors. The research on these factors is summarized, conclusions are drawn, and promising areas for future research are outlined. © 2013 ACM.",Depth perception | Distance estimation | Perception | Virtual environments | Virtual reality,ACM Computing Surveys,2014-07-01,Review,"Renner, Rebekka S.;Velichkovsky, Boris M.;Helmert, Jens R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2506375,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2542049,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893712252,10.1007/s10664-012-9215-y,Contributions of model checking and CoFI methodology to the development of space embedded software,"The role of embedded software in the last space accidents highlights the importance of verification and validation techniques for the development of space embedded software. In this context, this work analyses the contribution of two verification techniques applied to the onboard data handling software of space products. The first technique is model checking. The system is modeled by a set of timed automata and the verification of safety and liveness properties is performed using UPPAAL model checker. The verified model is then used to generate the embedded software. The second technique analyzed in this work is model based approach for the generation of test cases. The Conformance and Fault Injection (CoFI) testing methodology is used to guide the development of a set of Finite State Machine (FSM) models from the software specification. The test suite is automatically generated from the FSM models. The contributions of the two methodologies are analyzed based on the results provided by an experiment. Two software products are used as case study, each one implementing two services of the Packet Utilization Standard (PUS). These services represent the functionalities offered by a satellite onboard data handling computer. One of the products is developed with the aid of model checking, while the other is developed according to the practices currently used at the Instituto Nacional de Pesquisas Espaciais (INPE). Both software products are tested by the CoFI methodology. The experiment highlights the advantages and vulnerable points of model checking. It also demonstrates that the main contribution of CoFI testing methodology is to highlight problems related to situations that have not been considered in the software specification, such as the occurrence of inopportune events. This analysis helps to understand how different techniques can be integrated in the design of critical embedded software. © 2012 Springer Science+Business Media, LLC.",Embedded software | Model based testing | Model checking | Packet Utilization Standard (PUS) | Space application | Verification,Empirical Software Engineering,2014-02-01,Article,"Pontes, Rodrigo Pastl;Véras, Paulo Claudino;Ambrosio, Ana Maria;Villani, Emília",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84893811711,10.1007/s10664-012-9219-7,Static test case prioritization using topic models,"Software development teams use test suites to test changes to their source code. In many situations, the test suites are so large that executing every test for every source code change is infeasible, due to time and resource constraints. Development teams need to prioritize their test suite so that as many distinct faults as possible are detected early in the execution of the test suite. We consider the problem of static black-box test case prioritization (TCP), where test suites are prioritized without the availability of the source code of the system under test (SUT). We propose a new static black-box TCP technique that represents test cases using a previously unused data source in the test suite: the linguistic data of the test cases, i.e., their identifier names, comments, and string literals. Our technique applies a text analysis algorithm called topic modeling to the linguistic data to approximate the functionality of each test case, allowing our technique to give high priority to test cases that test different functionalities of the SUT. We compare our proposed technique with existing static black-box TCP techniques in a case study of multiple real-world open source systems: several versions of Apache Ant and Apache Derby. We find that our static black-box TCP technique outperforms existing static black-box TCP techniques, and has comparable or better performance than two existing execution-based TCP techniques. Static black-box TCP methods are widely applicable because the only input they require is the source code of the test cases themselves. This contrasts with other TCP techniques which require access to the SUT runtime behavior, to the SUT specification models, or to the SUT source code. © 2012 Springer Science+Business Media, LLC.",Test case prioritization | Testing and debugging | Topic models,Empirical Software Engineering,2014-02-01,Article,"Thomas, Stephen W.;Hemmati, Hadi;Hassan, Ahmed E.;Blostein, Dorothea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84899967362,10.1007/s10664-012-9224-x,Configuring latent Dirichlet allocation based feature location,"Feature location is a program comprehension activity, the goal of which is to identify source code entities that implement a functionality. Recent feature location techniques apply text retrieval models such as latent Dirichlet allocation (LDA) to corpora built from text embedded in source code. These techniques are highly configurable, and the literature offers little insight into how different configurations affect their performance. In this paper we present a study of an LDA based feature location technique (FLT) in which we measure the performance effects of using different configurations to index corpora and to retrieve 618 features from 6 open source Java systems. In particular, we measure the effects of the query, the text extractor configuration, and the LDA parameter values on the accuracy of the LDA based FLT. Our key findings are that exclusion of comments and literals from the corpus lowers accuracy and that heuristics for selecting LDA parameter values in the natural language context are suboptimal in the source code context. Based on the results of our case study, we offer specific recommendations for configuring the LDA based FLT. © 2012 Springer Science+Business Media, LLC.",Feature location | Program comprehension | Software evolution | Static analysis | Text retrieval,Empirical Software Engineering,2014-01-01,Article,"Biggers, Lauren R.;Bocovich, Cecylia;Capshaw, Riley;Eddy, Brian P.;Etzkorn, Letha H.;Kraft, Nicholas A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84899924240,10.1007/s10664-012-9232-x,An empirical study of control logic specifications for programmable logic controllers,"This paper presents an empirical study of control logic specifications used to document industrial control logic code in manufacturing applications. More than one hundred input/output related property specifications from ten different reusable function blocks were investigated. The main purpose of the study was to provide understanding of how the specifications are expressed by industrial practitioners, in order to develop new tools and methods for specifying control logic software, as well as for evaluating existing ones. In this paper, the studied specifications are used to evaluate linear temporal logic in general and the specification language ST-LTL, tailored for functions blocks, in particular. The study shows that most specifications are expressed as implications, that should always be fulfilled, between input and output conditions. Many of these implications are complex since the input and output conditions may be mixed and involve sequences, timer issues and non-boolean variables. Using ST-LTL it was possible to represent all implications of this study. The few non-implication specifications could be specified in ST-LTL as well after being altered to suit the specification language. The paper demonstrates some advantages of ST-LTL compared to standard linear temporal logic and discusses possible improvements such as support for automatic rewrite of complex specifications. © 2012 Springer Science+Business Media, LLC.",Formal specification | IEC 61131-3 | Industrial study | Programmable logic controller (PLC) | Software requirements and specification | Temporal logic,Empirical Software Engineering,2014-01-01,Article,"Ljungkrantz, Oscar;Åkesson, Knut;Fabian, Martin;Ebrahimi, Amir Hossein",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84899943828,10.1007/s10664-012-9234-8,Comprehensibility of UML-based software product line specifications A controlled experiment,"Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects. © 2012 Springer Science+Business Media, LLC.",Domain models | Empirical evaluation | Software product line engineering | UML | Variability management,Empirical Software Engineering,2014-01-01,Article,"Reinhartz-Berger, Iris;Sturm, Arnon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84901853220,10.1007/s10664-013-9244-1,On the variation and specialisation of workload - A case study of the Gnome ecosystem community,"Most empirical studies of open source software repositories focus on the analysis of isolated projects, or restrict themselves to the study of the relationships between technical artifacts. In contrast, we have carried out a case study that focuses on the actual contributors to software ecosystems, being collections of software projects that are maintained by the same community. To this aim, we defined a new series of workload and involvement metrics, as well as a novel approach - T̃ -graphs - for reporting the results of comparing multiple distributions. We used these techniques to statistically study how workload and involvement of ecosystem contributors varies across projects and across activity types, and we explored to which extent projects and contributors specialise in particular activity types. Using Gnome as a case study we observed that, next to coding, the activities of localization, development documentation and building are prevalent throughout the ecosystem. We also observed notable differences between frequent and occasional contributors in terms of the activity types they are involved in and the number of projects they contribute to. Occasional contributors and contributors that are involved in many different projects tend to be more involved in the localization activity, while frequent contributors tend to be more involved in the coding activity in a limited number of projects. © 2013 Springer Science+Business Media New York.",Case study | Developer community | Metrics | Open source | Software ecosystem,Empirical Software Engineering,2014-01-01,Article,"Vasilescu, Bogdan;Serebrenik, Alexander;Goeminne, Mathieu;Mens, Tom",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84901844397,10.1007/s10664-013-9253-0,Challenges and industry practices for managing software variability in small and medium sized enterprises,"Software variability is an ability to change (configure, customize, extend) software artefacts (e.g. code, product, domain requirements, models, design, documentation, test cases) for a specific context. Optimized variability management can lead a software company to 1) shorter development lead time, 2) improved customer and improved user satisfaction, 3) reduced complexity of product management (more variability, same $) and 4) reduced costs (same variability, less $). However, it is not easy for software companies, especially small and medium size of enterprises to deal with variability. In this paper we present variability challenges and used practices collected from five SMEs. Our study indicates that increased product complexity can lead growing SMEs to the time-consuming decision-making. Many of the analyzed medium size of companies also expect improved tool support to help them to boost their productivity when managing increasingly complex products and increasing amount of variants In fact, in many of the analysed SMEs, a high level of automation in design, release management and testing are or become a key factor for market success By introducing the challenges and used practices related to variability the paper deepens understanding of this highly relevant but relatively under-researched phenomenon and contributes to the literature on software product line engineering. © 2013 Springer Science+Business Media New York.",Software variability challenges | Variability practices,Empirical Software Engineering,2014-01-01,Article,"Ihme, Tuomas;Pikkarainen, Minna;Teppola, Susanna;Kääriäinen, Jukka;Biot, Olivier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907590620,10.1007/s10664-013-9285-5,Labeling source code with information retrieval methods: An empirical study,"To support program comprehension, software artifacts can be labeled - for example within software visualization tools - with a set of representative words, hereby referred to as labels. Such labels can be obtained using various approaches, including Information Retrieval (IR) methods or other simple heuristics. They provide a bird-eye's view of the source code, allowing developers to look over software components fast and make more informed decisions on which parts of the source code they need to analyze in detail. However, few empirical studies have been conducted to verify whether the extracted labels make sense to software developers. This paper investigates (i) to what extent various IR techniques and other simple heuristics overlap with (and differ from) labeling performed by humans; (ii) what kinds of source code terms do humans use when labeling software artifacts; and (iii) what factors - in particular what characteristics of the artifacts to be labeled - influence the performance of automatic labeling techniques. We conducted two experiments in which we asked a group of students (38 in total) to label 20 classes from two Java software systems, JHotDraw and eXVantage. Then, we analyzed to what extent the words identified with an automated technique - including Vector Space Models, Latent Semantic Indexing (LSI), latent Dirichlet allocation (LDA), as well as customized heuristics extracting words from specific source code elements - overlap with those identified by humans. Results indicate that, in most cases, simpler automatic labeling techniques - based on the use of words extracted from class and method names as well as from class comments - better reflect human-based labeling. Indeed, clustering-based approaches (LSI and LDA) are more worthwhile to be used for source code artifacts having a high verbosity, as well as for artifacts requiring more effort to be manually labeled. The obtained results help to define guidelines on how to build effective automatic labeling techniques, and provide some insights on the actual usefulness of automatic labeling techniques during program comprehension tasks.",Empirical studies | Information retrieval | Program comprehension | Software artifact labeling,Empirical Software Engineering,2014-10-01,Article,"De Lucia, Andrea;Di Penta, Massimiliano;Oliveto, Rocco;Panichella, Annibale;Panichella, Sebastiano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84907594075,10.1007/s10664-014-9305-0,AutoRELAX: Automatically RELAXing a goal model to address uncertainty,"Dynamically adaptive systems (DAS) must cope with system and environmental conditions that may not have been fully understood or anticipated during development. RELAX is a fuzzy logic-based specification language for identifying and assessing sources of environmental uncertainty, thereby making DAS requirements more tolerant of unanticipated conditions. This paper presents AutoRELAX, an approach that automatically generates RELAXed goal models to address environmental uncertainty. Specifically, AutoRELAX identifies goals to RELAX, which RELAX operators to apply, and the shape of the fuzzy logic function that establishes the goal satisfaction criteria. AutoRELAX generates different solutions by making tradeoffs between minimizing the number of RELAXed goals and maximizing delivered functionality by reducing the number of adaptations triggered by minor and adverse environmental conditions. In a recent extension, AutoRELAX uses a stepwise adaptation of weights to balance these two competing concerns and thereby further improve the utility of AutoRELAX. We apply it to two industry-based applications involving network management and a robotic controller, respectively.",Genetic algorithms | Goal models | Requirements engineering | Uncertainty,Empirical Software Engineering,2014-10-01,Article,"Fredericks, Erik M.;Devries, Byron;Cheng, Betty H.C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84909993591,10.1007/s10664-013-9261-0,An empirical study of identifier splitting techniques,"Researchers have shown that program analyses that drive software development and maintenance tools supporting search, traceability and other tasks can benefit from leveraging the natural language information found in identifiers and comments. Accurate natural language information depends on correctly splitting the identifiers into their component words and abbreviations. While conventions such as camel-casing can ease this task, conventions are not well-defined in certain situations and may be modified to improve readability, thus making automatic splitting more challenging. This paper describes an empirical study of state-of-the-art identifier splitting techniques and the construction of a publicly available oracle to evaluate identifier splitting algorithms. In addition to comparing current approaches, the results help to guide future development and evaluation of improved identifier splitting approaches.",Identifier names | Program comprehension | Software engineering tools | Source code text analysis,Empirical Software Engineering,2014-10-12,Article,"Hill, Emily;Binkley, David;Lawrie, Dawn;Pollock, Lori;Vijay-Shanker, K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84909998767,10.1007/s10664-013-9264-x,SWordNet: Inferring semantically related words from software context,"Code search is an integral part of software development and program comprehension. The difficulty of code search lies in the inability to guess the exact words used in the code. Therefore, it is crucial for keyword-based code search to expand queries with semantically related words, e.g., synonyms and abbreviations, to increase the search effectiveness. However, it is limited to rely on resources such as English dictionaries and WordNet to obtain semantically related words in software because many words that are semantically related in software are not semantically related in English. On the other hand, many words that are semantically related in English are not semantically related in software. This paper proposes a simple and general technique to automatically infer semantically related words (referred to as rPairs) in software by leveraging the context of words in comments and code. In addition, we propose a ranking algorithm on the rPair results and study cross-project rPairs on two sets of software with similar functionality, i.e., media browsers and operating systems. We achieve a reasonable accuracy in nine large and popular code bases written in C and Java. Our further evaluation against the state of art shows that our technique can achieve a higher precision and recall. In addition, the proposed ranking algorithm improves the rPair extraction accuracy by bringing correct rPairs to the top of the list. Our cross-project study successfully discovers overlapping rPairs among projects of similar functionality and finds that cross-project rPairs are more likely to be correct than project-specific rPairs. Since the cross-project rPairs are highly likely to be general for software of the same type, the discovered overlapping rPairs can benefit other projects of the same type that have not been analyzed.",Code search | Program comprehension | Semantically related words,Empirical Software Engineering,2014-10-12,Article,"Yang, Jinqiu;Tan, Lin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891601603,10.1016/j.jss.2013.08.036,Conceptual modeling of natural language functional requirements,"Requirements analysts consider a conceptual model to be an important artifact created during the requirements analysis phase of a software development life cycle (SDLC). A conceptual, or domain model is a visual model of the requirements domain in focus. Owing to its visual nature, the model serves as a platform for the deliberation of requirements by stakeholders and enables requirements analysts to further refine the functional requirements. Conceptual models may evolve into class diagrams during the design and execution phases of the software project. Even a partially automated conceptual model can save significant time during the requirements phase, by quickening the process of graphical communication and visualization. This paper presents a system to create a conceptual model from functional specifications, written in natural language in an automated manner. Classes and relationships are automatically identified from the functional specifications. This identification is based on the analysis of the grammatical constructs of sentences, and on Object Oriented principles of design. Extended entity-relationship (EER) notations are incorporated into the class relationships. Optimizations are applied to the identified entities during a post-processing stage, and the final conceptual model is rendered. The use of typed dependencies, combined with rules to derive class relationships offers a granular approach to the extraction of the design elements in the model. The paper illustrates the model creation process using a standard case study, and concludes with an evaluation of the usefulness of this approach for the requirements analysis. The analysis is conducted against both standard published models and conceptual models created by humans, for various evaluation parameters. © 2013 Elsevier Inc. All rights reserved.",Automated requirements analysis | Conceptual modeling | Natural language processing | Syntactic analysis,Journal of Systems and Software,2014-01-08,Article,"Vidya Sagar, Vidhu Bhala R.;Abirami, S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84891634221,10.1016/j.jss.2013.10.019,Recovering test-to-code traceability using slicing and textual analysis,"Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature. © 2013 Elsevier Inc. All rights reserved.",Dynamic slicing | Information retrieval | Test-to-code traceability,Journal of Systems and Software,2014-01-01,Article,"Qusef, Abdallah;Bavota, Gabriele;Oliveto, Rocco;De Lucia, Andrea;Binkley, Dave",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900631200,10.1016/j.jss.2013.11.1096,Delta-oriented model-based integration testing of large-scale systems,"Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain. © 2013 Elsevier Inc.",Large-scale systems | Model-based testing | Regression testing | Variable software architectures,Journal of Systems and Software,2014-05-01,Article,"Lochau, Malte;Lity, Sascha;Lachmann, Remo;Schaefer, Ina;Goltz, Ursula",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84898800045,10.1016/j.jss.2013.09.012,Mobile cloud middleware,"Mobile Cloud Computing (MCC) is arising as a prominent research area that is seeking to bring the massive advantages of the cloud to the constrained smartphones. Mobile devices are looking towards cloud-aware techniques, driven by their growing interest to provide ubiquitous PC-like functionality to mobile users these functionalities mainly target at increasing storage and computational capabilities. Smartphones may integrate those functionalities from different cloud levels, in a service oriented manner within the mobile applications, so that a mobile task can be delegated by direct invocation of a service. However, developing these kind of mobile cloud applications requires to integrate and consider multiple aspects of the clouds, such as resource-intensive processing, programmatically provisioning of resources (Web APIs) and cloud intercommunication. To overcome these issues, we have developed a Mobile Cloud Middleware (MCM) framework, which addresses the issues of interoperability across multiple clouds, asynchronous delegation of mobile tasks and dynamic allocation of cloud infrastructure. MCM also fosters the integration and orchestration of mobile tasks delegated with minimal data transfer. A prototype of MCM is developed and several applications are demonstrated in different domains. To verify the scalability of MCM, load tests are also performed on the hybrid cloud resources the detailed performance analysis of the middleware framework shows that MCM improves the quality of service for mobiles and helps in maintaining soft-real time responses for mobile cloud applications. © 2013 Elsevier Inc.",Code offloading | Interoperability | Mobile Cloud Computing | Task delegation,Journal of Systems and Software,2014-01-01,Article,"Flores, Huber;Srirama, Satish Narayana",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900826199,10.1016/j.jss.2014.02.058,From AADL to Timed Abstract State Machines: A verified model transformation,"Architecture Analysis and Design Language (AADL) is an architecture description language standard for embedded real-time systems widely used in the avionics and aerospace industry to model safety-critical applications. To verify and analyze the AADL models, model transformation technologies are often used to automatically extract a formal specification suitable for analysis and verification. In this process, it remains a challenge to prove that the model transformation preserves the semantics of the initial AADL model or, at least, some of the specific properties or requirements it needs to satisfy. This paper presents a machine checked semantics-preserving transformation of a subset of AADL (including periodic threads, data port communications, mode changes, and the AADL behavior annex) into Timed Abstract State Machines (TASM). The AADL standard itself lacks at present a formal semantics to make this translation validation possible. Our contribution is to bridge this gap by providing two formal semantics for the subset of AADL. The execution semantics provided by the AADL standard is formalized as Timed Transition Systems (TTS). This formalization gives a reference expression of AADL semantics which can be compared with the TASM-based translation (for verification purpose). Finally, the verified transformation is mechanized in the theorem prover Coq. © 2014 Elsevier Inc.",Architecture Analysis and Design Language (AADL) | Coq | Model transformation | Model-driven engineering | Semantics preservation | Timed Abstract State Machine (TASM),Journal of Systems and Software,2014-01-01,Article,"Yang, Zhibin;Hu, Kai;Ma, Dianfu;Bodeveix, Jean Paul;Pi, Lei;Talpin, Jean Pierre",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900827317,10.1016/j.jss.2014.03.006,"Assessment of institutions, scholars, and contributions on agile software development (2001-2012)","The number of scholarly publications on agile software development has grown significantly in recent years. Several researchers reviewed and attempted to synthesize studies on agile software development. However, no work has ranked the contributions of scholars and institutions to publications using a thorough process. This study presents findings on top publications, institutions, and scholars in the agile software development field from 2001 to 2012 based on the publication of such works in Science Citation Index journals. This paper highlights the key outlets for agile research and summarizes the most influential researchers and institutions as well as the most studied research areas. This study concludes by providing directions for future research. © 2014 Elsevier Inc.",Agile software development | Literature assessment | Research productivity,Journal of Systems and Software,2014-01-01,Article,"Chuang, Sun Wen;Luor, Tainyi;Lu, Hsi Peng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900826122,10.1016/j.jss.2014.02.023,A high capacity data hiding scheme for binary images based on block patterns,"This paper proposes a high capacity data hiding scheme for binary images based on block patterns, which can facilitate the authentication and annotation of scanned images. The scheme proposes block patterns for a 2 × 2 block to enforce specific block-based relationship in order to embed a significant amount of data without causing noticeable artifacts. In addition, two kinds of matching pair (MP) methods, internal adjustment MP and external adjustment MP, are designed to decrease the embedding changes. Shuffling is applied before embedding to reduce the distortion and improve the security. Experimental results show that the proposed scheme gives a significantly improved embedding capacity than previous approaches in the same level of embedding distortion. We also analyze the perceptual impact and discuss the robustness and security issues. © 2014 Elsevier Inc.",Authentication | Binary image | Data hiding | Watermarking,Journal of Systems and Software,2014-01-01,Article,"Wang, Chung Chuan;Chang, Ya Fen;Chang, Chin Chen;Jan, Jinn Ke;Lin, Chia Chen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905815244,10.1016/j.jss.2014.06.035,Empirical research methodologies and studies in Requirements Engineering: How far did we come?,"Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE. © 2014 Elsevier B.V. All rights reserved.",,Journal of Systems and Software,2014-01-01,Article,"Daneva, Maya;Damian, Daniela;Marchetto, Alessandro;Pastor, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905828775,10.1016/j.jss.2013.12.042,Software product management - An industry evaluation,"Product management is a key success factor for software products as it spans the entire life-cycle and thus ensures both a technical and business perspective. With its many interfaces to various business processes and stakeholders across the life-cycle, it is a primary driver for requirements engineering in its focus on value-orientation and consistency across releases. This article provides an overview on product management in software and IT. It summarizes experiences with introducing, improving and deploying the role of a product manager. In order to get a profound industry overview we performed a field study with interviews and concrete insight across fifteen different organizations world-wide on the role of the product manager and its success factors. As a technical solution we present four success factors identified from the research and show how they address the challenges we identified in practice. The novel part of this research and technical study is the industry survey and evaluation of resulting solution proposals. We found that with increasing institutionalization of a consistent and empowered product management role, the success rate of projects in terms of schedule predictability, quality and project duration improves. © 2014 Elsevier B.V. All rights reserved.",Industry survey | Product management | Software business,Journal of Systems and Software,2014-01-01,Article,"Ebert, Christof;Brinkkemper, Sjaak",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905864992,10.1016/j.jss.2014.03.082,On the use of software design models in software development practice: An empirical investigation,"Research into software design models in general, and into the UML in particular, focuses on answering the question how design models are used, completely ignoring the question if they are used. There is an assumption in the literature that the UML is the de facto standard, and that use of design models has had a profound and substantial effect on how software is designed by virtue of models giving the ability to do model-checking, code generation, or automated test generation. However for this assumption to be true, there has to be significant use of design models in practice by developers. This paper presents the results of a survey summarizing the answers of 3785 developers answering the simple question on the extent to which design models are used before coding. We relate their use of models with (i) total years of programming experience, (ii) open or closed development, (iii) educational level, (iv) programming language used, and (v) development type. The answer to our question was that design models are not used very extensively in industry, and where they are used, the use is informal and without tool support, and the notation is often not UML. The use of models decreased with an increase in experience and increased with higher level of qualification. Overall we found that models are used primarily as a communication and collaboration mechanism where there is a need to solve problems and/or get a joint understanding of the overall design in a group. We also conclude that models are seldom updated after initially created and are usually drawn on a whiteboard or on paper. © 2014 Elsevier B.V. All rights reserved.","Empirical industrial survey | Model-driven engineering (MDD, MDE, UML) | Software design models",Journal of Systems and Software,2014-01-01,Article,"Gorschek, Tony;Tempero, Ewan;Angelis, Lefteris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84892589237,10.1016/j.infsof.2013.11.004,Model-based requirements verification method: Conclusions from two controlled experiments,"Context Requirements engineering is one of the most important and critical phases in the software development life cycle, and should be carefully performed to build high quality and reliable software. However, requirements are typically gathered through various sources and are represented in natural language (NL), making requirements engineering a difficult, fault prone, and a challenging task. Objective To ensure high-quality software, we need effective requirements verification methods that can clearly handle and address inherently ambiguous nature of NL specifications. The objective of this paper is to propose a method that can address the challenges with NL requirements verification and to evaluate our proposed method through controlled experiments. Method We propose a model-based requirements verification method, called NLtoSTD, which transforms NL requirements into a State Transition Diagram (STD) that can help to detect and to eliminate ambiguities and incompleteness. The paper describes the NLtoSTD method to detect requirement faults, thereby improving the quality of the requirements. To evaluate the NLtoSTD method, we conducted two controlled experiments at North Dakota State University in which the participants employed the NLtoSTD method and a traditional fault checklist during the inspection of requirement documents to identify the ambiguities and incompleteness of the requirements. Results Two experiment results show that the NLtoSTD method can be more effective in exposing the missing functionality and, in some cases, more ambiguous information than the fault-checklist method. Our experiments also revealed areas of improvement that benefit the method's applicability in the future. Conclusion We presented a new approach, NLtoSTD, to verify requirements documents and two controlled experiments assessing our approach. The results are promising and have motivated the refinement of the NLtoSTD method and future empirical evaluation. © 2013 Elsevier B.V. All rights reserved.",Controlled experiments | Fault checklist | Model-based verification | NLtoSTD | Requirements verification,Information and Software Technology,2014-03-01,Article,"Aceituna, Daniel;Walia, Gursimran;Do, Hyunsook;Lee, Seok Won",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84889877572,10.1016/j.infsof.2013.08.003,An integrated approach based on execution measures for the continuous improvement of business processes realized by services,"Context Organizations are rapidly adopting Business Process Management (BPM) as they focus on their business processes (BPs), seeing them to be key elements in controlling and improving the way they perform their business. Business Process Intelligence (BPI) takes as its focus the collection and analysis of information from the execution of BPs for the support of decision making, based on the discovery of improvement opportunities. Realizing BPs by services introduces an intermediate service layer that enables us to separate the specification of BPs in terms of models from the technologies implementing them, thus improving their modifiability by decoupling the model from its implementation. Objective To provide an approach for the continuous improvement of BPs, based on their realization with services and execution measurement. It comprises an improvement process to integrate the improvements into the BPs and services, an execution measurement model defining and categorizing several measures for BPs and service execution, and tool support for both. Method We carried out a systematic literature review, to collect existing proposals related to our research work. Then, in close collaboration with business experts from the Hospital General de Ciudad Real (HGCR), Spain, and following design science principles, we developed the methods and artifacts described in this paper, which were validated by means of a case study. Results We defined an improvement process extending the BP lifecycle with measurement and improvement activities, integrating an execution measurement model comprising a set of execution measures. Moreover, we developed a plug-in for the ProM framework to visualize the measurement results as a proof-of-concept prototype. The case study with the HGCR has shown its feasibility. Conclusions Our improvement vision, based on BPs realized by services and on measurement of their execution, in conjunction with a systematic approach to integrate the detected improvements, provides useful guidance to organizations. © 2013 Elsevier B.V. All rights reserved.",Business Process execution measurement | Business Process Intelligence (BPI) | Business Process Management (BPM) | Continuous Process Improvement (CPI) | ProM framework | Service Oriented Computing (SOC),Information and Software Technology,2014-02-01,Article,"Delgado, Andrea;Weber, Barbara;Ruiz, Francisco;Garcia-Rodríguez De Guzmán, Ignacio;Piattini, Mario",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84897060718,10.1016/j.infsof.2014.01.008,Knowledge-based approaches in software documentation: A systematic literature review,"Context Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design. Objective The objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches. Method We use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol. Results Sixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation. Conclusions The findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively. © 2014 Elsevier B.V. All rights reserved.",Knowledge activity | Knowledge-based approach | Software architecture design | Software documentation | Systematic literature review,Information and Software Technology,2014-01-01,Review,"Ding, Wei;Liang, Peng;Tang, Antony;Van Vliet, Hans",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-84898831935,10.1016/j.infsof.2014.01.010,Metamodeling generalization and other directed relationships in UML,"Context Generalization is a fundamental relationship in object orientation and in the UML (Unified Modeling Language). The generalization relationship is represented in the UML metamodel as a ""directed relationship"". Objective Being a directed relationship corresponds to the nature of generalization in the semantic domain of object orientation: a relationship that is directed from the subclass to the superclass. However, we claim that the particular form this relationship adopts in the metamodel is erroneous, which entails a series of inconveniencies for model manipulation tools that try to adhere to the UML specification. Moreover, we think that this error could be due to a misinterpretation of the relationships between metamodeling levels in the UML: represented reality (M0), model (M1) and metamodel (M2). This problem also affects other directed relationships: Dependency and its various subtypes, Include and Extend between use cases, and others. Method We analyze the features of the generalization relationship in various domains and how it has been metamodeled in UML. We examine the problems, both theoretical and technological, posed by the UML metamodel of generalization. We then compare it with the metamodel of other directed relationships. Results We arrive at the conclusion that the metamodel of all directed relationships could be improved. Namely, we claim that, at level M2, the metamodel should not contain any one-way meta-associations: all meta-associations should be two-way, both for practical and theoretical reasons. Conclusions The rationale for our main claim can be summarized as follows: connected graphical symbols do know each other, and the goal of a metamodel is to specify the syntactic properties of a language, ergo meta-associations must be two-way. This, of course, does not preclude at all the use of one-way associations at the user model level (M1). © 2014 Elsevier B.V. All rights reserved.",Directed relationship | Generalization | Metamodel | Model engineering | Unified Modeling Language,Information and Software Technology,2014-01-01,Article,"Génova, Gonzalo;Llorens, Juan;Fraga, Anabel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84892614680,10.1016/j.infsof.2013.12.004,"A systematic review on security in Process-Aware Information Systems - Constitution, challenges, and future directions","Context Security in Process-Aware Information Systems (PAIS) has gained increased attention in current research and practice. However, a common understanding and agreement on security is still missing. In addition, the proliferation of literature makes it cumbersome to overlook and determine state of the art and further to identify research challenges and gaps. In summary, a comprehensive and systematic overview of state of the art in research and practice in the area of security in PAIS is missing. Objective This paper investigates research on security in PAIS and aims at establishing a common understanding of terminology in this context. Further it investigates which security controls are currently applied in PAIS. Method A systematic literature review is conducted in order to classify and define security and security controls in PAIS. From initially 424 papers, we selected in total 275 publications that related to security and PAIS between 1993 and 2012. Furthermore, we analyzed and categorized the papers using a systematic mapping approach which resulted into 5 categories and 12 security controls. Results In literature, security in PAIS often centers on specific (security) aspects such as security policies, security requirements, authorization and access control mechanisms, or inter-organizational scenarios. In addition, we identified 12 security controls in the area of security concepts, authorization and access control, applications, verification, and failure handling in PAIS. Based on the results, open research challenges and gaps are identified and discussed with respect to possible solutions. Conclusion This survey provides a comprehensive review of current security practice in PAIS and shows that security in PAIS is a challenging interdisciplinary research field that assembles research methods and principles from security and PAIS. We show that state of the art provides a rich set of methods such as access control models but still several open research challenges remain. © 2013 Elsevier B.V. All rights reserved.",Business Process Management | Business process security | Process-Aware Information Systems | Security | Systematic literature review | Workflow security,Information and Software Technology,2014-01-01,Review,"Leitner, Maria;Rinderle-Ma, Stefanie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84906314016,10.1016/j.infsof.2013.12.005,"How are software defects found? the role of implicit defect detection, individual responsibility, documents, and knowledge","Context Prior research has focused heavily on explicit defect detection, such as formal testing and reviews. However, in reality, humans find software defects in various activities. Implicit defect detection activities, such as preparing a product demonstration or updating a user manual, are not designed for defect detection, yet through such activities defects are discovered. In addition, the type of documentation, and knowledge used, in defect detection is diverse. Objective To understand how defect detection is affected by the perspectives of responsibility, activity, knowledge, and document use. To provide illustrative numbers concerning the multidimensionality of defect detection in an industrial context. Method The data were collected with a survey on four software development organizations in three different companies. We designed the survey based on our prior extensive work with these companies. Results We found that among our subjects (n = 105), implicit defect detection made a higher contribution than explicit defect detection in terms of found defects, 62% vs. 38%. We show that defect detection was performed by subjects in various roles supporting the earlier reports of testing being a cross-cutting activity in software development organizations. We found a low use of test cases (18%), but a high use of other documents in software defect detection, and furthermore, we found that personal knowledge was applied as an oracle in defect detection much more often than documented oracles. Finally, we recognize that contextual factors largely affect the transferability of our results, and we provide elaborate discussion about the most important contextual factors. Furthermore, we must be cautious as the results were obtained with a survey, and come from a small number of organizations. Conclusions In this paper, we show the large impact of implicit defect detection activities in four case organizations. Implicit defect detection has a large contribution to defect detection in practice, and can be viewed as an extremely low-cost way of detecting defects. Thus, harnessing and supporting it better may increase quality without increasing costs. For example, if an employee can update the user manual, and simultaneously detect defects from the software, then the defect detection part of this activity can be seen as cost-free. Additionally, further research is needed on how diverse types of useful documentation and knowledge can be utilized in defect detection. © 2014 Elsevier B.V. All rights reserved.",Activities | Defect detection | Documents | Human factors | Industrial questionnaire study | Software testing,Information and Software Technology,2014-01-01,Article,"Mäntylä, Mika V.;Itkonen, Juha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84897106264,10.1016/j.infsof.2014.01.011,Mockup-Driven Development: Providing agile support for Model-Driven Web Engineering,"Context Agile software development approaches are currently becoming the industry standard for Web Application development. On the other hand, Model-Driven Web Engineering (MDWE) methodologies are known to improve productivity when building this kind of applications. However, current MDWE methodologies tend to ignore important aspects of Web Applications development supported by agile processes, such as constant customer feedback or early design of user interfaces. Objective In this paper we analyze the difficulties of supporting agile features in MDWE methodologies. Then, we propose an approach that eases the incorporation of well-known agile practices to MDWE. Method We propose using User Interface prototypes (usually known as mockups) as a way to start the modeling process in the context of a mixed agile-MDWE process. To assist this process, we defined a lightweight metamodel that allows modeling features over mockups, interacting with end-users and generating MDWE models. Then, we conducted a statistical evaluation of both approaches (traditional vs. mockup-based modeling). Results First we comment on how agile features can be added to MDWE processes using mockups. Then, we show by means of a quantitative study that the proposed approach is faster, less error-prone and still as complete as traditional MDWE processes. Conclusion The use of mockups to guide the MDWE process helps in the reduction of the development cycle as well as in the incorporation of agile practices in the model-driven workflow. Complete MDWE models can be built and generated by using lightweight modeling over User Interface mockups, and this process suggests being more efficient, in terms of errors and effort, than traditional modeling in MDWE. © 2014 Elsevier B.V. All rights reserved.",Agile | MDD | Mockups | User-Interface | Web Engineering,Information and Software Technology,2014-01-01,Article,"Rivero, José Matías;Grigera, Julián;Rossi, Gustavo;Robles Luna, Esteban;Montero, Francisco;Gaedke, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905089674,10.1016/j.infsof.2014.04.010,Model-driven specification and enforcement of RBAC break-glass policies for process-aware information systems,"Context In many organizational environments critical tasks exist which - in exceptional cases such as an emergency - must be performed by a subject although he/she is usually not authorized to perform these tasks. Break-glass policies have been introduced as a sophisticated exception handling mechanism to resolve such situations. They enable certain subjects to break or override the standard access control policies of an information system in a controlled manner. Objective In the context of business process modeling a number of approaches exist that allow for the formal specification and modeling of process-related access control concepts. However, corresponding support for break-glass policies is still missing. In this paper, we aim at specifying a break-glass extension for process-related role-based access control (RBAC) models. Method We use model-driven development (MDD) techniques to provide an integrated, tool-supported approach for the definition and enforcement of break-glass policies in process-aware information systems. In particular, we provide modeling support on the computation independent model (CIM) layer as well as on the platform independent model (PIM) and platform specific model (PSM) layers. Results Our approach is generic in the sense that it can be used to extend process-aware information systems or process modeling languages with support for process-related RBAC and corresponding break-glass policies. Based on the formal CIM layer metamodel, we present a UML extension on the PIM layer that allows for the integrated modeling of processes and process-related break-glass policies via extended UML Activity diagrams. We evaluated our approach in a case study on real-world processes. Moreover, we implemented our approach at the PSM layer as an extension to the BusinessActivity library and runtime engine. Conclusion Our integrated modeling approach for process-related break-glass policies allows for specifying break-glass rules in process-aware information systems. © 2014 Elsevier B.V. All rights reserved.",Access control | Business process modeling | Model-driven development | UML,Information and Software Technology,2014-01-01,Article,"Schefer-Wenzl, Sigrid;Strembeck, Mark",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905117784,10.1016/j.infsof.2014.04.015,Study of advanced separation of concerns approaches using the GoF design patterns: A quantitative and qualitative comparison,"Context Since the emergence of the aspect oriented paradigm, several studies have been conducted to test the contribution of this new paradigm compared to the object paradigm. However, in addition to this type of studies, we need also comparative studies that assess the aspect approaches mutually. The motivations of the latter include the enhancement of each aspect approach, devising hybrid approaches or merely helping developers choosing the suitable approach according to their needs. Comparing advanced separation of concerns approaches is the context of our work. Objective We aim at making an assessment of how the aspect approaches deal with crosscutting concerns. This assessment is based on quantitative attributes such as coupling and cohesion that evaluate the modularity as well as on qualitative observations. Method We selected three of well-known aspect approaches: AspectJ, JBoss AOP and CaesarJ, all the three based on Java. We conducted then, a comparative study using the GoF design patterns. In order to be fair we asked a group of Master students to achieve the implementation of all patterns with the three approaches. The use of these implementations as hypothetical benchmarks allowed us to achieve two kinds of comparison: a quantitative one based on structural and performance metrics, and qualitative one based on observations collected during the implementation phase. Results The quantitative comparison shows some advantages like the using of fewer components with AspectJ and the strong cohesion with CaesarJ and weaknesses, as the high internal coupling caused by the inner classes of CaesarJ. The qualitative comparison gives comments about the approach understandability and others qualitative concepts. Conclusion This comparison highlighted strengths and weaknesses of each approach, and provided a referential work that can help choosing the right approach during software development, enhancing aspect approaches or devising hybrid approaches that combine best features. © 2014 Elsevier B.V. All rights reserved.",Advanced separation of concerns | Aspect oriented programming | AspectJ | CaesarJ | Empirical assessment | JBoss AOP,Information and Software Technology,2014-01-01,Article,"Soumeya, Debboub;Djamel, Meslati",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84900002321,10.1016/j.infsof.2014.02.009,A CSCW Requirements Engineering CASE Tool: Development and usability evaluation,"Context CSRML Tool 2012 is a Requirements Engineering CASE Tool for the Goal-Oriented Collaborative Systems Requirements Modeling Language (CSRML). Objective This work aims at evaluating the usability of CSRML Tool 2012, thus identifying any possible usability flaw to be solved in the next releases of the application, as well as giving a general overview on how to develop a DSL tool similar to the one evaluated in this work by means of Visual Studio Modelling SDK. Method In this evaluation, which was reported by following the ISO/IEC 25062:2006 Common Industry Format for usability tests, 28 fourth-course Computer Science students took part. They were asked to carry out a series of modifications to an incomplete CSRML requirements specification. Usability was assessed by measuring the task's completion rate, the elapsed time, number of accesses to the help system of the tool and the instructor's verbal assistance. The participants' arousal and pleasantness were assessed by analyzing both facial expressions and a USE questionnaire. Results In spite of obtaining high usability levels, the test outcome revealed some usability flaws that should be addressed. Conclusion The important lessons learnt from this evaluation are also applicable to the success of other usability tests as well as to the development of new CASE tools. © 2014 Elsevier B.V. All rights reserved.",CASE tool | CSCW | CSRML | ISO/IEC 25062:2006 | Requirements engineering | Usability evaluation,Information and Software Technology,2014-01-01,Article,"Teruel, Miguel A.;Navarro, Elena;López-Jaquero, Víctor;Montero, Francisco;González, Pascual",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951839299,10.1109/APSEC.2014.93,Empirical analysis of fault-proneness in methods by focusing on their comment lines,"This paper focuses on comments described in Java programs, and conducts an empirical analysis about relationships between comments and fault-proneness in the programs. The types of comments analyzed in this paper are comments described inside a method body (inner comments), and comments followed by a method declaration (documentation comments). Although both of them play important roles in the program comprehension, they seem to be described in different purposes; The inner comments are often added to present tips about code fragments, while the documentation comments usually work as a programmer's manual. In the field of code refactoring, wellwritten inner comments are said to be related to ""code smell"" since they may cover a lack of readability in a complicated code fragment. This paper analyzes the associations of comments with the code quality from the aspect of fault-proneness, with using four popular open source products. The empirical results show that a method having inner comments tends to be 1.8 - 3.0 times likely to be faulty. The key contribution of this work is to reveal the usefulness of inner comments to point at faulty methods.",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2014-01-01,Conference Paper,"Aman, Hirohisa;Amasaki, Sousuke;Sasaki, Takashi;Kawahara, Minoru",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84912064670,10.1109/SBES.2014.15,On the extraction of cookbooks for APIs from the crowd knowledge,"Developers of reusable software elements, such as libraries, usually have the responsibility to provide comprehensive and high quality documentation to enable effective software reuse. The effective reuse of libraries depends upon the quality of the API (Application Program Interface) documentation. Well established libraries typically have comprehensive API documentation, for example in Javadocs. However, they typically lack examples and explanations, which makes the effective reuse of the library difficult. StackOverflow.com (SO) is a Question and Answer service directed to issues related to software development. On SO, developers post questions related to a programming topic and other members of the SO community can provide answers to help them solving their problems. Despite the increasing adoption of SO, the information related to a particular topic is spread across the website. Thus, SO still lacks organization of its crowd knowledge. In this paper, we present an automatic approach that organizes the information available on SO in order to build cookbooks (recipe-oriented books) for APIs. The cookbooks are meant to be used through an exploration process (browsing). In order to evaluate the proposed approach, we have generated cookbooks for three APIs widely used by the software development community: SWT, STL and LINQ. Desired properties that cookbooks must meet were identified and a study was conducted to assess to what extent the generated cookbook meet those properties.",APIs documentation | crowd knowledge | Q&amp;A services,"Proceedings - 28th Brazilian Symposium on Software Engineering, SBES 2014",2014-10-31,Conference Paper,"Souza, Lucas B.L.De;Campos, Eduardo C.;Maia, Marcelo De A.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84920507826,10.1109/SOCA.2014.14,Offline trace checking of quantitative properties of service-based applications,"Service-based applications are often developed as compositions of partner services. A service integrator needs precise methods to specify the quality attributes expected by each partner service, as well as effective techniques to verify these attributes. In previous work, we identified the most common specification patterns related to provisioning service-based applications and developed an expressive specification language (SOLOIST) that supports them. SOLOIST is an extension of metric temporal logic with aggregate temporal modalities that can be used to write quantitative temporal properties. In this paper we address the problem of performing offline checking of service execution traces against quantitative requirements specifications written in SOLOIST. We present a translation of SOLOIST into CLTLB (D), a variant of linear temporal logic, and reduce the trace checking of SOLOIST to bounded satisfiability checking of CLTLB (D), which is supported by ZOT, an SMT-based verification toolkit. We detail the results of applying the proposed offline trace checking procedure to different types of traces, and compare its performance with previous work.",aggregate operators | quantitative properties | service-based applications | temporal logic | trace checking,"Proceedings - IEEE 7th International Conference on Service-Oriented Computing and Applications, SOCA 2014",2014-12-05,Conference Paper,"Bianculli, Domenico;Ghezzi, Carlo;Krstic, Srdan;Pietro, Pierluigi San",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84936748023,10.1109/ICIS.2014.6912159,Supporting program comprehension with program summarization,"A large amount of software maintenance effort is spent on program comprehension. How to accurately and quickly get the functional features in a program becomes a hot issue in program comprehension. Some studies in this area are focused on extracting the topics by analyzing linguistic information in the source code based on the textual mining techniques. However, the extracted topics are usually composed of some standalone words and difficult to understand. In this paper, we attempt to solve this problem based on a novel program summarization technique. First, we propose to use latent semantic indexing and clustering to group source artifacts with similar vocabulary to analyze the composition of each package in the program. Then, some topics composed of a vector of independent words can be extracted based on latent semantic indexing. Finally, we employ Minipar, a nature language parser, to help generate the summaries. The summaries can effectively organize the words from the topics in the form of the predefined sentence based on some rules. With such form of summaries, developers can understand what the features the program has and their corresponding source artifacts.",,"2014 IEEE/ACIS 13th International Conference on Computer and Information Science, ICIS 2014 - Proceedings",2014-09-26,Conference Paper,"Liu, Yu;Sun, Xiaobing;Liu, Xiangyue;Li, Yun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84898443631,10.1109/CSMR-WCRE.2014.6747188,"Comparison of feature implementations across languages, technologies, and styles","We describe and validate a method for comparing programming languages or technologies or programming styles in the context of implementing certain programming tasks. To this end, we analyze a number of 'little software systems' readily implementing a common feature set. We analyze source code, structured documentation, derived metadata, and other computed data. More specifically, we compare these systems on the grounds of the NCLOC metric while delegating more advanced metrics to future work. To reason about feature implementations in such a multi-language and multi-technological setup, we rely on an infrastructure which enriches traditional software artifacts (i.e., files in a repository) with additional metadata for implemented features as well as used languages and technologies. All resources are organized and exposed according to Linked Data principles so that they are conveniently explorable; both programmatic and interactive access is possible. The relevant formats and the underlying ontology are openly accessible and documented. © 2014 IEEE.",Feature location | Linked Data | Programming languages | Programming style | Programming technologies | Reverse engineering | Software chrestomathies | Software metrics,"2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings",2014-01-01,Conference Paper,"Lammel, Ralf;Leinberger, Martin;Schmorleiz, Thomas;Varanovich, Andrei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84898401218,10.1109/CSMR-WCRE.2014.6747228,Web API growing pains: Stories from client developers and their code,"Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. Our study is complemented with a set of observations regarding best practices for web API evolution. © 2014 IEEE.",,"2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings",2014-01-01,Conference Paper,"Espinha, Tiago;Zaidman, Andy;Gross, Hans Gerhard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84910606183,10.1145/2660267.2660287,AutoCog: Measuring the description-to-permission fidelity in android applications,"The booming popularity of smartphones is partly a result of application markets where users can easily download wide range of third-party applications. However, due to the open nature of markets, especially on Android, there have been several privacy and security concerns with these applications. On Google Play, as with most other markets, users have direct access to natural-language descriptions of those applications, which give an intuitive idea of the functionality including the security-related information of those applications. Google Play also provides the permissions requested by applications to access security and privacy-sensitive APIs on the devices. Users may use such a list to evaluate the risks of using these applications. To best assist the end users, the descriptions should reflect the need for permissions, which we term description-to-permission fidelity. In this paper, we present a system AutoCog to automatically assess description-to-permission fidelity of applications. AutoCog employs state-of-the-art techniques in natural language processing and our own learning-based algorithm to relate description with permissions. In our evaluation, Auto-Cog outperforms other related work on both performance of detection and ability of generalization over various permissions by a large extent. On an evaluation of eleven permissions, we achieve an average precision of 92.6% and an average recall of 92.0%. Our large-scale measurements over 45,811 applications demonstrate the severity of the problem of low description-to-permission fidelity. AutoCog helps bridge the long-lasting usability gap between security techniques and average users.",Android | Google play | Machine learning | Mobile | Natural language processing | Permissions,Proceedings of the ACM Conference on Computer and Communications Security,2014-11-03,Conference Paper,"Qu, Zhengyang;Rastogi, Vaibhav;Zhang, Xinyi;Chen, Yan;Zhu, Tiantian;Chen, Zhong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84958544952,10.1007/978-3-319-09195-2_4,"OCLR: A more expressive, pattern-based temporal extension of OCL","Modern enterprise information systems often require to specify their functional and non-functional (e.g., Quality of Service) requirements using expressions that contain temporal constraints. Specification approaches based on temporal logics demand a certain knowledge of mathematical logic, which is difficult to find among practitioners; moreover, tool support for temporal logics is limited. On the other hand, a standard language such as the Object Constraint Language (OCL), which benefits from the availability of several industrial-strength tools, does not support temporal expressions. In this paper we propose OCLR, an extension of OCL with support for temporal constraints based on well-known property specification patterns. With respect to previous extensions, we add support for referring to a specific occurrence of an event as well as for indicating a time distance between events and/or from scope boundaries. The proposed extension defines a new syntax, very close to natural language, paving the way for a rapid adoption by practitioners. We show the application of the language in a case study in the domain of eGovernment, developed in collaboration with a public service partner. © 2014 Springer International Publishing Switzerland.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Dou, Wei;Bianculli, Domenico;Briand, Lionel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.14279/tuj.eceasst.0.902,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84958544687,10.1007/978-3-319-07626-3_37,Management of visual clutter in annotated 3D CAD Models: A Comparative Study,"The use of annotations in CAD models has been an active area of research because of their ability to connect design information to specific aspects of the model's geometry. The effectiveness of annotations is determined by the ability to clearly communicate information. However, annotations can quickly create clutter and confusion as they increase both in number and complexity. Consequently, efficient interaction and visualization mechanisms become crucial. Despite recent standardizations of procedures for the presentation of textual information in CAD models, no explicit guidelines are available as to how to make annotated models more readable and manageable. In this paper, we present the results of a comparative study of different mechanisms to manage visual clutter in annotated 3D CAD models and offer recommendations based on our findings. Our results show that even basic interaction mechanisms have a substantial impact on user's performance. © 2014 Springer International Publishing Switzerland.",annotated 3D models | CAD model interaction | design communication | visual clutter,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Camba, Jorge;Contero, Manuel;Johnson, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84897693884,10.1504/IJCCBS.2014.059593,A formal framework to specify and verify real-time properties on critical systems,"We propose a verified approach to the formal verification of timed properties using model-checking techniques. We focus on properties commonly found during the analysis of reactive systems, expressed using real-time specification patterns. We use observers in order to transform the verification of these timed patterns into the verification of simpler LTL formulas. While the use of observers for model-checking is quite common, our contribution is original in several ways. First, we define a formal framework to verify that observers are correct and non-intrusive. Second, we define different classes of observers for each pattern and use a pragmatic approach in order to select the most efficient candidate in practice. This approach is implemented in an integrated verification tool chain for the Fiacre language. © 2014 Inderscience Enterprises Ltd.",Formal verification | Model checking | Patterns | Requirement specification | Time Petri nets | TPNs,International Journal of Critical Computer-Based Systems,2014-01-01,Article,"Abid, Nouha;Zilio, Silvano Dal;Botlan, Didier Le",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2597809.2597823,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84910658332,10.1145/2661136.2661159,Metamorphic domain-specific languages: A journey into the shapes of a language,"External or internal domain-specific languages (DSLs) or (fluent) APIs? Whoever you are - a developer or a user of a DSL - you usually have to choose side; you should not! What about metamorphic DSLs that change their shape according to your needs? Our 4-years journey of providing the ""right"" support (in the domain of feature modeling), led us to develop an external DSL, different shapes of an internal API, and maintain all these languages. A key insight is that there is no one-size-fits-all solution or no clear superiority of a solution compared to another. On the contrary, we found that it does make sense to continue the maintenance of an external and internal DSL. Based on our experience and on an analysis of the DSL engineering field, the vision that we foresee for the future of software languages is their ability to be self-adaptable to the most appropriate shape (including the corresponding integrated development environment) according to a particular usage or task. We call metamorphic DSL such a language, able to change from one shape to another shape.",Domain-specific languages | Metamorphic | Programming,"Onward! 2014 - Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, Part of SPLASH 2014",2014-10-20,Conference Paper,"Acher, Mathieu;Combemale, Benoit;Collet, Philippe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84985993355,10.1145/2593801.2593805,Supporting comprehension of unfamiliar programs by modeling an expert's perception,"Developers need to understand many Software Engineering (SE) artifacts while making changes to the code. In such cases, developers use cues extensively to establish relevance of an information with the task. Their familiarity with different kind of cues will help them in comprehending a program. But, developers face information overload because (a) there are many cues and (b) they might be unfamiliar with artifacts. So, we propose a novel approach to overcome information overload problem by modeling developer's perceived value of information based on cues. In this preliminary study, we validate one such model for common comprehension tasks. We also apply this model to summarize source code. An evaluation of the generated summaries resulted in 83% similarity with summaries recorded by developers. The promising results encourages us to create a repository of perception models that can later aid complex SE tasks.",Information scent | Program comprehension | Summarization,"3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2014 - Proceedings",2014-06-03,Conference Paper,"Kulkarni, Naveen;Varma, Vasudeva",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905662429,10.1145/2554850.2555016,Activities performed by programmers while using framework examples as a guide,"It is now a common approach pursued by programmers to develop new software systems using Object-Oriented Application Frameworks such as Spring, Struts and, Eclipse. This improves the quality and the maintainability of the code. Furthermore, it reduces development cost and time. However, the main problem is that these frameworks usually have a complicated Application Programming Interface (API), and typically suffer from the lack of enough documentation and appropriate user manuals. To solve these problems, programmers often refer to existing sample applications of those frameworks to learn how to implement the desired functionality in their own code. This is called the Monkey See, Monkey Do rule in software engineering literature. The aim of this paper is to investigate and analyze the activities programmers perform to achieve a successful use of this rule. The results of this analysis will help us to build automated tools which are helpful for programmers while perusing the aforementioned Monkey See, Monkey Do rule. Copyright 2014 ACM.",Monkey Do rule | Monkey See | Object-oriented application frameworks | Program comprehension,Proceedings of the ACM Symposium on Applied Computing,2014-01-01,Conference Paper,"Boghrati, Reihane;Heydarnoori, Abbas;Kazemitabaar, Majeed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84905966732,10.1007/978-3-319-10431-7_11,Trace checking of metric temporal logic with aggregating modalities using MapReduce,"Modern complex software systems produce a large amount of execution data, often stored in logs. These logs can be analyzed using trace checking techniques to check whether the system complies with its requirements specifications. Often these specifications express quantitative properties of the system, which include timing constraints as well as higher-level constraints on the occurrences of significant events, expressed using aggregate operators. In this paper we present an algorithm that exploits the MapReduce programming model to check specifications expressed in a metric temporal logic with aggregating modalities, over large execution traces. The algorithm exploits the structure of the formula to parallelize the evaluation, with a significant gain in time. We report on the assesment of the implementation - based on the Hadoop framework - of the proposed algorithm and comment on its scalability. © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Bianculli, Domenico;Ghezzi, Carlo;Krstić, Srdan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.14722/USEC.2014.23045,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84912144446,10.1109/QSIC.2014.22,The value of software documentation quality,"This paper presents the results of a study on software documentation quality in practice. Goal of this study is identifying the current state of software documentation quality and used analysis techniques for determining software documentation quality. Moreover, we aim at finding out, whether there is a demand for a tool-based software documentation quality analysis approach. This approach consists of a documentation quality model and a document checking tool, as proposed in previous work. We developed an online survey and asked about 300 experts to answer it. The survey was completed by 88 experts and the overall results confirm the importance of software documentation quality as well as the need for better tool support. The survey shows that the most important quality attributes with regard to documentation quality are accuracy, clarity, consistency, readability, structuredness, and understand ability. Most of these quality attributes are currently covered by our software documentation quality analysis approach, some of them (e.g., accuracy, structuredness) still need more attention, i.e. better support in our quality model and tool.",automatic evaluation of documentation quality | role of quality in documentation | software documentation quality,Proceedings - International Conference on Quality Software,2014-11-14,Conference Paper,"Plosch, Reinhold;Dautovic, Andreas;Saft, Matthias",Include,
10.1016/j.jss.2022.111515,2-s2.0-84951739587,10.1109/APSEC.2014.87,API document quality for resolving deprecated APIs,"Using deprecated APIs often results in security vulnerability or performance degradation. Thus, invocations to deprecated APIs should be immediately replaced by alternative APIs. To resolve deprecated APIs, most developers rely on API documents provided by service API libraries. However, the documents often do not have sufficient information. This makes many deprecated API usages remain unresolved, which leads programs to vulnerable states. This paper reports a result of studying document quality for deprecated APIs. We first collected 260 deprecated APIs of eight Java libraries as well as the corresponding API documents. These documents were manually investigated to figure out whether it provides alternative APIs, rationales, or examples. Then, we examined 2,126 API usages in 249 client applications and figured out whether those were resolved in the subsequent versions. This study revealed that 1) 3.6 APIs was deprecated and 3.6 deprecated APIs are removed from the library a month on average, 2) only 61% of API documents provided alternative APIs while rationale and examples were rarely documented, and 3) 62% of deprecate API usages in client applications were resolved if the corresponding API documents provided alternative APIs while 49% were resolved when the documents provided no alternative APIs. Based on these results, we draw future directions to encourage resolving deprecated APIs.",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2014-01-01,Conference Paper,"Ko, Deokyoon;Ma, Kyeongwook;Park, Sooyong;Kim, Suntae;Kim, Dongsun;Traon, Yves Le",Include,
10.1016/j.jss.2022.111515,2-s2.0-84898477814,10.1109/CSMR-WCRE.2014.6747170,Towards a context-aware IDE-based meta search engine for recommendation about programming errors and exceptions,"Study shows that software developers spend about 19% of their time looking for information in the web during software development and maintenance. Traditional web search forces them to leave the working environment (e.g., IDE) and look for information in the web browser. It also does not consider the context of the problems that the developers search solutions for. The frequent switching between web browser and the IDE is both time-consuming and distracting, and the keyword-based traditional web search often does not help much in problem solving. In this paper, we propose an Eclipse IDE-based web search solution that exploits the APIs provided by three popular web search engines-Google, Yahoo, Bing and a popular programming Q & A site, StackOverflow, and captures the content-relevance, context-relevance, popularity and search engine confidence of each candidate result against the encountered programming problems. Experiments with 75 programming errors and exceptions using the proposed approach show that inclusion of different types of contextual information associated with a given exception can enhance the recommendation accuracy of a given exception. Experiments both with two existing approaches and existing web search engines confirm that our approach can perform better than them in terms of recall, mean precision and other performance measures with little computational cost. © 2014 IEEE.",API mashup | Context code | Context-based search | Cosine similarity | IDE-based search,"2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings",2014-01-01,Conference Paper,"Rahman, Mohammad Masudur;Yeasmin, Shamima;Roy, Chanchal K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84903643072,10.1145/2618168.2618192,Software documentation: A standard for the 21<sup>st</sup> century,"This paper describes a desk study in which a user manual dating from 1925, describing how to operate a table loom, was verified for conformance to an ISO standard that lists requirements for software user documentation and was last revised in 2008. A remarkably high degree of conformance was established. It is discussed whether this is coincidental, or more fundamentally related to the way in which the documentation of software tools is regarded as quantitatively rather than qualitatively different from the documentation of physical tools. The paper concludes by making a case for academic research resulting in guidelines for software documentation that take into account the specific nature of software, and the specific problems that users encounter when working with software. © 2014 ACM.",documentation behaviour | documentation design | documentation interaction | end-user documentation | manuals | user manuals,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Van Loggem, Brigit",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2694344.2694372,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2694344.2694391,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961575598,10.1109/ESEM.2015.7321188,An Empirical Study on the Patterns of Eye Movement during Summarization Tasks,"Eye movement patterns are the order in which keywords or sections of keywords are read. These patterns are an important component of how programmers read source code. One strategy for determining how programmers perform summarization tasks is through eye tracking studies. These studies examine where people focus their attention while viewing text or images. In this study, we expand on eye tracking analysis to determine the eye movement patterns of programmers. We begin the study with a qualitative exploration of the eye movement patterns used by 10 professional programmers from an earlier study. We then use what we learned qualitatively to perform a quantitative analysis of those patterns. We found that all ten of the programmers followed nearly identical eye movement patterns. These patterns were analogous to eye movement patterns of reading natural language.",Companies | Computer languages | Gaze tracking | Natural languages | Tracking,International Symposium on Empirical Software Engineering and Measurement,2015-11-05,Conference Paper,"Rodeghero, Paige;McMillan, Collin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961627557,10.1109/ESEM.2015.7321197,Empirical Analysis of Change-Proneness in Methods Having Local Variables with Long Names and Comments,"This paper focuses on the local variable names and comments that are major artifacts reflecting the programmer's preference. It conducts an empirical analysis on the usefulness of those artifacts in assessing the software quality from the perspective of change-proneness in Java methods developed in six popular open source software products. The empirical results show: (1) a method having a longer named local variable is more change-prone, and (2) the presence of comments inside the method body strengthens the suspicions to be modified after the release. The above artifacts are worthy to find methods which can survive unscathed after the release.",Data collection | Encoding | Focusing | Java | Programming | Software | Standards,International Symposium on Empirical Software Engineering and Measurement,2015-11-05,Conference Paper,"Aman, Hirohisa;Amasaki, Sousuke;Sasaki, Takashi;Kawahara, Minoru",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84961616003,10.1109/ESEM.2015.7321190,An Exploratory Study on the Evolution of C Programming in the Unix Operating System,"Context: Numerous factors drive long term progress in programming practices. Goal: We study the evolution of C programming in the Unix operating system. Method: We extract, aggregate, and synthesize metrics from 66 snapshots obtained from an artificial software configuration management repository tracking the evolution of the Unix operating system over four decades. Results: C language programming practices appear to evolve over long term periods; our study identified some continuous trends with highly significant coefficients of determination. Many trends point toward increasing code quality through adherence to numerous programming guidelines, while some others indicate adoption that has reached maturity. In the area of commenting progress appears to have stalled. Conclusions: Studying the long term evolution of programming practices identifies areas where progress has been achieved along an expected path, as well as cases where there is room for improvement.",Guidelines | Long Term Evolution | Market research | Measurement | Operating systems | Programming,International Symposium on Empirical Software Engineering and Measurement,2015-11-05,Conference Paper,"Spinellis, Diomidis;Louridas, Panagiotis;Kechagia, Maria",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84961602227,10.1109/ESEM.2015.7321195,Does Quality of Requirements Specifications Matter? Combined Results of Two Empirical Studies,"[Background] Requirements Engineering is crucial for project success, and to this end, many measures for quality assurance of the software requirements specification (SRS) have been proposed. [Goal] However, we still need an empirical understanding on the extent to which SRS are created and used in practice, as well as the degree to which the quality of an SRS matters to subsequent development activities. [Method] We studied the relevance of SRS by relying on survey research and explored the impact of quality defects in SRS by relying on a controlled experiment. [Results] Our results suggest that the relevance of SRS quality depends both on particular project characteristics and what is considered as a quality defect; for instance, the domain of safety critical systems seems to motivate for an intense usage of SRS as a means for communication whereas defects hampering the pragmatic quality do not seem to be relevant as initially thought. [Conclusion] Efficient and effective quality assurance measures must be specific for carefully characterized contexts and carefully select defect classes.",Conferences | Correlation | Documentation | Quality assurance | Requirements engineering | Safety | Stakeholders,International Symposium on Empirical Software Engineering and Measurement,2015-11-05,Conference Paper,"Mund, Jakob;Méndez Fernández, Daniel;Femmer, Henning;Eckhardt, Jonas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961647343,10.1109/ESEM.2015.7321200,How to Specify Non-Functional Requirements to Support Seamless Modeling? A Study Design and Preliminary Results,"Context: Seamless model-based development provides integrated chains of models, covering all software engineering phases. Non-functional requirements (NFRs), like reusability, further play a vital role in software and systems engineering, but are often neglected in research and practice. It is still unclear how to integrate NFRs in a seamless model-based development. Goal: Our long-term goal is to develop a theory on the specification of NFRs such that they can be integrated in seamless model-based development. Method: Our overall study design includes a multi-staged procedure to infer an empirically founded theory on specifying NFRs to support seamless modeling. In this short paper, we present the study design and provide a discussion of (i) preliminary results obtained from a sample, and (ii) current issues related to the design. Results: Our study already shows significant fields of improvement, e.g., the low agreement during the classification. However, the results indicate to interesting points; for example, many of commonly used NFR classes concern system modeling concepts in a way that shows how blurry the borders between functional and NFRs are. Conclusions: We conclude so far that our overall study design seems suitable to obtain the envisioned theory in the long run, but we could also show current issues that are worth discussing within the empirical software engineering community. The main goal of this contribution is not to present and discuss current results only, but to foster discussions on the issues related to the integration of NFRs in seamless modeling in general and, in particular, discussions on open methodological issues.",Nonfunctional Requirements | Requirements | Requirements Engineering | Seamless Modeling,International Symposium on Empirical Software Engineering and Measurement,2015-11-05,Conference Paper,"Eckhardt, Jonas;Méndez Fernández, Daniel;Vogelsang, Andreas",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84961665145,10.1109/ESEM.2015.7321202,Improving Performance and Maintainability of Object Cloning with Lazy Clones: An Empirical Evaluation,"Object cloning is demanded by the prototype design pattern, copy-on-write strategy, some graph transformations, and many other scenarios. We have been developing a static analysis tool that clones objects frequently. In that context, issues related to performance, memory usage, and code maintainability might arise. Traditional deep cloning with dynamic allocation, reflection, and serialization, have not fulfilled those requirements. Thus, we developed novel implementations of lazy cloning with dynamic proxies and aspect-oriented programming (AOP). We defined benchmarks based on real workload to quantitatively assess the benefits of each implementation. AOP was chosen since it better harmonizes performance, memory usage and code maintainability. It was 88% faster than serialization, consumed 9 times less memory than reflection, and required 25 times less modifications on source code than dynamic allocation. In summary, we believe that the results can be extrapolated to broader contexts helping developers to make evidence-based decisions when object cloning is needed.",Empirical Study | Lazy Clone | Maintainability | Performance | Software Engineering,International Symposium on Empirical Software Engineering and Measurement,2015-11-05,Conference Paper,"Cartaxo, Bruno;Borba, Paulo;Soares, Sérgio;Fugimoto, Helio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960420078,10.1145/2786805.2786824,GR(1) synthesis for LTL specification patterns,"Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Two of the main challenges in bringing reactive synthesis to software engineering practice are its very high worst-case complexity { for linear temporal logic (LTL) it is double exponential in the length of the formula, and the difficulty of writing declarative specifications using basic LTL operators. To address the first challenge, Piterman et al. have suggested the General Reactivity of Rank 1 (GR(1)) fragment of LTL, which has an efficient polynomial time symbolic synthesis algorithm. To address the second challenge, Dwyer et al. have identified 55 LTL specification patterns, which are common in industrial specifications and make writing specifications easier. In this work we show that almost all of the 55 LTL specification patterns identified by Dwyer et al. can be expressed as assumptions and guarantees in the GR(1) fragment of LTL. Specifically, we present an automated, sound and complete translation of the patterns to the GR(1) form, which effectively results in an efficient reactive synthesis procedure for any specification that is written using the patterns. We have validated the correctness of the catalog of GR(1) templates we have created. The work is implemented in our reactive synthesis environment. It provides positive, promising evidence, for the potential feasibility of using reactive synthesis in practice.",Linear temporal logic | Specification patterns | Synthesis,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Maoz, Shahar;Ringert, Jan Oliver",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960362459,10.1145/2786805.2786855,How developers search for code: A case study,"With the advent of large code repositories and sophisticated search capabilities, code search is increasingly becoming a key software development activity. In this work we shed some light into how developers search for code through a case study performed at Google, using a combination of survey and log-analysis methodologies. Our study provides insights into what developers are doing and trying to learn when performing a search, search scope, query properties, and what a search session under different contexts usually entails. Our results indicate that programmers search for code very frequently, conducting an average of five search sessions with 12 total queries each workday. The search queries are often targeted at a particular code location and programmers are typically looking for code with which they are somewhat familiar. Further, programmers are generally seeking answers to questions about how to use an API, what code does, why something is failing, or where code is located.",Code search | Developer tools | User evaluation,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Sadowski, Caitlin;Stolee, Kathryn T.;Elbaum, Sebastian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960425174,10.1145/2786805.2786877,CLOTHO: Saving programs from malformed strings and incorrect string-handling,"Software is susceptible to malformed data originating from untrusted sources. Occasionally the programming logic or constructs used are inappropriate to handle the varied constraints imposed by legal and well-formed data. Consequently, softwares may produce unexpected results or even crash. In this paper, we present CLOTHO, a novel hybrid approach that saves such softwares from crashing when failures originate from malformed strings or inappropriate handling of strings. CLOTHO statically analyses a program to identify statements that are vulnerable to failures related to associated string data. CLOTHO then generates patches that are likely to satisfy constraints on the data, and in case of failures produces program behavior which would be close to the expected. The precision of the patches is improved with the help of a dynamic analysis. We have implemented CLOTHO for the Java String API, and our evaluation based on several popular open-source libraries shows that CLOTHO generates patches that are semantically similar to the patches generated by the programmers in the later versions. Additionally, these patches are activated only when a failure is detected, and thus CLOTHO incurs no runtime overhead during normal execution, and negligible overhead in case of failures.",Automatic program repair | Program analysis | Strings,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Dhar, Aritra;Purandare, Rahul;Dhawan, Mohan;Rangaswamy, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960346581,10.1145/2786805.2786876,Rule-based extraction of goal-use case models from text,"Goal and use case modeling has been recognized as a key approach for understanding and analyzing requirements. However, in practice, goals and use cases are often buried among other content in requirements specifications documents and written in unstructured styles. It is thus a time-consuming and error-prone process to identify such goals and use cases. In addition, having them embedded in natural language documents greatly limits the possibility of formally analyzing the requirements for problems. To address these issues, we have developed a novel rule-based approach to automatically extract goal and use case models from natural language requirements documents. Our approach is able to automatically categorize goals and ensure they are properly specified. We also provide automated semantic parameterization of artifact textual specifications to promote further analysis on the extracted goal-use case models. Our approach achieves 85% precision and 82% recall rates on average for model extraction and 88% accuracy for the automated parameterization.",Extraction | Goal-use case modeling | Semantic parameterization,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Nguyen, Tuong Huan;Grundy, John;Almorsy, Mohamed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960423371,10.1145/2786805.2786827,Summarizing and measuring development activity,"Software developers pursue a wide range of activities as part of their work, and making sense of what they did in a given time frame is far from trivial as evidenced by the large number of awareness and coordination tools that have been developed in recent years. To inform tool design for making sense of the information available about a developer's activity, we conducted an empirical study with 156 GitHub users to investigate what information they would expect in a summary of development activity, how they would measure development activity, and what factors influence how such activity can be condensed into textual summaries or numbers. We found that unexpected events are as important as expected events in summaries of what a developer did, and that many developers do not believe in measuring development activity. Among the factors that influence summarization and measurement of development activity, we identified development experience and programming languages.",Development activity | Empirical study | Summarization,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Treude, Christoph;Filho, Fernando Figueira;Kulesza, Uirà",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960395429,10.1145/2786805.2786817,"TLV: Abstraction through testing, learning, and validation","A (Java) class provides a service to its clients (i.e., programs which use the class). The service must satisfy certain specifications. Different specifications might be expected at different levels of abstraction depending on the client's objective. In order to effectively contrast the class against its specifications, whether manually or automatically, one essential step is to automatically construct an abstraction of the given class at a proper level of abstraction. The abstraction should be correct (i.e., over-approximating) and accurate (i.e., with few spurious traces). We present an automatic approach, which combines testing, learning, and validation, to constructing an abstraction. Our approach is designed such that a large part of the abstraction is generated based on testing and learning so as to minimize the use of heavy-weight techniques like symbolic execution. The abstraction is generated through a process of abstraction/refinement, with no user input, and converges to a specific level of abstraction depending on the usage context. The generated abstraction is guaranteed to be correct and accurate. We have implemented the proposed approach in a toolkit named TLV and evaluated TLV with a number of benchmark programs as well as three real-world ones. The results show that TLV generates abstraction for program analysis and verification more efficiently.",Automata learning | Behavior models | Program abstraction | Symbolic execution,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Sun, Jun;Xiao, Hao;Liu, Yang;Lin, Shang Wei;Qin, Shengchao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960413032,10.1145/2786805.2786867,Witness validation and stepwise testification across software verifiers,"It is commonly understood that a verification tool should provide a counterexample to witness a specification violation. Until recently, software verifiers dumped error witnesses in proprietary formats, which are often neither human- nor machine-readable, and an exchange of witnesses between different verifiers was impossible. To close this gap in softwareverification technology, we have defined an exchange format for error witnesses that is easy to write and read by verification tools (for further processing, e.g., witness validation) and that is easy to convert into visualizations that conveniently let developers inspect an error path. To eliminate manual inspection of false alarms, we develop the notion of stepwise testification: in a first step, a verifier finds a problematic program path and, in addition to the verification result false, constructs a witness for this path; in the next step, another verifier re-verifies that the witness indeed violates the specification. This process can have more than two steps, each reducing the state space around the error path, making it easier to validate the witness in a later step. An obvious application for testification is the setting where we have two verifiers: one that is efficient but imprecise and another one that is precise but expensive. We have implemented the technique of error-witness-driven program analysis in two state-of-the-art verification tools, CPAchecker and Ultimate Automizer, and show by experimental evaluation that the approach is applicable to a large set of verification tasks.",Counterexample validation | Error witness | Model checking | Program analysis | Software verification,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Beyer, Dirk;Dangl, Matthias;Dietsch, Daniel;Heizmann, Matthias;Stahlbauer, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960395601,10.1145/2786805.2786807,FLEX Java: Language support for safe and modular approximate programming,"Energy efficiency is a primary constraint in modern systems. Approximate computing is a promising approach that trades quality of result for gains in efficiency and performance. State- of-the-art approximate programming models require extensive manual annotations on program data and operations to guarantee safe execution of approximate programs. The need for extensive manual annotations hinders the practical use of approximation techniques. This paper describes FlexJava, a small set of language extensions, that significantly reduces the annotation effort, paving the way for practical approximate programming. These extensions enable programmers to annotate approximation-tolerant method outputs. The FlexJava compiler, which is equipped with an approximation safety analysis, automatically infers the operations and data that affect these outputs and selectively marks them approximable while giving safety guarantees. The automation and the language{compiler codesign relieve programmers from manually and explicitly annotating data declarations or operations as safe to approximate. FlexJava is designed to support safety, modularity, generality, and scalability in software development. We have implemented FlexJava annotations as a Java library and we demonstrate its practicality using a wide range of Java applications and by con- ducting a user study. Compared to EnerJ, a recent approximate programming system, FlexJava provides the same energy savings with significant reduction (from 2 × to 17 ×) in the number of annotations. In our user study, programmers spend 6 × to 12 × less time annotating programs using FlexJava than when using EnerJ.",Language design | Modular approximate programming,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Park, Jongse;Esmaeilzadeh, Hadi;Zhang, Xin;Naik, Mayur;Harris, William",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960459965,10.1145/2786805.2803181,Comprehensive service matching with matchbox,"Nowadays, many service providers offer software components in the form of Software as a Service. Requesters that want to discover those services in order to use or to integrate them, need to find out which service satisfies their requirements best. For this purpose, service matching ap-proaches determine how well the specifications of provided services satisfy their requirements (including structural, behavioral, and non-functional requirements). In this paper, we describe the tool-suite MatchBox that allows the integration of existing service matchers and their combination as part of flexibly configurable matching processes. Taking requirements and service specifications as an input, MatchBox is able to execute such matching processes and deliver rich matching results. In contrast to related tools, MatchBox allows users to take into account many different kinds of requirements, while it also provides the flexibility to control the matching process in many different ways.",Framework | Fuzzy matching | Matching processes | Service discovery | Service matching | Software requirements,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Bording, Paul;Bruns, Melanie;Platenius, Marie C.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84960355874,10.1145/2786805.2803180,Matrix miner: A red pill to architect informal product descriptions in the matrix,"Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condensed view of a product line. This paper introduces MatrixMiner: A tool for automatically synthesizing product comparison matrices (PCMs) from a set of product descriptions written in natural language. MatrixMiner is capable of identifying and organizing features and values in a PCM - despite the informality and absence of structure in the textual descriptions of products. Our empirical results of products mined from BestBuy show that the synthesized PCMs exhibit numerous quantitative, comparable information. Users can exploit MatrixMiner to visualize the matrix through a Web editor and review, refine, or complement the cell values thanks to the traceability with the original product descriptions and technical specifications.",Product comparison matrices | Software product lines | Variability mining,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Nasr, Sana Ben;Bécan, Guillaume;Acher, Mathieu;Filho, João Bosco Ferreira;Baudry, Benoit;Sannier, Nicolas;Davril, Jean Marc",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84960399450,10.1145/2786805.2804432,Evaluating a formal scenario-based method for the requirements analysis in automotive software engineering,"Automotive software systems often consist of multiple reactive components that must satisfy complex and safety- critical requirements. In automotive projects, the requirements are usually documented informally and are reviewed manually; this regularly causes inconsistencies to remain hidden until the integration phase, where their repair requires costly iterations. We therefore seek methods for the early automated requirement analysis and evaluated the scenario-based specification approach based on LSCs/MSDs; it promises to support an incremental and precise specification of requirements, and offers automated analysis through scenario execution and formal realizability checking. In a case study, we used ScenarioTools to model and analyze the requirements of a software to control a high-voltage coupling for electric vehicles. Our example contained 36 requirements and assumptions that we could successfully formalize, and we could successfully find specification defects by automated realizability checking. In this paper, we report on lessons learned, tool and method extensions we have introduced, and open challenges.",Automotive software | Modal sequence diagrams | Reactive systems | Realizability | Requirements analysis,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Greenyer, Joel;Haase, Maximilian;Marhenke, Jörg;Bellmer, Rene",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84960355904,10.1145/2786805.2804436,"Requirements, architecture, and quality in a mission critical system: 12 lessons learned","Public tender processes typically start with a comprehensive specification phase, where representatives of the eventual owner of the system, usually together with a hired group of consultants, spend a considerable amount of time to determine the needs of the owner. For the company that implements the system, this setup introduces two major challenges: (1) the written down requirements can never truly describe to a person, at least to one external to the specification process, the true intent behind the requirement; (2) the vision of the future system, stemming from the original idea, will change during the specification process - over time simultaneously invalidating at least some of the requirements. This paper reflects the experiences encountered in a large-scale mission critical information system - ERICA, an information system for the emergency services in Finland - regarding design, implementation, and deployment. Based on the experiences we propose more dynamic ways of system specification, leading to simpler design, implementation, and deployment phases and finally to a better perceived quality.",Architecture | External quality | Internal quality | Process quality | Requirements | User stories | Vision,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Koski, Aapo;Mikkonen, Tommi",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84960340045,10.1145/2786805.2807558,Improving energy consumption in android apps,"Mobile applications sometimes exhibit behaviors that can be attributed to energy bugs depending on developer implementation decisions. In other words, certain design decisions that are technically ""correct"" might affect the energy performance of applications. Such choices include selection of color palettes, libraries used, API usage and task scheduling order. We study the energy consumption of Android apps using a power model based on a multi-objective approach that minimizes the energy consumption, maximizes the contrast, and minimizes the distance between the chosen colors by com- paring the new options to the original palette. In addition, the usage of unnecessary resources can also be a cause of energy bugs depending on whether or not these are implemented correctly. We present an opportunity for continuous investigation of energy bugs by analyzing components in the background during execution on Android applications. This includes a potential new taxonomy type that is not covered by state-of-the-art approaches.",Empirical study | Energy consumption | Mobile applications,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Bernal-Cárdenas, Carlos",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84960369125,10.1145/2786805.2807560,Spotting familiar code snippet structures for program comprehension,"Developers deal with the persistent problem of understanding non-trivial code snippets. To understand the given implementation, its issues, and available choices, developers will benefit from reading relevant discussions and descriptions over the web. However, there is no easy way to know the relevant natural language terms so as to reach to such descriptions from a code snippet, especially if the documentation is inadequate and if the vocabulary used in the code is not helpful for web search. We propose an approach to solve this problem using a repository of topics and associated structurally variant snippets collected from a discussion forum. In this on-going work, we take Java methods from the code samples of three Java books, match them with the repository, and associate the topics with 76.9% precision and 66.7% recall.",Structure matching | Variant repository,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Vinayakarao, Venkatesh",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84960423792,10.1145/2786805.2807562,Textual domain specific language for requirement modelling,"Requirement specification is usually done with a combination of Natural Language (NL) and informal diagrams. Modelling approaches to support requirement engineering activities have involved a combination of text and graphical models. In this work, a textual domain specific modelling notation for requirement specification is presented. How certain requirement attributes are addressed using this notation is also described.",Domain specific languages | Requirement specification,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",2015-08-30,Conference Paper,"Olajubu, Oyindamola",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2784731.2789052,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2784731.2784741,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2784764,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2784756,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2784735,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961644558,10.1109/ICSM.2015.7332451,Developing a model of loop actions by mining loop characteristics from a large code corpus,"Some high level algorithmic steps require more than one statement to implement, but are not large enough to be a method on their own. Specifically, many algorithmic steps (e.g., count, compare pairs of elements, find the maximum) are implemented as loop structures, which lack the higher level abstraction of the action being performed, and can negatively affect both human readers and automatic tools. Additionally, in a study of 14,317 projects, we found that less than 20% of loops are documented to help readers. In this paper, we present a novel automatic approach to identify the high level action implemented by a given loop. We leverage the available, large source of high-quality open source projects to mine loop characteristics and develop an action identification model. We use the model and feature vectors extracted from loop code to automatically identify the high level actions implemented by loops. We have evaluated the accuracy of the loop action identification and coverage of the model over 7159 open source programs. The results show great promise for this approach to automatically insert internal comments and provide additional higher level naming for loop actions to be used by tools such as code search.",abstraction | documentation generation | mining code patterns,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Wang, Xiaoran;Pollock, Lori;Vijay-Shanker, K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961621919,10.1109/ICSM.2015.7332465,Program specialization and verification using file format specifications,"Programs that process data that reside in files are widely used in varied domains, such as banking, healthcare, and web-traffic analysis. Precise static analysis of these programs in the context of software transformation and verification tasks is a challenging problem. Our key insight is that static analysis of file-processing programs can be made more useful if knowledge of the input file formats of these programs is made available to the analysis. We instantiate this idea to solve two practical problems- specializing the code of a program to a given 'restricted' input file format, and verifying if a program 'conforms' to a given input file format. We then discuss an implementation of our approach, and also empirical results on a set of real and realistic programs. The results are very encouraging in the terms of both scalability as well as precision of the approach.",Automata | Banking | Context | Layout | Medical services | Software | Standards,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Medicherla, Raveendra Kumar;Komondoor, Raghavan;Narendran, S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961644231,10.1109/ICSM.2015.7332471,How do developers react to API evolution? the Pharo ecosystem case,"Software engineering research now considers that no system is an island, but it is part of an ecosystem involving other systems, developers, users, hardware,... When one system (e.g., a framework) evolves, its clients often need to adapt. Client developers might need to adapt to functionalities, client systems might need to be adapted to a new API, client users might need to adapt to a new User Interface. The consequences of such changes are yet unclear, what proportion of the ecosystem might be expected to react, how long might it take for a change to diffuse in the ecosystem, do all clients react in the same way? This paper reports on an exploratory study aimed at observing API evolution and its impact on a large-scale software ecosystem, Pharo, which has about 3,600 distinct systems, more than 2,800 contributors, and six years of evolution. We analyze 118 API changes and answer research questions regarding the magnitude, duration, extension, and consistency of such changes in the ecosystem. The results of this study help to characterize the impact of API evolution in large software ecosystems, and provide the basis to better understand how such impact can be alleviated.",Association rules | Computer science | Context | Ecosystems | History | Open source software,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Hora, Andre;Robbes, Romain;Anquetil, Nicolas;Etien, Anne;Ducasse, Stephane;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961666276,10.1109/ICSM.2015.7332473,Exploring API method parameter recommendations,"A number of techniques have been developed that support method call completion. However, there has been little research on the problem of method parameter completion. In this paper, we first present a study that helps us to understand how developers complete method parameters. Based on our observations, we developed a recommendation technique, called Parc, that collects parameter usage context using a source code localness property that suggests that developers tend to collocate related code fragments. Parc uses previous code examples together with contextual and static type analysis to recommend method parameters. Evaluating our technique against the only available state-of-the-art tool using a number of subject systems and different Java libraries shows that our approach has potential. We also explore the parameter recommendation support provided by the Eclipse Java Development Tools (JDT). Finally, we discuss limitations of our proposed technique and outline future research directions.",API methods | Code completion | Method parameter recommendations | Recommendations,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Asaduzzaman, Muhammad;Roy, Chanchal K.;Monir, Samiul;Schneider, Kevin A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961642176,10.1109/ICSM.2015.7332474,How can i improve my app? Classifying user reviews for software maintenance and evolution,"App Stores, such as Google Play or the Apple Store, allow users to provide feedback on apps by posting review comments and giving star ratings. These platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps. Previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks. However, in the case of the most popular apps, the large amount of received feedback, its unstructured nature and varying quality can make the identification of useful user feedback a very challenging task. In this paper we present a taxonomy to classify app reviews into categories relevant to software maintenance and evolution, as well as an approach that merges three techniques: (1) Natural Language Processing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify app reviews into the proposed categories. We show that the combined use of these techniques allows to achieve better results (a precision of 75% and a recall of 74%) than results obtained using each technique individually (precision of 70% and a recall of 67%).",Mobile Applications | Natural Language Processing | Sentiment Analysis | Text classification | User Reviews,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Panichella, Sebastiano;Di Sorbo, Andrea;Guzman, Emitza;Visaggio, Corrado A.;Canfora, Gerardo;Gall, Harald C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961637087,10.1109/ICSM.2015.7332478,Apiwave: Keeping track of API popularity and migration,"Every day new frameworks and libraries are created and existing ones evolve. To benefit from such newer or improved APIs, client developers should update their applications. In practice, this process presents some challenges: APIs are commonly backward-incompatible (causing client applications to fail when updating) and multiple APIs are available (making it difficult to decide which one to use). To address these challenges, we propose apiwave, a tool that keeps track of API popularity and migration of major frameworks/libraries. The current version includes data about the evolution of top 650 GitHub Java projects, from which 320K APIs were extracted. We also report an experience using apiwave on real-world scenarios.",Data mining | Databases | Java | Libraries | Market research | Software maintenance,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Hora, Andre;Valente, Marco Tulio",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84961574818,10.1109/ICSM.2015.7332481,PARC: Recommending API methods parameters,"APIs have grown considerably in size. To free developers from remembering every detail of an API, code completion has become an integral part of modern IDEs. Most work on code completion targets completing API method calls and leaves the task of completing method parameters to the developers. However, parameter completion is also a non-trivial task. We present an Eclipse plugin, called PARC, that supports automatic completion of API method parameters. The tool is based on the localness property of source code, which states that developers tend to put related code fragments close together. PARC combines contextual and static type analysis to support a wide range of parameter expression types.",API methods | Code completion | Eclipse plugin | Method parameter,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Asaduzzaman, Muhammad;Roy, Chanchal K.;Schneider, Kevin A.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84961669764,10.1109/ICSM.2015.7332486,How developers detect and fix performance bottlenecks in Android apps,"Performance of rapidly evolving mobile apps is one of the top concerns for users and developers nowadays. Despite the efforts of researchers and mobile API designers to provide developers with guidelines and best practices for improving the performance of mobile apps, performance bottlenecks are still a significant and frequent complaint that impacts the ratings and apps' chances for success. However, little research has been done into understanding actual developers' practices for detecting and fixing performance bottlenecks in mobile apps. In this paper, we present the results of an empirical study aimed at studying and understanding these practices by surveying 485 open source Android app and library developers, and manually analyzing performance bugs and fixes in their app repositories hosted on GitHub. The paper categorizes actual practices and tools used by real developers while dealing with performance issues. In general, our findings indicate that developers heavily rely on user reviews and manual execution of the apps for detecting performance bugs. While developers also use available tools to detect performance bottlenecks, these tools are mostly for profiling and do not help in detecting and fixing performance issues automatically.",Android | Bottlenecks | Developers | Performance,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Linares-Vasquez, Mario;Vendome, Christopher;Luo, Qi;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961654580,10.1109/ICSM.2015.7332491,Is this code written in English? A study of the natural language of comments and identifiers in practice,"Comments and identifiers are the main source of documentation of source-code and are therefore an integral part of the development and the maintenance of a program. As English is the world language, most comments and identifiers are written in English. However, if they are in any other language, a developer without knowledge of this language will almost perceive the code to be undocumented or even obfuscated. In absence of industrial data, academia is not aware of the extent of the problem of non-English comments and identifiers in practice. In this paper, we propose an approach for the language identification of source-code comments and identifiers. With the approach, a large-scale study has been conducted of the natural language of source-code comments and identifiers, analyzing multiple open-source and industry systems. The results show that a significant amount of the industry projects contain comments and identifiers in more than one language, whereas none of the analyzed open-source systems has this problem.",Documentation | Industries | Maintenance engineering | Natural languages | Open source software | Programming | Radiation detectors,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Pawelka, Timo;Juergens, Elmar",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84961566868,10.1109/ICSM.2015.7332493,"Experiences from performing software quality evaluations via combining benchmark-based metrics analysis, software visualization, and expert assessment","Software quality assessments are critical in organizations where the software has been produced by external vendors, or when the development and maintenance of a software product has been outsourced to external parties. These assessments are typically challenging because is not always possible to access the original developers (or sometimes is not even allowed), and in rare cases suppliers keep an account of the costs associated to code changes or defect fixes. In those situations, one is left with the artifacts (e.g., database, source code, and documentation) as the only sources of evidence for performing such evaluations. A major challenge is also to provide fact-based conclusions for supporting decision-making, instead of subjective interpretations based on expert assessments (an approach still very predominant in mainstream industrial practice). This paper describes an instance of a software quality evaluation process performed for an international logistics company, which combined: benchmark-based metrics threshold analysis, software visualization, and expert assessment. An interview was carried out afterwards with a member from the business division of the company, to assess the usefulness of the methodology and corresponding findings, and to explore avenues for future improvement.",Benchmark based assessments | COTS analysis | Expert assessment | Metrics threshold analysis | Software quality evaluations | Software visualization | Source code analysis,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Yamashita, Aiko",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84961634164,10.1109/ICSM.2015.7332495,An empirical evaluation of the effectiveness of inspection scenarios developed from a defect repository,"Abstracting and summarizing high-severity defects detected during inspections of previous software versions could lead to effective inspection scenarios in a subsequent version in software maintenance and evolution. We conducted an empirical evaluation of 456 defects detected from the requirement specification inspections conducted during the development of industrial software. The defects were collected from an earlier version, which included 59 high-severity defects, and from a later version, which included 48 high-severity defects. The results of the evaluation showed that nine defect types and their corresponding inspection scenarios were obtained by abstracting and summarizing 45 defects in the earlier version. The results of the evaluation also showed that 46 of the high-severity defects in the later version could be potentially detected using the obtained inspection scenarios. The study also investigated which inspection scenarios can be obtained by the checklist proposed in the value-based review (VBR). It was difficult to obtain five of the inspection scenarios using the VBR checklist. Furthermore, to investigate the effectiveness of cluster analysis for inspection scenario development, the 59 high-severity defects in the earlier version were clustered into similar defect groups by a clustering algorithm. The results indicated that cluster analysis can be a guide for selecting similar defects and help in the tasks of abstracting and summarizing defects.",defect abstraction | prioritizing inspection scenarios | Software inspection,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Kasubuchi, Kiyotaka;Morisaki, Shuji;Yoshida, Akiko;Ogawa, Chikako",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961642746,10.1109/ICSM.2015.7332500,Reverse engineering a visual age application,"This paper is an industrial case study of how a VisualAge application system on an IBM mainframe was reverse engineered into a system reference repository. The starting point was the code fragments generated by the VisualAge interactive development tool. The results of the reverse engineering process were a use case documentation, a module documentation and a system reference repository. In these documents, the names of the data and functions were extended to be more understandable. The process was in the end fully automated and took three months to implement. The resulting documentation is now being used as a basis for re-implementing the system in Java.",Data Dictionary | Post Documentation | Pseudo Code | Re-implementing Code | Renaming variables | Repository | Reverse Engineering | Use cases | VisualAge,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Sneed, Harry M.;Verhoef, Chris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961625694,10.1109/ICSM.2015.7332514,Using stereotypes in the automatic generation of natural language summaries for C++ methods,"An approach to automatically generate natural language documentation summaries for C++ methods is presented. The approach uses prior work by the authors on stereotyping methods along with the source code analysis framework srcML. First, each method is automatically assigned a stereotype(s) based on static analysis and a set of heuristics. Then, the approach uses the stereotype information, static analysis, and predefined templates to generate a natural-language summary for each method. This summary is automatically added to the code base as a comment for each method. The predefined templates are designed to produce a generic summary for specific method stereotypes. Static analysis is used to extract internal details about the method (e.g., parameters, local variables, calls, etc.). This information is used to specialize the generated summaries.",method stereotypes | program comprehension | source-code summarization | static analysis,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Abid, Nahla J.;Dragan, Natalia;Collard, Michael L.;Maletic, Jonathan I.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84961644822,10.1109/ICSM.2015.7332515,Keecle: Mining key architecturally relevant classes using dynamic analysis,"Reconstructing architectural components from existing software applications is an important task during the software maintenance cycle because either those elements do not exist or are outdated. Reverse engineering techniques are used to reduce the effort demanded during the reconstruction. Unfortunately, there is no widely accepted technique to retrieve software components from source code. Moreover, in several architectural descriptions of systems, a set of architecturally relevant classes are used to represent the set of architectural components. Based on this fact, we propose Keecle, a novel dynamic analysis approach for the detection of such classes from execution traces in a semi-automatic manner. Several mechanisms are applied to reduce the size of traces, and finally the reduced set of key classes is identified using Naïve Bayes classification. We evaluated the approach with two open source systems, in order to assess if the encountered classes map to the actual architectural classes defined in the documentation of those respective systems. The results were analyzed in terms of precision and recall, and suggest that the proposed approach is effective for revealing key classes that conceptualize architectural components, outperforming a state-of-the-art approach.",Accuracy | Computer architecture | Documentation | Software | Software architecture | Training | Vegetation,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Do Nascimento Vale, Liliane;De Maia, A. Marcelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961675248,10.1109/ICSM.2015.7332519,Supporting newcomers in software development projects,"The recent and fast expansion of OSS (Open-source software) communities has fostered research on how open source projects evolve and how their communities interact. Several research studies show that the inflow of new developers plays an important role in the longevity and the success of OSS projects. Beside that they also discovered that an high percentage of newcomers tend to leave the project because of the socio-technical barriers they meet when they join the project. However, such research effort did not generate yet concrete results in support retention and training of project newcomers. In this thesis dissertation we investigated problems arising when newcomers join software projects, and possible solutions to support them. Specifically, we studied (i) how newcomers behave during development activities and how they interact with others developers with the aim at (ii) developing tools and/or techniques for supporting them during the integration in the development team. Thus, among the various recommenders, we defined (i) a tool able to suggest appropriate mentors to newcomers during the training stage; then, with the aim at supporting newcomers during program comprehension we defined other two recommenders: A tool that (ii) generates high quality source code summaries and another tool able to (iii) provide descriptions of specific source code elements. For future work, we plan to improve the proposed recommenders and to integrate other kind of recommenders to better support newcomers in OSS projects.",Collaboration | Concrete | Documentation | Maintenance engineering | Mentoring | Software | Training,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",2015-11-19,Conference Paper,"Panichella, Sebastiano",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ISSRE.2015.7381796,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ISSRE.2015.7381797,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ISSRE.2015.7381811,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ISSRE.2015.7381837,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ISSRE.2015.7381840,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84964260921,10.1109/MiSE.2015.15,Towards executing dynamically updating finite-state controllers on a robot system,"Modern software systems are increasingly required to run for a long time and deliver uninterrupted service. Their requirements or their environments, however, may change. Therefore, these systems must be updated dynamically, at runtime. Typical examples can be found in manufacturing, transportation, or space applications, where stopping the system to deploy updates can be difficult, costly, or simply not possible. In previous work we proposed a model-driven approach that uses automatically synthesized finite-state controllers from scenariobased assume/guarantee specifications to safely and efficiently dynamically update the system. In this paper we describe an execution infrastructure of this approach, which allows us to execute and deploy newly synthesized dynamically updating controllers on embedded devices. We present a prototype implementation in Java for Lego Mindstorms robots. This experience gained can lead to a systematic approach to implement dynamic updates in the aforementioned critical software-intensive systems.",,"Proceedings - 7th International Workshop on Modeling in Software Engineering, MiSE 2015",2015-01-01,Conference Paper,"La Manna, Valerio Panzica;Greenyer, Joel;Clun, Donato;Ghezzi, Carlo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/MSR.2015.80,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957073517,10.1109/MSR.2015.12,Co-evolution of infrastructure and source code - An empirical study,"Infrastructure-as-code automates the process of configuring and setting up the environment (e.g., servers, VMs and databases) in which a software system will be tested and/or deployed, through textual specification files in a language like Puppet or Chef. Since the environment is instantiated automatically by the infrastructure languages' tools, no manual intervention is necessary apart from maintaining the infrastructure specification files. The amount of work involved with such maintenance, as well as the size and complexity of infrastructure specification files, have not yet been studied empirically. Through an empirical study of the version control system of 265 Open Stack projects, we find that infrastructure files are large and churn frequently, which could indicate a potential of introducing bugs. Furthermore, we found that the infrastructure code files are coupled tightly with the other files in a project, especially test files, which implies that testers often need to change infrastructure specifications when making changes to the test framework and tests.",Association rules | Cloud computing | Couplings | Maintenance engineering | Measurement | Servers,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Jiang, Yujuan;Adams, Bram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957054924,10.1109/MSR.2015.26,Ecosystems in GitHub and a method for ecosystem identification using reference coupling,"Software projects are not developed in isolation. Recent research has shifted to studying software ecosystems, communities of projects that depend on each other and are developed together. However, identifying technical dependencies at the ecosystem level can be challenging. In this paper, we propose a new method, known as reference coupling, for detecting technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We use our method to identify ecosystems in GitHub-hosted projects, and we identify several characteristics of the identified ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. The predominant type of ecosystems are those that develop tools to support software development. We also found that the project owners' social behaviour aligns well with the technical dependencies within the ecosystem, but project contributors' social behaviour does not align with these dependencies. We conclude with a discussion on future research that is enabled by our reference coupling method.",Cross-reference | Ecosystems | GitHub | Reference Coupling | Technical Dependencies,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Blincoe, Kelly;Harrison, Francis;Damian, Daniela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957101215,10.1109/MSR.2015.27,A historical analysis of Debian package incompatibilities,"Users and developers of software distributions are often confronted with installation problems due to conflicting packages. A prototypical example of this are the Linux distributions such as Debian. Conflicts between packages have been studied under different points of view in the literature, in particular for the Debian operating system, but little is known about how these package conflicts evolve over time. This article presents an extensive analysis of the evolution of package incompatibilities, spanning a decade of the life of the Debian stable and testing distributions for its most popular architecture, i386. Using the technique of survival analysis, this empirical study sheds some light on the origin and evolution of package incompatibilities, and provides the basis for building indicators that may be used to improve the quality of package-based distributions.",Analysis | Conflict | Debian | Dependency | Distribution | Empirical | Evolution | Maintenance | Package | Software,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Claes, Maelick;Mens, Tom;Di Cosmo, Roberto;Vouillon, Jérôme",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957065140,10.1109/MSR.2015.28,Recommending posts concerning API issues in developer Q&A sites,"API design is known to be a challenging craft, as API designers must balance their elegant ideals against 'real-world' concerns, such as utility, performance, backwards compatibility, and unforeseen emergent uses. However, to date, there is no principled method to collect or analyze API usability information that incorporates input from typical developers. In practice, developers often turn to Q&A websites such as stackoverflow.com (SO) when seeking expert advice on API use, the popularity of such sites has thus led to a very large volume of unstructured information that can be searched with diligence for answers to specific questions. The collected wisdom within such sites could, in principle, be of great help to API designers to better support developer needs, if only it could be collected, analyzed, and distilled for practical use. In this paper, we present a methodology that combines several techniques, including social network analysis and topic mining, to recommend SO posts that are likely to concern API design-related issues. To establish a comparison baseline, we introduce two more recommendation approaches: a reputation-based recommender and a random recommender. We have found that when applied to Q&A discussion of two popular mobile platforms, Android and iOS, our methodology achieves up to 93% accuracy and is more stable with its recommendations when compared to the two baseline techniques.",API usability | Application program interfaces | Online Q&A | Recommendation systems | Software ecosystems | Stackoverflow,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Wang, Wei;Malik, Haroon;Godfrey, Michael W.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957073802,10.1109/MSR.2015.29,An empirical study of architectural change in open-source software systems,"From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by conducting an empirical study of changes found in software architectures spanning several hundred versions of 14 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system's architecture during maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.",Architectural change | Architecture recovery | Open-source systems | Software architecture | Software evolution,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Le, Duc Minh;Behnamghader, Pooyan;Garcia, Joshua;Link, Daniel;Shahbazian, Arman;Medvidovic, Nenad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957107498,10.1109/MSR.2015.30,A study on the role of software architecture in the evolution and quality of software,"Conventional wisdom suggests that a software system's architecture has a significant impact on its evolution. Prior research has studied the evolution of software using the information of how its files have changed together in their revision history. No prior study, however, has investigated the impact of architecture on the evolution of software from its change history. This is mainly because most open-source software systems do not document their architectures. We have overcome this challenge using several architecture recovery techniques. We used the recovered models to examine if co-changes spanning multiple architecture modules are more likely to introduce bugs than co-changes that are within modules. The results show that the co-changes that cross architectural module boundaries are more correlated with defects than co-changes within modules, implying that, to improve accuracy, bug predictors should also take the software architecture of the system into consideration.",Defects | Software Architecture | Software Repositories,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Kouroshfar, Ehsan;Mirakhorli, Mehdi;Bagheri, Hamid;Xiao, Lu;Malek, Sam;Cai, Yuanfang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957047929,10.1109/MSR.2015.35,Are bullies more productive? Empirical study of affectiveness vs. issue fixing time,"Human Affectiveness, i.e., The emotional state of a person, plays a crucial role in many domains where it can make or break a team's ability to produce successful products. Software development is a collaborative activity as well, yet there is little information on how affectiveness impacts software productivity. As a first measure of this impact, this paper analyzes the relation between sentiment, emotions and politeness of developers in more than 560K Jira comments with the time to fix a Jira issue. We found that the happier developers are (expressing emotions such as JOY and LOVE in their comments), the shorter the issue fixing time is likely to be. In contrast, negative emotions such as SADNESS, are linked with longer issue fixing time. Politeness plays a more complex role and we empirically analyze its impact on developers' productivity.",Electronic publishing | Information services | Logistics | Measurement | Software | Software engineering | Training,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Ortu, Marco;Adams, Bram;Destefanis, Giuseppe;Tourani, Parastou;Marchesi, Michele;Tonelli, Roberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84957097555,10.1109/MSR.2015.37,A method to detect license inconsistencies in large-scale open source projects,"The reuse of free and open source software (FOSS) components is becoming more and more popular. They usually contain one or more software licenses describing the requirements and conditions which should be followed when been reused. Licenses are usually written in the header of source code files as program comments. Removing or modifying the license header by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. But to the best of our knowledge, no research has been devoted to investigate such kind of license infringements nor license inconsistencies. In this paper, we describe and categorize different types of license inconsistencies and propose a feasible method to detect them. Then we apply this method to Debian 7.5 and present the license inconsistencies found in it. With a manual analysis, we summarized various reasons behind these license inconsistencies, some of which imply license infringement and require the attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.",Electronic mail | Libraries | Licenses | Measurement | Open source software | Software reusability,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Wu, Yuhao;Manabe, Yuki;Kanda, Tetsuya;German, Daniel M.;Inoue, Katsuro",Include,
10.1016/j.jss.2022.111515,2-s2.0-84957103016,10.1109/MSR.2015.49,Summarizing complex development artifacts by mining heterogeneous data,"Summarization is hailed as a promising approach to reduce the amount of information that must be taken in by the person who wants to understand development artifacts, such as pieces of code, bug reports, emails, etc. However, existing approaches treat artifacts as pure textual entities, disregarding the heterogeneous and partially structured nature of most artifacts, which contain intertwined pieces of distinct type, such as source code, diffs, stack traces, human language, etc. We present a novel approach to augment existing summarization techniques (such as LexRank) to deal with the heterogeneous and multidimensional nature of complex artifacts. Our preliminary results on heterogeneous artifacts suggest our approach outperforms the current text-based approaches.",Holistic | Stack overfliow | Summarization,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Ponzanelli, Luca;Mocci, Andrea;Lanza, Michele",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957102495,10.1109/MSR.2015.53,Going green: An exploratory analysis of energy-related questions,"The popularity of smartphones - small computers that run on battery power - has exploded in the last decade. Unsurprisingly, power consumption is an overarching concern for mobile app developers, who are anxious to learn about power-related problems that are encountered by others. In this paper, we present an empirical study exploring the characteristics of energy-related questions posed in Stack Overflow, issues faced by the developers, and the most significantly discussed APIs. We extracted a sample of 5009 Stack Overflow questions, and manually analyzed 1000 posts of Android-related energy questions. Our study shows that developers are most concerned about energy-related issues that concern improper implementations, sensor, and radio utilization.",API | Energy | Power | StackOverflow,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Malik, Haroon;Zhao, Peng;Godfrey, Michael",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957079864,10.1109/MSR.2015.54,Mining StackOverflow to filter out off-topic IRC discussion,"Internet Relay Chat (IRC) is a commonly used tool by Open Source developers. Developers use IRC channels to discuss programming related problems, but much of the discussion is irrelevant and off-topic. Essentially if we treat IRC discussions like email messages, and apply spam filtering, we can try to filter out the spam (the off-topic discussions) from the ham (the programming discussions). Yet we need labelled data that unfortunately takes time to curate. To avoid costly cur ration in order to filter out off-topic discussions, we need positive and negative data-sources. On-line discussion forums, such as Stack Overflow, are very effective for solving programming problems. By engaging in open-data, Stack Overflow data becomes a powerful source of labelled text regarding programming. This work shows that we can train classifiers using Stack Overflow posts as positive examples of on-topic programming discussion. You Tube video comments, notorious for their lack of quality, serve as training set of off-topic discussion. By exploiting these datasets, accurate classifiers can be built, tested and evaluated that require very little effort for end-users to deploy and exploit.",IRC message filtering | Naive Bayes | Stackoverflow mining | SVM | Text classification | YouTube video comments,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Chowdhury, Shaiful Alam;Hindle, Abram",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957047799,10.1109/MSR.2015.56,Mining successful answers in stack overflow,"Recent research has shown that drivers of success in online question answering encompass presentation quality as well as temporal and social aspects. Yet, we argue that also the emotional style of a technical contribution influences its perceived quality. In this paper, we investigate how Stack Overflow users can increase the chance of getting their answer accepted. We focus on actionable factors that can be acted upon by users when writing an answer and making comments. We found evidence that factors related to information presentation, time and affect all have an impact on the success of answers.",Human Factors | Knowledge Sharing | Online Q&A | Sentiment Analysis,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Calefato, Fabio;Lanubile, Filippo;Marasciulo, Maria Concetta;Novielli, Nicole",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957099916,10.1109/MSR.2015.58,Intuition vs. truth: Evaluation of common myths about StackOverflow posts,"Posting and answering questions on Stack Overflow (SO) is everyday business for many developers. We asked a group of developers what they expect to be true about questions and answers on SO. Most of their expectations were related to the likelihood of getting an answer or to voting behavior. From their comments, we formulated nine myths that they think are true about the platform. Then, we proceeded to use rather simple methods from statistics to check if these myths are supported by the data in the SO dump provided. Through our analysis, we determined that there is an effect for eight of the nine myths the developers believed in. However, for only four of the myths the effect size is large enough to actually make a difference. Hence, we could bust five myths the developers believed in.",Myths | Posting Behavior | StackOverflow,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Honsel, Verena;Herbold, Steffen;Grabowski, Jens",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957065977,10.1109/MSR.2015.67,StORMeD: Stack overflow ready made data,"Stack Overflow is the de facto Question and Answer (Q&A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion.",H-ast | Island parsing | Unstructured data,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Ponzanelli, Luca;Mocci, Andrea;Lanza, Michele",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957075581,10.1109/MSR.2015.73,The Firefox temporal defect dataset,"The bug tracking repositories of software projects capture initial defect (bug) reports and the history of interactions among developers, testers, and customers. Extracting and mining information from these repositories is time consuming and daunting. Researchers have focused mostly on analyzing the frequency of the occurrence of defects and their attributes (e.g., The number of comments and lines of code changed, count of developers). However, the counting process eliminates information about the temporal alignment of events leading to changes in the attributes count. Software quality teams could plan and prioritize their work more efficiently if they were aware of these temporal sequences and knew their frequency of occurrence. In this paper, we introduce a novel dataset mined from the Fire fox bug repository (Bugzilla) which contains information about the temporal alignment of developer interactions. Our dataset covers eight years of data from the Fire fox project on activities throughout the project's lifecycle. Some of these activities have not been reported in frequency-based or other temporal datasets. The dataset we mined from the Fire fox project contains new activities, such as reporter experience, file exchange events, code-review process activities, and setting of milestones. We believe that this new dataset will improve analysis of bug reports and enable mining of temporal relationships so that practitioners can enhance their bug-fixing process.",Bug reports | Bug repositories | Dataset | Defect tracking | Temporal activities,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Habayeb, Mayy;Miranskyy, Andriy;Murtaza, Syed Shariyar;Buchanan, Leotis;Bener, Ayse",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84957054673,10.1109/MSR.2015.75,A dataset for API usage,"An Application Programming Interface (API) provides a specific set of functionalities to a developer. The main aim of an API is to encourage the reuse of already existing functionality. There has been some work done into API popularity trends, API evolution and API usage. For all the aforementioned research avenues there has been a need to mine the usage of an API in order to perform any kind of analysis. Each one of the approaches that has been employed in the past involved a certain degree of inaccuracy as there was no type check that takes place. We introduce an approach that takes type information into account while mining API method invocations and annotation usages. This approach accurately makes a connection between a method invocation and the class of the API to which the method belongs to. We try collecting as many usages of an API as possible, this is achieved by targeting projects hosted on GitHub. Additionally, we look at the history of every project to collect the usage of an API from earliest version onwards. By making such a large and rich dataset public, we hope to stimulate some more research in the field of APIs with the aid of accurate API usage samples.",API usage | Dataset | GitHub,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Sawant, Anand Ashok;Bacchelli, Alberto",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2814270.2814323,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2814270.2814291,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2814270.2814316,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2814270.2814279,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2814270.2814274,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2814270.2814313,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2814270.2814312,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2737924.2737964,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2737924.2737967,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2737924.2737957,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2737924.2737984,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2737924.2737993,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2676997,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2677009,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2676971,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84939531690,10.1145/2676726.2676964,From communicating machines to graphical choreographies,"Graphical choreographies, or global graphs, are general multiparty session specifications featuring expressive constructs such as forking, merging, and joining for representing application-level protocols. Global graphs can be directly translated into modelling notations such as BPMN and UML. This paper presents an algorithm whereby a global graph can be constructed from asynchronous interactions represented by communicating finite-state machines (CFSMs). Our results include: A sound and complete characterisation of a subset of safe CFSMs from which global graphs can be constructed; an algorithm to translate CFSMs to global graphs; a time complexity analysis; and an implementation of our theory, as well as an experimental evaluation.",Choreography | Communicating finite-state machines | Global graphs | Multiparty session types | Theory of regions,Conference Record of the Annual ACM Symposium on Principles of Programming Languages,2015-01-01,Conference Paper,"Lange, Julien;Tuosto, Emilio;Yoshida, Nobuko",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2676991,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2676974,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2677008,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2676977,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2677003,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2676726.2677006,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963542549,10.1109/SCAM.2015.7335401,Using changeset descriptions as a data source to assist feature location,"Feature location attempts to assist developers in discovering functionality in source code. Many textual feature location techniques utilize information retrieval and rely on comments and identifiers of source code to describe software entities. An interesting alternative would be to employ the changeset descriptions of the code altered in that changeset as a data source to describe such software entities. To investigate this we implement a technique utilizing changeset descriptions and conduct an empirical study to observe this technique's overall performance. Moreover, we study how the granularity (i.e. file or method level of software entities) and changeset range inclusion (i.e. most recent or all historical changesets) affect such an approach. The results of a preliminary study with Rhino and Mylyn. Tasks systems suggest that the approach could lead to a potentially efficient feature location technique. They also suggest that it is advantageous in terms of the effort to configure the technique at method level granularity and that older changesets from older systems may reduce the effectiveness of the technique.",Feature location | information retrieval | software repositories,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",2015-11-20,Conference Paper,"Chochlov, Muslim;English, Michael;Buckley, Jim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963620738,10.1109/SCAM.2015.7335404,Recommending insightful comments for source code using crowdsourced knowledge,"Recently, automatic code comment generation is proposed to facilitate program comprehension. Existing code comment generation techniques focus on describing the functionality of the source code. However, there are other aspects such as insights about quality or issues of the code, which are overlooked by earlier approaches. In this paper, we describe a mining approach that recommends insightful comments about the quality, deficiencies or scopes for further improvement of the source code. First, we conduct an exploratory study that motivates crowdsourced knowledge from Stack Overflow discussions as a potential resource for source code comment recommendation. Second, based on the findings from the exploratory study, we propose a heuristic-based technique for mining insightful comments from Stack Overflow Q & A site for source code comment recommendation. Experiments with 292 Stack Overflow code segments and 5,039 discussion comments show that our approach has a promising recall of 85.42%. We also conducted a complementary user study which confirms the accuracy and usefulness of the recommended comments.",code examples | code insight | comment recommendation | program analysis | Stack Overflow,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",2015-11-20,Conference Paper,"Rahman, Mohammad Masudur;Roy, Chanchal K.;Keivanloo, Iman",Include,
10.1016/j.jss.2022.111515,2-s2.0-84963585161,10.1109/SCAM.2015.7335405,Checking C++ codes for compatibility with operator overloading,"Operator overloading allows the semantic extension of existing code without the need for sweeping code changes. For example, automatic differentiation tools in C++ commonly use this feature to enhance the code with additional derivative computation. To this end, a floating point data type is changed to a complex user-defined type. While conceptually straightforward, this type change often leads to compilation errors that can be tedious to decipher and resolve. This is due to the fact that the built-in floating point types in C++ are treated differently than user-defined types, and code constructs that are legal for floating point types can be a violation of the C++ standard for complex user-defined types. We identify and classify such problematic code constructs and suggest how the code can be changed to avoid these errors, while still allowing the use of operator overloading. To automatically flag such occurrences, we developed a Clang-based tool for the static analysis of C++ code based on our assessment of constructs problematic in operator overloading for numeric types. It automatically finds instances of problematic code locations and prints Lint-like warning messages. To showcase the relevance of this topic and the usefulness of our tool, we consider the basic routines of the OpenFOAM CFD software package, consisting of 1,476 C++ source and header files, for a total of over 150,000 lines of code. Altogether, we found 74 distinct occurrences of problematic code constructs in 21 files. As some of these files are included in over 400 different locations in the OpenFOAM base, errors in these files create a torrent of error messages that often are difficult to comprehend. In summary, the classification of problematic instances aids developers in writing numerical code that is fit for operator overloading and the tool helps programmers that augment legacy code in spotting problematic code constructs.",,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",2015-11-20,Conference Paper,"Huck, Alexander;Bischof, Christian;Utke, Jean",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963610946,10.1109/SCAM.2015.7335413,When code smells twice as much: Metric-based detection of variability-aware code smells,"Code smells are established, widely used characterizations of shortcomings in design and implementation of software systems. As such, they have been subject to intensive research regarding their detection and impact on understandability and changeability of source code. However, current methods do not support highly configurable software systems, that is, systems that can be customized to fit a wide range of requirements or platforms. Such systems commonly owe their configurability to conditional compilation based on C preprocessor annotations (a. k. a. #ifdefs). Since annotations directly interact with the host language (e. g., C), they may have adverse effects on understandability and changeability of source code, referred to as variability-aware code smells. In this paper, we propose a metric-based method that integrates source code and C preprocessor annotations to detect such smells. We evaluate our method for one specific smell on five open-source systems of medium size, thus, demonstrating its general applicability. Moreover, we manually reviewed 100 instances of the smell and provide a qualitative analysis of its potential impact as well as common causes for the occurrence.",,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",2015-11-20,Conference Paper,"Fenske, Wolfram;Schulze, Sandro;Meyer, Daniel;Saake, Gunter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963579544,10.1109/SCAM.2015.7335418,"Recording and replaying system specific, source code transformations","During its lifetime, a software system is under continuous maintenance to remain useful. Maintenance can be achieved in activities such as adding new features, fixing bugs, improving the system's structure, or adapting to new APIs. In such cases, developers sometimes perform sequences of code changes in a systematic way. These sequences consist of small code changes (e.g., create a class, then extract a method to this class), which are applied to groups of related code entities (e.g., some of the methods of a class). This paper presents the design and proof-of-concept implementation of a tool called MacroRecorder. This tool records a sequence of code changes, then it allows the developer to generalize this sequence in order to apply it in other code locations. In this paper, we discuss MACRORECORDER's approach that is independent of both development and transformation tools. The evaluation is based on previous work on repetitive code changes related to rearchitecting. MacroRecorder was able to replay 92% of the examples, which consisted in up to seven code entities modified up to 66 times. The generation of a customizable, large-scale transformation operator has the potential to efficiently assist code maintenance.",Automated Code Transformation | Programming By Demonstration | Refactoring | Software Evolution | Software Maintenance,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",2015-11-20,Conference Paper,"Santos, Gustavo;Etien, Anne;Anquetil, Nicolas;Ducasse, Stephane;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963595986,10.1109/SCAM.2015.7335419,Discovering likely mappings between APIs using text mining,"Developers often release different versions of their applications to support various platform/programming-language application programming interfaces (APIs). To migrate an application written using one API (source) to another API (target), a developer must know how the methods in the source API map to the methods in the target API. Given a typical platform or language exposes a large number of API methods, manually writing API mappings is prohibitively resource-intensive and may be error prone. Recently, researchers proposed to automate the mapping process by mining API mappings from existing code-bases. However, these approaches require as input a manually ported (or at least functionally similar) code across source and target APIs. To address the shortcoming, this paper proposes TMAP: Text Mining based approach to discover likely API mappings using the similarity in the textual description of the source and target API documents. To evaluate our approach, we used TMAP to discover API mappings for 15 classes across: 1) Java and C# API, and 2) Java ME and Android API. We compared the discovered mappings with state-of-the-art source code analysis based approaches: Rosetta and StaMiner. Our results indicate that TMAP on average found relevant mappings for 57% more methods compared to previous approaches. Furthermore, our results also indicate that TMAP on average found exact mappings for 6.5 more methods per class with a maximum of 21 additional exact mappings for a single class as compared to previous approaches.",,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",2015-11-20,Conference Paper,"Pandita, Rahul;Jetley, Raoul Praful;Sudarsan, Sithu D.;Williams, Laurie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2745844.2745874,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960905669,10.1109/VISSOFT.2015.7332411,Visual analytics of software structure and metrics,"In terms of software maintenance and comprehension, the fields of software engineering and software visualization have produced several methods and tools. However, they are typically separate tools in practice. In this paper, we present a novel methodology of combining software analysis and software visualization tools via an interactive visual workflow modeling approach. Standard software analysis tools are also limited in that they support only well-known metrics or are too complicated to use for generating custom software metrics. To address these shortcomings, our approach focuses on visual elements, their configurations, and interconnectivity rather than a data ontology and querying language. In order to test and validate our methodology, we developed a prototype tool called VIMETRIK (Visual Specification of Metrics). Our preliminary evaluation study illustrates the intuitiveness and ease-of-use of our approach with regard to understanding software measurement and analysis data.",Software Comprehension | Software Maintenance | Software Measurement | Visual Analytics | Visual Queries,"2015 IEEE 3rd Working Conference on Software Visualization, VISSOFT 2015 - Proceedings",2015-11-19,Conference Paper,"Khan, Taimur;Barthel, Henning;Ebert, Achim;Liggesmeyer, Peter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960851926,10.1109/VISSOFT.2015.7332428,Visualization based API usage patterns refining,"Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Most of existing work provided different techniques to mine API usage patterns from client programs, in order to help developers to understand and use existing libraries. However, considering only client programs to identify API usage patterns, is a strong constraint as collecting several similar client programs for an API is not a trivial task. And even if these clients are available, all the usage scenarios of the API of interest may not be covered by those clients. In this paper, we propose a visualization based approach for the refinement of Client-based Usage Patterns. We first visualize the patterns structure. Then we enrich the patterns with API methods that are semantically related to them, and thus may contribute together to the implementation of a particular functionality for potential client programs.",Documentation | Layout | Libraries | Matrix decomposition | Semantics | Software | Visualization,"2015 IEEE 3rd Working Conference on Software Visualization, VISSOFT 2015 - Proceedings",2015-11-19,Conference Paper,"Saied, Mohamed Aymen;Benomar, Omar;Sahraoui, Houari",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84960857356,10.1109/VISSOFT.2015.7332439,XVIZIT: Visualizing cognitive units in spreadsheets,"Spreadsheets can be large and complex and their maintenance and comprehension difficult to end-users. Large numbers of cells, complex formulae and missing documentation can impede the understanding of a spreadsheet. Comprehension assesses different levels of a spreadsheet according to a specific maintenance task, ranging from single formulae over sets of cells to complex structural patterns. These levels of abstraction are subsumed under the term cognitive unit. XVIZIT helps end-users in maintaining and comprehending spreadsheets. It guides them through a spreadsheet model: Roles of cells and sheets, similar patterns and various concepts of modularity can be explored. It uses modularization algorithms to provide conceptional decompositions of a spreadsheet model, such as equivalence classes or data modules. XVIZIT's slice visualizations ease the evaluation of corrective modifications by showing the dependant cells. Furthermore, XVIZIT provides a number of complexity measures allowing end-users to estimate the effort to comprehend and maintain a spreadsheet.",Complexity theory | Computational modeling | Data visualization | Image color analysis | Maintenance engineering | Measurement | Switches,"2015 IEEE 3rd Working Conference on Software Visualization, VISSOFT 2015 - Proceedings",2015-11-19,Conference Paper,"Hodnigg, Karin;Pinzger, Martin",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.67,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.10,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.77,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.62,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.63,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.83,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.42,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.74,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2015.57,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951853572,10.1109/ICSE.2015.26,Automated data structure generation: Refuting common wisdom,"Common wisdom in the automated data structure generation community states that declarative techniques have better usability than imperative techniques, while imperative techniques have better performance. We show that this reasoning is fundamentally flawed: if we go to the declarative limit and employ constraint logic programming (CLP), the CLP data structure generation has orders of magnitude better performance than comparable imperative techniques. Conversely, we observe and argue that when it comes to realistically complex data structures and properties, the CLP specifications become more obscure, indirect, and difficult to implement and understand than their imperative counterparts.We empirically evaluate three competing generation techniques, CLP, Korat, and UDITA, to validate these observations on more complex and interesting data structures than any prior work in this area. We explain why these observations are true, and discuss possible techniques for attaining the best of both worlds.",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Dewey, Kyle;Nichols, Lawton;Hardekopf, Ben",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951846140,10.1109/ICSE.2015.60,Learning to log: Helping developers make informed logging decisions,"Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a ""learning to log"" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of ""learning to log"".",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Zhu, Jieming;He, Pinjia;Fu, Qiang;Zhang, Hongyu;Lyu, Michael R.;Zhang, Dongmei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951801291,10.1109/ICSE.2015.79,Compositional symbolic execution with memoized replay,"Symbolic execution is a powerful, systematic analysis that has received much visibility in the last decade. Scalability however remains a major challenge for symbolic execution. Compositional analysis is a well-known general purpose methodology for increasing scalability. This paper introduces a new approach for compositional symbolic execution. Our key insight is that we can summarize each analyzed method as a memoization tree that captures the crucial elements of symbolic execution, and leverage these memoization trees to efficiently replay the symbolic execution of the corresponding methods with respect to their calling contexts. Memoization trees offer a natural way to compose in the presence of heap operations, which cannot be dealt with by previous work that uses logical formulas as summaries for compositional symbolic execution. Our approach also enables efficient target oriented symbolic execution for error detection or program coverage. Initial experimental evaluation based on a prototype implementation in Symbolic PathFinder shows that our approach can be up to an order of magnitude faster than traditional non-compositional symbolic execution.",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Qiu, Rui;Yang, Guowei;Pâsâreanu, Corina S.;Khurshid, Sarfraz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951859252,10.1109/ICSE.2015.86,Specifying event-based systems with a counting fluent temporal logic,"Fluent linear temporal logic is a formalism for specifying properties of event-based systems, based on propositions called fluents, defined in terms of activating and deactivating events. In this paper, we propose complementing the notion of fluent by the related concept of counting fluent. As opposed to the boolean nature of fluents, counting fluents are numerical values, that enumerate event occurrences, and allow us to specify naturally some properties of reactive systems. Although by extending fluent linear temporal logic with counting fluents we obtain an undecidable, strictly more expressive formalism, we develop a sound (but incomplete) model checking approach for the logic, that reduces to traditional temporal logic model checking, and allows us to automatically analyse properties involving counting fluents, on finite event-based systems. Our experiments, based on relevant models taken from the literature, show that: (i) counting fluent temporal logic is better suited than traditional temporal logic for expressing properties in which the number of occurrences of certain events is relevant, and (ii) our model checking approach on counting fluent specifications is more efficient and scales better than model checking equivalent fluent temporal logic specifications.",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Regis, Germán;Degiovanni, Renzo;D'Ippolito, Nicolas;Aguirre, Nazareno",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951731731,10.1109/ICSE.2015.336,Graph-based statistical language model for code,"N-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n- gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75% of the cases, it can correctly suggest the API with only five candidates. ASTLan also has high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Nguyen, Anh Tuan;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951775563,10.1109/ICSE.2015.97,Discovering information explaining API types using text classification,"Many software development tasks require developers to quickly learn a subset of an Application Programming Interface (API). API learning resources are crucial for helping developers learn an API, but the knowledge relevant to a particular topic of interest may easily be scattered across different documents, which makes finding the necessary information more challenging. This paper proposes an approach to discovering tutorial sections that explain a given API type. At the core of our approach, we classify fragmented tutorial sections using supervised text classification based on linguistic and structural features. Experiments conducted on five tutorials show that our approach is able to discover sections explaining an API type with precision between 0.69 and 0.87 (depending on the tutorial) when trained and tested on the same tutorial. When trained and tested across tutorials, we obtained a precision between 0.74 and 0.94 and lower recall values.",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Petrosyan, Gayane;Robillard, Martin P.;De Mori, Renato",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84951865251,10.1109/ICSE.2015.101,An empirical study on real bug fixes,"Software bugs can cause significant financial loss and even the loss of human lives. To reduce such loss, developers devote substantial efforts to fixing bugs, which generally requires much expertise and experience. Various approaches have been proposed to aid debugging. An interesting recent research direction is automatic program repair, which achieves promising results, and attracts much academic and industrial attention. However, people also cast doubt on the effectiveness and promise of this direction. A key criticism is to what extent such approaches can fix real bugs. As only research prototypes for these approaches are available, it is infeasible to address the criticism by evaluating them directly on real bugs. Instead, in this paper, we design and develop BUGSTAT, a tool that extracts and analyzes bug fixes. With BUGSTAT's support, we conduct an empirical study on more than 9,000 real-world bug fixes from six popular Java projects. Comparing the nature of manual fixes with automatic program repair, we distill 15 findings, which are further summarized into four insights on the two key ingredients of automatic program repair: fault localization and faulty code fix. In addition, we provide indirect evidence on the size of the search space to fix real bugs and find that bugs may also reside in non-source files. Our results provide useful guidance and insights for improving the state-of-the-art of automatic program repair.",,Proceedings - International Conference on Software Engineering,2015-08-12,Conference Paper,"Zhong, Hao;Su, Zhendong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICPC.2015.9,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84961349381,10.1109/ICPC.2015.13,RCLinker: Automated Linking of Issue Reports and Commits Leveraging Rich Contextual Information,"Links between issue reports and their corresponding commits in version control systems are often missing. However, these links are important for measuring the quality of various parts of a software system, predicting defects, and many other tasks. A number of existing approaches have been designed to solve this problem by automatically linking bug reports to source code commits via comparison of textual information in commit messages with textual contents in the bug reports. Yet, the effectiveness of these techniques is oftentimes sub optimal when commit messages are empty or only contain minimum information, this particular problem makes the process of recovering trace ability links between commits and bug reports particularly challenging. In this work, we aim at improving the effectiveness of existing bug linking techniques by utilizing rich contextual information. We rely on a recently proposed tool, namely Change Scribe, which generates commit messages containing rich contextual information by using a number of code summarization techniques. Our approach then extracts features from these automatically generated commit messages and bug reports and inputs them into a classification technique that creates a discriminative model used to predict if a link exists between a commit message and a bug report. We compared our approach, coined as RCLinker (Rich Context Linker), to MLink, which is an existing state-of-the-art bug linking approach. Our experiment results on bug reports from 6 software projects show that RCLinker can outperform MLink in terms of F-measure by 138.66%.",ChangeScribe | Classification | Feature Extraction | Recovering Missing Links,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Le, Tien Duy B.;Linares-Vásquez, Mario;Lo, David;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961339840,10.1109/ICPC.2015.16,Could We Infer Unordered API Usage Patterns Only Using the Library Source Code?,"Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Much existing work has provided different techniques to mine API usage patterns from client programs in order to help developers on understanding and using existing libraries. However, considering only client programs to identify API usage patterns is a strong constraint as the client programs source code is not always available or the clients themselves do not exist yet for newly released APIs. In this paper, we propose a technique for mining Non Client-based Usage Patterns (NCBUP miner). We detect unordered API usage patterns as distinct groups of API methods that are structurally and semantically related and thus may contribute together to the implementation of a particular functionality for potential client programs. We evaluated our technique through four APIs. The obtained results are comparable to those of client-based approaches in terms of usage-patterns cohesion.",API Documentation | API Usage | Software Clustering | Usage Pattern,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Saied, Mohamed Aymen;Abdeen, Hani;Benomar, Omar;Sahraoui, Houari",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961372319,10.1109/ICPC.2015.17,Searching the State Space: A Qualitative Study of API Protocol Usability,"Application Programming Interfaces (APIs) often define protocols - restrictions on the order of client calls to API methods. API protocols are common and difficult to use, which has generated tremendous research effort in alternative specification, implementation, and verification techniques. However, little is understood about the barriers programmers face when using these APIs, and therefore the research effort may be misdirected. To understand these barriers better, we perform a two-part qualitative study. First, we study developer forums to identify problems that developers have with protocols. Second, we perform a think-aloud observational study, in which we systematically observe professional programmers struggle with these same problems to get more detail on the nature of their struggles and how they use available resources. In our observations, programmer time was spent primarily on four types of searches of the protocol state space. These observations suggest protocol-targeted tools, languages, and verification techniques will be most effective if they enable programmers to efficiently perform state search.",APIs | protocols | qualitative research | typestate,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Sushine, Joshua;Herbsleb, James D.;Aldrich, Jonathan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961334812,10.1109/ICPC.2015.19,"Code, Camera, Action: How Software Developers Document and Share Program Knowledge Using YouTube","Creating documentation is a challenging task in software engineering and most techniques involve the laborious and sometimes tedious job of writing text. This paper explores an alternative to traditional text-based documentation, the screen-cast, which captures a developer's screen while they narrate how a program or software tool works. We conducted a study to investigate how developers produce and share developer-focused screen casts using the You Tube social platform. First, we identified and analyzed a set of development screen casts to determine how developers have adapted to the medium to meet the demands of development-related documentation needs. We also explored the techniques and strategies used for sharing software knowledge. Second, we interviewed screen cast producers to understand their motivations for creating screen casts, and to uncover the perceived benefits and challenges in producing code-focused videos. Our findings reveal that video is a useful medium for communicating program knowledge between developers, and that developers build their online personas and reputation by sharing videos through social channels.",Code Walkthrough | Screencast | Software Documentation | YouTube,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"MacLeod, Laura;Storey, Margaret Anne;Bergen, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961340557,10.1109/ICPC.2015.21,Framework Instantiation Using Cookbooks Constructed with Static and Dynamic Analysis,"Software reuse is one of the major goals in software engineering. Frameworks promote the reuse of not only individual building blocks, but also of system design. However, framework instantiation requires a substantial understanding effort. High quality documentation is essential to minimize this effort. However, in most cases, appropriate documentation does not exist or is not updated. Our hypothesis is that the framework code itself and existing instantiations can serve as a guide for new instantiations. The challenge is that users still have to read large portions of code, which hinders the understanding process, thus our goal is to provide relevant information for framework instantiation with static and dynamic analysis of the framework and pre-existing instantiations. The final documentation is presented in a cookbook style, where recipes are composed of programming tasks and information about hotspots related to a feature instantiation. We conducted two preliminary experiments, the first to evaluate the recall of the approach and the second to study the practical usefulness of the recipe information for developers. Results reveal that our approach discloses accurate and relevant information about classes and methods used for framework instantiation.",code examples | cookbook | framework instantiation | reverse engineering,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Lafetá, Raquel F.Q.;Maia, Marcelo A.;Röthlisberger, David",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84961343017,10.1109/ICPC.2015.38,Towards Visual Reflexion Models,"Source code and models of a software system, like architectural views, tend to evolve separately and drift apart over time. Previous research has shown that it is possible to effectively relate them through a reflex ion model, defined as a ""summarization of a software system from the viewpoint of a particular high-level model"". While effective, the process of constructing and analyzing reflex ion models was supported by text-based tools with limited visual representation. With the original approach, it was relatively hard to understand which parts of the system were represented, and which parts of the system contributed to specific relations in the reflex ion model. We present our vision on augmenting the construction and analysis of reflex ion models with visual support, effectively providing the basis for visual reflex ion models. We describe our approach, implemented as a web-based application, and two promising case studies involving two open-source projects.",architecture | domain model | reflexion models,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Romanelli, Marcello;Mocci, Andrea;Lanza, Michele",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84961195074,10.1145/2745802.2745804,Supporting architecture documentation: A comparison of two ontologies for knowledge retrieval,"Context: Software architecture documentation is used to communicate architectural knowledge. It is often difficult for document users to find all the architectural knowledge they need to do their tasks, and this results in wasted time and mistakes during development. Objective: In this pa- per we investigate how ontology-based documentation may support users in finding the architectural knowledge they need. Method: We executed a controlled experiment to test for differences in knowledge retrieval efficiency and ef- fectiveness between two groups of master students that used two ontologies built from different understandings of the ar- chitectural knowledge needs of document users. Results: Use of the ontology built based on a better understanding of architectural knowledge needs was significantly more effi- cient or effective for retrieving part of the knowledge needed by document users. We analysed participants' search ac- tions and identified which organisation of knowledge in the ontologies resulted in efficient and effective knowledge re- trieval. Conclusion: We found that an improved under- standing of knowledge needs allows for the construction of an ontology from which document users retrieve knowledge more efficiently and effectively. In some cases we found that the ontology support for knowledge needs had to be traded off against ontology design criteria.",Knowledge retrieval | Ontology engineering | Ontology-based documentation | Software architecture documentation,ACM International Conference Proceeding Series,2015-04-27,Conference Paper,"De Graaf, Klaas Andries;Liang, Peng;Tang, Antony;Van Vliet, Hans",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961154468,10.1145/2745802.2745819,Analyzing confidentiality and privacy concerns: Insights from android issue logs,"Context: Post-release user feedback plays an integral role in improving software quality and informing new features. Given its growing importance, feedback concerning security enhancements is particularly noteworthy. In considering the rapid uptake of Android we have examined the scale and severity of Android security threats as reported by its stakeholders. Objective: We systematically mine Android issue logs to derive insights into stakeholder perceptions and experiences in relation to certain Android security issues. Method: We employed contextual analysis techniques to study issues raised regarding confidentiality and privacy in the last three major Android releases, considering covariance of stakeholder comments, and the level of consistency in user preferences and priorities. Results: Confidentiality and privacy concerns varied in severity, and were most prevalent over Jelly Bean releases. Issues raised in regard to confidentiality related mostly to access, user credentials and permission management, while privacy concerns were mainly expressed about phone locking. Community users also expressed divergent preferences for new security features, ranging from more relaxed to very strict. Conclusions: Strategies that support continuous corrective measures for both old and new Android releases would likely maintain stakeholder confidence. An approach that provides users with basic default security settings, but with the power to configure additional security features if desired, would provide the best balance for Android's wide cohort of stakeholders.",Android | Confidentiality | Content analysis | Empirical analysis | Privacy | Security,ACM International Conference Proceeding Series,2015-04-27,Conference Paper,"Licorish, Sherlock A.;MacDonell, Stephen G.;Clear, Tony",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961169984,10.1145/2745802.2745837,Analyzing program readability based on WordNet,"Comments to describe the intent of the code is crucial to measure the program readability, especially for the methods and their comments in a program. Existing program readability techniques mainly focus on matching method and its comments on whether there is the same content between them. But these techniques cannot accurately analyze polysemy and synonyms in the program. In this paper, we propose an approach to analyze program readability based on WordNet, which is able to expand the range of keyword search and solve the problem of semantic ambiguity. Based on the same semantic query function of WordNet, we match keywords between comments and methods, and analyze the readability of the classes and packages in a program.",Program comprehension | Program readability | WordNet,ACM International Conference Proceeding Series,2015-04-27,Conference Paper,"Liu, Yangchao;Sun, Xiaobing;Duan, Yucong",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-84951069755,10.1007/978-3-319-21690-4_1,A trusted mechanised specification of javascript: One year on,"The JSCert project provides a Coq mechanised specification of the core JavaScript language. A key part of the project was to develop a methodology for establishing trust, by designing JSCert in such a way as to provide a strong connection with the JavaScript standard, and by developing JSRef, a reference interpreter which was proved correct with respect to JSCert and tested using the standard Test262 test suite. In this paper, we assess the previous state of the project at POPL’14 and the current state of the project at CAV’15. We evaluate the work of POPL’14, providing an analysis of the methodology as a whole and a more detailed analysis of the tests. We also describe recent work on extending JSRef to include Google’s V8 Array library, enabling us to cover more of the language and to pass more tests.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Gardner, Philippa;Smith, Gareth;Watt, Conrad;Wood, Thomas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951139875,10.1007/978-3-319-21690-4_7,Skipping refinement,"We introduce skipping refinement, a new notion of correctness for reasoning about optimized reactive systems. Reasoning about reactive systems using refinement involves defining an abstract, highlevel specification system and a concrete, low-level implementation system. One then shows that every behavior allowed by the implementation is also allowed by the specification. Due to the difference in abstraction levels, it is often the case that the implementation requires many steps to match one step of the specification, hence, it is quite useful for refinement to directly account for stuttering. Some optimized implementations, however, can actually take multiple specification steps at once. For example, a memory controller can buffer the commands to the memory and at a later time simultaneously update multiple memory locations, thereby skipping several observable states of the abstract specification, which only updates one memory location at a time. We introduce skipping simulation refinement and provide a sound and complete characterization consisting of “local” proof rules that are amenable to mechanization and automated verification. We present case studies that highlight the applicability of skipping refinement: a JVM-inspired stack machine, a simple memory controller and a scalar to vector compiler transformation. Our experimental results demonstrate that current model-checking and automated theorem proving tools have difficulty automatically analyzing these systems using existing notions of correctness, but they can analyze the systems if we use skipping refinement.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Jain, Mitesh;Manolios, Panagiotis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951009232,10.1007/978-3-319-21690-4_18,Learning commutativity specifications,"In this work we present a new sampling-based “black box” inference approach for learning the behaviors of a library component. As an application, we focus on the problem of automatically learning commutativity specifications of data structures. This is a very challenging problem, yet important, as commutativity specifications are fundamental to program analysis, concurrency control and even lower bounds. Our approach is enabled by three core insights: (i) type-aware sampling which drastically improves the quality of obtained examples, (ii) relevant predicate discovery critical for reducing the formula search space, and (iii) an efficient search based on weighted-set cover for finding formulas ranging over the predicates and capturing the examples. More generally, our work learns formulas belonging to fragments consisting of quantifier-free formulas over a finite number of relation symbols. Such fragments are expressive enough to capture useful specifications (e.g., commutativity) yet are amenable to automated inference. We implemented a tool based on our approach and have shown that it can quickly learn non-trivial and important commutativity specifications of fundamental data types such as hash maps, sets, array lists, union find and others. We also showed experimentally that learning these specifications is beyond the capabilities of existing techniques.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Gehr, Timon;Dimitrov, Dimitar;Vechev, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951136582,10.1007/978-3-319-21690-4_19,Angelic verification: Precise verification modulo unknowns,"Verification of open programs can be challenging in the presence of an unconstrained environment. Verifying properties that depend on the environment yields a large class of uninteresting false alarms. Using a verifier on a program thus requires extensive initial investment in modeling the environment of the program. We propose a technique called angelic verification for verification of open programs, where we constrain a verifier to report warnings only when no acceptable environment specification exists to prove the assertion. Our framework is parametric in a vocabulary and a set of angelic assertions that allows a user to configure the tool. We describe a few instantiations of the framework and an evaluation on a set of real-world benchmarks to show that our technique is competitive with industrial-strength tools even without models of the environment.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Das, Ankush;Lahiri, Shuvendu K.;Lal, Akash;Li, Yi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951109750,10.1007/978-3-319-21668-3_13,Deductive program repair,"We present an approach to program repair and its application to programs with recursive functions over unbounded data types. Our approach formulates program repair in the framework of deductive synthesis that uses existing program structure as a hint to guide synthesis. We introduce a new specification construct for symbolic tests. We rely on such user-specified tests as well as automatically generated ones to localize the fault and speed up synthesis. Our implementation is able to eliminate errors within seconds from a variety of functional programs, including symbolic computation code and implementations of functional data structures. The resulting programs are formally verified by the Leon system.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Kneuss, Etienne;Koukoutos, Manos;Kuncak, Viktor",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84951099253,10.1007/978-3-319-21668-3_19,Measuring with timed patterns,"We propose a declarative measurement specification language for quantitative performance evaluation of hybrid (discrete-continuous) systems based on simulation traces. We use timed regular expressions with events to specify patterns that define segments of simulation traces over which measurements are to be taken. In addition, we associate measure specifications over these patterns to describe a particular type of performance evaluation (maximization, average, etc.) to be done over the matched signal segments. The resulting language enables expressive and versatile specification of measurement objectives. We develop an algorithm for our measurement framework, implement it in a prototype tool, and apply it in a case study of an automotive communication protocol. Our experiments demonstrate that the proposed technique is usable with very low overhead to a typical (computationally intensive) simulation.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Ferrère, Thomas;Maler, Oded;Ničković, Dejan;Ulus, Dogan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2789052,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2784731.2784741,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2784764,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2784756,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2784731.2784735,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2610375,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2629506,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2685613,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930158786,10.1145/2699697,aToucan: An automated framework to derive UML analysis models from use case models,"The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model comprising class, sequence and activity diagrams from a use case model and to automatically establish traceability links between model elements of the use case model and the generated analysis model. Note that our goal is to save effort through automated support, not to replace human abstraction and decision making. Seven (six) case studies were performed to compare class (sequence) diagrams generated by aToucan to the ones created by experts, Masters students, and trained, fourth-year undergraduate students. Results show that aToucan performs well regarding consistency (e.g., 88% class diagram consistency) and completeness (e.g., 80% class completeness) when comparing generated class diagrams with reference class diagrams created by experts and Masters students. Similarly, sequence diagrams automatically generated by aToucan are highly consistent with the ones devised by experts and are also rather complete, for instance, 91% and 97% message consistency and completeness, respectively. Further, statistical tests show that aToucan significantly outperforms fourth-year engineering students in this respect, thus demonstrating the value of automation. We also conducted two industrial case studies demonstrating the applicability of aToucan in two different industrial domains. Results showed that the vast majority of model elements generated by aToucan are correct and that therefore, in practice, such models would be good initial models to refine and augment so as to converge towards to correct and complete analysis models. A performance analysis shows that the execution time of aToucan (when generating class and sequence diagrams) is dependent on the number of simple sentences contained in the use case model and remains within a range of a few minutes. Five different software system descriptions (18 use cases altogether) were performed to evaluate the generation of activity diagrams. Results show that aToucan can generate 100% complete and correct control flow information of activity diagrams and on average 85% data flAow information completeness. Moreover, we show that aToucan outperforms three commercial tools in terms of activity diagram generation.",Activity diagram | Analysis model | Automation | Class diagram | Sequence diagram | Traceability | Transformation | UML | Use case modeling,ACM Transactions on Software Engineering and Methodology,2015-05-01,Article,"Yue, Tao;Briand, Lionel C.;Labiche, Yvan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930177029,10.1145/2699696,Documenting design-pattern instances: A family of experiments on source-code comprehensibility,"Design patterns are recognized as a means to improve software maintenance by furnishing an explicit specification of class and object interactions and their underlying intent [Gamma et al. 1995]. Only a few empirical investigations have been conducted to assess whether the kind of documentation for design patterns implemented in source code affects its comprehensibility. To investigate this aspect, we conducted a family of four controlled experiments with 88 participants having different experience (i.e., professionals and Bachelor, Master, and PhD students). In each experiment, the participants were divided into three groups and asked to comprehend a nontrivial chunk of an open-source software system. Depending on the group, each participant was, or was not, provided with graphical or textual representations of the design patterns implemented within the source code. We graphically documented design-pattern instances with UML class diagrams. Textually documented instances are directly reported source code as comments. Our results indicate that documenting design-pattern instances yields an improvement in correctness of understanding source code for those participants with an adequate level of experience.",Controlled experiment | Design patterns | Maintenance | Replications | Software models | Source-code comprehension,ACM Transactions on Software Engineering and Methodology,2015-05-01,Article,"Scanniello, Giuseppe;Gravino, Carmine;Risi, Michele;Tortora, Genoveffa;Dodero, Gabriella",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84930154899,10.1145/2729976,Platys: An active learning framework for place-aware application development and its evaluation,"We introduce a high-level abstraction of location called place. A place derives its meaning from a user's physical space, activities, or social context. In this manner, place can facilitate improved user experience compared to the traditional representation of location, which is spatial coordinates. We propose the Platys framework as a way to address the special challenges of place-aware application development. The core of Platys is a middleware that (1) learns a model of places specific to each user via active learning, a machine learning paradigm that seeks to reduce the user-effort required for training the middleware, and (2) exposes the learned user-specific model of places to applications at run time, insulating application developers from dealing with both low-level sensors and user idiosyncrasies in perceiving places. We evaluated Platys via two studies. First, we collected place labels and Android phone sensor readings from 10 users. We applied Platys' active learning approach to learn each user's places and found that Platys (1) requires fewer place labels to learn a user's places with a desired accuracy than do two traditional supervised approaches, and (2) learns places with higher accuracy than two unsupervised approaches. Second, we conducted a developer study to evaluate Platys' efficiency in assisting developers and its effectiveness in enabling usable applications. In this study, 46 developers employed either Platys or the Android location API to develop a place-aware application. Our results indicate that application developers employing Platys, when compared to those employing the Android API, (1) develop a place-aware application faster and perceive reduced difficulty and (2) produce applications that are easier to understand (for developers) and potentially more usable and privacy preserving (for application users).",Active learning | Contextaware | Location-aware | Middleware | Mobile application development | Place recognition | Place-aware | Privacy | Semi-supervised learning | Usability,ACM Transactions on Software Engineering and Methodology,2015-05-01,Article,"Murukannaiah, Pradeep K.;Singh, Munindar P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84941555928,10.1145/2699688,Does automated unit test generation really help software testers? A controlled empirical study,"Work on automated test generation has produced several tools capable of generating test data which achieves high structural coverage over a program. In the absence of a specification, developers are expected to manually construct or verify the test oracle for each test input. Nevertheless, it is assumed that these generated tests ease the task of testing for the developer, as testing is reduced to checking the results of tests. While this assumption has persisted for decades, there has been no conclusive evidence to date confirming it. However, the limited adoption in industry indicates this assumption may not be correct, and calls into question the practical value of test generation tools. To investigate this issue, we performed two controlled experiments comparing a total of 97 subjects split between writing tests manually and writing tests with the aid of an automated unit test generation tool, EVOSUITE. We found that, on one hand, tool support leads to clear improvements in commonly applied quality metrics such as code coverage (up to 300% increase). However, on the other hand, there was no measurable improvement in the number of bugs actually found by developers. Our results not only cast some doubt on how the research community evaluates test generation tools, but also point to improvements and future work necessary before automated test generation tools will be widely adopted by practitioners.",Automated test generation | Branch coverage | Empirical software engineering | Unit testing,ACM Transactions on Software Engineering and Methodology,2015-08-01,Conference Paper,"Fraser, Gordon;Staats, Matt;McMinn, Phil;Arcuri, Andrea;Padberg, Frank",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928107075,10.1109/TSE.2014.2367027,The impact of API change- and fault-proneness on the user ratings of android apps,"The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received.",Android | API changes | Empirical Studies | Mining Software Repositories,IEEE Transactions on Software Engineering,2015-04-01,Article,"Bavota, Gabriele;Linares-Vásquez, Mario;Bernal-Cárdenas, Carlos Eduardo;Di Penta, Massimiliano;Oliveto, Rocco;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84929331583,10.1109/TSE.2014.2372785,The oracle problem in software testing: A survey,"Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the 'test oracle problem'. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the test oracle problem and an analysis of trends in this important area of software testing research and practice.",Automatic testing | Test oracle | Testing formalism,IEEE Transactions on Software Engineering,2015-05-01,Article,"Barr, Earl T.;Harman, Mark;McMinn, Phil;Shahbaz, Muzammil;Yoo, Shin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84933054135,10.1109/TSE.2014.2387172,Extracting development tasks to navigate software documentation,"Knowledge management plays a central role in many software development organizations. While much of the important technical knowledge can be captured in documentation, there often exists a gap between the information needs of software developers and the documentation structure. To help developers navigate documentation, we developed a technique for automatically extracting tasks from software documentation by conceptualizing tasks as specific programming actions that have been described in the documentation. More than 70 percent of the tasks we extracted from the documentation of two projects were judged meaningful by at least one of two developers. We present TaskNavigator, a user interface for search queries that suggests tasks extracted with our technique in an auto-complete list along with concepts, code elements, and section headers. We conducted a field study in which six professional developers used TaskNavigator for two weeks as part of their ongoing work. We found search results identified through extracted tasks to be more helpful to developers than those found through concepts, code elements, and section headers. The results indicate that task descriptions can be effectively extracted from software documentation, and that they help bridge the gap between documentation structure and the information needs of software developers.",Auto-Complete | Development Tasks | Natural Language Processing | Navigation | Software Documentation,IEEE Transactions on Software Engineering,2015-06-01,Article,"Treude, Christoph;Robillard, Martin P.;Dagenais, Barthélémy",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84937702668,10.1109/TSE.2015.2398877,"Aligning Qualitative, Real-Time, and Probabilistic Property Specification Patterns Using a Structured English Grammar","Formal methods offer an effective means to assert the correctness of software systems through mathematical reasoning. However, the need to formulate system properties in a purely mathematical fashion can create pragmatic barriers to the application of these techniques. For this reason, Dwyer et al. invented property specification patterns which is a system of recurring solutions to deal with the temporal intricacies that would make the construction of reactive systems very hard otherwise. Today, property specification patterns provide general rules that help practitioners to qualify order and occurrence, to quantify time bounds, and to express probabilities of events. Nevertheless, a comprehensive framework combining qualitative, real-time, and probabilistic property specification patterns has remained elusive. The benefits of such a framework are twofold. First, it would remove the distinction between qualitative and quantitative aspects of events; and second, it would provide a structure to systematically discover new property specification patterns. In this paper, we report on such a framework and present a unified catalogue that combines all known plus 40 newly identified or extended patterns. We also offer a natural language front-end to map patterns to a temporal logic of choice. To demonstrate the virtue of this new framework, we applied it to a variety of industrial requirements, and use PSPWizard , a tool specifically developed to work with our unified pattern catalogue, to automatically render concrete instances of property specification patterns to formulae of an underlying temporal logic of choice.",Probabilistic Properties | Real-time Properties | Specification Patterns,IEEE Transactions on Software Engineering,2015-07-01,Article,"Autili, Marco;Grunske, Lars;Lumpe, Markus;Pelliccione, Patrizio;Tang, Antony",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84937448734,10.1109/TSE.2015.2419611,COVERT: Compositional Analysis of Android Inter-App Permission Leakage,"Android is the most popular platform for mobile devices. It facilitates sharing of data and services among applications using a rich inter-app communication system. While access to resources can be controlled by the Android permission system, enforcing permissions is not sufficient to prevent security violations, as permissions may be mismanaged, intentionally or unintentionally. Android's enforcement of the permissions is at the level of individual apps, allowing multiple malicious apps to collude and combine their permissions or to trick vulnerable apps to perform actions on their behalf that are beyond their individual privileges. In this paper, we present COVERT, a tool for compositional analysis of Android inter-app vulnerabilities. COVERT's analysis is modular to enable incremental analysis of applications as they are installed, updated, and removed. It statically analyzes the reverse engineered source code of each individual app, and extracts relevant security specifications in a format suitable for formal verification. Given a collection of specifications extracted in this way, a formal analysis engine (e.g., model checker) is then used to verify whether it is safe for a combination of applications - holding certain permissions and potentially interacting with each other - to be installed together. Our experience with using COVERT to examine over 500 real-world apps corroborates its ability to find inter-app vulnerabilities in bundles of some of the most popular apps on the market.",Android | Formal Verification | Inter-App Vulnerabilities | Static Analysis,IEEE Transactions on Software Engineering,2015-09-01,Article,"Bagheri, Hamid;Sadeghi, Alireza;Garcia, Joshua;Malek, Sam",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961822007,10.1109/TSE.2015.2442238,An Eye-Tracking Study of Java Programmers and Application to Source Code Summarization,"Source Code Summarization is an emerging technology for automatically generating brief descriptions of code. Current summarization techniques work by selecting a subset of the statements and keywords from the code, and then including information from those statements and keywords in the summary. The quality of the summary depends heavily on the process of selecting the subset: a high-quality selection would contain the same statements and keywords that a programmer would choose. Unfortunately, little evidence exists about the statements and keywords that programmers view as important when they summarize source code. In this paper, we present an eye-tracking study of 10 professional Java programmers in which the programmers read Java methods and wrote English summaries of those methods. We apply the findings to build a novel summarization tool. Then, we evaluate this tool. Finally, we further analyze the programmers' method summaries to explore specific keyword usage and provide evidence to support the development of source code summarization systems.",program comprehension | Source code summaries,IEEE Transactions on Software Engineering,2015-11-01,Article,"Rodeghero, Paige;Liu, Cheng;McBurney, Paul W.;McMillan, Collin",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84961690128,10.1145/2775441.2775471,Putting GamerGate in context: How group documentation informs social media activity,"In this paper, I begin the process of examining the value of organizational documentation in evaluating social media activism. I take a rhetorical analysis approach to some of the documentation associated with the GamerGate movement and compare traits within the documentation to social media artifacts and metadata to determine how well the documents reflect the active network in regards to desired behavior and goals. Additionally, I examine the challenges in this approach given the anonymity and diffusion of authorship. Finally, I look at how documentation analysis can help inform big data analysis in online activism.",Online activism | Online harassment | Public relations documentation | Rhetorical analysis | Social media,SIGDOC 2015 - Proceedings of the 33rd Annual International Conference on the Design of Communication,2015-07-16,Conference Paper,"Trice, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961705063,10.1145/2775441.2775463,Envisioning mobile apps for audio description: Exploring universal design of National Park Service brochures,"""Unigrid"" design specifications created by Massimo Vignelli have provided the standards for the layout of paper brochures at U.S. National Park Service sites for more than three decades. These brochures offer visitors a familiar analog presentation of visual information, blending text, photographs, maps, and illustrations. These materials, however, are not accessible to people who are blind, have low vision, or a print disability. The National Park Service for decades has been challenged - by requirements and principle - to offer alternate formats that provide equivalent experiences and information of these print materials. In other words, people who are blind or visually impaired should have access to a ""brochure"" experience, too. This exploratory study, funded by the National Park Service, takes a new approach to this long-term problem by conducting a content analysis of current Unigrid brochures to determine their fundamental components, found in practice. This components-based approach is intended to provide clear pathways for cross-modal translation of the printed material into audio-described media, which then, can be efficiently distributed via mobile apps, as an extension of these original components.",Accessibility | Audio description | Cross-modal translation | Disability studies | Information design | Intercultural communication | Mobile | National Park Service | Universal design,SIGDOC 2015 - Proceedings of the 33rd Annual International Conference on the Design of Communication,2015-07-16,Conference Paper,"Oppegaard, Brett;Conway, Thomas;Conway, Megan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961718561,10.1145/2775441.2775457,User preferences of software documentation genres,"Today's technical software users find a wide variety of content online, from videos to forum posts to online articles. This study examines the extent to which the genre of this content matters to the users searching for it. It presents the findings of a new exploratory study that addresses the significance of genre preferences to the users, given that genre is secondary to their actual business goals. It looks at documentation users' usage of, and attitudes towards, documentation genres, and how factors such as work role and past experience come into play. Our conclusions can help individuals who formulate content strategies to understand the extent and strength of genre usage. This in turn can help them determine a strategic mix of documentation genres.",Genre | Online content | Source selection | User satisfaction,SIGDOC 2015 - Proceedings of the 33rd Annual International Conference on the Design of Communication,2015-07-16,Conference Paper,"Earle, Ralph H.;Rosso, Mark A.;Alexander, Kathryn E.",Exclude,
10.1016/j.jss.2022.111515,,10.1145/2556270,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2632296,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84911192041,10.1145/2655691,Urban Sensing Using Mobile Phone Network Data: A Survey of Research,"The recent development of telecommunication networks is producing an unprecedented wealth of information and, as a consequence, an increasing interest in analyzing such data both from telecoms and from other stakeholders' points of view. In particular,mobile phone datasets offer access to insights into urban dynamics and human activities at an unprecedented scale and level of detail, representing a huge opportunity for research and real-world applications. This article surveys the new ideas and techniques related to the use of telecommunication data for urban sensing. We outline the data that can be collected from telecommunication networks as well as their strengths and weaknesses with a particular focus on urban sensing. We survey existing filtering and processing techniques to extract insights from this data and summarize them to provide recommendations on which datasets and techniques to use for specific urban sensing applications. Finally, we discuss a number of challenges and open research areas currently being faced in this field. We strongly believe the material and recommendations presented here will become increasingly important as mobile phone network datasets are becoming more accessible to the research community.",Data mining | Mobile phone data | Urban planning | Urban transportation,ACM Computing Surveys,2015-04-16,Article,"Calabrese, Francesco;Ferrari, Laura;Blondel, Vincent D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2659796,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84937130447,10.1145/2666003,Rich vehicle routing problem: Survey,"The Vehicle Routing Problem (VRP) is a well-known research line in the optimization research community. Its different basic variants have been widely explored in the literature. Even though it has been studied for years, the research around it is still very active. The new tendency is mainly focused on applying this study case to real-life problems. Due to this trend, the Rich VRP arises: combining multiple constraints for tackling realistic problems. Nowadays, some studies have considered specific combinations of real-life constraints to define the emerging Rich VRP scopes. This work surveys the state of the art in the field, summarizing problem combinations, constraints defined, and approaches found.",Routing | Transportation | Vehicle routing problem,ACM Computing Surveys,2015-09-29,Article,"Caceres-Cruz, Jose;Arias, Pol;Guimarans, Daniel;Riera, Daniel;Juan, Angel A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924971479,10.1145/2674559,Low-rank modeling and its applications in image analysis,"Low-rank modeling generally refers to a class of methods that solves problems by representing variables of interest as low-rank matrices. It has achieved great success in various fields including computer vision, data mining, signal processing, and bioinformatics. Recently, much progress has been made in theories, algorithms, and applications of low-rank modeling, such as exact low-rank matrix recovery via convex programming and matrix completion applied to collaborative filtering. These advances have brought more and more attention to this topic. In this article, we review the recent advances of low-rank modeling, the state-of-the-art algorithms, and the related applications in image analysis. We first give an overview of the concept of low-rank modeling and the challenging problems in this area. Then, we summarize the models and algorithms for low-rank matrix recovery and illustrate their advantages and limitations with numerical experiments. Next, we introduce a few applications of low-rank modeling in the context of image analysis. Finally, we conclude this article with some discussions.",Image analysis | Low-rank modeling | Matrix factorization | Optimization,ACM Computing Surveys,2015-01-08,Article,"Zhou, Xiaowei;Yang, Can;Zhao, Hongyu;Yu, Weichuan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84929497454,10.1145/2700381,Graph-based label propagation in digital media: A review,"The expansion of the Internet over the last decade and the proliferation of online social communities, such as Facebook, Google+, and Twitter, as well as multimedia sharing sites, such as YouTube, Flickr, and Picasa, has led to a vast increase of available information to the user. In the case of multimedia data, such as images and videos, fast querying and processing of the available information requires the annotation of the multimedia data with semantic descriptors, that is, labels. However, only a small proportion of the available data are labeled. The rest should undergo an annotation-labeling process. The necessity for the creation of automatic annotation algorithms gave birth to label propagation and semi-supervised learning. In this study, basic concepts in graph-based label propagation methods are discussed. Methods for proper graph construction based on the structure of the available data and label inference methods for spreading label information from a few labeled data to a larger set of unlabeled data are reviewed. Applications of label propagation algorithms in digital media, as well as evaluation metrics for measuring their performance, are presented.",Label propagation | Semi-supervised learning,ACM Computing Surveys,2015-04-16,Review,"Zoidi, Olga;Fotiadou, Eftychia;Nikolaidis, Nikos;Pitas, Ioannis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930619661,10.1145/2719921,A survey of interactive Remote rendering systems,"Remote rendering means rendering 3D graphics on a computing device and displaying the results on another computing device connected through a network. The concept was originally developed for sharing computing resources remotely. It has been receiving increasing attention from researchers in both academia and industry in recent years due to the proliferation of cloud computing and mobile devices. In this article, we survey the interactive remote rendering systems proposed in the literature, analyze how to improve the state of the art, and summarize the related technologies. The readers of this article will understand the history of remote rendering systems and obtain some inspirations of the future research directions in this area.",Cloud computing | Cloud games | Cloud rendering | Distributed rendering | Qoe | Qos | Video streaming,ACM Computing Surveys,2015-09-29,Article,"Shi, Shu;Hsu, Cheng Hsin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921701362,10.1007/s10664-013-9277-5,HAZOP-based identification of events in use cases: An empirical study,"Completeness is one of the main quality attributes of requirements specifications. If functional requirements are expressed as use cases, one can be interested in event completeness. A use case is event complete if it contains description of all the events that can happen when executing the use case. Missing events in any use case can lead to higher project costs. Thus, the question arises of what is a good method of identification of events in use cases and what accuracy and review speed one can expect from it. The goal of this study was to check if (1) HAZOP-based event identification is more effective than ad hoc review and (2) what is the review speed of these two approaches. Two controlled experiments were conducted in order to evaluate ad hoc approach and H4U method to event identification. The first experiment included 18 students, while the second experiment was conducted with the help of 82 professionals. In both cases, accuracy and review speed of the investigated methods were measured and analyzed. Moreover, the usage of HAZOP keywords was analyzed. In both experiments, a benchmark specification based on use cases was used. The first experiment with students showed that a HAZOP-based review is more effective in event identification than ad hoc review and this result is statistically significant. However, the reviewing speed of HAZOP-based reviews is lower. The second experiment with professionals confirmed these results. These experiments showed also that event completeness is hard to achieve. It on average ranged from 0.15 to 0.26. HAZOP-based identification of events in use cases is an useful alternative to ad hoc reviews. It can achieve higher event completeness at the cost of an increase in effort.",Controlled experiment | HAZOP | Requirements engineering | Software quality | Use cases,Empirical Software Engineering,2015-02-01,Article,"Jurkiewicz, Jakub;Nawrocki, Jerzy;Ochodek, Mirosław;Głowacki, Tomasz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928708161,10.1007/s10664-014-9312-1,Do topics make sense to managers and developers?,"Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability and issue report traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted match the perception that industrial Program Managers and Developers have about the effort put into addressing certain topics. We then replicate this study again on Open Source Developers using issue reports from issue trackers instead of requirements, confirming our previous industrial conclusions. We found that efforts extracted as commits from version control systems relevant to a topic often matched the perception of the managers and developers of what actually occurred at that time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics and issue/bug report topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements and issues.",LDA | LDA topics | Requirements | Requirements topic | User study,Empirical Software Engineering,2015-04-01,Article,"Hindle, Abram;Bird, Christian;Zimmermann, Thomas;Nagappan, Nachiappan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928707280,10.1007/s10664-014-9311-2,Modelling the ‘hurried’ bug report reading process to summarize bug reports,"Although bug reports are frequently consulted project assets, they are communication logs, by-products of bug resolution, and not artifacts created with the intent of being easy to follow. To facilitate bug report digestion, we propose a new, unsupervised, bug report summarization approach that estimates the attention a user would hypothetically give to different sentences in a bug report, when pressed with time. We pose three hypotheses on what makes a sentence relevant: discussing frequently discussed topics, being evaluated or assessed by other sentences, and keeping focused on the bug report’s title and description. Our results suggest that our hypotheses are valid, since the summaries have as much as 12 % improvement in standard summarization evaluation metrics compared to the previous approach. Our evaluation also asks developers to assess the quality and usefulness of the summaries created for bug reports they have worked on. Feedback from developers not only shows the summaries are useful, but also points out important requirements for this, and any bug summarization approach, and indicates directions for future work.",Bug reports | Empirical study | Natural language processing | Productivity | Summarization,Empirical Software Engineering,2015-04-01,Article,"Lotufo, Rafael;Malik, Zeeshan;Czarnecki, Krzysztof",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84929522239,10.1007/s10664-013-9299-z,Achieving scalable mutation-based generation of whole test suites,"Without complete formal specification, automatically generated software tests need to be manually checked in order to detect faults. This makes it desirable to produce the strongest possible test set while keeping the number of tests as small as possible. As commonly applied coverage criteria like branch coverage are potentially weak, mutation testing has been proposed as a stronger criterion. However, mutation based test generation is hampered because usually there are simply too many mutants, and too many of these are either trivially killed or equivalent. On such mutants, any effort spent on test generation would per definition be wasted. To overcome this problem, our search-based EvoSuite test generation tool integrates two novel optimizations: First, we avoid redundant test executions on mutants by monitoring state infection conditions, and second we use whole test suite generation to optimize test suites towards killing the highest number of mutants, rather than selecting individual mutants. These optimizations allowed us to apply EvoSuite to a random sample of 100 open source projects, consisting of a total of 8,963 classes and more than two million lines of code, leading to a total of 1,380,302 mutants. The experiment demonstrates that our approach scales well, making mutation testing a viable test criterion for automated test case generation tools, and allowing us to analyze the relationship of branch coverage and mutation testing in detail.",Mutation testing | Search-based testing | Test case generation | Testing classes | Unit testing,Empirical Software Engineering,2015-06-15,Article,"Fraser, Gordon;Arcuri, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930540420,10.1007/s10664-014-9317-9,The impact of imperfect change rules on framework API evolution identification: an empirical study,"Software frameworks keep evolving. It is often time-consuming for developers to keep their client code up-to-date. Not all frameworks have documentation about the upgrading process. Many approaches have been proposed to ease the impact of non-documented framework evolution on developers by identifying change rules between two releases of a framework, but these change rules are imperfect, i.e., not 100 % correct. To the best of our knowledge, there is no empirical study to show the usefulness of these imperfect change rules. Therefore, we design and conduct an experiment to evaluate their impact. In the experiment, the subjects must find the replacements of 21 missing methods in the new releases of three open-source frameworks with the help of (1) all-correct, (2) imperfect, and (3) no change rules. The statistical analysis results show that the precision of the replacements found by the subjects with the three sets of change rules are significantly different. The precision with all-correct change rules is the highest while that with no change rules is the lowest, while imperfect change rules give a precision in between. The effect size of the difference between the subjects with no and imperfect change rules is large and that between the subjects with imperfect and correct change rules is moderate. The results of this study show that the change rules generated by framework API evolution approaches do help developers, even they are not always correct. The imperfect change rules can be used by developers upgrading their code when documentation is not available or as a complement to partial documentation. The moderate difference between results from subjects with imperfect and all-correct change rules also suggests that improving precision of change rules will still help developers.",Change rule | Framework API evolution | Software maintenance | Usefulness,Empirical Software Engineering,2015-08-08,Article,"Wu, Wei;Serveaux, Adrien;Guéhéneuc, Yann Gaël;Antoniol, Giuliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84940718612,10.1007/s10664-014-9325-9,How the Apache community upgrades dependencies: an evolutionary study,"Software ecosystems consist of multiple software projects, often interrelated by means of dependency relations. When one project undergoes changes, other projects may decide to upgrade their dependency. For example, a project could use a new version of a component from another project because the latter has been enhanced or subject to some bug-fixing activities. In this paper we study the evolution of dependencies between projects in the Java subset of the Apache ecosystem, consisting of 147 projects, for a period of 14 years, resulting in 1,964 releases. Specifically, we investigate (i) how dependencies between projects evolve over time when the ecosystem grows, (ii) what are the product and process factors that can likely trigger dependency upgrades, (iii) how developers discuss the needs and risks of such upgrades, and (iv) what is the likely impact of upgrades on client projects. The study results—qualitatively confirmed by observations made by analyzing the developers’ discussion—indicate that when a new release of a project is issued, it triggers an upgrade when the new release includes major changes (e.g., new features/services) as well as large amount of bug fixes. Instead, developers are reluctant to perform an upgrade when some APIs are removed. The impact of upgrades is generally low, unless it is related to frameworks/libraries used in crosscutting concerns. Results of this study can support the understanding of the of library/component upgrade phenomenon, and provide the basis for a new family of recommenders aimed at supporting developers in the complex (and risky) activity of managing library/component upgrade within their software projects.",Mining software repositories | Project dependency upgrades | Software ecosystems,Empirical Software Engineering,2015-10-04,Article,"Bavota, Gabriele;Canfora, Gerardo;Di Penta, Massimiliano;Oliveto, Rocco;Panichella, Sebastiano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84944346113,10.1007/s10664-014-9323-y,Recommending reference API documentation,"Reference documentation is an important source of information on API usage. However, information useful to programmers can be buried in irrelevant text, or attached to a non-intuitive API element, making it difficult to discover. We propose to detect and recommend fragments of API documentation potentially important to a programmer who has already decided to use a certain API element. We categorize text fragments in API documentation based on whether they contain information that is indispensable, valuable, or neither. From the fragments that contain knowledge worthy of recommendation, we extract word patterns, and use these patterns to automatically find new fragments that contain similar knowledge in unseen documentation. We implemented our technique in a tool, Krec, that supports both information filtering and discovery. In an evaluation study with randomly-sampled method definitions from ten open source systems, we found that with a training set derived from about 1000 documentation units, we could issue recommendations with 90 % precision and 69 % recall. In a study involving ten independent assessors, indispensable knowledge items recommended for API types were judged useful 57 % of the time and potentially useful an additional 30 % of the time.",API documentation | Application programming interfaces | Natural language processing | Recommendation systems | Text classification,Empirical Software Engineering,2015-12-01,Article,"Robillard, Martin P.;Chhetri, Yam B.",Include,
10.1016/j.jss.2022.111515,2-s2.0-84944355850,10.1007/s10664-014-9343-7,Charting the API minefield using software telemetry data,"Programs draw significant parts of their functionality through the use of Application Programming Interfaces (APIs). Apart from the way developers incorporate APIs in their software, the stability of these programs depends on the design and implementation of the APIs. In this work, we report how we used software telemetry data to analyze the causes of API failures in Android applications. Specifically, we got 4.9 gb worth of crash data that thousands of applications sent to a centralized crash report management service. We processed that data to extract approximately a million stack traces, stitching together parts of chained exceptions, and established heuristic rules to draw the border between applications and the API calls. We examined a set of more than a half million stack traces associated with risky API calls to map the space of the most common application failure reasons. Our findings show that the top ones can be attributed to memory exhaustion, race conditions or deadlocks, and missing or corrupt resources. Given the classes of the crash causes we identified, we recommend API design and implementation choices, such as specific exceptions, default resources, and non-blocking algorithms, that can eliminate common failures. In addition, we argue that development tools like memory analyzers, thread debuggers, and static analyzers can prevent crashes through early code testing and analysis. Finally, some execution platform and framework designs for process and memory management can also eliminate some application crashes.",Application programming interfaces | Mobile applications | Reliability | Stack traces,Empirical Software Engineering,2015-12-01,Article,"Kechagia, Maria;Mitropoulos, Dimitris;Spinellis, Diomidis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84941285696,10.1016/j.jss.2015.07.039,Ahab's legs in scenario-based requirements validation: An experiment to study communication mistakes,"The correct identification of requirements is a crucial step for the implementation of a satisfactory software system. In the validation of requirements with scenarios, a straightforward communication is central to obtain a good participation from stakeholders. Technical specifications are translated into scenarios to make them concrete and easy to understand for non-technical users, and contextual details are added to encourage user engagement. However, additional contextual details (Ahab's legs) could generate a negative impact on the requirements' validation by leading to proliferating comments that are not pertinent to session objective. The objective of this study is to evaluate the impact of Ahab's leg to scenario-based requirement validation sessions. We conducted a controlled experiment with human participants and measured the pertinence of the comments formulated by participants when discussing the requirements. The results of our experiment suggest that the potentially negative impact of Ahab's leg can be effectively controlled by the analyst.",Human factors of requirement Engineering | Requirement validation,Journal of Systems and Software,2015-11-01,Article,"Sabatucci, Luca;Ceccato, Mariano;Marchetto, Alessandro;Susi, Angelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84941266999,10.1016/j.jss.2015.08.001,A Model-Driven approach for functional test case generation,"Test phase is one of the most critical phases in software engineering life cycle to assure the final system quality. In this context, functional system test cases verify that the system under test fulfills its functional specification. Thus, these test cases are frequently designed from the different scenarios and alternatives depicted in functional requirements. The objective of this paper is to introduce a systematic process based on the Model-Driven paradigm to automate the generation of functional test cases from functional requirements. For this aim, a set of metamodels and transformations and also a specific language domain to use them is presented. The paper finishes stating learned lessons from the trenches as well as relevant future work and conclusions that draw new research lines in the test cases generation context.",Early testing | Model-Driven testing | Software quality assurance,Journal of Systems and Software,2015-11-01,Article,"Gutiérrez, J. J.;Escalona, M. J.;Mejías, M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84944061567,10.1016/j.jss.2015.08.018,Effective and efficient detection of software theft via dynamic API authority vectors,"Software theft has become a very serious threat to both the software industry and individual software developers. A software birthmark indicates unique characteristics of a program in question, which can be used for analyzing the similarity of a pair of programs and detecting theft. This paper proposes a novel birthmark, a dynamic API authority vector (DAAV). DAAV satisfies four essential requirements for good birthmarks - credibility, resiliency, scalability, and packing-free - while existing static birthmarks are unable to handle the packed programs and existing dynamic birthmarks do not satisfy credibility and resiliency. Through our extensive experiments with a set of Windows applications, DAAV is shown to have not only the credibility and resiliency higher than the existing dynamic birthmarks but also the accuracy comparable to that of existing static birthmarks. This result indicates that our proposed birthmark provides high accuracy and also covers packed programs successfully in detecting software theft.",Birthmark | Similarity analysis | Software theft detection,Journal of Systems and Software,2015-12-01,Article,"Chae, Dong Kyu;Kim, Sang Wook;Cho, Seong Je;Kim, Yesol",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84944029939,10.1016/j.jss.2015.08.026,Evolution of software in automated production systems: Challenges and research directions,"Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems.",Automated production systems | Automation | Evolution | Software engineering,Journal of Systems and Software,2015-12-01,Article,"Vogel-Heuser, Birgit;Fay, Alexander;Schaefer, Ina;Tichy, Matthias",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84944076649,10.1016/j.jss.2015.07.040,A goal-oriented approach for representing and using design patterns,"Design patterns are known as proven solutions to recurring design problems. The role of pattern documentation format is to transfer experience thus making pattern employment a viable technique. This research line proposes a goal-oriented pattern documentation that highlights decision-relevant information. The contribution of this paper is twofold. First, it presents a semi-structural visual notation that visualizes context, forces, alternative solutions and consequences in a compact format. Second, it introduces a systematic reuse process, in which the use of goal-oriented patterns aids the practitioner in selecting and customizing design patterns. An empirical study has been conducted the results of which supports the hypothesis that the goal-oriented format provides benefits for the practitioner. The experiment revealed a trend in which solutions better address requirements when the subjects are equipped with the new pattern documentation.",Design patterns | Goal modeling | Goal reasoning,Journal of Systems and Software,2015-12-01,Article,"Sabatucci, Luca;Cossentino, Massimo;Susi, Angelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84912534729,10.1016/j.jss.2014.09.042,"Cost, benefits and quality of software development documentation: A systematic mapping","Context: Software documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit. Objectives: In this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality. Method: We use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively. Results: We present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1-11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one ""System under Study"" (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study. Conclusion: Our study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies; (2) studies involving large-scale development projects, or from large number of study participants of various organizations; (3) more industry-academia collaborations; (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost.",Documentation benefit | Software documentation | Systematic mapping,Journal of Systems and Software,2015-01-01,Article,"Zhi, Junji;Garousi-Yusifoʇlu, Vahid;Sun, Bo;Garousi, Golara;Shahnewaz, Shawn;Ruhe, Guenther",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-84919383927,10.1016/j.jss.2014.10.010,A controlled experiment to evaluate the understandability of KAOS and i∗ for modeling Teleo-Reactive systems,"Context Teleo-Reactive (TR) specifications allow engineers to define the behavior of reactive systems while taking into account goals and changes in the state of the environment. Objective This article evaluates two different Goal Oriented Requirements Engineering notations, i∗ and KAOS, to determine their understandability level for specifying TR systems. Method A controlled experiment was performed by two groups of Bachelor students. Each group first analyzed a requirements model of a mobile robotic system, specified using one of the evaluated languages, and then they filled in a questionnaire to evaluate its understandability. Afterwards, each group proceeded similarly with the model of another system specified with the second language. Results The statistical analysis of the data obtained by means of the experiment showed that the understandability of i∗ is higher than that of KAOS when modeling TR systems. Conclusion Both languages are suitable for specifying TR systems although their notations should be specialized to maximize the understandability attribute. i∗ surpasses KAOS due to two main reasons: i∗ models represent dependencies between agents and goals or tasks; and notational differences between tasks and goals in i∗ are more evident than those between goals and requirements in KAOS.",Controlled experiment | Requirements engineering | Teleo-Reactive,Journal of Systems and Software,2015-02-01,Article,"Morales, José Miguel;Navarro, Elena;Sánchez, Pedro;Alonso, Diego",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84919339677,10.1016/j.jss.2014.10.014,Web API growing pains: Loosely coupled yet strongly tied,"Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution.",API | Breaking changes | Software evolution | Web,Journal of Systems and Software,2015-02-01,Article,"Espinha, Tiago;Zaidman, Andy;Gross, Hans Gerhard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84919344103,10.1016/j.jss.2014.10.015,Enhanced healthcare personnel rostering solution using mobile technologies,"This paper presents a novel personnel rostering system for healthcare units, which incorporates mobile technologies to minimize time overheads and boost personnel satisfaction. This way, doctors nurses and administrative staff may provide solutions and suggestions to the process of shifts' scheduling and rostering in a group based - social and organized manner, at any given time, using their smartphone or tablet. This system is designed and implemented according to wide research on requirements' specification, carried out in Greek public hospitals and based on a study of healthcare units' organization, at a practical and legal level. The personnel rostering system anticipates to facilitate the staff administration task, through real-time communication between hospital's personnel. It enables the formation of a micro-community with enhanced social communication tools, to provide dynamic management, recording and updating of changes that occur in scheduled duties, without mediators and delays. The proposed solution includes an intelligent mobile device application, designed for smartphones and tablets. It is provided to the personnel and enables them to participate in the process of scheduling duties and shifts. The XML based, back-end, supporting information system offers services that allow a smoother operation of the unit, minimize time overheads in case of arbitrary changes and maximize satisfaction of personnel. The overall operation of the units, that reclaim the features offered by this system, can be improved. Minimizing the time and other bureaucratic delays in personnel scheduling is a vital part of the way a healthcare facility is organized. Thus, facilitating this process, with any available technology, may prove to be cost effective and crucial. Systems that incorporate mobile applications are already widely accepted, and become increasingly important to the healthcare sector, as well. The mobile based, personnel shifts' scheduling solution shown is an approach that already receives encouraging support and indicates that it assists in achieving remarkable results.",Health information systems | Mobile applications | Personnel scheduling,Journal of Systems and Software,2015-02-01,Article,"Paschou, Mersini;Papadimitiriou, Christos;Nodarakis, Nikolaos;Korezelidis, Konstantinos;Sakkopoulos, Evangelos;Tsakalidis, Athanasios",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84919452028,10.1016/j.jss.2014.10.036,Integrating usability work into a large inter-organisational agile development project: Tactics developed by usability designers,"In this paper we examine the integration of usability activities into a large inter-organisational agile development project. Inter-organisational agile projects possess unique attributes. They involve multiple stakeholders from different organisational contexts and are thus characterised by competing priorities. Team members also lack a mutual awareness of what constitutes work. These issues make the collaboration between project teams challenging. Meanwhile collaboration between usability designers and agile project teams is an integral part of the integration of usability activities into agile development projects. We carried out an interpretive case study on a large inter-organisational agile development project to examine how usability designers and agile project teams collaborate in this project type. Results showed integration goals were achieved through five tactics deployed by the usability designers. These tactics were negotiating inclusion; upward influencing, placating expert users, establishing credibility and diffusing designs. The implications of these findings are summarised in the form of three propositions that pertain to how usability designer-agile project team collaborations might be organised in agile development projects. Further, the role of the usability designer in ensuring the integration of usability activities is also emphasised.",Agile software development | Tactics | Usability,Journal of Systems and Software,2015-02-01,Article,"Wale-Kolade, Adeola Yetunde",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84919360062,10.1016/j.jss.2014.10.031,Profiling and classifying the behavior of malicious codes,"Malware is a major security threat confronting computer systems and networks and has increased in scale and impact from the early days of ICT. Traditional protection mechanisms are largely incapable of dealing with the diversity and volume of malware variants which is evident today. This paper examines the evolution of malware including the nature of its activity and variants, and the implication of this for computer security industry practices. As a first step to address this challenge, I propose a framework to extract features statically and dynamically from malware that reflect the behavior of its code such as the Windows Application Programming Interface (API) calls. Similarity based mining and machine learning methods have been employed to profile and classify malware behaviors. This method is based on the sequences of API sequence calls and frequency of appearance. Experimental analysis results using large datasets show that the proposed method is effective in identifying known malware variants, and also classifies malware with high accuracy and low false alarm rates. This encouraging result indicates that classification is a viable approach for similarity detection to help detect malware. This work advances the detection of zero-day malware and offers researchers another method for understanding impact.",Cybercrime | Malware | Profiling,Journal of Systems and Software,2015-02-01,Article,"Alazab, Mamoun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84919343973,10.1016/j.jss.2014.10.054,A benchmarking process to assess software requirements documentation for space applications,"Poorly written requirements are a common source of software defects and, in application areas like space systems, the cost of malfunctioning software can be very high. This work proposes a benchmarking procedure for assessing the quality of software requirements that adopt the Packet Utilization Standard (PUS) defined by the European Cooperation for Space Standardization (ECSS) standards. The benchmark uses three checklists that aim at guaranteeing that the specifications comply with the PUS standard, consider faulty behaviour, and do not include errors typically found in this type of documents. The benchmark is defined for two services of the PUS standard: the telecommand verification and on board operating scheduling. A benchmark validation approach is also proposed in the paper. It uses the concept of fault injection to insert known errors in software requirements specification documents. The benchmark validation is performed through its application to three projects from different countries. Results show that our proposal provides a simple and effective way for identifying weaknesses and compare the degree of maturity of requirements documents.",Benchmarking | ECSS standards | Software requirements quality,Journal of Systems and Software,2015-02-01,Article,"Véras, Paulo C.;Villani, Emilia;Ambrosio, Ana Maria;Vieira, Marco;Madeira, Henrique",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84919362638,10.1016/j.jss.2014.10.038,Extracting REST resource models from procedure-oriented service interfaces,"During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-∗ stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service descriptions are analyzed, using natural language processing techniques and graph transformations, in order to yield a collection of hierarchically organized elements forming REST resources that semantically correspond to the functionality offered by the service. The proposed approach has been applied as a proof of concept with positive results, for the extraction of resource models from a sizable number of procedure-oriented Web Service interfaces that have been obtained from an open service directory.",REST | Service oriented architectures | Software reengineering,Journal of Systems and Software,2015-02-01,Article,"Athanasopoulos, Michael;Kontogiannis, Kostas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921634197,10.1016/j.jss.2014.11.041,An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review,"Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessmentmethods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience.",Assessment method design | Software process assessment | Systematic literature review,Journal of Systems and Software,2015-03-01,Article,"Zarour, Mohammad;Abran, Alain;Desharnais, Jean Marc;Alarifi, Abdulrahman",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84923178582,10.1016/j.jss.2014.12.042,Semi-automatic architectural pattern identification and documentation using architectural primitives,"In this article, we propose an interactive approach for the semi-automatic identification and documentation of architectural patterns based on a domain-specific language. To address the rich concepts and variations of patterns, we firstly propose to support pattern description through architectural primitives. These are primitive abstractions at the architectural level that can be found in realizations of multiple patterns, and they can be leveraged by software architects for pattern annotation during software architecture documentation or reconstruction. Secondly, using these annotations, our approach automatically suggests possible pattern instances based on a reusable catalog of patterns and their variants. Once a pattern instance has been documented, the annotated component models and the source code get automatically checked for consistency and traceability links are automatically generated. To study the practical applicability and performance of our approach, we have conducted three case studies for existing, non-trivial open source systems.",Architectural component views | Architectural pattern | Software architecture,Journal of Systems and Software,2015-04-01,Article,"Haitzer, Thomas;Zdun, Uwe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84929158133,10.1016/j.jss.2015.03.066,Input-based adaptive randomized test case prioritization: A local beam search approach,"Test case prioritization assigns the execution priorities of the test cases in a given test suite. Many existing test case prioritization techniques assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in real-world software development projects. This paper proposes a novel family of input-based local-beam-search adaptive-randomized techniques. They make adaptive tree-based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite. We report a validation experiment on a suite of four medium-size benchmarks. The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code-coverage-based greedy or search-based prioritization techniques, including Genetic, Greedy and ART, in both our controlled experiment and case study. Our techniques are also significantly more efficient than the Genetic and Greedy, but are less efficient than ART.",Adaptive test case prioritization | Randomized algorithm | Regression testing,Journal of Systems and Software,2015-07-01,Article,"Jiang, Bo;Chan, W. K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930801342,10.1016/j.jss.2015.04.062,Information infrastructure risk prediction through platform vulnerability analysis,"Abstract The protection of information infrastructures is important for the function of other infrastructure sectors. As vital parts for the information infrastructure operation, software-based platforms, face a series of vulnerabilities and threats. This paper aims to provide a complementary approach to existing vulnerability prediction solutions and launch the measurement of zero-day risk by introducing a risk prediction methodology for an information infrastructure. The proposed methodology consists of four steps and utilizes the outcomes of a proper analysis of security measurements provided by specifications from the Security Content Automation Protocol. First, we identify software platform assets that support an information infrastructure and second we measure the historical rate of vulnerability occurrences. Third, we use a distribution fitting procedure to estimate the statistical correlation between empirical and reference probability distributions and verify the statistical significance of the distribution fitting results with the Kolmogorov - Smirnov test. Fourth, we develop conditional probability tables that constitute a Bayesian Belief Network topology as means to enable risk prediction and estimation on security properties. The practicality of the risk prediction methodology is demonstrated with an implementation example from the electronic banking sector. The contribution of the proposed methodology is to provide auditors with a proactive approach about zero-day risks.",Bayesian belief network (BBN) | Kolmogorov-Smirnov test | Zero-day risk,Journal of Systems and Software,2015-08-01,Article,"Chatzipoulidis, Aristeidis;Michalopoulos, Dimitrios;Mavridis, Ioannis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930795664,10.1016/j.jss.2015.04.066,An exploratory study on exception handling bugs in Java programs,"Abstract Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers' perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat. Only 27% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70% of the organizations there are no specific tests for the exception handling code. Also, 61% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40% of the respondents consider the quality of exception handling code to be either good or very good and only 14% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs. Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools.",Bugs | Exception handling | Repository mining,Journal of Systems and Software,2015-08-01,Article,"Ebert, Felipe;Castor, Fernando;Serebrenik, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930792473,10.1016/j.jss.2015.04.089,Toward the tools selection in model based system engineering for embedded systems - A systematic literature review,"Abstract Model based system engineering (MBSE) is a systematic approach of modeling which is frequently used to support requirement specification, design, verification and validation activities of system development. However, it is difficult to customize MBSE approach for the development of embedded systems due to their diverse behavioral aspects. Furthermore, appropriate tools selection to perform particular MBSE activities is always challenging. This paper focuses on the identification and classification of recent research practices pertaining to embedded systems development through MBSE approach. Consequently, a comprehensive analysis of various MBSE tools has been presented. Systematic literature review (SLR) has been used to identify 61 research practices published during 2008-2014. The identified researches have been classified into six different categories to analyze various aspects of MBSE approach for embedded systems. Consequently, 39 preliminary tools are identified that have been used in recent researches. Furthermore, classification and evaluation of tools have been presented. This research highlights important trends and approaches of MBSE to support development of embedded systems. A comprehensive investigation of tools in this article facilitates researchers, practitioners and developers to select appropriate tools according to their requirements.",Embedded systems | MBSE | Tools,Journal of Systems and Software,2015-08-01,Article,"Rashid, Muhammad;Anwar, Muhammad Waseem;Khan, Aamir M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922680287,10.1016/j.infsof.2014.06.002,"The impact of global dispersion on coordination, team performance and software quality-A systematic literature review","Context: Global software development (GSD) contains different context setting dimensions, which are essential for effective teamwork and success of projects. Although considerable research effort has been made in this area, as yet, no agreement has been reached about the impact of these dispersion dimensions on team coordination and project outcomes. Objective: This paper summarizes empirical evidence on the impact of global dispersion dimensions on coordination, team performance and project outcomes. Method: We performed a systematic literature review of 46 publications from 25 journals and 19 conference and workshop proceedings, which were published between 2001 and 2013. Thematic analysis was used to identify global dimensions and their measures. Vote counting was used to decide on the impact trends of dispersion dimensions on team performance and software quality. Results: Global dispersion dimensions are consistently conceptualized, but quantified in many different ways. Different dispersion dimensions are associated with a distinct set of coordination challenges. Overall, geographical dispersion tends to have a negative impact on team performance and software quality. Temporal dispersion tends to have a negative impact on software quality, but its impact on team performance is inconsistent and can be explained by type of performance. Conclusion: For researchers, we reveal several opportunities for future research, such as coordination challenges in inter-organizational software projects, impact of processes and practices mismatches on project outcomes, evolution of coordination needs and mechanism over time and impact of dispersion dimensions on open source project outcomes. For practitioners, they should consider the tradeoff between cost and benefits while dispersing tasks, alignment impact of dispersion dimensions with individual and organizational objectives, coordination mechanisms as situational approaches and collocation of development activities of high quality demand components in GSD projects.",Global dispersion | Global software development | Meta analysis | Performance | Software quality | Systematic literature review,Information and Software Technology,2015-01-01,Conference Paper,"Nguyen-Duc, Anh;Cruzes, Daniela S.;Conradi, Reidar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922597745,10.1016/j.infsof.2014.05.019,Shared service recommendations from requirement specifications: A hybrid syntactic and semantic toolkit,"Context: Software Requirement Specifications (SRSs) are central to software lifecycles. An SRS defines the functionalities and constraints of a desired software system, hence it often serves as reference for further development. Software lifecycles concerned with the conversion of traditional systems into more serviceoriented infrastructures can benefit from understanding potential shared capabilities through the analysis of SRSs. Objective: In this paper, we propose an automated approach capable of recommending shared software services from multiple text-based SRSs created by different organizations. Our goal is to facilitate the identification of overlapping requirements in these specifications and subsequently recommend shared components, which promotes software reuse. The shared components can be implemented as services that are invoked across different systems. Method: Our approach leverages the syntactic similarity of the SRS text augmented with semantic information derived from the WordNet database. This work extends our earlier studies by introducing an algorithm that utilizes noun, verb, and predicate relations to enhance the discovery of equivalent requirements and the recommendation of reusable services. A prototype system is implemented to evaluate the approach and experimental results have shown effective recommendation of requirements and their realized shared services. Results: Our automatic recommendation approach generates recommendations in few minutes compared to 9 h when services are manually inspected by developers. Our approach is also able to recommend services that are overlooked by the same developers, and to identify similarity between requirements even if these requirements are reworded. Conclusion: We show through experimentation that we can efficiently recommend services by leveraging both the syntactical structure and the semantic information of a requirements document and that our approach is more effective than the manual selection of services by experts. We also show that our approach is effective in detecting similar requirements for a single system and hence discovering opportunities for software reuse.",Requirements engineering | Shared service discovery | Similarity analysis,Information and Software Technology,2015-01-01,Conference Paper,"Blake, M. Brian;Saleh, Iman;Wei, Yi;Schlesinger, Ian D.;Yale-Loehr, Alexander;Liu, Xuanzhe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84913582085,10.1016/j.infsof.2014.10.002,Semantics for consistent activation in context-oriented systems,"Context: Context-oriented programming languages provide dedicated programming abstractions to define behavioral adaptations and means to combine those adaptations dynamically according to sensed context changes. Some of these languages feature programming abstractions to explicitly define interaction dependencies among contexts. However, the semantics of context activation and the meaning of dependency relations have been described only informally, which in some cases has led to incorrect specifications, faulty implementations and inconsistent system behavior. Objective: With the aim of avoiding faulty implementations and inconsistencies during system execution, this paper proposes both a formal and run-time model of contexts, context activation and context interaction. Method: As a formal and computational basis, we introduce context Petri nets, a model based on Petri nets, which we found to match closely the structure of contexts in context-oriented systems. The operational semantics of Petri nets permits the modeling of run-time context activations. Existing Petri net analyses allow us to reason about system properties. As validation, we carried out small and medium-sized case studies. Results: In the cases explored, context Petri nets served effectively as underlying run-time model to ensure that declared context interaction constraints remain consistent during context manipulation. Moreover, context Petri nets enabled us to analyze certain properties regarding the activation state of particular contexts. Conclusion: Context Petri nets thus proved to be appropriate to encode and manage the semantics of context activation, both formally and computationally, so as to preserve the consistency of context-oriented systems.",Context-oriented programming | Dynamic behavior adaptation | Petri nets | Program semantics | Self-adaptiveness,Information and Software Technology,2015-02-01,Article,"Cardozo, Nicolás;González, Sebastián;Mens, Kim;Van Der Straeten, Ragnhild;Vallejos, Jorge;D'Hondt, Theo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942063258,10.1016/j.infsof.2015.06.006,On organisational influences in software standards and their open source implementations,"Context It is widely acknowledged that standards implemented in open source software can reduce risks for lock-in, improve interoperability, and promote competition on the market. However, there is limited knowledge concerning the relationship between standards and their implementations in open source software. This paper reports from an investigation of organisational influences in software standards and open source software implementations of software standards. The study focuses on the RDFa standard and its implementation in the Drupal project. Objective The overarching goal of the study is to establish organisational influences in software standards and their implementations in open source software. More specifically, our objective is to establish organisational influences in the RDFa standard and its implementation in the Drupal project. Method By conduct of a case study of the RDFa standard and its implementation in the Drupal project we investigate organisational influences in software standards and their implementations in open source software. Specifically, the case study involved quantitative analyses of issue tracker data for different issue trackers for W3C RDFa and the Drupal implementation of RDFa. Results The case study provides details on how and to what extent organisational influences occur in W3C RDFa and the Drupal implementation of RDFa, by specifically providing a characterisation of issues and results concerning contribution to issue raising and commenting, organisational involvement over time, and individual and organisational collaboration on issues. Conclusion We find that widely deployed standards can benefit from contributions provided by a range of different individuals, organisations, and types of organisations either directly to a standardisation project or indirectly via an open source project implementing the standard. Further, we also find that open processes for standardisation adopted by W3C may also contribute to open source projects implementing specific standards.",Case study | Drupal | Open source | Open standard | Organisational influence | RDFa,Information and Software Technology,2015-11-01,Conference Paper,"Gamalielsson, Jonas;Lundell, Björn;Feist, Jonas;Gustavsson, Tomas;Landqvist, Fredric",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922572514,10.1016/j.infsof.2014.08.003,Usage and usefulness of technical software documentation: An industrial case study,"Context: Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the value, degree of usage and usefulness of documentation during development and maintenance. Objective: Motivated by the needs of NovAtel Inc. (NovAtel), a world-leading company developing software systems in support of global navigation satellite systems, and based on the results of a former systematic mapping study, we aimed at better understanding of the usage and the usefulness of various technical documents during software development and maintenance. Method: We utilized the results of a former systematic mapping study and performed an industrial case study at NovAtel. From the joint definition of the analysis goals, the research method incorporates qualitative and quantitative analysis of 55 documents (design, test and process related) and 1630 of their revisions. In addition, we conducted a survey on the usage and usefulness of documents. A total of 25 staff members from the industrial partner, all having a medium to high level of experience, participated in the survey. Results: In the context of the case study, a number of findings were derived. They include that (1) technical documentation was consulted least frequently for maintenance purpose and most frequently as an information source for development, (2) source code was considered most frequently as the preferred information source during software maintenance, (3) there is no significant difference between the usage of various documentation types during both development and maintenance, and (4) initial hypotheses stating that up-to-date information, accuracy and preciseness have the highest impact on usefulness of technical documentation. Conclusions: It is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the tasks to be completed (e.g., development and maintenance). The results have been confirmed to be helpful for the company under study, and the firm is currently implementing some of the recommendations given.",Case study | Industrial context | Technical software documentation | Usage | Usefulness,Information and Software Technology,2015-01-01,Conference Paper,"Garousi, Golara;Garousi-Yusifoʇlu, Vahid;Ruhe, Guenther;Zhi, Junji;Moussavi, Mahmoud;Smith, Brian",Include,
10.1016/j.jss.2022.111515,2-s2.0-84921053895,10.1016/j.infsof.2014.11.003,"Synergy between Activity Theory and goal/scenario modeling for requirements elicitation, analysis, and evolution","Context It is challenging to develop comprehensive, consistent, analyzable requirements models for evolving requirements. This is particularly critical for certain highly interactive types of socio-technical systems that involve a wide range of stakeholders with disparate backgrounds; system success is often dependent on how well local social constraints are addressed in system design. Objective This paper describes feasibility research, combining a holistic social system perspective provided by Activity Theory (AT), a psychological paradigm, with existing system development methodologies and tools, specifically goal and scenario modeling. Method AT is used to understand the relationships between a system, its stakeholders, and the system's evolving context. The User Requirements Notation (URN) is used to produce rigorous, analyzable specifications combining goal and scenario models. First, an AT language was developed constraining the framework for automation, second consistency heuristics were developed for constructing and analyzing combined AT/URN models, third a combined AT/URN methodology was developed, and consequently applied to a proof-of-concept system. Results An AT language with limited tool support was developed, as was a combined AT/URN methodology. This methodology was applied to an evolving disease management system to demonstrate the feasibility of adapting AT for use in system development with existing methodologies and tools. Bi-directional transformations between the languages allow proposed changes in system design to be propagated to AT models for use in stakeholder discussions regarding system evolution. Conclusions The AT framework can be constrained for use in requirements elicitation and combined with URN tools to provide system designs that include social system perspectives. The developed AT/URN methodology can help engineers to track the impact on system design due to requirement changes triggered by changes in the system's social context. The methodology also allows engineers to assess the impact of proposed system design changes on the social elements of the system context.",Activity Theory | Goal modeling | Requirements engineering | Scenario modeling | User Requirements Notation,Information and Software Technology,2015-03-01,Article,"Georg, Geri;Mussbacher, Gunter;Amyot, Daniel;Petriu, Dorina;Troup, Lucy;Lozano-Fuentes, Saul;France, Robert",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84929517039,10.1016/j.infsof.2015.02.014,How Java APIs break - An empirical study,"Context It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs. Objective We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and the impact this has on programs using these libraries. Method This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for API analysis, this tool is based on the popular ASM byte code analysis library. Results We found that for most of the programs we investigated, APIs are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API. Conclusion It is concluded that API instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution.",API evolution | Backward compatibility | Binary compatibility | Byte-code | Java,Information and Software Technology,2015-09-01,Article,"Jezek, Kamil;Dietrich, Jens;Brada, Premek",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922665274,10.1016/j.infsof.2014.06.006,Generating optimized configurable business process models in scenarios subject to uncertainty,"Context: The quality of business process models (i.e., software artifacts that capture the relations between the organizational units of a business) is essential for enhancing the management of business processes. However, such modeling is typically carried out manually. This is already challenging and time consuming when (1) input uncertainty exists, (2) activities are related, and (3) resource allocation has to be considered. When including optimization requirements regarding flexibility and robustness it becomes even more complicated potentially resulting into non-optimized models, errors, and lack of flexibility. Objective: To facilitate the human work and to improve the resulting models in scenarios subject to uncertainty, we propose a software-supported approach for automatically creating configurable business process models from declarative specifications considering all the aforementioned requirements. Method: First, the scenario is modeled through a declarative language which allows the analysts to specify its variability and uncertainty. Thereafter, a set of optimized enactment plans (each one representing a potential execution alternative) are generated from such a model considering the input uncertainty. Finally, to deal with this uncertainty during run-time, a flexible configurable business process model is created from these plans. Results: To validate the proposed approach, we conduct a case study based on a real business which is subject to uncertainty. Results indicate that our approach improves the actual performance of the business and that the generated models support most of the uncertainty inherent to the business. Conclusions: The proposed approach automatically selects the best part of the variability of a declarative specification. Unlike existing approaches, our approach considers input uncertainty, the optimization of multiple objective functions, as well as the resource and the control-flow perspectives. However, our approach also presents a few limitations: (1) it is focused on the control-flow and the data perspective is only partially addressed and (2) model attributes need to be estimated.",Configurable business process models | Constraint programming | Constraint-based business process models | Flexibility | Planning and scheduling | Robustness,Information and Software Technology,2015-01-01,Conference Paper,"Jiménez-Ramírez, Andrés;Weber, Barbara;Barba, Irene;Del Valle, Carmelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84913554456,10.1016/j.infsof.2014.09.011,Automated events identification in use cases,"Context: Use cases are a popular method of expressing functional requirements. One contains a main scenario and a set of extensions, each consisting of an event and an alternative sequence of activities. Events omitted in requirements specification can lead to rework. Unfortunately, as it follows from the previous research, manual identification of events is rather ineffective (less than 1/3 of events are identified) and it is slow. Objective: The goal of this paper is to propose an automatic method of identification of events in use cases and evaluate its quality. Method: Each step of a main scenario is analyzed by a sequence of NLP tools to identify its performer, activity type and information object. It has been observed that performer, activity type and some attributes of information objects determine types of events that can occur when that activity is performed. That empirical knowledge is represented as a set of axioms and two inference rules have been proposed which allow to identify types of possible events. For each event type an NLG pattern is proposed which allows to generate description of the event type in natural language. The proposed method was compared with two manual approaches to identification of events: ad hoc and HAZOP-based. Also a kind of Turing test was performed to evaluate linguistic quality of generated descriptions. Results: Accuracy of the proposed method is about 80% (for manual approaches it is less than 1/3) and its speed is about 11 steps/minute (ad hoc approach is 4 times slower, and HAZOP-based approach is 20 times slower). Understandability of the generated event descriptions was not worse than understandability of the descriptions written by humans. Conclusions: The proposed method could be used to enhance contemporary tools for managing use cases.",Functional requirements | Natural language processing | Requirements engineering | Use cases,Information and Software Technology,2015-02-01,Article,"Jurkiewicz, J.;Nawrocki, J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928139326,10.1016/j.infsof.2015.03.006,Two controlled experiments on model-based architectural decision making,"Context In recent years, architectural design decisions are becoming more and more common for documenting software architectures. Rather than describing the structure of software systems, architectural decisions capture the design rationale and - often reusable - architectural knowledge. Many approaches and tools have been proposed in the literature to support architectural decision making and documentation (for instance, based on models, ontologies, or templates). In this context, the capturing, organization, and effective reuse of architectural knowledge has gained a lot of attention. Objective However, there is little empirical evidence about the supportive effect of reusable architectural knowledge on the effectiveness and efficiency of architectural decision making. Method To investigate these aspects, we conducted two separate controlled experiments with software architecture students in which we tested the supportive effect of reusable decision models in decision making and documentation. Results Our results show that the use of reusable decision models can significantly increase both the efficiency and the effectiveness of novice architects. Conclusion We can report, that our findings are in line with similar studies and support the claims regarding reusable architectural design decisions in principle.",Architectural decision model | Architectural design decision | Architectural knowledge | Controlled experiment,Information and Software Technology,2015-07-01,Article,"Lytra, Ioanna;Gaubatz, Patrick;Zdun, Uwe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921712991,10.1016/j.infsof.2014.12.002,Evidence management for compliance of critical systems with safety standards: A survey on the state of practice,"Context Demonstrating compliance of critical systems with safety standards involves providing convincing evidence that the requirements of a standard are adequately met. For large systems, practitioners need to be able to effectively collect, structure, and assess substantial quantities of evidence. Objective This paper aims to provide insights into how practitioners deal with safety evidence management for critical computer-based systems. The information currently available about how this activity is performed in the industry is very limited. Method We conducted a survey to determine practitioners' perspectives and practices on safety evidence management. A total of 52 practitioners from 15 countries and 11 application domains responded to the survey. The respondents indicated the types of information used as safety evidence, how evidence is structured and assessed, how evidence evolution is addressed, and what challenges are faced in relation to provision of safety evidence. Results Our results indicate that (1) V&V artefacts, requirements specifications, and design specifications are the most frequently used safety evidence types, (2) evidence completeness checking and impact analysis are mostly performed manually at the moment, (3) text-based techniques are used more frequently than graphical notations for evidence structuring, (4) checklists and expert judgement are frequently used for evidence assessment, and (5) significant research effort has been spent on techniques that have seen little adoption in the industry. The main contributions of the survey are to provide an overall and up-to-date understanding of how the industry addresses safety evidence management, and to identify gaps in the state of the art. Conclusion We conclude that (1) V&V plays a major role in safety assurance, (2) the industry will clearly benefit from more tool support for collecting and manipulating safety evidence, and (3) future research on safety evidence management needs to place more emphasis on industrial applications.",Safety assurance | Safety certification | Safety evidence | Safety-critical systems | State of the practice,Information and Software Technology,2015-04-01,Article,"Nair, Sunil;De La Vara, Jose Luis;Sabetzadeh, Mehrdad;Falessi, Davide",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84940268355,10.1016/j.infsof.2014.06.007,A semi-automated approach to adapt activity diagrams for new use cases,"Context: Web engineering methodologies generally assign a crucial role to design models. Therefore, providing a model reuse approach is very interesting since it reduces development costs and improves quality. Current works on model reuse mainly focus on retrieval of the promising reusable assets, and much less is done regarding adaptation of the retrieved assets. This research proposes a semi-automatic approach for adaptation of UML activity diagrams to new use cases. Objective: UML use case diagrams and activity diagrams are traditionally used for the brief and the detailed specification of the functional requirements. Since many web applications have similar functionalities, and hence similar functional requirements, this research proposes an approach to take a use case diagram as input and semi-automatically create corresponding activity diagrams by adapting existing activity diagrams. Method: The proposed approach includes five main components: (1) a model repository, (2) an ontology repository as a source of domain knowledge, (3) an algorithm for annotating activity diagrams, (4) a similarity metric for retrieval of similar use cases, and (5) an adaptation algorithm for creating activity diagram of a new use case from an existing activity diagram The proposed approach uses the semantic web data model as the underlying representation format. Results: The initial experiments show that the proposed approach is promising and it provides an average reuse percent of 76%. However, it has still some weaknesses like being much dependent on the quality of the model repository and having low tolerance in case of inconsistency in the model repository. Conclusion: Enabling model reuse in the early stages of a model based development approach is very important in reducing development costs. This paper proposes a semi-automatic approach to reuse activity diagrams through their adaptation for new use cases. The approach is demonstrated to be promising although it has still some limitations.",Activity diagram | Adaptation | Model reuse | Semantic web | Use case,Information and Software Technology,2015-01-01,Conference Paper,"Paydar, Samad;Kahani, Mohsen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921790999,10.1016/j.infsof.2015.01.001,Facilitating construction of safety cases from formal models in Event-B,"Context Certification of safety-critical software systems requires submission of safety assurance documents, e.g., in the form of safety cases. A safety case is a justification argument used to show that a system is safe for a particular application in a particular environment. Different argumentation strategies (informal and formal) are applied to determine the evidence for a safety case. For critical software systems, application of formal methods is often highly recommended for their safety assurance. Objective The objective of this paper is to propose a methodology that combines two activities: formalisation of system safety requirements of critical software systems for their further verification as well as derivation of structured safety cases from the associated formal specifications. Method We propose a classification of system safety requirements in order to facilitate the mapping of informally defined requirements into a formal model. Moreover, we propose a set of argument patterns that aim at enabling the construction of (a part of) a safety case from a formal model in Event-B. Results The results reveal that the proposed classification-based mapping of safety requirements into formal models facilitates requirements traceability. Moreover, the provided detailed guidelines on construction of safety cases aim to simplify the task of the argument pattern instantiation for different classes of system safety requirements. The proposed methodology is illustrated by numerous case studies. Conclusion Firstly, the proposed methodology allows us to map the given system safety requirements into elements of the formal model to be constructed, which is then used for verification of these requirements. Secondly, it guides the construction of a safety case, aiming to demonstrate that the safety requirements are indeed met. Consequently, the argumentation used in such a constructed safety case allows us to support it with formal proofs and model checking results used as the safety evidence.",Argument patterns | Event-B | Formal development and verification | Safety cases | Safety requirements | Safety-critical software systems,Information and Software Technology,2015-04-01,Article,"Prokhorova, Yuliya;Laibinis, Linas;Troubitsyna, Elena",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921033182,10.1016/j.infsof.2014.11.007,Improving the management of product lines by performing domain knowledge extraction and cross product line analysis,"Context Increase in market competition is one of the main reasons for developing and maintaining families of systems, termed Product Lines (PLs). Managing those PLs is challenging, let alone the management of several related PLs. Currently, those PLs are managed separately or their relations are analyzed assuming explicit specification of dependencies or use of an underlying terminology. Such assumptions may not hold when developing the PLs in different departments or companies applying various engineering processes. Objective In this work we call for utilizing the knowledge gained from developing and maintaining different PLs in the same domain in order to recommend on improvements to the management of PLs. Method The suggested approach conducts domain knowledge extraction and cross PL analysis on feature diagrams - the main aid for modeling PL variability. The domain knowledge is extracted by applying similarity metrics, clustering, and mining techniques. Based on the created domain models, the approach performs cross PL analysis that examines relations in the domain models and generates improvement recommendations to existing PLs and overall management recommendations (e.g., merging or splitting PLs). Results The approach outcomes were evaluated by humans in a domain of mobile phones. The evaluation results may provide evidence that the outcomes of the approach in general and its recommendations in particular meet human perception of the given domain. Conclusion We conclude that through domain knowledge extraction and cross PL analysis the suggested approach may generate recommendations useful to the management of individual PLs, as well as to the overall management of different PLs in the same domain.",Domain analysis | Feature modeling | Software product line engineering | Variability management,Information and Software Technology,2015-03-01,Article,"Reinhartz-Berger, Iris;Wulf-Hadash, Ora",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924652882,10.1016/j.infsof.2015.01.009,Automated measurement of API usability: The API Concepts Framework,"Context Usability is an important software quality attribute for APIs. Unfortunately, measuring it is not an easy task since many things like experienced evaluators, suitable test users, and a functional product are needed. This makes existing usability measurement methods difficult to use, especially for non-professionals. Objective To make API usability measurement easier, an automated and objective measurement method would be needed. This article proposes such a method. Since it would be impossible to find and integrate all possible factors that influence API usability in one step, the main goal is to prove the feasibility of the introduced approach, and to define an extensible framework so that additional factors can easily be defined and added later. Method A literature review is conducted to find potential factors influencing API usability. From these factors, a selected few are investigated more closely with usability studies. The statistically evaluated results from these studies are used to define specific elements of the introduced framework. Further, the influence of the user as a critical factor for the framework's feasibility is evaluated. Results The API Concepts Framework is defined, with an extensible structure based on concepts that represent the user's actions, measurable properties that define what influences the usability of these concepts, and learning effects that represent the influence of the user's experience. A comparison of values calculated by the framework with user studies shows promising results. Conclusion It is concluded that the introduced approach is feasible and provides useful results for evaluating API usability. The extensible framework easily allows to add new concepts and measurable properties in the future.",API design | API usability | Complexity measures | Metrics,Information and Software Technology,2015-05-01,Article,"Scheller, Thomas;Kühn, Eva",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922518885,10.1016/j.infsof.2014.09.003,An empirically-based characterization and quantification of information seeking through mailing lists during open source developers' software evolution,"Context: Several authors have proposed information seeking as an appropriate perspective for studying software evolution. Empirical evidence in this area suggests that substantial time delays can accrue, due to the unavailability of required information, particularly when this information must travel across geographically distributed sites. Objective: As a first step in addressing the time delays that can occur in information seeking for distributed Open Source (OS) programmers during software evolution, this research characterizes the information seeking of OS developers through their mailing lists. Method: A longitudinal study that analyses 17 years of developer mailing list activity in total, over 6 different OS projects is performed, identifying the prevalent information types sought by developers, from a qualitative, grounded analysis of this data. Quantitative analysis of the number-of-responses and response time-lag is also performed. Results: The analysis shows that Open Source developers are particularly implementation centric and team focused in their use of mailing lists, mirroring similar findings that have been reported in the literature. However novel findings include the suggestion that OS developers often require support regarding the technology they use during development, that they refer to documentation fairly frequently and that they seek implementation-oriented specifics based on system design principles that they anticipate in advance. In addition, response analysis suggests a large variability in the response rates for different types of questions, and particularly that participants have difficulty ascertaining information on other developer's activities. Conclusion: The findings provide insights for those interested in supporting the information needs of OS developer communities: They suggest that the tools and techniques developed in support of co-located developers should be largely mirrored for these communities: that they should be implementation centric, and directed at illustrating ""how"" the system achieves its functional goals and states. Likewise they should be directed at determining the reason for system bugs: a type of question frequently posed by OS developers but less frequently responded to.",Information seeking software maintenance | Open source software | Qualitative empirical study,Information and Software Technology,2015-01-01,Conference Paper,"Sharif, Khaironi Y.;English, Michael;Ali, Nour;Exton, Chris;Collins, J. J.;Buckley, Jim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921029939,10.1016/j.infsof.2014.11.001,A systematic literature review on the barriers faced by newcomers to open source software projects,"Context Numerous open source software projects are based on volunteers collaboration and require a continuous influx of newcomers for their continuity. Newcomers face barriers that can lead them to give up. These barriers hinder both developers willing to make a single contribution and those willing to become a project member. Objective This study aims to identify and classify the barriers that newcomers face when contributing to open source software projects. Method We conducted a systematic literature review of papers reporting empirical evidence regarding the barriers that newcomers face when contributing to open source software (OSS) projects. We retrieved 291 studies by querying 4 digital libraries. Twenty studies were identified as primary. We performed a backward snowballing approach, and searched for other papers published by the authors of the selected papers to identify potential studies. Then, we used a coding approach inspired by open coding and axial coding procedures from Grounded Theory to categorize the barriers reported by the selected studies. Results We identified 20 studies providing empirical evidence of barriers faced by newcomers to OSS projects while making a contribution. From the analysis, we identified 15 different barriers, which we grouped into five categories: social interaction, newcomers' previous knowledge, finding a way to start, documentation, and technical hurdles. We also classified the problems with regard to their origin: newcomers, community, or product. Conclusion The results are useful to researchers and OSS practitioners willing to investigate or to implement tools to support newcomers. We mapped technical and non-technical barriers that hinder newcomers' first contributions. The most evidenced barriers are related to socialization, appearing in 75% (15 out of 20) of the studies analyzed, with a high focus on interactions in mailing lists (receiving answers and socialization with other members). There is a lack of in-depth studies on technical issues, such as code issues. We also noticed that the majority of the studies relied on historical data gathered from software repositories and that there was a lack of experiments and qualitative studies in this area.",Barriers to entry | Joining | Newcomers | Onboarding | Open source software | Systematic literature review,Information and Software Technology,2015-03-01,Review,"Steinmacher, Igor;Graciotto Silva, Marco Aurelio;Gerosa, Marco Aurelio;Redmiles, David F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84921046811,10.1016/j.infsof.2014.10.006,Defining the resource perspective in the development of processes-aware information systems,"Context The resource perspective has impact on the performance of business processes. However, current Workflow Management Systems (WfMSs) provide disparate support to its implementation and business process modeling languages provide limited capabilities for its definition. Thus, it is difficult to specify requirements regarding this perspective and to select an appropriate WfMS to support them in order to obtain a technological solution aligned with the organizational needs. Objective To provide support to the definition, implementation, verification and validation of resource perspective requirements in the development of Process-Aware Information Systems (PAISs) based on WfMSs. Method The following activities were carried out: (i) identification of resource perspective aspects in executable workflow specifications, (ii) analysis of the elements provided by the BPMN modeling language to represent these aspects, (iii) development of a framework based on BPMN for defining and implementing these aspects by using the extension mechanism provided by this language, (iv) development of a model-driven development method that leverages the framework to develop PAISs, and (v) validation of the proposed framework and method through the development of a tool supporting them, a case study, and the evaluation against the Workflow Resource Patterns. Results A framework, a method and a tool that support the definition of the resource perspective in the development of PAISs. Conclusion By using the proposed framework and method, practitioners are able to: define the resource perspective requirements in conceptual process models, select a WfMS as implementation platform, and define the implementation of these requirements maintaining the consistency between the conceptual process models and the workflow specifications.",BPMN | Business process | Process-aware information system | Resource perspective | Workflow,Information and Software Technology,2015-03-01,Article,"Stroppi, Luis Jesús Ramón;Chiotti, Omar;Villarreal, Pablo David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942053656,10.1016/j.infsof.2015.06.004,A systematic literature review of use case specifications research,"Context Use cases have been widely accepted and acknowledged as a specification tool for specifying the functional requirements of a software system. Many variations of use cases exist which tries to address the issues such as their completeness, degree of formalism, automated information extraction, usability, and pertinence. Objective The aim of this systematic review is to examine the existing literature for the evolution of the use cases, their applications, quality assessments, open issues, and the future directions. Method We perform keyword-based extensive search to identify the relevant studies related to use case specifications research reported in journal articles, conference papers, workshop papers, bulletins and book chapters. Results The specified search process resulted 119 papers, which were published between 1992 and February 2014. This included, 54 journal articles, 42 conference papers, 2 ACM/IEEE bulletins, 12 book chapters, 6 workshop papers and 3 white papers. We found that as many as twenty use case templates have been proposed and applied for various software specification problems ranging from informal descriptions with paragraph-style text to more formal keyword-oriented templates. Conclusion Use cases have been evolved from initial plain, semi-formal textual descriptions to a more formal template structure facilitating automated information extraction in various software development life cycle activities such as requirement documentation, requirement analysis, requirement validation, domain modeling, test case generation, planning and estimation, and maintenance. The issues that remain to be sorted out are (1) the right degree of formalism, (2) the efficient change management, (3) the industrial relevance, and (4) assessment of the quality of the specification. Additionally, its synergy with other software models that are used in the development processes is an issue that needs to be addressed.",Evolution | Guidelines | Quality | Systematic reviews | Use case specifications | Use case templates,Information and Software Technology,2015-11-01,Conference Paper,"Tiwari, Saurabh;Gupta, Atul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84922592234,10.1016/j.infsof.2014.06.005,"Special section from the international conference on evaluation and assessment in software engineering, 2013",,,Information and Software Technology,2015-01-01,Editorial,"Travassos, Guilherme Horta",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924662751,10.1016/j.infsof.2015.01.002,Comparing development approaches and reuse strategies: An empirical evaluation of developer views from the aerospace industry,"Context There is a debate in the aerospace industry whether lessons from reuse successes and failures in nonembedded software can be applied to embedded software. Reuse supposedly reduces development time and errors. The aerospace industry was an early advocate of reuse, but in Aerospace, not all reuse experiences have been as successful as expected. Some major projects experienced large overruns in time, budget, as well as inferior performance, at least in part, due to the gap between reuse expectations and reuse outcomes. This seemed to be especially the case for embedded systems. Objective Our goal is to discover software reuse practices in the aerospace industry. In particular, we wish to learn whether practitioners who develop embedded systems use the same development approaches and artifacts as software practitioners who develop nonembedded systems. We wish to learn whether reuse influences selection of development approaches and artifacts and whether outcomes are impacted. Method We developed a survey given to software practitioners in a major Aerospace Corporation developing either embedded or nonembedded systems. The survey probed to identify development methods used, artifacts reused and outcomes resulting from the reuse. We used qualitative and quantitative methods such as descriptive statistics, MANOVA, Principle Component Analysis and an analysis of freeform comments to compare reuse practices between embedded systems and nonembedded systems development. Results We found that embedded systems were more likely to include component based development, product line development and model based development in their development approach, whereas nonembedded systems were more likely to include Ad Hoc and COTS/GOTS in their development approach. Embedded systems developers tended to reuse more and different reuse artifacts. Conclusion We found that, while outcomes were nearly identical, the development approaches and artifacts used did, in fact, differ. In particular, the tight coupling between code and the platform in embedded systems often dictated the development approach and reuse artifacts and identified some of the reasons.",Embedded systems | Empirical study | Nonembedded systems | Software reuse,Information and Software Technology,2015-05-01,Article,"Varnell-Sarjeant, Julia;Amschler Andrews, Anneliese;Lucente, Joe;Stefik, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84942011818,10.1016/j.infsof.2015.06.002,Comparative case studies of open source software peer review practices,"Context The power of open source software peer review lies in the involvement of virtual communities, especially users who typically do not have a formal role in the development process. As communities grow to a certain extent, how to organize and support the peer review process becomes increasingly challenging. A universal solution is likely to fail for communities with varying characteristics. Objective This paper investigates differences of peer review practices across different open source software communities, especially the ones engage distinct types of users, in order to offer contextualized guidance for developing open source software projects. Method Comparative case studies were conducted in two well-established large open source communities, Mozilla and Python, which engage extremely different types of users. Bug reports from their bug tracking systems were examined primarily, complemented by secondary sources such as meeting notes, blog posts, messages from mailing lists, and online documentations. Results The two communities differ in the key activities of peer review processes, including different characteristics with respect to bug reporting, design decision making, to patch development and review. Their variances also involve the designs of supporting technology. The results highlight the emerging role of triagers, who bridge the core and peripheral contributors and facilitate the peer review process. The two communities demonstrate alternative designs of open source software peer review and their tradeoffs were discussed. Conclusion It is concluded that contextualized designs of social and technological solutions to open source software peer review practices are important. The two cases can serve as learning resources for open source software projects, or other types of large software projects in general, to cope with challenges of leveraging enormous contributions and coordinating core developers. It is also important to improve support for triagers, who have not received much research effort yet.",Design | Open source software | Software peer review | Virtual community,Information and Software Technology,2015-11-01,Conference Paper,"Wang, Jing;Shih, Patrick C.;Wu, Yu;Carroll, John M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84924690259,10.1016/j.infsof.2015.01.005,Requirements simulation for early validation using Behavior Trees and Datalog,"Context The role of formal specification in requirements validation and analysis is generally considered to be limited because considerable expertise is required in developing and understanding the mathematical proofs. However, formal semantics of a language can provide a basis for step-by-step execution of requirements specification by building an easy to use simulator to assist in requirements elicitation, validation and analysis. Objective The objective of this paper is to illustrate the usefulness of a simulator that executes requirements and captures system states as rules and facts in a database. The database can then be queried to carry out analysis after all the requirements have been executed a given number of times Method Behavior Trees (BTs)1 are automatically translated into Datalog facts and rules through a simulator called SimTree. The translation process involves model-to-model (M2M) transformation and model-to-text (M2T) transformation which automatically generates the code for a simulator called SimTree. SimTree on execution produces Datalog code. The effectiveness of the simulator is evaluated using the specifications of a published case study - Ambulatory Infusion Pump (AIP)2. Results The BT specification of the AIP was transformed into SimTree code for execution. The simulator produced a complete state-space for a predetermined number of runs in the form of Datalog facts and rules, which were then analyzed for various properties of interest like safety and liveness. Conclusion Queries of the resultant Datalog code were found to be helpful in identifying defects in the specification. However, probability values had to be manually assigned to all the events to ensure reachability to all leaf nodes of the tree and timely completion of all the runs. We identify optimization of execution paths to reduce execution time as our future work.",Behavior Trees | Datalog | Formal analysis | Formal methods | Requirements engineering | Simulation,Information and Software Technology,2015-05-01,Article,"Zafar, Saad;Farooq-Khan, Naurin;Ahmed, Musharif",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84943329814,10.1109/WICSA.2015.17,A Proactive Support for Conceptual Interoperability Analysis in Software Systems,"Successfully integrating a software system with an existing other software system requires, beyond technical mismatches, identifying and resolving conceptual mismatches that might result in worthless integration and costly rework. Often, not all relevant architectural information about the system to integrate with is publicly available, as it is hidden in internal architectural documents and not exposed in the public API documentation. Thus, we propose a framework of conceptual interoperability information and a formalization of it. Based on this framework, a system's architect can semi-automatically extract interoperability-relevant parts from his architecture and lower-level design documentation and publish it in a standardized and formalized way. The goal is to keep the additional effort for providing the interoperability-relevant information as low as possible and to encourage architects to provide it proactively. Thus, we extract from UML diagrams and textual documentation information that is relevant for conceptual interoperability. Companies that aim at interoperation of their systems with others, e.g. Companies initiating an ecosystem, should be highly motivated to provide such interoperability information in order to grow their business impact by more successful interoperations. In a more advanced level, also the architect, who is integrating his system with a provided one, could extract interoperability-related information about his existing system and we envision to automatically match the pieces of both sides and identify conceptual mismatches.",black-box interoperation | conceptual interoperability | conceptual mismatches | information extraction | interoperability analysis,"Proceedings - 12th Working IEEE/IFIP Conference on Software Architecture, WICSA 2015",2015-07-14,Conference Paper,"Abukwaik, Hadil;Naab, Matthias;Rombach, Dieter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84962213915,10.1109/SBES.2015.27,Challenges in Analyzing Software Documentation in Portuguese,"Many tools that automatically analyze, summarize, or transform software artifacts rely on natural language processing tooling for the interpretation of natural language text produced by software developers, such as documentation, code comments, commit messages, or bug reports. Processing natural language text produced by software developers is challenging because of unique characteristics not found in other texts, such as the presence of code terms and the systematic use of incomplete sentences. In addition, texts produced by Portuguese-speaking developers mix languages since many keywords and programming concepts are referred to by their English name. In this paper, we provide empirical insights into the challenges of analyzing software artifacts written in Portuguese. We analyzed 100 question titles from the Portuguese version of Stack Overflow with two Portuguese language tools and identified multiple problems which resulted in very few sentences being tagged completely correctly. Based on these results, we propose heuristics to improve the analysis of natural language text produced by software developers in Portuguese.",Documentation | natural language processing,"Proceedings - 29th Brazilian Symposium on Software Engineering, SBES 2015",2015-11-11,Conference Paper,"Treude, Christoph;Prolo, Carlos A.;Filho, Fernando Figueira",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84958292443,10.1109/SEAA.2015.20,On the coherence between comments and implementations in source code,"Source code comments provide useful information on the implementation of a software and on the intent behind design decisions and goals. Writing informative and useful comments is far from being a trivial task. Moreover, source code comments tend to remain mostly unchanged during maintenance activities. As a consequence, the information provided in the comment of a method and in its corresponding implementation may be not coherent with each other (i.e., The comment does not properly describe the implementation). In this paper, we present the results of a manual assessment on the coherence between comments and implementations of 3636 methods, gathered from 3 Java open source software systems (for one of these systems, we considered 2 different subsequent versions). Resulting evaluations have been collected in a dataset, we made publicly available on the web. The defined protocol used for the creation of this dataset is also described. This lets researchers evaluate the goodness of our dataset and eases its future possible extensions. Another contribution of our paper consists in investigating on a possible link between coherence and lexical similarity between source code and comments. Our preliminary outcomes suggest that this similarity is higher in case the comment of methods and their implementations are coherent. However, the obtained similarity values are generally low and are not much higher than those for non-coherent method implementations and comments.",Agreement Assessment | Dataset | Experimental Protocol | Source Code Comment,"Proceedings - 41st Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2015",2015-10-20,Conference Paper,"Corazza, Anna;Maggio, Valerio;Scanniello, Giuseppe",Include,
10.1016/j.jss.2022.111515,,10.1109/MYSEC.2015.7475208,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928681873,10.1109/SANER.2015.7081848,CloCom: Mining existing source code for automatic comment generation,"Code comments are an integral part of software development. They improve program comprehension and software maintainability. The lack of code comments is a common problem in the software industry. Therefore, it is beneficial to generate code comments automatically. In this paper, we propose a general approach to generate code comments automatically by analyzing existing software repositories. We apply code clone detection techniques to discover similar code segments and use the comments from some code segments to describe the other similar code segments. We leverage natural language processing techniques to select relevant comment sentences. In our evaluation, we analyze 42 million lines of code from 1,005 open source projects from GitHub, and use them to generate 359 code comments for 21 Java projects. We manually evaluate the generated code comments and find that only 23.7% of the generated code comments are good. We report to the developers the good code comments, whose code segments do not have an existing code comment. Amongst the reported code comments, seven have been confirmed by the developers as good and committable to the software repository while the rest await for developers' confirmation. Although our approach can generate good and committable comments, we still have to improve the yield and accuracy of the proposed approach before it can be used in practice with full automation.",comment generation | documentation | program comprehension,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings",2015-04-08,Conference Paper,"Wong, Edmund;Liu, Taiyue;Tan, Lin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84928693330,10.1109/SANER.2015.7081842,The influence of App churn on App success and StackOverflow discussions,"Gauging the success of software systems has been difficult in the past as there was no uniform measure. With mobile Application (App) Stores, users rate each App according to a common rating scheme. In this paper, we study the impact of App churn on the App success through the analysis of 154 free Android Apps that have a total of 1.2k releases. We provide a novel technique to extract Android API elements used by Apps that developers change between releases. We find that high App churn leads to lower user ratings. For example, we find that on average, per release, poorly rated Apps change 140 methods compared to the 82 methods changed by positively rated Apps. Our findings suggest that developers should not release new features at the expense of churn and user ratings. We also investigate the link between how frequently API classes and methods are changed by App developers relative to the amount of discussion of these code elements on StackOverflow. Our findings indicate that classes and methods that are changed frequently by App developers are in more posts on StackOverflow. We add to the growing consensus that StackOverflow keeps up with the documentation needs of practitioners.",Android | API elements | Changes | StackOverflow,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings",2015-04-08,Conference Paper,"Guerrouj, Latifa;Azad, Shams;Rigby, Peter C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928679361,10.1109/SANER.2015.7081813,An observational study on API usage constraints and their documentation,"Nowadays, APIs represent the most common reuse form when developing software. However, the reuse benefits depend greatly on the ability of client application developers to use correctly the APIs. In this paper, we present an observational study on the API usage constraints and their documentation. To conduct the study on a large number of APIs, we implemented and validated strategies to automatically detect four types of usage constraints in existing APIs. We observed that some of the constraint types are frequent and that for three types, they are not documented in general. Surprisingly, the absence of documentation is, in general, specific to the constraints and not due to the non documenting habits of developers.",,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings",2015-04-08,Conference Paper,"Saied, Mohamed Aymen;Sahraoui, Houari;Dufour, Bruno",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84928665538,10.1109/SANER.2015.7081812,Mining multi-level API usage patterns,"Software developers need to cope with complexity of Application Programming Interfaces (APIs) of external libraries or frameworks. However, typical APIs provide several thousands of methods to their client programs, and such large APIs are difficult to learn and use. An API method is generally used within client programs along with other methods of the API of interest. Despite this, co-usage relationships between API methods are often not documented. We propose a technique for mining Multi-Level API Usage Patterns (MLUP) to exhibit the co-usage relationships between methods of the API of interest across interfering usage scenarios. We detect multi-level usage patterns as distinct groups of API methods, where each group is uniformly used across variable client programs, independently of usage contexts. We evaluated our technique through the usage of four APIs having up to 22 client programs per API. For all the studied APIs, our technique was able to detect usage patterns that are, almost all, highly consistent and highly cohesive across a considerable variability of client programs.",API Documentation | API Usage | Software Clustering | Usage Pattern,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings",2015-04-08,Conference Paper,"Saied, Mohamed Aymen;Benomar, Omar;Abdeen, Hani;Sahraoui, Houari",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84959874688,10.1109/VLHCC.2015.7356972,"Tutorons: Generating context-relevant, on-demand explanations and demonstrations of online code","Programmers frequently turn to the web to solve problems and find example code. For the sake of brevity, the snippets in online instructions often gloss over the syntax of languages like CSS selectors and Unix commands. Programmers must compensate by consulting external documentation. In this paper, we propose language-specific routines called Tutorons that automatically generate context-relevant, on-demand micro-explanations of code. A Tutoron detects explainable code in a web page, parses it, and generates in-situ natural language explanations and demonstrations of code. We build Tutorons for CSS selectors, regular expressions, and the Unix command wget. We demonstrate techniques for generating natural language explanations through template instantiation, synthesizing code demonstrations by parse tree traversal, and building compound explanations of co-occurring options. Through a qualitative study, we show that Tutoron-generated explanations can reduce the need for reference documentation in code modification tasks.",,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2015-12-14,Conference Paper,"Head, Andrew;Appachu, Codanda;Hearst, Marti A.;Hartmann, Bjorn",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84945893804,10.1109/RCoSE.2015.13,Continuous API Design for Software Ecosystems,"Today more and more software operates in a software ecosystem where software products and services are shared over a common technological platform between business actors. In such a setting, a software platform API becomes the interface between the platform and its owner (s) on the one side and other actors and their extensions on the other side. In such ecosystem context, platform API fitness for application development has a huge impact on the success of the ecosystem. However, software ecosystems are usually evolving at a high pace given their dynamic structure and the continuously changing business strategies of ecosystem actors. The design of APIs for software ecosystems therefore has to co-evolve. In this position paper, we propose a continuous (re-)assessment method for platform API's that takes into account ecosystem needs. We illustrate the method with insights from an ongoing case study.",,"Proceedings - 2nd International Workshop on Rapid Continuous Software Engineering, RCoSE 2015",2015-07-24,Conference Paper,"Hammouda, Imed;Knauss, Eric;Costantini, Leonardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/HotWeb.2015.19,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84940991650,10.1145/2724719,MAPE-K formal templates to rigorously design behaviors for self-adaptive systems,"Designing software systems that have to deal with dynamic operating conditions, such as changing availability of resources and faults that are difficult to predict, is complex. A promising approach to handle such dynamics is self-adaptation that can be realized by a MAPE-K feedback loop (Monitor-Analyze-Plan-Execute plus Knowledge). To provide evidence that the system goals are satisfied, given the changing conditions, the state of the art advocates the use of formal methods. However, little research has been done on consolidating design knowledge of self-adaptive systems. To support designers, this paper contributes with a set of formally specified MAPE-K templates that encode design expertise for a family of self-adaptive systems. The templates comprise: (1) behavior specification templates for modeling the different components of a MAPE-K feedback loop (based on networks of timed automata), and (2) property specification templates that support verification of the correctness of the adaptation behaviors (based on timed computation tree logic). To demonstrate the reusability of the formal templates, we performed four case studies in which final-year Masters students used the templates to design different self-adaptive systems.",D.2.4 [software/program verification]: Formal methods | Design | Formal templates | MAPE-K | Self-adaptation | Verification,ACM Transactions on Autonomous and Adaptive Systems,2015-08-01,Article,"Gil De La Iglesia, Didac;Weyns, Danny",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/s11704-015-4409-2,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84947074827,10.1007/978-3-319-20609-7_24,Usability of educational technology APIs: Findings and guidelines,"This paper describes a project that reviewed the usability of existing Educational Technology Application Programming Interfaces (EdTech APIs). The focus was on web-based APIs and the portals through which these are offered to developers. After analysing the state of art with regard to existing EdTech APIs and after conducting a literature review on API usability, a survey was circulated among developers and CTOs of EdTech organisations. The results of the aforementioned three steps were triangulated and resulted in usability guidelines for EdTech APIs. The contribution of this project is twofold: firstly, the production of a concrete set of EdTech API usability guidelines and, secondly, their implementation in a proof-of-concept a portal for two different EdTech offerings.",API | Programming | Usability,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Kapros, Evangelos;Peirce, Neil",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/978-3-319-14130-5_23,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84962320596,10.1109/MS.2014.80,How API Documentation Fails,"Formal documentation can be a crucial resource for learning to how to use an API. However, producing high-quality documentation can be nontrivial. Researchers investigated how 10 common documentation problems manifested themselves in practice. The results are based on two surveys of a total of 323 professional software developers and analysis of 179 API documentation units. The three severest problems were ambiguity, incompleteness, and incorrectness of content. The respondents often mentioned six of the 10 problems as 'blockers"" that forced them to use another API.",API | documentation | software development | software engineering | user study,IEEE Software,2015-07-01,Article,"Uddin, Gias;Robillard, Martin P.",Include,
10.1016/j.jss.2022.111515,2-s2.0-84948807989,10.1587/transinf.2015EDP7107,Lines of comments as a noteworthy metric for analyzing fault-proneness in methods,"This paper focuses on the power of comments to predict fault-prone programs. In general, comments along with executable statements enhance the understandability of programs. However, comments may also be used to mask the lack of readability in the program, therefore well-written comments are referred to as ""deodorant to mask code smells"" in the field of code refactoring. This paper conducts an empirical analysis to examine whether Lines of Comments (LCM) written inside a method's body is a noteworthy metric for analyzing fault-proneness in Java methods. The empirical results show the following two findings: (1) morecommented methods (the methods having more comments than the amount estimated by size and complexity of the methods) are about 1.6-2.8 times more likely to be faulty than the others, and (2) LCM can be a useful factor in fault-prone method prediction models along with the method size and the method complexity.",Comments | Fault-prone method prediction | Product metrics | Regression model,IEICE Transactions on Information and Systems,2015-12-01,Article,"Aman, Hirohisa;Amasaki, Sousuke;Sasaki, Takashi;Kawahara, Minoru",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84958641245,10.1145/2804360.2804364,The driving forces of api evolution,"Evolving an Application Programming Interface (API) is a delicate activity, as modifications to them can significantly impact their users. The increasing use of APIs means that software development organisations must take an empirical and scientific approach to the way they manage the evolution of their APIs. If no attempt at analysing or quantifying the evolution of an API is made, there will be a diminished understanding of the evolution, and possible improvements to the maintenance strategy will be dificult to identify. We believe that long-standing software evolution theories can provide additional insight to the field of APIs, and can be of great use to companies maintaining APIs. In this case study, we conduct a qualitative investigation to understand what drives the evolution of an industry company's existing API, by examining two versions of the API interface. The changes were analysed based on two software evolution theories, and the extent to which we could reverse engineer the change decisions was determined by interviewing an architect of the API. The results of this analysis show that the largest driving force of the APIs evolution was the desire for new functionality. Our findings which show that changes happen sporadically, rather than continuously, appear to show that the law of Conservation of Organisational Stability was not a considerable factor for the evolution of the API. We also found that it is possible to reverse engineer change decisions and in doing so, identified that the feedback loop of an API is an important area of improvement.",API Design | Software Evolution | Software Maintenance,International Workshop on Principles of Software Evolution (IWPSE),2015-08-30,Conference Paper,"Granli, William;Burchell, John;Hammouda, Imed;Knauss, Eric",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84964282425,10.1145/2811237.2811298,A first analysis of string APIs: The case of pharo,"Most programming languages natively provide an abstraction of character strings. However, it is difficult to assess the design or the API of a string library. There is no comprehensive analysis of the needed operations and their different variations. There are no real guidelines about the different forces in presence and how they structure the design space of string manipulation. In this article, we harvest and structure a set of criteria to describe a string API. We propose an analysis of the Pharo 4 String library as a first experience on the topic.",API | Design | Library | Strings | Style,"IWST 2015 - Proceedings of the 10th International Workshop on Smalltalk Technologies, in conjunction with the 23rd International Smalltalk Joint Conference",2015-07-15,Conference Paper,"Pollet, Damien;Ducasse, Stéphane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84916233649,10.1002/spe.2215,Some structural measures of API usability,"In this age of collaborative software development, the importance of usable APIs is well recognized. There already exists a rich body of literature that addresses issues ranging from how to design usable APIs to assessing qualitatively the usability of a given API. However, there does not yet exist a set of general-purpose metrics that can be pressed into service for a more quantitative assessment of API usability. The goal of this paper is to remedy this shortcoming in the literature. Our work presents a set of formulas that examine the API method declarations from the perspective of several commonly held beliefs regarding what makes APIs difficult to use.We validate the numerical characterizations of API usability as produced by our metrics through the APIs of several software systems.",API usability | Application programming interface | Metrics,Software - Practice and Experience,2015-01-01,Article,"Rama, Girish Maskeri;Kak, Avinash",Include,
10.1016/j.jss.2022.111515,,10.1007/s11219-015-9303-5,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84944057714,10.1016/B978-0-12-411519-4.00006-9,Latent Dirichlet Allocation: Extracting Topics from Software Engineering Data,"Topic analysis is a powerful tool that extracts ""topics"" from document collections. Unlike manual tagging, which is effort intensive and requires expertise in the documents' subject matter, topic analysis (in its simplest form) is an automated process. Relying on the assumption that each document in a collection refers to a small number of topics, it extracts bags of words attributable to these topics. These topics can be used to support document retrieval or to relate documents to each other through their associated topics. Given the variety and amount of textual information included in software repositories, in issue reports, in commit and source-code comments, and in other forms of documentation, this method has found many applications in the software-engineering field of mining software repositories.This chapter provides an overview of the theory underlying latent Dirichlet allocation (LDA), the most popular topic-analysis method today. Next, it illustrates, with a brief tutorial introduction, how to employ LDA on a textual data set. Third, it reviews the software-engineering literature for uses of LDA for analyzing textual software-development assets, in order to support developers' activities. Finally, we discuss the interpretability of the automatically extracted topics, and their correlation with tags provided by subject-matter experts.",Latent Dirichlet allocation | Software engineering | Topic modeling | Tutorial,The Art and Science of Analyzing Software Data,2015-09-01,Book Chapter,"Campbell, Joshua Charles;Hindle, Abram;Stroulia, Eleni",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84944055539,10.1016/B978-0-12-411519-4.00017-3,Code Comment Analysis for Improving Software Quality,"Code comments contain a rich amount of information that can be leveraged to improve software maintainability and reliability. This chapter on studying and analyzing free-form and semistructured code comments will help practitioners, researchers, and students learn about (1) the characteristics and content of code comments, (2) the techniques, tools, and measures for studying and analyzing code comments automatically or semiautomatically, and (3) future research directions and challenges. Readers will acquire a unique blend of interdisciplinary techniques, including natural language processing, machine learning, and program analysis, which are also useful for analyzing other software engineering text. This chapter can serve as an introduction to comment study and analysis for practitioners, researchers, and students who are interested in conducting research in this area, applying comment analysis techniques in industry, and learning about this area.",Annotations | Bug detection | Code comment analysis | Machine learning | Natural language processing | Software quality | Specification mining | Text analytics,The Art and Science of Analyzing Software Data,2015-09-01,Book Chapter,"Tan, Lin",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84944057714,10.1016/B978-0-12-411519-4.00006-9,Latent Dirichlet Allocation: Extracting Topics from Software Engineering Data,"Topic analysis is a powerful tool that extracts ""topics"" from document collections. Unlike manual tagging, which is effort intensive and requires expertise in the documents' subject matter, topic analysis (in its simplest form) is an automated process. Relying on the assumption that each document in a collection refers to a small number of topics, it extracts bags of words attributable to these topics. These topics can be used to support document retrieval or to relate documents to each other through their associated topics. Given the variety and amount of textual information included in software repositories, in issue reports, in commit and source-code comments, and in other forms of documentation, this method has found many applications in the software-engineering field of mining software repositories.This chapter provides an overview of the theory underlying latent Dirichlet allocation (LDA), the most popular topic-analysis method today. Next, it illustrates, with a brief tutorial introduction, how to employ LDA on a textual data set. Third, it reviews the software-engineering literature for uses of LDA for analyzing textual software-development assets, in order to support developers' activities. Finally, we discuss the interpretability of the automatically extracted topics, and their correlation with tags provided by subject-matter experts.",Latent Dirichlet allocation | Software engineering | Topic modeling | Tutorial,The Art and Science of Analyzing Software Data,2015-09-01,Book Chapter,"Campbell, Joshua Charles;Hindle, Abram;Stroulia, Eleni",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.22230/SRC.2015V6N2A220,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84930918188,10.14257/ijseia.2015.9.5.21,Collaborative documentation process model,"The communication efficiency among participants is an important key to successful software documentation because its failure leads to document quality degradation incurred by increasing document's inconsistencies, loss, and ambiguities. The existing documentation systems do not support efficient interaction and communication during software documentation activities. In order to solve this problem, this paper presents a collaborative documentation process (CDP) model for supporting efficient interaction and information sharing among participants, and proposes a collaborative documentation management system (CDMS) based on the model. The CDMS is an easy-o-use documentation supporting tool which supports automatic messages sending depending on documents states and events. The advantage is that it greatly improves document consistencies, and reduces the time and cost spent in documentation activities.",Change management | Collaborative system | Document consistency | Software documentation,International Journal of Software Engineering and its Applications,2015-01-01,Article,"Jang, Sung Bong",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84936806101,10.1109/ITNG.2015.68,Investigating the Link between User Stories and Documentation Debt on Software Projects,"Technical debt is a metaphor that describes the effect of immature artefacts in software development. One of its types is documentation debt, which can be identified by locating missing, inadequate or incomplete artefacts in software projects. Nowadays, we can observe more organizations using agile methods to support their activities. In particular, the use of user stories reduces the focus on requirement specification tasks and, as a consequence, creates difficulties that need to be overcame by the development team. In order to investigate these difficulties and assess whether they create a favourable scenario for incurring documentation debt, this paper presents the results of a literature review and an exploratory study. The results from both studies allowed us to identify a list of causes that can lead the development team to incur documentation debt when working with agile requirements. This is an important step in order to manage the technical debt from a preventive perspective.",Agile Requirements | Documentation Debt | Exploratory Study | Literature Review | Technical Debt | User Stories,"Proceedings - 12th International Conference on Information Technology: New Generations, ITNG 2015",2015-05-26,Conference Paper,"Soares, Henrique F.;Alves, Nicolli S.R.;Mendes, Thiago S.;Mendonca, Manoel;Spinola, Rodrigo O.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963549003,10.1109/MTD.2015.7332621,A Contextualized Vocabulary Model for identifying technical debt on code comments,"The identification of technical debt (TD) is an important step to effectively manage it. In this context, a set of indicators has been used by automated approaches to identify TD items, but some debt may not be directly identified using only metrics collected from the source code. In this work we propose CVM-TD, a model to support the identification of technical debt through code comment analysis. We performed an exploratory study on two large open sources projects with the goal of characterizing the feasibility of the proposed model to support the detection of TD through code comments analysis. The results indicate that (1) developers use the dimensions considered by CVM-TD when writing code comments, (2) CVM-TD provides a vocabulary that may be used to detect TD items, and (3) the proposed model needs to be calibrated in order to reduce the difference between comments returned by the vocabulary and those that may indicate a TD item. Code comments analysis can be used to detect TD in software projects and CVM-TD may support the development team to perform this task.",Code comment analysis | mining comment | Technical debt identification | technical debt indicator,"2015 IEEE 7th International Workshop on Managing Technical Debt, MTD 2015 - Proceedings",2015-11-19,Conference Paper,"Farias, Mario Andre De Freitas;Neto, Manoel Gomes De Mendonca;Silva, Andre Batista Da;Spinola, Rodrigo Oliveira",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1109/ICCNEEE.2015.7381407,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84937458434,10.1007/978-3-319-19243-7_10,To document or not to document? An exploratory study on developers’ motivation to document code,"Technical debt represents the situation in a project where developers accept compromises in one dimension of a system in order to meet urgent demands in other dimensions. These compromises incur a “debt”, on which “interest” has to be paid to maintain the long-term health of the project. One of the elements of technical debt is documentation debt due to under-documentation of the evolving system. In this exploratory study, our goal is to examine the different aspects of developers’ motivation to document code. Specifically, we aim to identify the motivating and hindering aspects of documentation as perceived by the developers. The motivating aspects of code documenting we find include improving code comprehensibility, order, and quality. The hindering aspects include developers’ perception of documenting as a tedious, difficult, and time consuming task that interrupts the coding process. These findings may serve as a basis for developing guidelines toward improving documentation practices and encouraging developers to document their code thus reducing documentation debt.",Documentation | Motivation | Technical debt,Lecture Notes in Business Information Processing,2015-01-01,Conference Paper,"Shmerlin, Yulia;Hadar, Irit;Kliger, Doron;Makabee, Hayim",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84944911705,10.1002/cpe.3526,Science gateways today and tomorrow: Positive perspectives of nearly 5000 members of the research community,"Science gateways are digital interfaces to advanced technologies that support science/engineering research/education. Frequently implemented as Web and mobile applications, they provide access to community resources such as software, data, collaboration tools, instrumentation, and high-performance computing. We anticipate opportunities for growth within a fragmented community. Through a large-scale survey, we measured the extent and characteristics of the gateway community (reliance on gateways and nature of existing resources) to understand useful services and support for builders and users. We administered an online survey to nearly 29,000 principal investigators, senior administrators, and people with gateway affiliations. Nearly 5000 respondents represented diverse expertise and geography. The majority of researchers/educators indicated that specialized online resources were important to their work. They choose technologies by asking colleagues and looking for documentation, demonstrated reliability, and technical support; adaptability via customizing or open-source standards was another priority. Research groups commonly provide their own resources, but public/academic institutions and commercial services also provide substantial offerings. Application creators and administrators welcome external services providing guidance such as technology selection, sustainability planning, evaluation, and specialized expertise (e.g., quality assurance and design). Technologies are diverse, so flexibility and ongoing community input are essential, as is offering specific, easy-to-access training, community support, and professional development.",cyberinfrastructure | portals | science/engineering gateways | software development | survey | Web interfaces,Concurrency and Computation: Practice and Experience,2015-11-01,Article,"Lawrence, Katherine A.;Zentner, Michael;Wilkins-Diehr, Nancy;Wernert, Julie A.;Pierce, Marlon;Marru, Suresh;Michael, Scott",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/s11704-015-4409-2,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85020178100,10.7717/peerj-cs.12,Mining usage patterns for the Android API,"API methods are not used alone, but in groups and following patterns. However, despite being a key information for API users, most usage patterns are not described in official API documents. In this article, we report a study that evaluates the feasibility of automatically enriching API documents with information on usage patterns. For this purpose, we mine and analyze 1,952 usage patterns, from a set of 396 Android applications. As part of our findings, we report that the Android API has many undocumented and non-trivial usage patterns, which can be inferred using association rule mining algorithms. We also describe a field study where a version of the original Android documentation is instrumented with the extracted usage patterns. During 17 months, this documentation received 77,863 visits from professional Android developers.",Android | Application programming interfaces | Usage patterns,PeerJ Computer Science,2015-01-01,Article,"Borges, Hudson S.;Valente, Marco Tulio",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1007/s11219-015-9267-5,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1145/2872362.2872363,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2872362.2872399,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84991593709,10.1145/2961111.2962616,A Study of Documentation in Agile Software Projects,"Although agile methods have become established in software engineering, documentation in projects is rare. Employing a theoretical model of information and documentation, our paper analyzes documentation practices in agile software projects in their entirety. Our analysis uses method triangulation: partly-structured interviews, observation and online survey. We demonstrate the correlation between satisfaction with information searches and the amount of documentation that exists for most types of information as an example. Also digital searches demand nearly twice as much time as documentation. In the conclusion, we provide recommendations on the use of supporting methods or tools to shape agile documentation.",agile documentation | Information behavior,International Symposium on Empirical Software Engineering and Measurement,2016-09-08,Conference Paper,"Voigt, Stefan;Von Garrel, Jörg;Müller, Julia;Wirth, Dominic",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84991648507,10.1145/2961111.2962591,How Are Discussions Associated with Bug Reworking?: An Empirical Study on Open Source Projects,"Background: Bug fixing is one major activity in software maintenance to solve unexpected errors or crashes of software systems. However, a bug fix can also be incomplete and even introduce new bugs. In such cases, extra effort is needed to rework the bug fix. The reworking requires to inspect the problem again, and perform the code change and verification when necessary. Discussions throughout the bug fixing process are important to clarify the reported problem and reach a solution. Aims: In this paper, we explore how discussions during the initial bug fix period (i.e., before the bug reworking occurs) associate with future bug reworking. We focus on two types of ""reworked bug fixes"": 1) the initial bug fix made in a re-opened bug report; and 2) the initially submitted patch if multiple patches are submitted for a single bug report. Method: We perform a case study using five open source projects (i.e., Linux, Firefox, PDE, Ant and HTTP). The discussions are studied from six perspectives (i.e., duration, number of comments, dispersion, frequency, number of developers and experience of developers). Furthermore, we extract topics of discussions using Latent Dirichlet Allocation (LDA). Results: We find that the occurrence of bug reworking is associated with various perspectives of discussions. Moreover, discussions on some topics (e.g., code inspection and code testing) can decrease the frequency of bug reworking. Conclusions: The discussions during the initial bug fix period may serve as an early indicator of what bug fixes are more likely to be reworked.",bug reworking | discussions | Latent Dirichlet Allocation | re-open | re-patch,International Symposium on Empirical Software Engineering and Measurement,2016-09-08,Conference Paper,"Zhao, Yu;Zhang, Feng;Shihab, Emad;Zou, Ying;Hassan, Ahmed E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84991633796,10.1145/2961111.2962596,Detection of Requirement Errors and Faults via a Human Error Taxonomy: A Feasibility Study,"Background: Developing correct software requirements is important for overall software quality. Most existing quality improvement approaches focus on detection and removal of faults (i.e. problems recorded in a document) as opposed identifying the underlying errors that produced those faults. Accordingly, developers are likely to make the same errors in the future and fail to recognize other existing faults with the same origins. Therefore, we have created a Human Error Taxonomy (HET) to help software engineers improve their software requirement specification (SRS) documents. Aims: The goal of this paper is to analyze whether the HET is useful for classifying errors and for guiding developers to find additional faults. Methods: We conducted a empirical study in a classroom setting to evaluate the usefulness and feasibility of the HET. Results: First, software developers were able to employ error categories in the HET to identify and classify the underlying sources of faults identified during the inspection of SRS documents. Second, developers were able to use that information to detect additional faults that had gone unnoticed during the initial inspection. Finally, the participants had a positive impression about the usefulness of the HET. Conclusions: The HET is effective for identifying and classifying requirements errors and faults, thereby helping to improve the overall quality of the SRS and the software.",Human Errors | Software Quality Improvement | Software Requirements,International Symposium on Empirical Software Engineering and Measurement,2016-09-08,Conference Paper,"Hu, Wenhua;Carver, Jeffrey C.;Anu, Vaibhav K.;Walia, Gursimran S.;Bradshaw, Gary",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84991706356,10.1145/2961111.2962622,Semantic Coupling between Classes: Corpora or Identifiers?,"Context: Conceptual coupling is a measure of how loosely or closely related two software artifacts are, by considering the semantic information embedded in the comments and identifiers. This type of coupling is typically evaluated using the semantic information from source code into a words corpus. The extraction of words corpora can be lengthy, especially when systems are large and many classes are involved. Goal: This study investigates whether using only the class identifiers (e.g., the class names) can be used to evaluate the conceptual coupling between classes, as opposed to the words corpora of the entire classes. Method: In this study, we analyze two Java systems and extract the conceptual coupling between pairs of classes, using (i) a corpus-based approach; and (ii) two identifier-based tools. Results: Our results show that measuring the semantic similarity between classes using (only) their identifiers is similar to using the class corpora. Additionally, using the identifiers is more efficient in terms of precision, recall, and computation time. Conclusions: Using only class identifiers to measure their semantic similarity can save time on program comprehension tasks for large software projects; the findings of this paper support this hypothesis, for the systems that were used in the evaluation and can also be used to guide researchers developing future generations of tools supporting program comprehension.",Corpora | Corpus | Latent Semantic Indexing (LSI) | Object-oriented software (OO) | Open-source software (OSS) | Semantic coupling | Semantic similarity | Vector Space Model (VSM),International Symposium on Empirical Software Engineering and Measurement,2016-09-08,Conference Paper,"Ajienka, Nemitari;Capiluppi, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2950290.2994161,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997497980,10.1145/2950290.2950337,Titanium: Efficient analysis of evolving alloy specifications,"The Alloy specification language, and the corresponding Alloy Analyzer, have received much attention in the last two decades with applications in many areas of software engineering. Increasingly, formal analyses enabled by Alloy are desired for use in an on-line mode, where the specifications are automatically kept in sync with the running, possibly changing, software system. However, given Alloy Analyzer's reliance on computationally expensive SAT solvers, an important challenge is the time it takes for such analyses to execute at runtime. The fact that in an on-line mode, the analyses are often repeated on slightly revised versions of a given specification, presents us with an opportunity to tackle this challenge. We present Titanium, an extension of Alloy for formal analysis of evolving specifications. By leveraging the results from previous analyses, Titanium narrows the state space of the revised specification, thereby greatly reducing the required computational effort. We describe the semantic basis of Titanium in terms of models specified in relational logic. We show how the approach can be realized atop an existing relational logic model finder. Our experimental results show Titanium achieves a significant speed-up over Alloy Analyzer when applied to the analysis of evolving specifications.",Evolving Software | Formal Verication | Partial Models | Relational Logic,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Bagheri, Hamid;Malek, Sam",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997470964,10.1145/2950290.2950314,Mining performance specifications,"Functional testing is widespread and supported by a multitude of tools, including tools to mine functional specifications. In contrast, non-functional attributes like performance are often less well understood and tested. While many profiling tools are available to gather raw performance data, interpreting this raw data requires expert knowledge and a thorough understanding of the underlying software and hardware infrastructure. In this work we present an approach that mines performance specifications from running systems autonomously. The tool creates performance models during runtime. The mined models are analyzed further to create compact and comprehensive performance assertions. The resulting assertions can be used as an evidence-based performance specification for performance regression testing, performance monitoring, or as a foundation for more formal performance specifications.",Performance Modeling | Regression Testing | Specification Mining,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Brönink, Marc;Rosenblum, David S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997206922,10.1145/2950290.2950340,Proteus: Computing disjunctive loop summary via path dependency analysis,"Loops are challenging structures for program analysis, especially when loops contain multiple paths with complex interleaving executions among these paths. In this paper, we first propose a classification of multi-path loops to understand the complexity of the loop execution, which is based on the variable updates on the loop conditions and the execution order of the loop paths. Secondly, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects on the variables. The key contribution is to use a path dependency automaton (PDA) to capture the execution dependency between the paths. A DFS-based algorithm is proposed to traverse the PDA to summarize the effect for all feasible executions in the loop. The experimental results show that Proteus is effective in three applications: Proteus can 1) compute a more precise bound than the existing loop bound analysis techniques; 2) significantly outperform state-of-The-Art tools for loop verification; and 3) generate test cases for deep loops within one second, while KLEE and Pex either need much more time or fail.",Disjunctive Summary | Loop Summarization,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Xie, Xiaofei;Chen, Bihuan;Liu, Yang;Le, Wei;Li, Xiaohong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997523703,10.1145/2950290.2950325,How to break an API: Cost negotiation and community values in three software ecosystems,"Change introduces conict into software ecosystems: breaking changes may ripple through the ecosystem and trigger rework for users of a package, but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs. We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change, Eclipse, R/CRAN, and Node.js/npm, to understand how developers make decisions about change and change-related costs and what practices, tooling, and policies are used. We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem. Our results illustrate that there is a large design space in how to build an ecosystem, its policies and its supporting infrastructure; and there is value in making community values and accepted tradeos explicit and transparent in order to resolve conicts and negotiate change-related costs.",Collaboration | Dependency Management | Qualitative Research | Semantic Versioning | Software Ecosystems,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Bogart, Christopher;Kästner, Christian;Herbsleb, James;Thung, Ferdian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997497035,10.1145/2950290.2950348,Detecting sensitive data disclosure via bi-directional text correlation analysis,"Traditional sensitive data disclosure analysis faces two challenges: to identify sensitive data that is not generated by specific API calls, and to report the potential disclosures when the disclosed data is recognized as sensitive only after the sink operations. We address these issues by developing BIDTEXT, a novel static technique to detect sensitive data disclosures. BIDTEXT formulates the problem as a type system, in which variables are typed with the text labels that they encounter (e.g., during key-value pair operations). The type system features a novel bi-directional propagation technique that propagates the variable label sets through forward and backward data-flow. A data disclosure is reported if a parameter at a sink point is typed with a sensitive text label. BIDTEXT is evaluated on 10,000 Android apps. It reports 4,406 apps that have sensitive data disclosures, with 4,263 apps having log based disclosures and 1,688 having disclosures due to other sinks such as HTTP requests. Existing techniques can only report 64.0% of what BIDTEXT reports. And manual inspection shows that the false positive rate for BIDTEXT is 10%.",Android apps | Bi-directional Text Correlation | Sensitive Data Disclosure,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Huang, Jianjun;Zhang, Xiangyu;Tan, Lin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997542840,10.1145/2950290.2950298,API Deprecation: A retrospective analysis and detection method for code examples on the web,"Deprecation allows the developers of application programming interfaces (APIs) to signal to other developers that a given API item ought to be avoided. But little is known about deprecation practices beyond anecdotes. We examine how API deprecation has been used in 26 open source Java frameworks and libraries, finding that the classic deprecate-replace-remove cycle is often not followed, as many APIs were removed without prior deprecation, many deprecated APIs were subsequently un-deprecated, and removed APIs are even resurrected with surprising frequency. Furthermore, we identify several problems in the information commonly (not) provided to help API consumers transition their dependent code. As a consequence of deprecation, coding examples on the web- an increasingly important source of information for developers- can easily become outdated. Code examples that demonstrate how to use deprecated APIs can be difficult to disregard and a waste of time for developers. We propose a lightweight, version-sensitive framework to detect deprecated API usages in source code examples on the web so developers can be informed of such usages before they invest time and energy into studying them. We reify the framework as a prototype tool (DEPRECATION WATCHER). Our evaluation on detecting deprecated ANDROID API usages in code examples on STACK OVERFLOW shows our tool obtains a precision of 100% and a recall of 86% in a random sample of 200 questions.",Api Deprecation | Deprecation Practices | Deprecation Watcher | Web-Based Documentation,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Zhou, Jing;Walker, Robert J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997207387,10.1145/2950290.2950306,When should internal interfaces be promoted to public?,"Commonly, software systems have public (and stable) interfaces, and internal (and possibly unstable) interfaces. Despite being discouraged, client developers often use internal interfaces, which may cause their systems to fail when they evolve. To overcome this problem, API producers may promote internal interfaces to public. In practice, however, API producers have no assistance to identify public interface candidates. In this paper, we study the transition from internal to public interfaces. We aim to help API producers to deliver a better product and API clients to benefit sooner from public interfaces. Our empirical investigation on five widely adopted Java systems present the following observations. First, we identified 195 promotions from 2,722 internal interfaces. Second, we found that promoted internal interfaces have more clients. Third, we predicted internal interface promotion with precision between 50%{80%, recall 26%{82%, and AUC 74%{85%. Finally, by applying our predictor on the last version of the analyzed systems, we automatically detected 382 public interface candidates.",API Usage | Internal Interface Analysis | Software Evolution,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Hora, Andre;Valente, Marco Tulio;Robbes, Romain;Anquetil, Nicolas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997208356,10.1145/2950290.2950349,On-The-fly decomposition of specifications in software model checking,"Major breakthroughs have increased the efficiency and effiectiveness of software model checking considerably, such that this technology is now applicable to industrial-scale software. However, verifying the full formal specification of a software system is still considered too complex, and in practice, sets of properties are verified one by one in isolation. We propose an approach that takes the full formal specification as input and first tries to verify all properties simultaneously in one run of the verifier. Our verification algorithm monitors itself and detects situations for which the full set of properties is too complex. In such cases, we perform an automatic decomposition of the full set of properties into smaller sets, and continue the verification seamlessly. To avoid state-space explosion for large sets of properties, we introduce on-The-y property weaving: properties get weaved into the program's transition system on the y, during the analysis; which properties to weave and verify is determined dynamically during the verification process. We perform an extensive evaluation based on verification tasks that were derived from 4 336 Linux kernel modules, and a set of properties that define the correct usage of the Linux API. Checking several properties simultaneously can lead to a significant performance gain, due to the fact that abstract models share many parts among different properties.",Decomposition | Formal Methods | Multi-Property Verification | Program Analysis | Software Model Checking | Specification,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Apel, Sven;Beyer, Dirk;Mordan, Vitaly;Mutilin, Vadim;Stahlbauer, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997236595,10.1145/2950290.2950318,Lightweight specification and analysis of dynamic systems with rich configurations,"Model-checking is increasingly popular in the early phases of the software development process. To establish the correctness of a software design one must usually verify both structural and behavioral (or temporal) properties. Unfortunately, most specification languages, and accompanying model-checkers, excel only in analyzing either one or the other kind. This limits their ability to verify dynamic systems with rich con-gurations: systems whose state space is characterized by rich structural properties, but whose evolution is also expected to satisfy certain temporal properties. To address this problem, we first propose Electrum, an extension of the Alloy specification language with temporal logic operators, where both rich con-gurations and expressive temporal properties can easily be defined. Two alternative model-checking techniques are then proposed, one bounded and the other unbounded, to verify systems expressed in this language, namely to verify that every desirable temporal property holds for every possible con-guration.",Formal Specification Language | Model-Checking,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Macedo, Nuno;Brunel, Julien;Chemouil, David;Cunha, Alcino;Kuperberg, Denis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997235946,10.1145/2950290.2950299,What would users change in my App? Summarizing app reviews for recommending software changes,"Mobile app developers constantly monitor feedback in user reviews with the goal of improving their mobile apps and better meeting user expectations. Thus, automated approaches have been proposed in literature with the aim of reducing the effort required for analyzing feedback contained in user reviews via automatic classification/prioritization according to specific topics. In this paper, we introduce SURF (Summarizer of User Reviews Feedback), a novel approach to condense the enormous amount of information that developers of popular apps have to manage due to user feedback received on a daily basis. SURF relies on a conceptual model for capturing user needs useful for developers performing maintenance and evolution tasks. Then it uses sophisticated summarisation techniques for summarizing thousands of reviews and generating an interactive, structured and condensed agenda of recommended software changes. We performed an end-To-end evaluation of SURF on user reviews of 17 mobile apps (5 of them developed by Sony Mobile), involving 23 developers and researchers in total. Results demonstrate high accuracy of SURF in summarizing reviews and the usefulness of the recommended changes. In evaluating our approach we found that SURF helps developers in better understanding user needs, substantially reducing the time required by developers compared to manually analyzing user (change) requests and planning future software changes.",Mobile Application | Text Summarization | User Feedback,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Sorbo, Andrea Di;Panichella, Sebastiano;Alexandru, Carol V.;Shimagaki, Junji;Visaggio, Corrado A.;Canfora, Gerardo;Gall, Harald C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997078032,10.1145/2950290.2950355,Detecting and fixing precision-specific operations for measuring floating-point errors,"The accuracy of the oating-point calculation is critical to many applications and different methods have been proposed around oating-point accuracies, such as detecting the errors in the program, verifying the accuracy of the program, and optimizing the program to produce more accurate results. These approaches need a specification of the program to understand the ideal calculation performed by the program, which is usually approached by interpreting the program in a precision-unspecific way. However, many operations programmed in existing code are inherently precision-specific, which cannot be easily inter-preted in a precision-unspecific way. In fact, the semantics used in existing approaches usually fail to interpret precision-specific operations correctly. In this paper, we present a systematic study on precision-specific operations. First, we propose a detection approach to detect precision-specific operations. Second, we propose a fixing approach to enable the tuning of precisions under the presence of precision-specific operations. Third, we studied the precision-specific operations in the GNU C standard math library based on our detection and fixing approaches. Our results show that (1) a significant number of code fragments in the standard C math library are precision-specific operations, and some large inaccuracies reported in existing studies are false positives or potential false positives due to precision-specific operations; (2) our detection approach has high precision and recall; (3) our fixing approach can lead to overall more accurate result.",Floating-point accuracy | Precision-specific operations.,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Wang, Ran;Zou, Daming;He, Xinrui;Xiong, Yingfei;Zhang, Lu;Huang, Gang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997448805,10.1145/2950290.2983983,Sustainable software design,"Although design plays a central role in software development, the information produced in this activity is often left to progressively evaporate as the result of software evolution, loss of artifacts, or the fading of related knowledge held by the development team. This paper introduces the concept of sustainability for software design, and calls for its integration into the existing catalog of design quality attributes. Applied to software design, sustainability conveys the idea that a particular set of design decisions and their rationale can be succinctly reected in the host technology and/or described in documentation in a way that is checkable for conformance with the code and generally resistant to evaporation. The paper discusses the relation between sustainability and existing research areas in software engineering, and highlights future research challenges related to sustainable software design.",Software Design | Software Evolution,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Robillard, Martin P.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997417532,10.1145/2950290.2983988,Studying developer gaze to empower software engineering research and practice,"A new research paradigm is proposed that leverages developer eye gaze to improve the state of the art in software engineering research and practice. The vision of this new paradigm for use on software engineering tasks such as code summarization, code recommendations, prediction, and continuous traceability is described. Based on this new paradigm, it is foreseen that new benchmarks will emerge based on developer gaze. The research borrows from cognitive psychology, artificial intelligence, information retrieval, and data mining. It is hypothesized that new algorithms will be discovered that work with eye gaze data to help improve current IDEs, thus improving developer productivity. Conducting empirical studies using an eye tracker will lead to inventing, evaluating, and applying innovative methods and tools that use eye gaze to support the developer. The implications and challenges of this paradigm for future software engineering research is discussed.",Benchmarks | Continuous traceability | Eye tracking | Mining gaze data | Predictions | Recommendations | Summarizations,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Sharif, Bonita;Clark, Benjamin;Maletic, Jonathan I.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997503500,10.1145/2950290.2983955,Bing developer assistant: Improving developer productivity by recommending sample code,"In programming practice, developers often need sample code in order to learn how to solve a programming-related problem. For example, how to reuse an Application Programming Interface (API) of a large-scale software library and how to implement a certain functionality. We believe that previously written code can help developers understand how others addressed the similar problems and can help them write new programs. We develop a tool called Bing Developer Assistant (BDA), which improves developer productivity by recommending sample code mined from public software repositories (such as GitHub) and web pages (such as Stack Overflow). BDA can automatically mine code snippets that implement an API or answer a code search query. It has been implemented as a free-downloadable extension of Microsoft Visual Studio and has received more than 670K downloads since its initial.",API | API Usage Extraction | Code Search | GitHub | Software Reuse,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Zhang, Hongyu;Jain, Anuj;Khandelwal, Gaurav;Kaushik, Chandrashekhar;Ge, Scott;Hu, Wenxiang",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997113453,10.1145/2950290.2983935,PUMConf: A tool to configure product specific use case and domain models in a product line,"We present PUMConf, a tool for supporting configuration that currently focuses on requirements and enables effective product line management in the context of use case-driven development. By design, it relies exclusively on variability modeling for artifacts that are commonly used in such contexts (i.e., use case diagram, specifications and domain model). For given Product Line (PL) use case and domain models, PUMConf checks the consistency of the models, interactively receives configuration decisions from analysts, automatically checks decision consistency, and generates Product Specific (PS) use case and domain models from the PL models and decisions. It has been evaluated on an industrial case study in the automotive domain. lines;",Product Line Engineering | Use Case-Driven Development.,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Hajri, Ines;Goknil, Arda;Briand, Lionel C.;Stephany, Thierry",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997522817,10.1145/2950290.2983931,T2API: Synthesizing API code usage templates from english texts with statistical translation,"In this work, we develop T2API, a statistical machine translationbased tool that takes a given English description of a programming task as a query, and synthesizes the API usage template for the task by learning from training data. T2API works in two steps. First, it derives the API elements relevant to the task described in the input by statistically learning from a StackOverflow corpus of text descriptions and corresponding code. To infer those API elements, it also considers the context of the words in the textual input and the context of API elements that often go together in the corpus. The inferred API elements with their relevance scores are ensembled into an API usage by our novel API usage synthesis algorithm that learns the API usages from a large code corpus via a graph-based language model. Importantly, T2API is capable of generating new API usages from smaller, previously-seen usages.",API Usage Synthesis | Graph-based Statistical Machine Translation | Text-To-Code Translation,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Nguyen, Thanh;Rigby, Peter C.;Nguyen, Anh Tuan;Karanfil, Mark;Nguyen, Tien N.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997241535,10.1145/2950290.2983940,JBSE: A symbolic executor for Java programs with complex heap inputs,"We present the Java Bytecode Symbolic Executor (JBSE), a symbolic executor for Java programs that operates on complex heap inputs. JBSE implements both the novel Heap EXploration Logic (HEX), a symbolic execution approach to deal with heap inputs, and the main state-of-The-Art approaches that handle data structure constraints expressed as either executable programs (repOk methods) or declarative specifications. JBSE is the first symbolic executor specifically designed to deal with programs that operate on complex heap inputs, to experiment with the main state-of-The-Art approaches, and to combine different decision procedures to explore possible synergies among approaches for handling symbolic data structures.",Alloy | Heap data structures | Heap Exploration Logic | Pointer Assertion Logic | RepOk | Symbolic Execution,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Braione, Pietro;Denaro, Giovanni;Pezzè, Mauro",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997501279,10.1145/2950290.2983938,ARdoc: App reviews development oriented classifier,"Google Play, Apple App Store and Windows Phone Store are well known distribution platforms where users can download mobile apps, rate them and write review comments about the apps they are using. Previous research studies demonstrated that these reviews contain important information to help developers improve their apps. However, analyzing reviews is challenging due to the large amount of reviews posted every day, the unstructured nature of reviews and its varying quality. In this demo we present ARdoc, a tool which combines three techniques: (1) Natural Language Parsing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify useful feedback contained in app reviews important for performing software maintenance and evolution tasks. Our quantitative and qualitative analysis (involving mobile professional developers) demonstrates that ARdoc correctly classiffes feedback useful for maintenance perspectives in user reviews with high precision (ranging between 84% and 89%), recall (ranging between 84% and 89%), and F-Measure (ranging between 84% and 89%). While evaluating our tool developers of our study confirmed the usefulness of ARdoc in extracting important maintenance tasks for their mobile applications.",Mobile Applications | Natural Language Processing | Sentiment Analysis | Text Classification | User Reviews,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Panichella, Sebastiano;Di Sorbo, Andrea;Guzman, Emitza;Visaggio, Corrado A.;Canfora, Gerardo;Gall, Harald",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997523693,10.1145/2950290.2983946,Data structure synthesis,"All mainstream languages ship with libraries implementing lists, maps, sets, trees, and other common data structures. These libraries are su-cient for some use cases, but other applications need specialized data structures with different operations. For such applications, the standard libraries are not enough. I propose to develop techniques to automatically synthesize data structure implementations from high-level specifications. My initial results on a large class of collection data struc-tures demonstrate that this is possible and lend hope to the prospect of general data structure synthesis. Synthesized implementations can save programmer time and improve correctness while matching the performance of handwritten code.",Automatic Programming | Data Structures | Program Synthesis,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Loncaric, Calvin",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997482050,10.1145/2950290.2983960,Effective assignment and assistance to software developers and reviewers,"Human reliance and dominance are ubiquitous in sustaining a high-quality large software system. Automatically assign-ing the right solution providers to the maintenance task at hand is arguably as important as providing the right tool support for it. Two maintenance tasks related to assign-ment and assistance to software developers and reviewers are addressed, and solutions are proposed. The key insight behind these proposed solutions is the analysis and use of micro-levels of human-To-code and human-To-human inter-actions (eg., code review). We analyzed code reviews that are managed by Gerrit and found different markers of devel-oper expertise associated with the source code changes and their acceptance, time line, and human roles and feedback involved in the reviews. We formed a developer-expertise model from these markers and showed its application in bug triaging. Specifically, we derived a developer recom-mendation approach for an incoming change request, named rDevX , from this expertise model. Additionally, we present an approach, namely cHRev, to automatically recommend reviewers who are best suited to participate in a given re-view, based on their historical contributions as demonstrated in their prior reviews. Furthermore, a comparative study on other previous approaches for developer recommendation and reviewer recommendation was performed. The metrics recall and MRR were used to measure their quantitative effectiveness. Results show that the proposed approaches outperform the subjected competitors with statistical significance.",Developer Recommendation | Reviewer recommendation,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Zanjani, Motahareh Bahrami",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997530299,10.1145/2950290.2983961,RABIEF: Range analysis based integer error fixing,"We propose RABIEF, a novel and fully automatic approach to fix C integer errors based on range analysis. RABIEF is inspired by the following insights: (1) fixes for various integer errors have typical patterns including sanitization, explicit cast and declared type alteration; (2) range analysis provides sound basis for error detection and guides fix generation. We implemented RABIEF into a tool ARGYI. Its effectiveness and efficiency have been substantiated by the facts that: (1) ARGYI succeeds in fixing 93.9% of 5414 integer bugs from Juliet test suite, scaling to 600 KLOC within 5500 seconds; (2) ARGYI is confirmed to correctly fix 20 errors from 4 real-world programs within only 240 seconds.",Fixing pattern | Integer error | Range analysis,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Cheng, Xi",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84997241505,10.1145/2950290.2983972,Static loop analysis and its applications,"Loops are challenging structures in program analysis, and an effective loop analysis is crucial in the applications, such as symbolic execution and program verification. In the research, we will first perform a deep analysis and propose a classification according to the complexity of the loops. Then try to propose techniques for analyzing and summarizing different loops. At last, we apply the techniques in multiple applications.",Loop summarization | Program verification | Termination,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2016-11-01,Conference Paper,"Xie, Xiaofei",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2951913.2951934,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2951913.2951939,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.32,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.25,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.94,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.17,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.11,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.64,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.35,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.68,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.66,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.45,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.80,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.70,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.20,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.22,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.56,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.61,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1109/ICSME.2016.36,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85013243516,10.1109/ISSRE.2016.37,ORPLocator: Identifying Read Points of Configuration Options via Static Analysis,"Configuration options are widely used for customizing the behavior and initial settings of software applications, server processes, and operating systems. Their distinctive property is that each option is processed, defined, and described in different parts of a software project - namely in code, in configuration file, and in documentation. This creates a challenge for maintaining project consistency as it evolves. It also promotes inconsistencies leading to misconfiguration issues in production scenarios. We propose an approach for detection of inconsistencies between source code and documentation based on static analysis. Our approach automatically identifies source code locations where options are read, and for each such location retrieves the name of the option. Inconsistencies are then detected by comparing the results against the option names listed in documentation. We evaluated our approach on multiple components of Apache Hadoop, a complex framework with more than 800 options. Our tool ORPLocator was able to successfully locate at least one read point for 93% to 96% of documented options within four Hadoop components. A comparison with a previous state-of-the-art technique shows that our tool produces more accurate results. Moreover, our evaluation has uncovered 4 previously unknown, real-world inconsistencies between documented options and source code.",Configuration options | Empirical study | Inconsistency detection | Static analysis,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2016-12-05,Conference Paper,"Dong, Zhen;Andrzejak, Artur;Lo, David;Costa, Diego",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85013288101,10.1109/ISSRE.2016.31,Proving Concurrent Data Structures Linearizable,"Linearizability of concurrent data structure implementationsis notoriously hard to prove. Consequently, currentverification techniques can only prove linearizability for certainclasses of data structures. We introduce a generic, sound, andpractical technique to statically check the linearizability of concurrentdata structure implementations. Our technique involvesspecifying the concurrent operations as a list of sub-operationsand passing this specification on to an automated checker thatverifies linearizability using relationships between individual sub-operations. We have proven the soundness of our technique. Ourapproach is expressive: we have successfully verified the linearizabilityof 12 popular concurrent data structure implementationsincluding algorithms that are considered to be challenging toprove linearizable such as elimination back-off stack, lazy linkedlist, and time-stamped stack. Our checker is effective, as it canverify the specifications in less than a second.",concurrent data structures | Linearizability | verification,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2016-12-05,Conference Paper,"Singh, Vineet;Neamtiu, Iulian;Gupta, Rajiv",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85013271814,10.1109/ISSRE.2016.18,Bear: A Framework for Understanding Application Sensitivity to OS (Mis) Behavior,"Applications are generally written assuming a predictable and well-behaved OS. In practice, they experience unpredictable misbehavior at the OS level and across OSes: different OSes can handle network events differently, APIs can behave differently across OSes, and OSes may be compromised or buggy. This unpredictability is challenging because its sources typically manifest during deployment and are hard to reproduce. This paper introduces Bear, a framework for statistical analysis of application sensitivity to OS unpredictability that can help developers build more resilient software, discover challenging bugs and identify the scenarios that most need validation. Bear analyzes a program with a set of perturbation strategies on a set of commonly used system calls in order to discover the most sensitive system calls for each application, the most impactful strategies, and how they predict abnormal program outcome. We evaluated Bear with 113 CPU and IO-bound programs, and our results show that null memory dereferencing and erroneous buffer operations are the most impactful strategies for predicting abnormal program execution and that their impacts increase ten-fold with workload increase (e.g. number of network requests from 10 to 1000). Generic system calls are more sensitive than specialized system calls - for example, write and sendto can both be used to send data through a socket, but the sensitivity of write is twice that of sendto. System calls with an array parameter (e.g. read) are more sensitive to perturbations than those having a struct parameter with a buffer (e.g readv). Moreover, the fewer parameters a system call has, the more sensitive it is.",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2016-12-05,Conference Paper,"Sun, Ruimin;Lee, Andrew;Chen, Aokun;Porter, Donald E.;Bishop, Matt;Oliveira, Daniela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85013231727,10.1109/ISSRE.2016.42,Experience Report: An Empirical Study of API Failures in OpenStack Cloud Environments,"Stories about service outages in cloud environments have been making the headlines recently. In many cases, the reliability of cloud infrastructure Application Programming Interfaces (APIs) were at fault. Hence, understanding the factors affecting the reliability of these APIs is important to improve the availability of cloud services. In this study, we mined bugs of 25 modules within the 5 most important OpenStack APIs to understand API failures and characteristics. Our results show that in OpenStack, only one third of all API-related changes are due to fixing failures, with 7% of all fixes even changing the API interface, potentially breaking clients. Through qualitative analysis of 230 sampled API failures we observed that the majority of API related failures are due to small programming faults. Fortunately, the subject, message and stack trace as well as reply lag between comments included in these failures' bug reports provide a good indication of the cause of the failure.",API Failure | Cloud | Empirical Study | OpenStack,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2016-12-05,Conference Paper,"Musavi, Pooya;Adams, Bram;Khomh, Foutse",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84973444206,10.1145/2896982.2896994,An end-to-end domain specific modeling and analysis platform,"Software architecture models are specifications of the principal design decisions about a software system that primarily govern its structure, behavior, and quality. They serve as a basis for experimentation and rationalization of design decisions. While many techniques utilize and support software architecture modeling and analysis, a recurring obstacle is that often advances in one area (e.g., architecture-based modeling) tend to be disconnected from those in another area (e.g., simulation). In this work we aim to provide an end-to-end model-driven engineering approach, called DomainPro, that bridges this gap and supports engineers throughout the software modeling, analysis, and implementation process by automatically synthesizing a range of model interpreters (MI). DomainPro is also available to download at https://goo.gl/4sRT9B. You can also watch the demo video describing DomainPro's prominent features at https://youtu.be/6rg7pC6bhO0.",,"Proceedings - 8th International Workshop on Modeling in Software Engineering, MiSE 2016",2016-05-14,Conference Paper,"Shahbazian, Arman;Edwards, George;Medvidovic, Nenad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84973441844,10.1145/2896982.2896988,Architecture-centric derivation of products in a software product line,"It is essential to architecture-centric product line development that product line architecture can be used to drive activities specific to product line development, such as product derivation. This requires a mechanism that can automatically derive the architecture and code of a product instance from the customization of product line architecture. In this paper, we analyze the insufficiency of two existing solutions in this area and present an architecture-centric approach that meets the requirement. The approach can support product line differences in platforms and functions, and generate both product line code and product code. It is based on a product line implementation mechanism that combines a code generation and separation pattern with an architecture-based code annotation technique. We have implemented the approach, and finished a preliminary evaluation with a chat application.",Architecture-centric development | Product line architecture | Software architecture,"Proceedings - 8th International Workshop on Modeling in Software Engineering, MiSE 2016",2016-05-14,Conference Paper,"Cu, Cuong;Zheng, Yongjie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974588152,10.1145/2901739.2901748,How Android app developers manage power consumption? An empirical study by mining power management commits,"As Android platform becomes more and more popular, a large amount of Android applications have been developed. When developers design and implement Android applications, power consumption management is an important factor to consider since it affects the usability of the applications. Thus, it is important to help developers adopt proper strategies to manage power consumption. Interestingly, today, there is a large number of Android application repositories made publicly available in sites such as GitHub. These repositories can be mined to help crystalize common power management activities that developers do. These in turn can be used to help other developers to perform similar tasks to improve their own Android applications. In this paper, we present an empirical study of power management commits in Android applications. Our study extends that of Moura et al. who perform an empirical study on energy aware commits; however they do not focus on Android applications and only a few of the commits that they study come from Android applications. Android applications are often different from other applications (e.g., those running on a server) due to the issue of limited battery life and the use of specialized APIs. As subjects of our empirical study, we obtain a list of open source Android applications from F-Droid and crawl their commits from Github. We get 468 power management commits after we filter the commits using a set of keywords and by performing manual analysis. These 468 power management commits are from 154 different Android applications and belong to 15 different application categories. Furthermore, we use open card sort to categorize these power management commits and we obtain 6 groups which correspond to different power management activities. Our study also reveals that for different kinds of Android application (e.g., Games, Connectivity, Navigation, Internet, Phone & SMS, Time, etc.), the dominant power management activities differ. For example, the percentage of power management commits belonging to Power Adaptation activity is larger for Navigation applications than those belonging to other categories.",Empirical study | Mining software repository | Power consumption | Power management,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Bao, Lingfeng;Lo, David;Xia, Xin;Wang, Xinyu;Tian, Cong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974577705,10.1145/2901739.2901762,Inter-app communication in android: Developer challenges,"The Android platform is designed to support mutually untrusted third-party apps, which run as isolated processes but may interact via platform-controlled mechanisms, called Intents. Interactions among third-party apps are intended and can contribute to a rich user experience, for example, the ability to share pictures from one app with another. The Android platform presents an interesting point in a design space of module systems that is biased toward isolation, extensibility, and untrusted contributions. The Intent mechanism essentially provides message channels among modules, in which the set of message types is extensible. However, the module system has design limitations including the lack of consistent mechanisms to document message types, very limited checking that a message conforms to its specifications, the inability to explicitly declare dependencies on other modules, and the lack of checks for backward compatibility as message types evolve over time. In order to understand the degree to which these design limitations result in real issues, we studied a broad corpus of apps and cross-validated our results against app documentation and Android support forums. Our fndings suggest that design limitations do indeed cause development problems. Based on our results, we outline further research questions and propose possible mitigation strategies.",,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Ahmad, Waqar;Kästner, Christian;Sunshine, Joshua;Aldrich, Jonathan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974588541,10.1145/2901739.2901757,Understanding the exception handling strategies of Java libraries: An empirical study,"This paper presents an empirical study whose goal was to investigate the exception handling strategies adopted by Java libraries and their potential impact on the client applications. In this study, exception flow analysis was used in combination with manual inspections in order: (i) to characterize the exception handling strategies of existing Java libraries from the perspective of their users; and (ii) to identify exception handling anti-patterns. We extended an existing static analysis tool to reason about exception flows and handler actions of 656 Java libraries selected from 145 categories in the Maven Central Repository. The study findings suggest a current trend of a high number of undocumented API runtime exceptions (i.e., @throws in Javadoc) and Unintended Handler problem. Moreover, we could also identify a considerable number of occurrences of exception handling anti-patterns (e.g. Catch and Ignore). Finally, we have also analyzed 647 bug issues of the 7 most popular libraries and identified that 20.71% of the reports are defects related to the problems of the exception strategies and anti-patterns identified in our study. The results of this study point to the need of tools to better understand and document the exception handling behavior of libraries.",Empirical study | Exception flows analysis | Exception handling | Exception handling anti-patterns | Software libraries | Static analysis tool,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Sena, Demóstenes;Coelho, Roberta;Kulesza, Uirá;Bonifácio, Rodrigo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974569621,10.1145/2901739.2901752,"Mining Valence, arousal, and Dominance - Possibilities for detecting burnout and productivity?","Similar to other industries, the software engineering domain is plagued by psychological diseases such as burnout, which lead developers to lose interest, exhibit lower activity and/or feel powerless. Prevention is essential for such diseases, which in turn requires early identification of symptoms. The emotional dimensions of Valence, Arousal and Dominance (VAD) are able to derive a person's interest (attraction), level of activation and perceived level of control for a particular situation from textual communication, such as emails. As an initial step towards identifying symptoms of productivity loss in software engineering, this paper explores the VAD metrics and their properties on 700,000 Jira issue reports containing over 2,000,000 comments, since issue reports keep track of a developer's progress on addressing bugs or new features. Using a general-purpose lexicon of 14,000 English words with known VAD scores, our results show that issue reports of different type (e.g., Feature Request vs. Bug) have a fair variation of Valence, while increase in issue priority (e.g., from Minor to Critical) typically increases Arousal. Furthermore, we show that as an issue's resolution time increases, so does the arousal of the individual the issue is assigned to. Finally, the resolution of an issue increases valence, especially for the issue Reporter and for quickly addressed issues. The existence of such relations between VAD and issue report activities shows promise that text mining in the future could offer an alternative way for work health assessment surveys.",,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Mäntylä, Mika;Adams, Bram;Destefanis, Giuseppe;Graziotin, Daniel;Ortu, Marco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974588134,10.1145/2901739.2901771,On mining crowd-based speech documentation,"Despite the globalization of software development, relevant documentation of a project, such as requirements and design documents, often still is missing, incomplete or outdated. However, parts of that documentation can be found outside the project, where it is fragmented across hundreds of textual web documents like blog posts, email messages and forum posts, as well as multimedia documents such as screencasts and podcasts. Since dissecting and filtering multimedia information based on its relevancy to a given project is an inherently difficult task, it is necessary to provide an automated approach for mining this crowd-based documentation. In this paper, we are interested in mining the speech part of YouTube screencasts, since this part typically contains the rationale and insights of a screencast. We introduce a methodology that transcribes and analyzes the transcribed text using various Information Extraction (IE) techniques, and present a case study to illustrate the applicability of our mining methodology. In this case study, we extract use case scenarios from WordPress tutorial videos and show how their content can supplement existing documentation. We then evaluate how well existing rankings of video content are able to pinpoint the most relevant videos for a given scenario.",Crowd-based documentation | Information Extraction | Mining video content | Software documentation | Speech analysis,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Moslehi, Parisa;Adams, Bram;Rilling, Juergen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974574214,10.1145/2901739.2901751,Using dynamic and contextual features to predict issue lifetime in GitHub projects,"Methods for predicting issue lifetime can help software project managers to prioritize issues and allocate resources accordingly. Previous studies on issue lifetime prediction have focused on models built from static features, meaning features calculated at one snapshot of the issue's lifetime based on data associated to the issue itself. However, during its lifetime, an issue typically receives comments from various stakeholders, which may carry valuable insights into its perceived priority and difficulty and may thus be exploited to update lifetime predictions. Moreover, the lifetime of an issue depends not only on characteristics of the issue itself, but also on the state of the project as a whole. Hence, issue lifetime prediction may benefit from taking into account features capturing the issue's context (contextual features). In this work, we analyze issues from more than 4000 GitHub projects and build models to predict, at different points in an issue's lifetime, whether or not the issue will close within a given calendric period, by combining static, dynamic and contextual features. The results show that dynamic and contextual features complement the predictive power of static ones, particularly for long-term predictions.",Issue lifetime prediction | Issue tracking | Mining software repositories,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Kikas, Riivo;Dumas, Marlon;Pfahl, Dietmar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974604506,10.1145/2901739.2901742,A large-scale empirical study on self-admitted technical debt,"Technical debt is a metaphor introduced by Cunningham to indicate ""not quite right code which we postpone making it right"". Examples of technical debt are code smells and bug hazards. Several techniques have been proposed to detect different types of technical debt. Among those, Potdar and Shihab defined heuristics to detect instances of self-admitted technical debt in code comments, and used them to perform an empirical study on five software systems to investigate the phenomenon. Still, very little is known about the diffusion and evolution of technical debt in software projects. This paper presents a differentiated replication of the work by Potdar and Shihab. We run a study across 159 software projects to investigate the diffusion and evolution of self-admitted technical debt and its relationship with software quality. The study required the mining of over 600K commits and 2 Billion comments as well as a qualitative analysis performed via open coding. Our main findings show that self-admitted technical debt (i) is diffused, with an average of 51 instances per system, (ii) is mostly represented by code (30%), defect, and requirement debt (20% each), (iii) increases over time due to the introduction of new instances that are not fixed by developers, and (iv) even when fixed, it survives long time (over 1,000 commits on average) in the system.",Empirical software engineering | Mining software repositories | Technical debt,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Bavota, Gabriele;Russo, Barbara",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974602592,10.1145/2901739.2901759,"A large-scale study on repetitiveness, containment, and composability of routines in open-source projects","Source code in software systems has been shown to have a good degree of repetitiveness at the lexical, syntactical, and API usage levels. This paper presents a large-scale study on the repetitiveness, containment, and composability of source code at the semantic level. We collected a large dataset consisting of 9,224 Java projects with 2.79M class files, 17.54M methods with 187M SLOCs. For each method in a project, we build the program dependency graph (PDG) to represent a routine, and compare PDGs with one another as well as the subgraphs within them. We found that within a project, 12.1% of the routines are repeated, and most of them repeat from 2-7 times. As entirety, the routines are quite project-specific with only 3.3% of them exactly repeating in 1-4 other projects with at most 8 times. We also found that 26.1% and 7.27% of the routines are contained in other routine(s), i.e., implemented as part of other routine(s) elsewhere within a project and in other projects, respectively. Except for trivial routines, their repetitiveness and containment is independent of their complexity. Defining a subroutine via a per-variable slicing subgraph in a PDG, we found that 14.3% of all routines have all of their subroutines repeated. A high percentage of subroutines in a routine can be found/reused elsewhere. We collected 8,764,971 unique subroutines (with 323,564 unique JDK subroutines) as basic units for code searching/synthesis. We also provide practical implications of our findings to automated tools.",Code reuse | Composability | Containment | Repetitiveness,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Nguyen, Anh Tuan;Nguyen, Hoan Anh;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974530136,10.1145/2901739.2901767,From query to usable code: An analysis of Stack Overflow code snippets,"Enriched by natural language texts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries. With the goal of developing automated tools with the Stack Overflow snippets and surrounding text, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When using text search engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets? A total of 3M code snippets are analyzed across four languages: C#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C# proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.",Automatic program generation | Code mining,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Yang, Di;Hussain, Aftab;Lopes, Cristina Videira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974597181,10.1145/2901739.2903506,MUBench: A benchmark for API-misuse detectors,"Over the last few years, researchers proposed a multitude of automated bug-detection approaches that mine a class of bugs that we call API misuses. Evaluations on a variety of software products show both the omnipresence of such misuses and the ability of the approaches to detect them. This work presents MuBench, a dataset of 89 API misuses that we collected from 33 real-world projects and a survey. With the dataset we empirically analyze the prevalence of API misuses compared to other types of bugs, finding that they are rare, but almost always cause crashes. Furthermore, we discuss how to use it to benchmark and compare API-misuse detectors.",API-misuse detection | Benchmark | Bug detection,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Amann, Sven;Nadi, Sarah;Nguyen, Hoan A.;Nguyen, Tien N.;Mezini, Mira",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84974576965,10.1145/2901739.2903493,Judging a commit by its cover: Correlating commit message entropy with build status on Travis-CI,"Developers summarize their changes to code in commit messages. When a message seems ""unusual"", however, this puts doubt into the quality of the code contained in the commit. We trained n-gram language models and used cross-entropy as an indicator of commit message ""unusualness"" of over 120,000 commits from open source projects. Build statuses collected from Travis-CI were used as a proxy for code quality. We then compared the distributions of failed and successful commits with regards to the ""unusualness"" of their commit message. Our analysis yielded significant results when correlating cross-entropy with build status.",Build status | Commit message | Cross entropy | Github | N-gram language model | Open source | Travis-CI,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",2016-05-14,Conference Paper,"Santos, Eddie Antonio;Hindle, Abram",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/2983990.2984032,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984027,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984033,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984009,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984034,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2983991,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984017,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984040,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984036,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2983992,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2983996,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984012,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984011,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2983990.2984014,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908119,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908107,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908082,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908091,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908110,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908122,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2908080.2908125,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84965042795,10.1145/2837614.2837649,Symbolic computation of differential equivalences,"Ordinary differential equations (ODEs) are widespread in many natural sciences including chemistry, ecology, and systems biology, and in disciplines such as control theory and electrical engineering. Building on the celebrated molecules-As-processes paradigm, they have become increasingly popular in computer science, with highlevel languages and formal methods such as Petri nets, process algebra, and rule-based systems that are interpreted as ODEs. We consider the problem of comparing and minimizing ODEs automatically. Influenced by traditional approaches in the theory of programming, we propose differential equivalence relations. We study them for a basic intermediate language, for which we have decidability results, that can be targeted by a class of highlevel specifications. An ODE implicitly represents an uncountable state space, hence reasoning techniques cannot be borrowed from established domains such as probabilistic programs with finite-state Markov chain semantics. We provide novel symbolic procedures to check an equivalence and compute the largest one via partition refinement algorithms that use satisfiability modulo theories. We illustrate the generality of our framework by showing that differential equivalences include (i) well-known notions for the minimization of continuous-time Markov chains (lumpability), (ii) bisimulations for chemical reaction networks recently proposed by Cardelli et al., and (iii) behavioral relations for process algebra with ODE semantics. With a prototype implementation we are able to detect equivalences in biochemical models from the literature that cannot be reduced using competing automatic techniques.",Ordinary differential equations | Partition refinement | Quantitative equivalence relations | Satisfiability modulo theory,ACM SIGPLAN Notices,2016-04-08,Article,"Cardelli, Luca;Tribastone, Mirco;Tschaikowski, Max;Vandin, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2837614.2837630,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2837614.2837645,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2837614.2837615,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2837614.2837666,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2837614.2837628,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2837614.2837629,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85010775264,10.1109/SCAM.2016.18,Conc2Seq: A Frama-C plugin for verification of parallel compositions of c programs,"Frama-C is an extensible modular framework for analysis of C programs that offers different analyzers in the form of collaborating plugins. Currently, Frama-C does not support the proof of functional properties of concurrent code. We present Conc2Seq, a new code transformation based tool realized as a Frama-C plugin and dedicated to the verification of concurrent C programs. Assuming the program under verification respects an interleaving semantics, Conc2Seq transforms the original concurrent C program into a sequential one in which concurrency is simulated by interleavings. User specifications are automatically reintegrated into the new code without manual intervention. The goal of the proposed code transformation technique is to allow the user to reason about a concurrent program through the interleaving semantics using existing Frama-C analyzers.",C language | Code transformation | Concurrent code | Formal verification | Specification transformation,"Proceedings - 2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation, SCAM 2016",2016-12-12,Conference Paper,"Blanchard, Allan;Kosmatov, Nikolai;Lemerre, Matthieu;Loulergue, Frederic",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85010722899,10.1109/SCAM.2016.20,Effects dependence graph: A key data concept for c source-to-source compilers,"Optimizations, transformations and analyses are applied to programs by compilers at the intermediate representation level, which usually does not include explicit variable declarations. This description level is fine for middle-ends and for source-to-source optimizers of simple languages. Meanwhile, the C language has become much more flexible since the C99 standard, and let variable and type declarations appear almost anywhere in source code. We present in this paper a new concept to manage C99 declarations in a source-to-source compiler: the Effects Dependence Graph, which is an extension of the classical Data Dependence Graph. It deals particularly efficiently with user-defined type declarations or dependent types like Variable-Length Array. It is also interesting because no legal scheduling transformation is hindered and because existing algorithms are either not or slightly modified. Finally it reduces the need for variable, struct and array privatization or live range analyses in automatic parallelizers. To the best of our knowledge, the declaration issue is ignored in the literature: existing C source-to-source compilers either do not support C99, or accept only restricted portions of code, and production compilers use low-level intermediate representations, possibly with annotations. In this way our solution addresses a wider range of compiler analysis issues.",C Language | Data Dependence Graph | Declaration Scheduling | Source-to-Source Compiler,"Proceedings - 2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation, SCAM 2016",2016-12-12,Conference Paper,"Lossing, Nelson;Guillou, Pierre;Irigoin, Francois",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2896377.2901477,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85010380020,10.1109/VISSOFT.2016.23,A Visualization Tool for 3D Graphics Program Comprehension and Debugging,"Real-time 3D graphics programs are based on a 3D pipeline structure. In a 3D pipeline, data is loaded, prepared, transferred from CPU to GPU, and then processed on GPU to create images. Regular debuggers and special graphics debuggers primarily focus on displaying data but do not show how data is transferred from CPU to GPU. If there is an error in data transfer, programmers usually need to locate the error by reading the source code, which many of our students find difficult. We have developed a tool to visualize data flow between CPU and GPU in a 3D pipeline. The visualization helps students understand 3D graphics programs, especially some of the implicit data connections in 3D graphics APIs. It also helps students quickly locate bugs related to data transfer in a 3D pipeline. We demonstrate our tool with an example that shows how a subtle error can be quickly detected in our visualization.",3D graphics program | Debugging | Software comprehension | Software visualization,"Proceedings - 2016 IEEE Working Conference on Software Visualization, VISSOFT 2016",2016-12-09,Conference Paper,"Podila, Sahithi;Zhu, Ying",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85010297386,10.1109/VISSOFT.2016.19,MetaVis: Exploring actionable visualization,"Software visualization can be very useful for answering complex questions that arise in the software development process. Although modern visualization engines offer expressive APIs for building such visualizations, developers often have difficulties to (1) identify a suitable visualization technique to answer their particular development question, and to (2) implement that visualization using the existing APIs. Examples that illustrate the usage of an engine to build concrete visualizations offer a good starting point, but developers may have to traverse long lists of categories and analyze examples one-by-one to find a suitable one. We propose MetaVis, a tool that fills the gap between existing visualization techniques and their practical applications during software development. We classify questions frequently formulated by software developers and for each, based on our expertise, identify suitable visualizations. MetaVis uses tags mined from these questions to offer a tag-iconic cloud-based visualization. Each tag links to suitable visualizations that developers can explore, modify and try out. We present initial results of an implementation of MetaVis in the Pharo programming environment. The tool visualizes 76 developers' questions assigned to 49 visualization examples.",Recommendation systems | Software visualization | User needs,"Proceedings - 2016 IEEE Working Conference on Software Visualization, VISSOFT 2016",2016-12-09,Conference Paper,"Merino, Leonel;Ghafari, Mohammad;Nierstrasz, Oscar;Bergel, Alexandre;Kubelka, Juraj",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2970276.2985779,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84989186733,10.1145/2970276.2970312,Taming android fragmentation: Characterizing and detecting compatibility issues for android apps,"Android ecosystem is heavily fragmented. The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps. As a result, various compatibility issues arise, causing poor user experience. However, little is known on the characteristics of such fragmentationinduced compatibility issues and no mature tools exist to help developers quickly diagnose and fix these issues. To bridge the gap, we conducted an empirical study on 191 real-world compatibility issues collected from popular opensource Android apps. Our study characterized the symptoms and root causes of compatibility issues, and disclosed that the patches of these issues exhibit common patterns. With these findings, we propose a technique named FicFinder to automatically detect compatibility issues in Android apps. FicFinder performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues are triggered. FicFinder reports actionable debugging information to developers when it detects potential issues. We evaluated FicFinder with 27 large-scale open-source Android apps. The results show that FicFinder can precisely detect compatibility issues in these apps and uncover previously-unknown issues.",Android fragmentation | Compatibility issues,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Wei, Lili;Liu, Yepang;Cheung, Shing Chi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989177864,10.1145/2970276.2970355,LockPeeker: Detecting latent locks in Java APIs,"Detecting lock-related defects has long been a hot research topic in software engineering. Many efforts have been spent on detecting such deadlocks in concurrent software systems. However, latent locks may be hidden in application programming interface (API) methods whose source code may not be accessible to developers. Many APIs have latent locks. For example, our study has shown that J2SE alone can have 2,000+ latent locks. As latent locks are less known by developers, they can cause deadlocks that are hard to perceive or diagnose. Meanwhile, the state-of-the-art tools mostly handle API methods as black boxes, and cannot detect deadlocks that involve such latent locks. In this paper, we propose a novel black-box testing approach, called LockPeeker, that reveals latent locks in Java APIs. The essential idea of LockPeeker is that latent locks of a given API method can be revealed by testing the method and summarizing the locking effects during testing execution. We have evaluated LockPeeker on ten real-world Java projects. Our evaluation results show that (1) LockPeeker detects 74.9% of latent locks in API methods, and (2) it enables state-of-the-art tools to detect deadlocks that otherwise cannot be detected.",API method | Deadlock detection | Latent lock,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Lin, Ziyi;Zhong, Hao;Chen, Yuting;Zhao, Jianjun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989182873,10.1145/2970276.2970309,Sound static deadlock analysis for C/Pthreads,"We present a static deadlock analysis for C/Pthreads. The design of our method has been guided by the requirement to analyse real-world code. Our approach is sound (i.e., misses no deadlocks) for programs that have defined behaviour according to the C standard and the Pthreads specification, and is precise enough to prove deadlock-freedom for a large number of such programs. The method consists of a pipeline of several analyses that build on a new contextand thread-sensitive abstract interpretation framework. We further present a lightweight dependency analysis to identify statements relevant to deadlock analysis and thus speed up the overall analysis. In our experimental evaluation, we succeeded to prove deadlock-freedom for 292 programs from the Debian GNU/Linux distribution with in total 2.3MLOC in 4 hours.",Abstract interpretation | Deadlock analysis | Static analysis,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Kroening, Daniel;Poetzl, Daniel;Schrammel, Peter;Wachter, Björn",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989154042,10.1145/2970276.2970305,Inferring annotations for device drivers from verification histories,"This paper studies and optimizes automated program verification. Detailed reasoning about software behavior is often facilitated by program invariants that hold across all program executions. Finding program invariants is in fact an essential step in automated program verification. Automatic discovery of precise invariants, however, can be very difficult in practice. The problem can be simplified if one has access to a candidate set of assertions (or annotations) and the search for invariants is limited over the space defined by these annotations. Then, the main challenge is to automatically generate quality program annotations. We present an approach that infers program annotations automatically by leveraging the history of verifying related programs. Our algorithm extracts high-quality annotations from previous verification attempts, and then applies them for verifying new programs. We present a case study where we applied our algorithm to Microsoft's Static Driver Verifier (SDV). SDV is an industrial-strength tool for verification of Windows device drivers that uses manually-tuned heuristics for obtaining a set of annotations. Our technique inferred program annotations comparable in performance to the existing annotations used in SDV that were devised manually by human experts over years. Additionally, the inferred annotations together with the existing ones improved the performance of SDV overall, proving correct 47% of drivers more while running 22% faster in our experiments.",Big Code | Invariant generation | Learning invariants | Program verification | Verification history,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Pavlinovic, Zvonimir;Lal, Akash;Sharma, Rahul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989206850,10.1145/2970276.2970310,Array length inference for C library bindings,"Simultaneous use of multiple programming languages (polyglot programming) assists in creating efficient, coherent, modern programs in the face of legacy code. However, manually creating bindings to low-level languages like C is tedious and error-prone. We offer relief in the formof an automated suite of analyses, designed to enhance the quality of automatically produced bindings. These analyses recover high-level array length information that is missing from C's type system. We emit annotations in the style of GObject-Introspection, which produces bindings from annotations on function signatures. We annotate each array argument as terminated by a special sentinel value, fixed-length, or of length determined by another argument. These properties help produce more idiomatic, efficient bindings. We correctly annotate at least 70% of all arrays with these length types, and our results are comparable to those produced by human annotators, but take far less time to produce.",Bindings | FFI | Foreign function interfaces | Libraries | Static analysis | Type inference,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Maas, Alisa J.;Nazaré, Henrique;Liblit, Ben",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989162862,10.1145/2970276.2970332,Conc-iSE: Incremental symbolic execution of concurrent software,"Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be reexplored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.",Concurrency | Partial order reduction | Symbolic execution | Weakest precondition,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Guo, Shengjian;Kusano, Markus;Wang, Chao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989189561,10.1145/2970276.2970323,Identifying domain elements from textual specifications,"Analysis modeling refers to the task of identifying domain objects, their attributes and operations, and the relationships between these objects from software requirements speci fications which are usually written in some natural language. There have been a few efforts to automate this task, but they seem to be largely constrained by the language related issues as well as the lack of a systematic transformation process. In this paper, we propose a systematic, automated transformation approach which first interprets the specification sentences based on the Hornby's verb patterns, and then uses semantic relationships between the words in the sentences, obtained from Type Dependencies using Stanford NL Parser, to identify the domain elements from them. With the help of a controlled experiment, we show that the analysis class diagrams generated by the proposed approach are far more correct, far more complete and less redundant than those generated by the exiting automated approaches.",Analysis class diagram | Analysis modeling | Automated approach | Model transformation | Natural Language Processing,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Thakur, Jitendra Singh;Gupta, Atul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989172598,10.1145/2970276.2970356,How good are the specs? A study of the bug-finding effectiveness of existing Java api specifications,"Runtime verification can be used to find bugs early, during software development, by monitoring test executions against formal specifications (specs). The quality of runtime verification depends on the quality of the specs. While previous research has produced many specs for the Java API, manually or through automatic mining, there has been no large-scale study of their bug-finding effectiveness. We present the first in-depth study of the bug-finding effectiveness of previously proposed specs. We used JavaMOP to monitor 182 manually written and 17 automatically mined specs against more than 18K manually written and 2.1M automatically generated tests in 200 open-source projects. The average runtime overhead was under 4.3×. We inspected 652 violations of manually written specs and (randomly sampled) 200 violations of automatically mined specs. We reported 95 bugs, out of which developers already fixed 74. However, most violations, 82.81% of 652 and 97.89% of 200, were false alarms. Our empirical results show that (1) runtime verification technology has matured enough to incur tolerable runtime overhead during testing, and (2) the existing API specifications can find many bugs that developers are willing to fix; however, (3) the false alarm rates are worrisome and suggest that substantial effort needs to be spent on engineering better specs and properly evaluating their effectiveness.",Empirical study | Runtime verification | Specification quality,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Legunsen, Owolabi;Ul Hassan, Wajih;Xu, Xinyue;Roşu, Grigore;Marinov, Darko",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84989170960,10.1145/2970276.2970286,Lightweight collection and storage of software repository data with datarover,"The ease of setting up collaboration infrastructures for software engineering projects creates a challenge for researchers that aim to analyze the resulting data. As teams can choose from various available software-as-a-service solutions and can configure them with a few clicks, researchers have to create and maintain multiple implementations for collecting and aggregating the collaboration data in order to perform their analyses across different setups. The DataRover system presented in this paper simplifies this task by only requiring custom source code for API authentication and querying. Data transformation and linkage is performed based on mappings, which users can define based on sample responses through a graphical front end. This allows storing the same input data in formats and databases most suitable for the intended analysis without requiring additional coding. Furthermore, API responses are continuously monitored to detect changes and allow users to update their mappings and data collectors accordingly. A screencast of the described use cases is available at https: //youtu.be/mt4ztff4SfU.",API monitoring | Data Collection | Data Mapping Definition | Data Storage | Link Discovery,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Kowark, Thomas;Matthies, Christoph;Uflacker, Matthias;Plattner, Hasso",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84989203072,10.1145/2970276.2970287,Visual contract extractor: A tool for reverse engineering visual contracts using dynamic analysis,"Visual contracts model the operations of classes, components or services by preand post-conditions formalised as graph transformation rules. They provide a precise but intuitive notation to test, document and analyse software systems. However, due to their detailed level of specification of data states and transformations, modelling a real application is a complex and error-prone process. Rather than adopting a top-down modelling approach, we follow a dynamic bottom-up approach to reverse engineer visual contracts from object-oriented programs based on tracing the execution of operations. We developed the Visual Contract Extractor (VCE), a dynamic analysis tool which supports the reverse engineering of visual operation contracts from Java programs. We explore the main features of the tool using two case studies and discuss usage scenarios ranging from traditional program understanding to novel applications in the field of model-based engineering. A screencast demonstrating the tool is provided at https://www.youtube.com/watch?v=VtTx8UHgRGo.",Dynamic analysis | Graph transformation | Model extraction | Reverse engineering | Specification mining | Visual contracts,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Alshanqiti, Abdullah;Heckel, Reiko;Kehrer, Timo",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84989173797,10.1145/2970276.2970289,AnModeler: A tool for generating domain models from textual specifications,"This paper presents AnModeler, a tool for generating analysis models from software requirements specified using use cases. The tool uses the Stanford natural language parser to extract type dependencies (TDs) and parts of speech tags (POS-tags) of sentences from input Use Case Specification (UCS). Then, it identifies sentence structures using a set of rules framed based on Hornby's verb patterns. With the information of the TDs, POS tags, and the identified sentence structures, the tool discovers domain elements, viz.: domain objects (including their attributes and operations) and interactions between them; it consolidates the domain information as a class diagram (as well as a sequence diagram). An experiment conducted on 10 UCSs with two industry experts as subjects showed that the analysis class diagrams generated by AnModeler were remarkably close to those generated by the two industry experts. Being lightweight and easy to use, the tool can also be used to assist students and young developers in acquiring object-oriented domain modeling skills quickly.",Analysis class diagram | Automated approach | Automated tool for analysis modeling | Model transformation | Problem level sequence diagram | Template code,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Thakur, Jitendra Singh;Gupta, Atul",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84971455382,10.1145/2884781.2884855,Toward a framework for detecting privacy policy violations in android application code,"Mobile applications frequently access sensitive personal information to meet user or business requirements. Because such information is sensitive in general, regulators increasingly require mobileapp developers to publish privacy policies that describe what information is collected. Furthermore, regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps. To help mobile-app developers check their privacy policies against their apps' code for consistency, we propose a semi-automated framework that consists of a policy terminology-API method map that links policy phrases to API methods that produce sensitive information, and information flow analysis to detect misalignments. We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of mappings from API methods to policy phrases. Our empirical evaluation on 477 top Android apps discovered 341 potential privacy policy violations.",Android Applications | Privacy Policies | Violation Detection,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Slavin, Rocky;Wang, Xiaoyin;Hosseini, Mitra Bokaei;Hester, James;Krishnan, Ram;Bhatia, Jaspreet;Breaux, Travis D.;Niu, Jianwei",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971483495,10.1145/2884781.2884829,Performance issues and optimizations in Java script: An empirical study,"As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-effciency of thousands of programs. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that ineffcient usage of APIs is the most prevalent root cause. Furthermore, we find that most issues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Selakovic, Marija;Pradel, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971462338,10.1145/2884781.2884843,Guiding dynamic symbolic execution toward unverified program executions,"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the.NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Christakis, Maria;Müller, Peter;Wüstholz, Valentin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971384484,10.1145/2884781.2884856,Synthesizing framework models for symbolic execution,"Symbolic execution is a powerful program analysis technique, but it is diffcult to apply to programs built using frameworks such as Swing and Android, because the framework code itself is hard to symbolically execute. The standard solution is to manually create a framework model that can be symbolically executed, but developing and maintaining a model is diffcult and error-prone. In this paper, we present Pasket, a new system that takes a first step toward automatically generating Java framework models to support symbolic execution. Pasket's focus is on creating models by instantiating design patterns. Pasket takes as input class, method, and type information from the framework API, together with tutorial programs that exercise the framework. From these artifacts and Pasket's internal knowledge of design patterns, Pasket synthesizes a framework model whose behavior on the tutorial programs matches that of the original framework. We evaluated Pasket by synthesizing models for subsets of Swing and Android. Our results show that the models derived by Pasket are suffcient to allow us to use off-the-shelf symbolic execution tools to analyze Java programs that rely on frameworks.",Framework Model | Program Synthesis | Sketch | Symbolic Execution,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Jeon, Jinseong;Qiu, Xiaokang;Fetter-Degges, Jonathan;Foster, Jeffrey S.;Solar-Lezamay, Armando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971377495,10.1145/2884781.2884808,SWIM: Synthesizing what i mean code search and idiomatic snippet synthesis,"Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expres-sions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe swim, a tool which suggests code snippets given API-related natural language queries such as\generate md5 hash code"". The query does not need to contain framework-specific trivia such as the type names or methods of interest. We translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce structured call sequences to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis. We evaluated swim with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workow is also very responsive, at an average of 1:5 seconds per snippet.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Raghothaman, Mukund;Wei, Yi;Hamadi, Youssef",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971484615,10.1145/2884781.2884881,Automatic model generation from documentation for Java API functions,"Modern software systems are becoming increasingly com-plex, relying on a lot of third-party library support. Li-brary behaviors are hence an integral part of software be-haviors. Analyzing them is as important as analyzing the software itself. However, analyzing libraries is highly chal-lenging due to the lack of source code, implementation in different languages, and complex optimizations. We observe that many Java library functions provide excellent documen-tation, which concisely describes the functionalities of the functions. We develop a novel technique that can construct models for Java API functions by analyzing the documen-tation. These models are simpler implementations in Java compared to the original ones and hence easier to analyze. More importantly, they provide the same functionalities as the original functions. Our technique successfully models 326 functions from 14 widely used Java classes. We also use these models in static taint analysis on Android apps and dynamic slicing for Java programs, demonstrating the effectiveness and efficiency of our models.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Zhai, Juan;Huang, Jianjun;Ma, Shiqing;Zhang, Xiangyu;Tan, Lin;Zhao, Jianhua;Qin, Feng",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84971450222,10.1145/2884781.2884800,Augmenting API documentation with insights from stack overflow,"Software developers need access to different kinds of information which is often dispersed among different documentation sources, such as API documentation or Stack Overow. We present an approach to automatically augment API documentation with\insight sentences"" from Stack Overow| sentences that are related to a particular API type and that provide insight not contained in the API documentation of that type. Based on a development set of 1,574 sentences, we compare the performance of two state-of-the-art summarization techniques as well as a pattern-based approach for insight sentence extraction. We then present SISE, a novel machine learning based approach that uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on the development set. In a comparative study with eight software developers, we found that SISE resulted in the highest number of sentences that were considered to add useful information not found in the API documentation. These results indicate that taking into account the meta data available on Stack Overow as well as part-of-speech tags can significantly improve unsupervised extraction approaches when applied to Stack Overow data.",API documentation | Insight sentences | Stack Overow,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Treude, Christoph;Robillard, Martin P.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84971476736,10.1145/2884781.2884868,Code anomalies flock together,"Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies-popularly known as code smells-may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to ""flock together"" to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Oizumi, Willian;Garcia, Alessandro;Da Silva Sousa, Leonardo;Cafeo, Bruno;Zhao, Yixue",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971484605,10.1145/2884781.2884792,Disseminating architectural knowledge on open-source projects a case study of the book architecture of open-source applications,"This paper reports on an interview-based study of 18 authors of different chapters of the two-volume book\Architecture of Open-Source Applications"". The main contributions are a synthesis of the process of authoring essay-style documents (ESDs) on software architecture, a series of observations on important factors that inuence the content and presentation of architectural knowledge in this documentation form, and a set of recommendations for readers and writers of ESDs on software architecture. We analyzed the inuence of three factors in particular: the evolution of a system, the community involvement in the project, and the personal characteristics of the author. This study provides the first systematic investigation of the creation of ESDs on software architecture. The observations we collected have implications for both readers and writers of ESDs, and for architecture documentation in general.",Architecture Description | Open-Source Software,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Robillard, Martin P.;Medvidovic, Nenad",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84971389478,10.1145/2884781.2884816,StubDroid: Automatic inference of precise data-flow summaries for the android framework,"Smartphone users suffer from insuffcient information on how commercial as well as malicious apps handle sensitive data stored on their phones. Automated taint analyses address this problem by allowing users to detect and investigate how applications access and handle this data. A current problem with virtually all those analysis approaches is, though, that they rely on explicit models of the Android runtime library. In most cases, the existence of those models is taken for granted, despite the fact that the models are hard to come by: Given the size and evolution speed of a modern smartphone operating system it is prohibitively expensive to derive models manually from code or documentation. In this work, we therefore present StubDroid, the first fully automated approach for inferring precise and effcient library models for taint-analysis problems. StubDroid automatically constructs these summaries from a binary distribution of the library. In our experiments, we use Stub-Droid-inferred models to prevent the static taint analysis FlowDroid from having to re-analyze the Android runtime library over and over again for each analyzed app. As the results show, the models make it possible to analyze apps in seconds whereas most complete re-analyses would time out after 30 minutes. Yet, StubDroid yields comparable precision. In comparison to manually crafted summaries, StubDroid's cause the analysis to be more precise and to use less time and memory.",Framework model | Library | Model inference | Static analysis | Summary,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Arzt, Steven;Bodden, Eric",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971383851,10.1145/2884781.2884849,An empirical study on the impact of C++ lambdas and programmer experience,"Lambdas have seen increasing use in mainstream programming languages, notably in Java 8 and C++ 11. While the technical aspects of lambdas are known, we conducted the first randomized controlled trial on the human factors impact of C++ 11 lambdas compared to iterators. Because there has been recent debate on having students or professionals in experiments, we recruited undergraduates across the academic pipeline and professional programmers to evaluate these findings in a broader context. Results afford some doubt that lambdas benefit developers and show evidence that students are negatively impacted in regard to how quickly they can write correct programs to a test specification and whether they can complete a task. Analysis from log data shows that participants spent more time with compiler errors, and have more errors, when using lambdas as compared to iterators, suggesting difficulty with the syntax chosen for C++. Finally, experienced users were more likely to complete tasks, with or without lambdas, and could do so more quickly, with experience as a factor explaining 45.7% of the variance in our sample in regard to completion time.",C++11 | Human Factors | Lambda Expressions,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Uesbeck, Phillip Merlin;Stefik, Andreas;Hanenberg, Stefan;Pedersen, Jan;Daleiden, Patrick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971467079,10.1145/2884781.2884788,Are non-functional requirements really non-functional? an investigation of non-functional requirements in practice,"Non-functional requirements (NFRs) are commonly distinguished from functional requirements by differentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also inuences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions. As a result, they remain diffcult to analyze and test. Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classification of 530 NFRs extracted from 11 industrial requirements specifications and analyze to which extent these NFRs describe system behavior. Our results suggest that most\non-functional"" requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.",Classification | Empirical studies | Model-based development | Non-functional requirements,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Eckhardt, Jonas;Vogelsang, Andreas;Fernández, Daniel Méndez",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971433442,10.1145/2884781.2884801,Probing for requirements knowledge to stimulate architectural thinking,"Software requirements specifications (SRSs) often lack the detail needed to make informed architectural decisions. Architects therefore either make assumptions, which can lead to incorrect decisions, or conduct additional stakeholder interviews, resulting in potential project delays. We previously observed that software architects ask Probing Questions (PQs) to gather information crucial to architectural decision-making. Our goal is to equip Business Analysts with appropriate PQs so that they can ask these questions themselves. We report a new study with over 40 experienced architects to identify reusable PQs for five areas of functionality and organize them into structured flows. These PQflows can be used by Business Analysts to elicit and specify architecturally relevant information. Additionally, we leverage machine learning techniques to determine when a PQ-flow is appropriate for use in a project, and to annotate individual PQs with relevant information extracted from the existing SRS. We trained and evaluated our approach on over 8,000 individual requirements from 114 requirements specifications and also conducted a pilot study to validate its usefulness.",Architecturally significant requirements | Automated requirement classification | Functional requirements | PQ-flows | Probing Questions (PQs) | Requirements knowledge,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Anish, Preethu Rose;Balasubramaniam, Balaji;Sainani, Abhishek;Cleland-Huang, Jane;Daneva, Maya;Wieringa, Roel J.;Ghaisas, Smita",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971393358,10.1145/2884781.2884832,Efficient large-scale trace checking using mapreduce,"The problem of checking a logged event trace against a temporal logic specification arises in many practical cases. Unfortunately, known algorithms for an expressive logic like MTL (Metric Temporal Logic) do not scale with respect to two crucial dimensions: the length of the trace and the size of the time interval of the formula to be checked. The former issue can be addressed by distributed and parallel trace checking algorithms that can take advantage of modern cloud computing and programming frameworks like MapReduce. Still, the latter issue remains open with current stateof-the-art approaches. In this paper we address this memory scalability issue by proposing a new semantics for MTL, called lazy semantics. This semantics can evaluate temporal formulae and boolean combinations of temporal-only formulae at any arbitrary time instant. We prove that lazy semantics is more expressive than point-based semantics and that it can be used as a basis for a correct parametric decomposition of any MTL formula into an equivalent one with smaller, bounded time intervals. We use lazy semantics to extend our previous distributed trace checking algorithm for MTL. The evaluation shows that the proposed algorithm can check formulae with large intervals, on large traces, in a memory-effcient way.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Bersani, Marcello M.;Bianculli, Domenico;Ghezzi, Carlo;Krstic, Srdan;Pietro, Pierluigi San",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971384501,10.1145/2884781.2884790,Jumping through hoops: Why do Java developers struggle with cryptography APIs?,"To protect sensitive data processed by current applications, developers, whether security experts or not, have to rely on cryptography. While cryptography algorithms have become increasingly advanced, many data breaches occur because developers do not correctly use the corresponding APIs. To guide future research into practical solutions to this problem, we perform an empirical investigation into the obstacles developers face while using the Java cryptography APIs, the tasks they use the APIs for, and the kind of (tool) support they desire. We triangulate data from four separate studies that include the analysis of 100 StackOverflow posts, 100 GitHub repositories, and survey input from 48 developers. We find that while developers find it difficult to use certain cryptographic algorithms correctly, they feel surprisingly confident in selecting the right cryptography concepts (e.g., encryption vs. signatures). We also find that the APIs are generally perceived to be too low-level and that developers prefer more task-based solutions.",API misuse | Cryptography | Empirical software engineering,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Nadi, Sarah;Kruger, Stefan;Mezini, Mira;Bodden, Eric",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971482960,10.1145/2884781.2884841,Nomenest omen: Exploring and exploiting similarities between argument and parameter names,"Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We find that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to the corresponding parameter than any alternative argument available in the call site's scope. As applications of our findings, we present an anomaly detection technique that identifies 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83%.",Empirical study | Identifier names | Method arguments | Name-based program analysis | Static analysis,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Liu, Hui;Liu, Qiurong;Staicu, Cristian Alexandru;Pradel, Michael;Luo, Yue",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84971474073,10.1145/2884781.2884882,Locking discipline inference and checking,"Concurrency is a requirement for much modern software, but the implementation of multithreaded algorithms comes at the risk of errors such as data races. Programmers can prevent data races by documenting and obeying a locking discipline, which indicates which locks must be held in order to access which data. This paper introduces a formal semantics for locking specifications that gives a guarantee of race freedom. A notable difference from most other semantics is that it is in terms of values (which is what the runtime system locks) rather than variables. The paper also shows how to express the formal semantics in two different styles of analysis: abstract interpretation and type theory. We have implemented both analyses, in tools that operate on Java. To the best of our knowledge, these are the first tools that can soundly infer and check a locking discipline for Java. Our experiments compare the implementations with one another and with annotations written by programmers, showing that the ambiguities and unsoundness of previous formulations are a problem in practice.",,Proceedings - International Conference on Software Engineering,2016-05-14,Conference Paper,"Ernst, Michael D.;Lovato, Alberto;Macedonio, Damiano;Spoto, Fausto;Thaine, Javier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84979776445,10.1109/ICPC.2016.7503707,Improving code readability models with textual features,"Code reading is one of the most frequent activities in software maintenance; before implementing changes, it is necessary to fully understand source code often written by other developers. Thus, readability is a crucial aspect of source code that may significantly influence program comprehension effort. In general, models used to estimate software readability take into account only structural aspects of source code, e.g., line length and a number of comments. However, source code is a particular form of text; therefore, a code readability model should not ignore the textual aspects of source code encapsulated in identifiers and comments. In this paper, we propose a set of textual features aimed at measuring code readability. We evaluated the proposed textual features on 600 code snippets manually evaluated (in terms of readability) by 5K+ people. The results demonstrate that the proposed features complement classic structural features when predicting code readability judgments. Consequently, a code readability model based on a richer set of features, including the ones proposed in this paper, achieves a significantly higher accuracy as compared to all of the state-of-the-art readability models.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Scalabrino, Simone;Linares-Vasquez, Mario;Poshyvanyk, Denys;Oliveto, Rocco",Include,
10.1016/j.jss.2022.111515,2-s2.0-84979735897,10.1109/ICPC.2016.7503709,Navigating the WordPress plugin landscape,"WordPress includes a plugin mechanism that allows user-provided code to be executed in response to specific system events and input/output requests. The large number of extension points provided by WordPress makes it challenging for new plugin developers to understand which extension points they should use, while the thousands of existing plugins make it hard to find existing extension point handler implementations for use as examples when creating a new plugin. In this paper, we present a lightweight analysis, supplemented with information mined from source comments and the webpages hosted by WordPress for each plugin, that guides developers to the right extension points and to existing implementations of handlers for these extension points. We also present empirical information about how plugins are used in practice, providing guidance to both tool and prospective plugin developers.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Hills, Mark",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84979763131,10.1109/ICPC.2016.7503716,Can we find stable alternatives for unstable Eclipse interfaces?,"The Eclipse framework is a popular and widely adopted framework that has been evolving for over a decade. Like many other evolving software systems, the Eclipse framework provides both stable and supported interfaces (APIs) and unstable, discouraged, and unsupported interfaces (non-APIs). However, despite being discouraged by Eclipse, the usage of bad interfaces is not uncommon. Our previous research has confirmed t hat a s E clipse s tates, i ndeed A PIs a re stable while non-APIs are unstable. Applications using non-APIs face compatibility challenges in new Eclipse releases. Furthermore, our previous studies further revealed that the reason why application developers use the unstable interfaces is because they cannot find s table i nterfaces w ith t he f unctionality t hat they require. Moreover, in a study we conducted, Eclipse application developers stated that they manually find t he functionality from Eclipse. Eclipse being a very large complex software system with a large number of committers, we hypothesize that as developers manually search for the functionality they they require, it is possible that they miss stable interfaces offering the same functionality. To this end, using code clone detection techniques, we analyzed 18 major releases of Eclipse for possible clones. Our findings a re t hree fold: i ) we discover that indeed there exist clones in Eclipse, ii) we also discovered that some of the identified c lones o riginate f rom different Eclipse projects, iii) our findings r eveal t hat t here i s no significant number of APIs (less than 1%) offering the same or similar functionality as the non-APIs in all Eclipse releases we studied. This reveals that there are very few syntactic clones between API and non-API, thus developers were forced to either use non-API or find a n API t hat e xists, t hat i s similar in functionality, but not in syntax.",code clone | Eclipse | Evolution | Stable Interfaces | Unstable interfaces,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Kawuma, Simon;Businge, John;Bainomugisha, Engineer",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84979735975,10.1109/ICPC.2016.7503721,On automatically detecting similar Android apps,"Detecting similar applications is a challenging problem, since it implies that similar high-level features and their low-level implementations can be detected and matched automatically. We propose an approach for automatically detecting Closely reLated applications in ANdroid (CLANdroid) by relying on advanced Information Retrieval techniques and five semantic anchors: identifiers, Android APIs, intents, permissions, and sensors. To evaluate CLANdroid we created a benchmark consisting of 14,450 apps along with information on similar apps provided by Google Play. We also compared effectiveness of different semantic anchors for detecting similar apps as perceived by 27 users. The results show that using Android-specific semantic anchors are useful for detecting similar Android apps across different categories. We also measured the impact of third-party libraries and obfuscated code when identifying similar Android apps, and our results suggest that there is significant difference in the accuracy when third-party libraries are excluded.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Linares-Vasquez, Mario;Holtzhauer, Andrew;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84979741000,10.1109/ICPC.2016.7503724,Investigating the android apps' success: An empirical study,"Measuring the success of software systems was not a trivial task in the past. Nowadays, mobile apps provide a uniform schema, i.e., the average ratings provided by the apps' users to gauge their success. While recent research has focused on examining the relationship between change-and fault-proneness and apps' lack of success, as well as qualitatively analyzing the reasons behind the apps' users dissatisfaction, there is little empirical evidence on the factors related to the success of mobile apps. In this paper, we explore the relationships between the mobile apps' success and a set of metrics that not only characterize the apps themselves but also the quality of the APIs used by the apps, as well as user attributes when they interact with the apps. In particular, we measure API quality in terms of bugs fixed in APIs used by apps and changes that occurred in the API methods. We examine different kinds of changes including changes in the interfaces, implementation, and exception handling. For user-related factors, we leverage the number of app's downloads and installations, and users' reviews. Through an empirical study of 474 free Android apps, we find that factors such as the number of users' reviews provided for an app, app's category and size appear to have an impact on the app's success.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Guerrouj, Latifa;Baysal, Olga",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84979735895,10.1109/ICPC.2016.7503731,Detecting exploratory programming behaviors for introductory programming exercises,"Developers often perform the repeating cycle of implementation and evaluation when they need to deal with the unfamiliar portion of the source code. This cycle is named as exploratory programming. We regard exploratory programming as an effective way not only to improve novice's programming skill but also to support educators in programming exercise in University. Because when novices often use the exploratory programming, it means novices struggle to solve their assignments. Therefore, educators should grasp which elements, APIs or blocks novices often used exploratory programming for. In this paper, firstly we propose the definition of novice's exploratory programming to collect logs of exploratory based on various granularity by novices. Secondly, we propose an algorithm based on our proposed definition to automatically detect exploratory programming behaviors. We also conducted a small case study. As a result of automatic detection, our proposed algorithm allows us to know what elements of program novices often feel difficult and struggle for.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Makihara, Erina;Igaki, Hiroshi;Yoshida, Norihiro;Fujiwara, Kenji;Iida, Hajimu",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84979732328,10.1109/ICPC.2016.7503735,Comprehending source code of large software system for reuse,"Comprehending source code of large software system for code reuse is an important problem. It is due to very high cost of software maintenance. In this paper, the author shares the experience on comprehending code-base of PRISM-a model-based program analysis tool developed by Tata Consultancy Services-containing in total more than 500 KLOC. During this activity, the author not only detected two bugs in existing PRISM software system while comprehending more than half of the total code (approximately 300 KLOC), but also added feature of Application Programming Interface (API) for Intermediate Representation (IR) transformation utility for 'C' language consisting of 1 KLOC. The author applied combination of key techniques such as lexical and dynamic analyses, models such as Brook, Soloway to understand code-base and, mixed top-down and bottom-up approaches for comprehension. The author observed that speed of comprehension and reuse later increased multi-fold because half of code-base (approximately 250 KLOC) is automatically generated from specification and thus by focusing on understanding less than 25 KLOC.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Kulkarni, Aniket",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84979747846,10.1109/ICPC.2016.7503736,Defending against the attack of the micro-clones,"Micro-clones are small pieces of redundant code, such as repeated subexpressions or statements. In this paper, we establish the considerations and value toward automated detection and removal of micro-clones at scale. We leverage the Boa software mining infrastructure to detect micro-clones in a data set containing 380,125 Java repositories, and yield thousands of instances where redundant code may be safely removed. By filtering our results to target popular Java projects on GitHub, we proceed to issue 43 pull requests that patch micro-clones. In summary, 95% of our patches to active GitHub repositories are merged rapidly (within 15 hours on average). Moreover, none of our patches were contested; they either constituted a real flaw, or have not been considered due to repository inactivity. Our results suggest that the detection and removal of micro-clones is valued by developers, can be automated at scale, and may be fixed with rapid turnaround times.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Van Tonder, Rijnard;Le Goues, Claire",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84979735719,10.1109/ICPC.2016.7503744,WAVI: A reverse engineering tool for web applications,"Web developers face some unique challenges when trying to understand, modify and document the structure of their web applications. The heterogeneity and complexity of the underlying technologies and languages heighten comprehension problems. In particular, JavaScript, as an essential part of the Web ecosystem, is a language that offers a flexibility that can make its code hard to grasp, when it comes to comprehension and documentation tasks. In this paper, we present the first iteration of WAVI (WebAppViewer), a reverse engineering tool that uses static analysis and a filter-based mechanism to retrieve and document the structure of a Web application. WAVI is able to extract elements coming from essential web languages and frameworks such as HTML, JavaScript, CSS and Node.js. The tool makes use of some simple, effective heuristics to accurately retrieve dependency links for files and methods. WAVI also offers the visualisation of the extracted information as force-directed graphs and customized class diagrams. The effectiveness of WAVI is evaluated with experiments that demonstrate that (i) it can resolve JavaScript calls better than a recent technique, and (ii) its visualisation modules are intuitive and scalable.",class diagram | force-directed diagrams | JavaScript | reverse engineering | static analysis | web application,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Cloutier, Jonathan;Kpodjedo, Segla;El Boussaidi, Ghizlane",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84979735899,10.1109/ICPC.2016.7503746,STAC: A tool for Static Textual Analysis of Code,"Static textual analysis techniques have been recently applied to process and synthesize source code. The underlying tenet is that important information is embedded in code identifiers and internal code comments. Such information can be analyzed to provide automatic aid for several software engineering activities. To facilitate this line of work, we present STAC, a tool for supporting Static Textual Analysis of Code. STAC is designed as a light-weight stand-alone tool that provides a practical one-stop solution for code indexing. Code indexing is the process of extracting important textual information from source code. Accurate indexing has been found to significantly influence the performance of code retrieval and analysis methods. STAC provides features for extracting and processing textual patterns found in Java, C++, and C# code artifacts. These features include identifier splitting, stemming, lemmatization, and spell-checking. STAC is also provided as an API to help researchers to integrate basic code indexing features into their code.",,IEEE International Conference on Program Comprehension,2016-07-05,Conference Paper,"Khatiwada, Saket;Kelly, Michael;Mahmoud, Anas",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84978505258,10.1145/2915970.2915971,A study of the state-of-the-art of PaaS interoperability,"Background: Platform as a Service (PaaS) model of Cloud Computing is at an early stage of adoption in spite of its perceived benefits, especially in speeding up the development cycle of Cloud-aware applications. Wider adoption of this service model is impeded by multiple inhibitors, one of which is a lack of interoperability between proprietary services of public PaaS providers. The paper presents the state-of-the-art of PaaS interoperability initiatives from industry and academia. Although these initiatives evidence interest in solving the issue, they are focused either on system infrastructure services or on enabling application portability. PaaS service interoperability has not received much attention yet. Aim: The paper proposes a Semantic-Agent framework as an effective solution for spanning multiple public PaaSs and integrating differentiated application infrastructure services. The proposed interoperability framework will be independent of specific offerings and targeted at the needs of developers. Methodology: This PhD (of which the current study is a subset) encompasses three primary tasks. First, an OWL-S based ontology for semantic annotation of PaaS services will be designed. Second, a multi-agent mechanism will be devised to enable semantic matching and composition of services. Third, a proof-of-concept solution that is able to span multiple public PaaSs and establish service interoperability will be developed. Contribution: The two main contributions of this research are: i) establishing a comprehensive ontology for PaaS services supporting Cloud application development, and ii) devising a novel framework enabling their interoperability.",Agent | Application services | Cloud computing | Development | Integration | Interoperability | Ontology | PaaS | Semantics,ACM International Conference Proceeding Series,2016-06-01,Conference Paper,"Hoare, Suchismita",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84978468668,10.1145/2915970.2915984,A review-based comparative study of bad smell detection tools,"Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.",Bad smells | Comparative study | Detection tools | Systematic literature review,ACM International Conference Proceeding Series,2016-06-01,Conference Paper,"Fernandes, Eduardo;Oliveira, Johnatan;Vale, Gustavo;Paiva, Thanis;Figueiredo, Eduardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978516868,10.1145/2915970.2915981,What we have learnt adopting evidence-based software engineering for industrial practice,,Continuous improvement | Evidence | Learning from experience | Software engineering | Systematic review,ACM International Conference Proceeding Series,2016-06-01,Conference Paper,"Anderson, Steve;Sagar, Paul;Smith, Beth;Wallace, Ken",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84978519663,10.1145/2915970.2915985,A systematic mapping study on requirements scoping,"Context: Requirements scoping is one of the key activities in requirements management but also a major risk for project management. Continuously changing scope may create a congestion state in handling the requirements inflow which causes negative consequences, e.g. delays or scope creep. Objectives: In this paper, we look at requirements scoping literature outside Software Product Line (SPL) by exploring the current literature on the phenomenon, summarizing publication trends, performing thematic analysis and analyzing the strength of the evidence in the light of rigor and relevance assessment. Method: We run a Systematic Mapping Study (SMS) using snowballing procedure, supported by a database search for the start set identification, and identified 21 primary studies and 2 secondary studies. Results: The research interest in this area steadily increases and includes mainly case studies, validation or evaluation studies. The results were categorized into four themes: definitions, negative effects associated with scoping, challenges and identified methods/tools. The identified scope management techniques are also matched against the identified requirements scoping challenges.",Requirements scoping | Snowballing | Systematic mapping study,ACM International Conference Proceeding Series,2016-06-01,Conference Paper,"Wnuk, Krzysztof;Kollu, Ravichandra Kumar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978473838,10.1145/2915970.2915986,An empirical analysis of reopened bugs based on open source projects,"Background: Bug fixing is a long-term and time-consuming activity. A software bug experiences a typical life cycle from newly reported to finally closed by developers, but it could be reopened afterwards for further actions due to reasons such as unclear description given by the bug reporter and developer negligence. Bug reopening is neither desirable nor could be completely avoided in practice, and it is more likely to bring unnecessary workloads to already-busy developers. Aims: To the best of our knowledge, there has been a little previous work on software bug reopening. In order to further study in this area, we perform an empirical analysis to provide a comprehensive understanding of this special area. Method: Based on four open source projects from Eclipse product family, they are CDT, JDT, PDE and Platform, we first quantitatively analyze reopened bugs from perspectives of proportion, impacts and time distribution. After initial exploration on their characteristics, we then qualitatively summarize root causes for bug reopening, this is carried out by investigating developer discussions recorded in Eclipse Bugzilla. Results: Results show that 6%-10% of total bugs will lead to reopening eventually. Over 93% of reopened bugs place serious inuence on the normal operation of the system being developed. Several key reasons for bug reopening have been identified in our empirical study. Conclusions: Although reopened bugs have significant impacts on both end users and developers, it is quite possible to reduce bug reopening rate through the adoption of appropriate methods, such as promoting effective and efficient communication among bug reporters and developers, which is supported by empirical evidence in this study.",Bug reports | Bug tracking system | Empirical software engineering | Open source projects | Reopened bugs,ACM International Conference Proceeding Series,2016-06-01,Conference Paper,"Mi, Qing;Keung, Jacky",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994552861,10.1109/ICGSE.2016.35,Hiring in the global stage: Profiles of online contributions,"Managers are increasingly using online contributions to make hiring decisions. However, it is nontrivial to find the relevant information of candidates in large online, global communities. We present Visual Resume, a novel tool that aggregates information on contributions across two different types of peer production sites (a code hosting site and a technical Q&A forum). Visual Resume displays summaries of developers' contributions, and allows easy access to contribution details. It also facilitates pairwise comparisons of candidates through a card-based design. Our study, involving participants from global organizations or corporations that draw from the global community, indicates that Visual Resume facilitated hiring decisions, both technical and soft skills were important when making these decisions.",Developer profiles | hiring decisions | online contributions,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016",2016-09-26,Conference Paper,"Sarma, Anita;Chen, Xiaofan;Kuttal, Sandeep;Dabbish, Laura;Wang, Zhendong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994627456,10.1109/ICGSE.2016.19,Global software engineering: Evolution and trends,"Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.",global software engineering | GSE | ICGSE | longitudinal study | near-shoring | offshoring | outsourcing,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016",2016-09-26,Conference Paper,"Ebert, Christof;Kuhrmann, Marco;Prikladnicki, Rafael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978922994,10.1007/978-3-319-41528-4_6,Structural synthesis for GXW specifications,"We define the GXW fragment of linear temporal logic (LTL ) as the basis for synthesizing embedded control software for safetycritical applications. Since GXW includes the use of a weak-until operator we are able to specify a number of diverse programmable logic control (PLC ) problems, which we have compiled from industrial training sets. For GXW controller specifications, we develop a novel approach for synthesizing a set of synchronously communicating actor-based controllers. This synthesis algorithm proceeds by means of recursing over the structure of GXW specifications, and generates a set of dedicated and synchronously communicating sub-controllers according to the formula structure. In a subsequent step, 2QBF constraint solving identifies and tries to resolve potential conflicts between individual GXW specifications. This structural approach to GXW synthesis supports traceability between requirements and the generated control code as mandated by certification regimes for safety-critical software. Our experimental results suggest that GXW synthesis scales well to industrial-sized control synthesis problems with 20 input and output ports and beyond.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Cheng, Chih Hong;Hamza, Yassine;Ruess, Harald",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978924627,10.1007/978-3-319-41528-4_7,Bounded cycle synthesis,"We introduce a new approach for the synthesis of Mealy machines from specifications in linear-time temporal logic (LTL), where the number of cycles in the state graph of the implementation is limited by a given bound. Bounding the number of cycles leads to implementations that are structurally simpler and easier to understand. We solve the synthesis problem via an extension of SAT-based bounded synthesis, where we additionally construct a witness structure that limits the number of cycles. We also establish a triple-exponential upper and lower bound for the potential blow-up between the length of the LTL formula and the number of cycles in the state graph.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Finkbeiner, Bernd;Klein, Felix",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978803252,10.1007/978-3-319-41528-4_22,Automatic verification of iterated separating conjunctions using symbolic execution,"In permission logics such as separation logic, the iterated separating conjunction is a quantifier denoting access permission to an unbounded set of heap locations. In contrast to recursive predicates, iterated separating conjunctions do not prescribe a structure on the locations they range over, and so do not restrict how to traverse and modify these locations. This flexibility is important for the verification of randomaccess data structures such as arrays and data structures that can be traversed in multiple ways such as graphs. Despite its usefulness, no automatic program verifier natively supports iterated separating conjunctions; they are especially difficult to incorporate into symbolic execution engines, the prevalent technique for building verifiers for these logics. In this paper, we present the first symbolic execution technique to support general iterated separating conjunctions. We propose a novel representation of symbolic heaps and flexible support for logical specifications that quantify over heap locations. Our technique exhibits predictable and fast performance despite employing quantifiers at the SMT level, by carefully controlling quantifier instantiations. It is compatible with other features of permission logics such as fractional permissions, recursive predicates, and abstraction functions. Our technique is implemented as an extension of the Viper verification infrastructure.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Müller, Peter;Schwerhoff, Malte;Summers, Alexander J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978903969,10.1007/978-3-319-41528-4_29,Automatic reachability analysis for nonlinear hybrid models with C2E2,"C2E2 is a bounded reachability analysis tool for nonlinear dynamical systems and hybrid automaton models. Previously it required users to annotate each system of differential equations of the hybrid automaton with discrepancy functions, and since these annotations are difficult to get for general nonlinear differential equations, the tool had limited usability. This version of C2E2 is improved in several ways, the most prominent among which is the elimination of the need for userprovided discrepancy functions. It automatically computes piece-wise (or local) discrepancy functions around the reachable parts of the state space using symbolically computed Jacobian matrix and eigenvalue perturbation bounds. The special cases of linear and constant rate differential equations are handled with more efficient algorithm. In this paper, we discuss these and other new features that make the new C2E2 a usable tool for bounded reachability analysis of hybrid systems.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Fan, Chuchu;Qi, Bolun;Mitra, Sayan;Viswanathan, Mahesh;Duggirala, Parasara Sridhar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978910864,10.1007/978-3-319-41540-6_22,BDD-based boolean functional synthesis,"Boolean functional synthesis is the process of automatically obtaining a constructive formalization from a declarative relation that is given as a Boolean formula. Recently, a framework was proposed for Boolean functional synthesis that is based on Craig Interpolation and in which Boolean functions are represented as And-Inverter Graphs (AIGs). In this work we adapt this framework to the setting of Binary Decision Diagrams (BDDs), a standard data structure for representation of Boolean functions. Our motivation in studying BDDs is their common usage in temporal synthesis, a fundamental technique for constructing control software/hardware from temporal specifications, in which Boolean synthesis is a basic step. Rather than using Craig Interpolation, our method relies on a technique called Self-Substitution, which can be easily implemented by using existing BDD operations. We also show that this yields a novel way to perform quantifier elimination for BDDs. In addition, we look at certain BDD structures called input-first, and propose a technique called TrimSubstitute, tailored specifically for such structures. Experiments on scalable benchmarks show that both Self- Substitution and TrimSubstitute scale well for benchmarks with good variable orders and significantly outperform current Boolean-synthesis techniques.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Fried, Dror;Tabajara, Lucas M.;Vardi, Moshe Y.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978924211,10.1007/978-3-319-41540-6_24,Property directed equivalence via abstract simulation,"We present a novel approach for automated incremental verification that employs both reusable and relational specifications of software to incrementally verify pairs of programs with possibly nested loops. It analyzes two programs, P - the one already verified, and Q - the one needed to be verified, and proceeds by detecting an abstraction αP of P and a simulation ρ, such that αP simulates Q via ρ. The key idea behind our simulation synthesis is to drive construction of both αP and ρ by the safe inductive invariants of P, thus guaranteeing the property preservations by the results. Finally, our approach allows effective lifting of the safe inductive invariants of P to Q using only αP and ρ. Based on our evaluation, in many cases when the absolute equivalence between programs cannot be proven, our approach is able to establish the property directed equivalence, confirming that the program Q is safe.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Fedyukovich, Grigory;Gurfinkel, Arie;Sharygina, Natasha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978887905,10.1007/978-3-319-41540-6_25,Combining model learning and model checking to analyze TCP implementations,"We combinemodel learning andmodel checking in a challenging case study involving Linux, Windows and FreeBSD implementations of TCP. We use model learning to infer models of different software components and then apply model checking to fully explore what may happen when these components (e.g. a Linux client and a Windows server) interact. Our analysis reveals several instances in which TCP implementations do not conform to their RFC specifications.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Fiterău-Broştean, Paul;Janssen, Ramon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2951913.2951934,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2951913.2951939,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994552861,10.1109/ICGSE.2016.35,Hiring in the global stage: Profiles of online contributions,"Managers are increasingly using online contributions to make hiring decisions. However, it is nontrivial to find the relevant information of candidates in large online, global communities. We present Visual Resume, a novel tool that aggregates information on contributions across two different types of peer production sites (a code hosting site and a technical Q&A forum). Visual Resume displays summaries of developers' contributions, and allows easy access to contribution details. It also facilitates pairwise comparisons of candidates through a card-based design. Our study, involving participants from global organizations or corporations that draw from the global community, indicates that Visual Resume facilitated hiring decisions, both technical and soft skills were important when making these decisions.",Developer profiles | hiring decisions | online contributions,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016",2016-09-26,Conference Paper,"Sarma, Anita;Chen, Xiaofan;Kuttal, Sandeep;Dabbish, Laura;Wang, Zhendong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994627456,10.1109/ICGSE.2016.19,Global software engineering: Evolution and trends,"Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.",global software engineering | GSE | ICGSE | longitudinal study | near-shoring | offshoring | outsourcing,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016",2016-09-26,Conference Paper,"Ebert, Christof;Kuhrmann, Marco;Prikladnicki, Rafael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2744200,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2818639,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84969961445,10.1145/2876443,Inflow and retention in oss communities with commercial involvement: A case study of three hybrid projects,"Motivation: Open-source projects are often supported by companies, but such involvement often affects the robust contributor inflow needed to sustain the project and sometimes prompts key contributors to leave. To capture user innovation and to maintain quality of software and productivity of teams, these projects need to attract and retain contributors. Aim: We want to understand and quantify how inflow and retention are shaped by policies and actions of companies in three application server projects. Method: We identified three hybrid projects implementing the same JavaEE specification and used published literature, online materials, and interviews to quantify actions and policies companies used to get involved. We collected project repository data, analyzed affiliation history of project participants, and used generalized linear models and survival analysis to measure contributor inflow and retention. Results: We identified coherent groups of policies and actions undertaken by sponsoring companies as three models of community involvement and quantified tradeoffs between the inflow and retention each model provides. We found that full control mechanisms and high intensity of commercial involvement were associated with a decrease of external inflow and with improved retention. However, a shared control mechanism was associated with increased external inflow contemporaneously with the increase of commercial involvement. Implications: Inspired by a natural experiment, our methods enabled us to quantify aspects of the balance between community and private interests in open- source software projects and provide clear implications for the structure of future open-source communities.",Commercial involvement | Contributor inflow | Contributor retention | Extent and intensity of involvement | Hybrid project | Natural experiment,ACM Transactions on Software Engineering and Methodology,2016-04-01,Article,"Zhou, Minghui;Mockus, Audris;Ma, Xiujuan;Zhang, L. U.;Mei, Hong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84975317416,10.1145/2907942,Mining privacy goals from privacy policies using hybridized task recomposition,"Privacy policies describe high-level goals for corporate data practices; regulators require industries to make available conspicuous, accurate privacy policies to their customers. Consequently, software requirements must conform to those privacy policies. To help stakeholders extract privacy goals from policies, we introduce a semiautomated framework that combines crowdworker annotations, natural language typed dependency parses, and a reusable lexicon to improve goal-extraction coverage, precision, and recall. The framework evaluation consists of a five-policy corpus governing web and mobile information systems, yielding an average precision of 0.73 and recall of 0.83. The results show that no single framework element alone is sufficient to extract goals; however, the overall framework compensates for elemental limitations. Human annotators are highly adaptive at discovering annotations in new texts, but those annotations can be inconsistent and incomplete; dependency parsers lack sophisticated, tacit knowledge, but they can perform exhaustive text search for prospective requirements indicators; and while the lexicon may never completely saturate, the lexicon terms can be reliably used to improve recall. Lexical reuse reduces false negatives by 41%, increasing the average recall to 0.85. Last, crowd workers were able to identify and remove false positives by around 80%, which improves average precision to 0.93.",Crowdsourcing | Natural language processing | Privacy | Requirements extraction,ACM Transactions on Software Engineering and Methodology,2016-06-01,Article,"Bhatia, Jaspreet;Breaux, Travis D.;Schaub, Florian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2968444,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84962885356,10.1109/TSE.2015.2465386,Automatic Source Code Summarization of Context for Java Methods,"Source code summarization is the task of creating readable summaries that describe the functionality of software. Source code summarization is a critical component of documentation generation, for example as Javadocs formed from short paragraphs attached to each method in a Java program. At present, a majority of source code summarization is manual, in that the paragraphs are written by human experts. However, new automated technologies are becoming feasible. These automated techniques have been shown to be effective in select situations, though a key weakness is that they do not explain the source code's context. That is, they can describe the behavior of a Java method, but not why the method exists or what role it plays in the software. In this paper, we propose a source code summarization technique that writes English descriptions of Java methods by analyzing how those methods are invoked. We then performed two user studies to evaluate our approach. First, we compared our generated summaries to summaries written manually by experts. Then, we compared our summaries to summaries written by a state-of-the-art automatic summarization tool. We found that while our approach does not reach the quality of human-written summaries, we do improve over the state-of-the-art summarization tool in several dimensions by a statistically-significant margin.",automatic documentation | program comprehension | Source code summarization,IEEE Transactions on Software Engineering,2016-02-01,Article,"McBurney, Paul W.;McMillan, Collin",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84963864904,10.1109/TSE.2015.2465387,Evaluating the Effects of Architectural Documentation: A Case Study of a Large Scale Open Source Project,"Sustaining large open source development efforts requires recruiting new participants; however, a lack of architectural documentation might inhibit new participants since large amounts of project knowledge are unavailable to newcomers. We present the results of a multitrait, multimethod analysis of the effects of introducing architectural documentation into a substantial open source project - the Hadoop Distributed File System (HDFS). HDFS had only minimal architectural documentation, and we wanted to discover whether the putative benefits of architectural documentation could be observed over time. To do this, we created and publicized an architecture document and then monitored its usage and effects on the project. The results were somewhat ambiguous: by some measures the architecture documentation appeared to effect the project but not by others. Perhaps of equal importance is our discovery that the project maintained, in its web-accessible JIRA archive of software issues and fixes, enough architectural discussion to support architectural thinking and reasoning. This 'emergent' architecture documentation served an important purpose in recording core project members' architectural concerns and resolutions. However, this emergent architecture documentation did not serve all project members equally well; it appears that those on the periphery of the project - newcomers and adopters - still require explicit architecture documentation, as we will show.",documentation | open source software | Software architecture,IEEE Transactions on Software Engineering,2016-03-01,Article,"Kazman, Rick;Goldenson, Dennis;Monarch, Ira;Nichols, William;Valetto, Giuseppe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963808208,10.1109/TSE.2015.2478001,Metamorphic Testing for Software Quality Assessment: A Study of Search Engines,"Metamorphic testing is a testing technique that can be used to verify the functional correctness of software in the absence of an ideal oracle. This paper extends metamorphic testing into a user-oriented approach to software verification, validation, and quality assessment, and conducts large scale empirical studies with four major web search engines: Google, Bing, Chinese Bing, and Baidu. These search engines are very difficult to test and assess using conventional approaches owing to the lack of an objective and generally recognized oracle. The results are useful for both search engine developers and users, and demonstrate that our approach can effectively alleviate the oracle problem and challenges surrounding a lack of specifications when verifying, validating, and evaluating large and complex software systems.",lack of system specification | metamorphic testing | oracle problem | quality assessment | search engine | Software quality | user-oriented testing | validation | verification,IEEE Transactions on Software Engineering,2016-03-01,Article,"Zhou, Zhi Quan;Xiang, Shaowen;Chen, Tsong Yueh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84968813606,10.1109/TSE.2015.2479232,"Mapping Bug Reports to Relevant Files: A Ranking Model, a Fine-Grained Benchmark, and Feature Evaluation","When a new bug report is received, developers usually need to reproduce the bug and perform code reviews to find the cause, a process that can be tedious and time consuming. A tool for ranking all the source files with respect to how likely they are to contain the cause of the bug would enable developers to narrow down their search and improve productivity. This paper introduces an adaptive ranking approach that leverages project knowledge through functional decomposition of source code, API descriptions of library components, the bug-fixing history, the code change history, and the file dependency graph. Given a bug report, the ranking score of each source file is computed as a weighted combination of an array of features, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. We evaluate the ranking system on six large scale open source Java projects, using the before-fix version of the project for every bug report. The experimental results show that the learning-to-rank approach outperforms three recent state-of-the-art methods. In particular, our method makes correct recommendations within the top 10 ranked source files for over 70 percent of the bug reports in the Eclipse Platform and Tomcat projects.",Bug reports | Learning to rank | software maintenance,IEEE Transactions on Software Engineering,2016-04-01,Article,"Ye, Xin;Bunescu, Razvan;Liu, Chang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84983239068,10.1109/TSE.2016.2520468,Model checking software with first order logic specifications using AIG solvers,"Static verification techniques leverage Boolean formula satisfiability solvers such as SAT and SMT solvers that operate on conjunctive normal form and first order logic formulae, respectively, to validate programs. They force bounds on variable ranges and execution time and translate the program and its specifications into a Boolean formula. They are limited to programs of relatively low complexity for the following reasons. (1) A small increase in the bounds can cause a large increase in the size of the translated formula. (2) Boolean satisfiability solvers are restricted to using optimizations that apply at the level of the formula. Finally, (3) the Boolean formulae often need to be regenerated with higher bounds to ensure the correctness of the translation. We present a method that uses And-Inverter-Graph (AIG) sequential circuits, and AIG synthesis and verification frameworks to validate programs. An AIG is a Boolean formula with memory elements, logically complete negated conjunction gates, and a hierarchical structure. Encoding the validation problem of a program as an AIG (1) typically provides a more succinct representation than a Boolean formulae encoding with no memory elements, (2) preserves the high-level structure of the program, and (3) enables the use of a number of powerful automated analysis techniques that have no counterparts for other Boolean formulae such as CNF. Our method takes an imperative program with a first order logic specification consisting of a precondition and a postcondition pair, and a bound on the program variable ranges, and produces an AIG with a designated output that is ${true}$ when the program violates the specification. Our method uses AIG synthesis reduction techniques to reduce the AIG, and then uses AIG verification techniques to check the satisfiability of the designated output. The results show that our method can validate designs that are not possible with other state of the art techniques, and with bounds that are an order of magnitude larger.",Boolean satisfiability solvers | Hoare triplet | Software verification | static analysis,IEEE Transactions on Software Engineering,2016-08-01,Article,"Noureddine, Mohammad A.;Zaraket, Fadi A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84988843656,10.1109/TSE.2016.2527000,Probabilistic Interface Automata,"System specifications have long been expressed through automata-based languages, which allow for compositional construction of complex models and enable automated verification techniques such as model checking. Automata-based verification has been extensively used in the analysis of systems, where they are able to provide yes/no answers to queries regarding their temporal properties. Probabilistic modelling and checking aim at enriching this binary, qualitative information with quantitative information, more suitable to approaches such as reliability engineering. Compositional construction of software specifications reduces the specification effort, allowing the engineer to focus on specifying individual component behaviour to then analyse the composite system behaviour. Compositional construction also reduces the validation effort, since the validity of the composite specification should be dependent on the validity of the components. These component models are smaller and thus easier to validate. Compositional construction poses additional challenges in a probabilistic setting. Numerical annotations of probabilistically independent events must be contrasted against estimations or measurements, taking care of not compounding this quantification with exogenous factors, in particular the behaviour of other system components. Thus, the validity of compositionally constructed system specifications requires that the validated probabilistic behaviour of each component continues to be preserved in the composite system. However, existing probabilistic automata-based formalisms do not support specification of non-deterministic and probabilistic component behaviour which, when observed through logics such as pCTL, is preserved in the composite system. In this paper we present a probabilistic extension to Interface Automata which preserves pCTL properties under probabilistic fairness by ensuring a probabilistic branching simulation between component and composite automata. The extension not only supports probabilistic behaviour but also allows for weaker prerequisites to interfacing composition, that supports delayed synchronisation that may be required because of internal component behaviour. These results are equally applicable as an extension to non-probabilistic Interface Automata.",Behaviour models | interface automata | model checking | probability,IEEE Transactions on Software Engineering,2016-09-01,Article,"Pavese, Esteban;Braberman, Victor;Uchitel, Sebastian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84988845027,10.1109/TSE.2016.2527791,The Role of Method Chains and Comments in Software Readability and Comprehension-An Experiment,"Software readability and comprehension are important factors in software maintenance. There is a large body of research on software measurement, but the actual factors that make software easier to read or easier to comprehend are not well understood. In the present study, we investigate the role of method chains and code comments in software readability and comprehension. Our analysis comprises data from 104 students with varying programming experience. Readability and comprehension were measured by perceived readability, reading time and performance on a simple cloze test. Regarding perceived readability, our results show statistically significant differences between comment variants, but not between method chain variants. Regarding comprehension, there are no significant differences between method chain or comment variants. Student groups with low and high experience, respectively, show significant differences in perceived readability and performance on the cloze tests. Our results do not show any significant relationships between perceived readability and the other measures taken in the present study. Perceived readability might therefore be insufficient as the sole measure of software readability or comprehension. We also did not find any statistically significant relationships between size and perceived readability, reading time and comprehension.",comments | experiment | method chains | software comprehension | software measurement | Software readability,IEEE Transactions on Software Engineering,2016-09-01,Article,"Borstler, Jurgen;Paech, Barbara",Include,
10.1016/j.jss.2022.111515,2-s2.0-84990240335,10.1109/TSE.2015.2510633,A Multi-Objective Technique to Prioritize Test Cases,"While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.",Regression testing | requirements | test case prioritization | testing,IEEE Transactions on Software Engineering,2016-10-01,Article,"Marchetto, Alessandro;Islam, Md Mahfuzul;Asghar, Waseem;Susi, Angelo;Scanniello, Giuseppe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85006783309,10.1109/TSE.2016.2553032,An Industrial Survey of Safety Evidence Change Impact Analysis Practice,"Context. In many application domains, critical systems must comply with safety standards. This involves gathering safety evidence in the form of artefacts such as safety analyses, system specifications, and testing results. These artefacts can evolve during a system's lifecycle, creating a need for change impact analysis to guarantee that system safety and compliance are not jeopardised. Objective. We aim to provide new insights into how safety evidence change impact analysis is addressed in practice. The knowledge about this activity is limited despite the extensive research that has been conducted on change impact analysis and on safety evidence management. Method. We conducted an industrial survey on the circumstances under which safety evidence change impact analysis is addressed, the tool support used, and the challenges faced. Results. We obtained 97 valid responses representing 16 application domains, 28 countries, and 47 safety standards. The respondents had most often performed safety evidence change impact analysis during system development, from system specifications, and fully manually. No commercial change impact analysis tool was reported as used for all artefact types and insufficient tool support was the most frequent challenge. Conclusion. The results suggest that the different artefact types used as safety evidence co-evolve. In addition, the evolution of safety cases should probably be better managed, the level of automation in safety evidence change impact analysis is low, and the state of the practice can benefit from over 20 improvement areas.",change impact analysis | safety evidence | Safety-critical system | state of the practice | survey research,IEEE Transactions on Software Engineering,2016-12-01,Article,"De La Vara, Jose Luis;Borg, Markus;Wnuk, Krzysztof;Moonen, Leon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2767005,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2764901,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2840722,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84950272635,10.1145/2840724,A survey of automatic protocol reverse engineering tools,"Computer network protocols define the rules in which two entities communicate over a network of unique hosts. Many protocol specifications are unknown, unavailable, or minimally documented, which prevents thorough analysis of the protocol for security purposes. For example, modern botnets often use undocumented and unique application-layer communication protocols to maintain command and control over numerous distributed hosts. Inferring the specification of closed protocols has numerous advantages, such as intelligent deep packet inspection, enhanced intrusion detection system algorithms for communications, and integration with legacy software packages. Themultitude of closed protocols coupled with existing time-intensive reverse engineering methodologies has spawned investigation into automated approaches for reverse engineering of closed protocols. This article summarizes and organizes previously presented automatic protocol reverse engineering tools by approach. Approaches that focus on reverse engineering the finite state machine of a target protocol are separated from those that focus on reverse engineering the protocol format.",Communication security | Protocol reverse engineering,ACM Computing Surveys,2016-02-08,Review,"Narayan, John;Shukla, Sandeep K.;Clancy, T. Charles",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/2896377.2901477,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84955082401,10.1007/s10664-014-9341-9,Employing secure coding practices into industrial applications: a case study,"Industrial Control Systems (ICS) are the vital part of modern critical infrastructures. Recent attacks to ICS indicate that these systems have various types of vulnerabilities. A large number of vulnerabilities are due to secure coding problems in industrial applications. Several international and national organizations like: NIST, DHS, and US-CERT have provided extensive documentation on securing ICS; however proper details on securing software application for industrial setting were not presented. The notable point that makes securing a difficult task is the contradictions between security priorities in ICS and IT systems. In addition, none of the guidelines highlights the implications on modification of general IT security solutions to industrial settings. Moreover based on the best of our knowledge, steps to develop a successful real-world secure industrial application have not been reported. In this paper, the first attempts to employ secure coding best practices into a real world industrial application (Supervisory Control and Data Acquisition) called OpenSCADA is presented. Experiments indicate that resolving the vulnerabilities of OpenSCADA in addition to possible improvement in its availability, does not jeopardize other dimensions of security. In addition, all experiments are backed up with proper statistical tests to see whether or not, improvements are statistically significant.",Availability | Industrial control system | Memory leak | SCADA | Secure coding | Time critical process,Empirical Software Engineering,2016-02-01,Article,"Khalili, Abdullah;Sami, Ashkan;Azimi, Mahdi;Moshtari, Sara;Salehi, Zahra;Ghiasi, Mahboobe;Safavi, Ali Akbar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84955169475,10.1007/s10664-014-9344-6,An empirical study of the textual similarity between source code and source code summaries,"Source code documentation often contains summaries of source code written by authors. Recently, automatic source code summarization tools have emerged that generate summaries without requiring author intervention. These summaries are designed for readers to be able to understand the high-level concepts of the source code. Unfortunately, there is no agreed upon understanding of what makes up a “good summary.” This paper presents an empirical study examining summaries of source code written by authors, readers, and automatic source code summarization tools. This empirical study examines the textual similarity between source code and summaries of source code using Short Text Semantic Similarity metrics. We found that readers use source code in their summaries more than authors do. Additionally, this study finds that accuracy of a human written summary can be estimated by the textual similarity of that summary to the source code.",Automatic documentation generation | Documentation | Source code summarization | Textual similarity,Empirical Software Engineering,2016-02-01,Article,"McBurney, Paul W.;McMillan, Collin",Include,
10.1016/j.jss.2022.111515,2-s2.0-84955206453,10.1007/s10664-014-9347-3,Weighing lexical information for software clustering in the context of architecture recovery,"In this paper, we present a software clustering approach that leverages the information conveyed by the zone in which each lexeme appears in the classes of object oriented systems. We define six zones in the source code: Class Name, Attribute Name, Method Name, Parameter Name, Comment, and Source Code Statement. These zones may convey information with different levels of relevance, and so their contribution should be differently weighed according to the software system under study. To this aim, we define a probabilistic model of the lexemes distribution whose parameters are automatically estimated by the Expectation-Maximization algorithm. The weights of the zones are then exploited to compute similarities among source code classes, which are then grouped by a k-Medoid clustering algorithm. To assess the validity of our solution in the software architecture recovery field, we applied our approach to 19 software systems from different application domains. We observed that the use of our probabilistic model and the defined zones improves the quality of clustering results so that they are close to a theoretical upper bound we have proved.",Probabilistic model | Reengineering | Software clustering | Software maintenance | Software understanding,Empirical Software Engineering,2016-02-01,Article,"Corazza, Anna;Di Martino, Sergio;Maggio, Valerio;Scanniello, Giuseppe",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84955200920,10.1007/s10664-014-9350-8,Linguistic antipatterns: what they are and how developers perceive them,"Antipatterns are known as poor solutions to recurring problems. For example, Brown et al. and Fowler define practices concerning poor design or implementation solutions. However, we know that the source code lexicon is part of the factors that affect the psychological complexity of a program, i.e., factors that make a program difficult to understand and maintain by humans. The aim of this work is to identify recurring poor practices related to inconsistencies among the naming, documentation, and implementation of an entity—called Linguistic Antipatterns (LAs)—that may impair program understanding. To this end, we first mine examples of such inconsistencies in real open-source projects and abstract them into a catalog of 17 recurring LAs related to methods and attributes. Then, to understand the relevancy of LAs, we perform two empirical studies with developers—30 external (i.e., not familiar with the code) and 14 internal (i.e., people developing or maintaining the code). Results indicate that the majority of the participants perceive LAs as poor practices and therefore must be avoided—69 % and 51 % of the external and internal developers, respectively. As further evidence of LAs’ validity, open source developers that were made aware of LAs reacted to the issue by making code changes in 10 % of the cases. Finally, in order to facilitate the use of LAs in practice, we identified a subset of LAs which were universally agreed upon as being problematic; those which had a clear dissonance between code behavior and lexicon.",Developers’ perception | Empirical study | Linguistic antipatterns | Source code identifiers,Empirical Software Engineering,2016-02-01,Article,"Arnaoudova, Venera;Di Penta, Massimiliano;Antoniol, Giuliano",Include,
10.1016/j.jss.2022.111515,2-s2.0-84924262028,10.1007/s10664-015-9369-5,Automatic identifier inconsistency detection using code dictionary,"Inconsistent identifiers make it difficult for developers to understand source code. In particular, large software systems written by several developers can be vulnerable to identifier inconsistency. Unfortunately, it is not easy to detect inconsistent identifiers that are already used in source code. Although several techniques have been proposed to address this issue, many of these techniques can result in false alarms since such techniques do not accept domain words and idiom identifiers that are widely used in programming practice. This paper proposes an approach to detecting inconsistent identifiers based on a custom code dictionary. It first automatically builds a Code Dictionary from the existing API documents of popular Java projects by using an Natural Language Processing (NLP) parser. This dictionary records domain words with dominant part-of-speech (POS) and idiom identifiers. This set of domain words and idioms can improve the accuracy when detecting inconsistencies by reducing false alarms. The approach then takes a target program and detects inconsistent identifiers of the program by leveraging the Code Dictionary. We provide CodeAmigo, a GUI-based tool support for our approach. We evaluated our approach on seven Java based open-/proprietary- source projects. The results of the evaluations show that the approach can detect inconsistent identifiers with 85.4 % precision and 83.59 % recall values. In addition, we conducted an interview with developers who used our approach, and the interview confirmed that inconsistent identifiers frequently and inevitably occur in most software projects. The interviewees then stated that our approach can help to better detect inconsistent identifiers that would have been missed through manual detection.",Code dictionary | Code readability | Inconsistent identifiers | Part-of-speech analysis | Refactoring | Source code,Empirical Software Engineering,2016-04-01,Article,"Kim, Suntae;Kim, Dongsun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84926030795,10.1007/s10664-015-9362-z,On the detection of custom memory allocators in C binaries,"Many reverse engineering techniques for data structures rely on the knowledge of memory allocation routines. Typically, they interpose on the system’s malloc and free functions, and track each chunk of memory thus allocated as a data structure. However, many performance-critical applications implement their own custom memory allocators. Examples include webservers, database management systems, and compilers like gcc and clang. As a result, current binary analysis techniques for tracking data structures fail on such binaries. We present MemBrush, a new tool to detect memory allocation and deallocation functions in stripped binaries with high accuracy. We evaluated the technique on a large number of real world applications that use custom memory allocators. We demonstrate that MemBrush can detect allocators/deallocators with a high accuracy which is 52 out of 59 for allocators, and 29 out of 31 for deallocators in SPECINT 2006. As we show, we can furnish existing reverse engineering tools with detailed information about the memory management API, and as a result perform an analysis of the actual application specific data structures designed by the programmer. Our system uses dynamic analysis and detects memory allocation and deallocation routines by searching for functions that comply with a set of generic characteristics of allocators and deallocators.",Custom memory allocation | Dynamic binary analysis,Empirical Software Engineering,2016-06-01,Article,"Chen, Xi;Slowinska, Asia;Bos, Herbert",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84928137502,10.1007/s10664-015-9379-3,What are mobile developers asking about? A large scale study using stack overflow,"The popularity of mobile devices has been steadily growing in recent years. These devices heavily depend on software from the underlying operating systems to the applications they run. Prior research showed that mobile software is different than traditional, large software systems. However, to date most of our research has been conducted on traditional software systems. Very little work has focused on the issues that mobile developers face. Therefore, in this paper, we use data from the popular online Q&A site, Stack Overflow, and analyze 13,232,821 posts to examine what mobile developers ask about. We employ Latent Dirichlet allocation-based topic models to help us summarize the mobile-related questions. Our findings show that developers are asking about app distribution, mobile APIs, data management, sensors and context, mobile tools, and user interface development. We also determine what popular mobile-related issues are the most difficult, explore platform specific issues, and investigate the types (e.g., what, how, or why) of questions mobile developers ask. Our findings help highlight the challenges facing mobile developers that require more attention from the software engineering research and development communities in the future and establish a novel approach for analyzing questions asked on Q&A forums.",Mobile issues | Mobile software development | Stack overflow,Empirical Software Engineering,2016-06-01,Article,"Rosen, Christoffer;Shihab, Emad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84966415665,10.1007/s10664-015-9389-1,"What Java developers know about compatibility, and why this matters","Real-world programs are neither monolithic nor static—they are constructed using platform and third party libraries, and both programs and libraries continuously evolve in response to change pressure. In case of the Java language, rules defined in the Java Language and Java Virtual Machine Specifications define when library evolution is safe. These rules distinguish between three types of compatibility—binary, source and behavioural. We claim that some of these rules are counter intuitive and not well-understood by many developers. We present the results of a survey where we quizzed developers about their understanding of the various types of compatibility. 414 developers responded to our survey. We find that while most programmers are familiar with the rules of source compatibility, they generally lack knowledge about the rules of binary and behavioural compatibility. This can be problematic when organisations switch from integration builds to technologies that require dynamic linking, such as OSGi. We have assessed the gravity of the problem by studying how often linkage-related problems are referenced in issue tracking systems, and find that they are common.",API compatibility | Binary compatibility | Empirical study | Java | Linking | User survey,Empirical Software Engineering,2016-06-01,Article,"Dietrich, Jens;Jezek, Kamil;Brada, Premek",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84950252940,10.1007/s10664-015-9411-7,An exploratory study of api changes and usages based on apache and eclipse ecosystems,"Frameworks are widely used in modern software development to reduce development costs. They are accessed through their Application Programming Interfaces (APIs), which specify the contracts with client programs. When frameworks evolve, API backward-compatibility cannot always be guaranteed and client programs must upgrade to use the new releases. Because framework upgrades are not cost-free, observing API changes and usages together at fine-grained levels is necessary to help developers understand, assess, and forecast the cost of each framework upgrade. Whereas previous work studied API changes in frameworks and API usages in client programs separately, we analyse and classify API changes and usages together in 22 framework releases from the Apache and Eclipse ecosystems and their client programs. We find that (1) missing classes and methods happen more often in frameworks and affect client programs more often than the other API change types do, (2) missing interfaces occur rarely in frameworks but affect client programs often, (3) framework APIs are used on average in 35 % of client classes and interfaces, (4) most of such usages could be encapsulated locally and reduced in number, and (5) about 11 % of APIs usages could cause ripple effects in client programs when these APIs change. Based on these findings, we provide suggestions for developers and researchers to reduce the impact of API evolution through language mechanisms and design strategies.",API changes | API usages | Data mining | Empirical study | Framework ecosystems | Framework evolution,Empirical Software Engineering,2016-12-01,Article,"Wu, Wei;Khomh, Foutse;Adams, Bram;Guéhéneuc, Yann Gaël;Antoniol, Giuliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84933573820,10.1007/s10664-015-9392-6,Improving the performance of OCL constraint solving with novel heuristics for logical operations: a search-based approach,"A common practice to specify constraints on the Unified Modeling Language (UML) models is using the Object Constraint Language (OCL). Such constraints serve various purposes, ranging from simply providing precise meaning to the models to supporting complex verification and validation activities. In many applications, these constraints have to be solved to obtain values satisfying the constraints, for example, in the case of model-based testing (MBT) to generate test data for the purpose of generating executable test cases. In our previous work, we proposed novel heuristics for various OCL constructs to efficiently solve them using search algorithms. These heuristics are enhanced in this paper to further improve the performance of OCL constraint solving. We performed an empirical evaluation comprising of three case studies using three search algorithms: Alternating Variable Method (AVM), (1 + 1) Evolutionary Algorithm (EA), and a Genetic Algorithm (GA) and in addition Random Search (RS) was used as a comparison baseline. In the first case study, we evaluated each heuristics using carefully designed artificial problems. In the second case study, we evaluated the heuristics on various constraints of Cisco’s Video Conferencing Systems defined to support MBT. Finally, the third case study is about EU-Rent Car Rental specification and is obtained from the literature. The results of the empirical evaluation showed that (1 + 1) EA and AVM with the improved heuristics significantly outperform the rest of the algorithms.",Empirical evaluation | OCL | Search-based testing | Test data | UML,Empirical Software Engineering,2016-12-01,Article,"Ali, Shaukat;Iqbal, Muhammad Zohaib;Khalid, Maham;Arcuri, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84986877763,10.1016/j.jss.2016.08.053,Requirement-driven evolution in software product lines: A systematic mapping study,"CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products). CONCLUSION. Analyses of the results indicate that “Solution proposals” are the most common type of contribution (31%). Regarding the evolution activity, “Implement change” (43%) and “Analyze and plan change” (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.",Evolution | Software product lines | Systematic mapping study,Journal of Systems and Software,2016-12-01,Article,"Montalvillo, Leticia;Díaz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84949683066,10.1016/j.jss.2015.11.039,Safe reconfiguration of Coqcots and Pycots components,"Software systems have to face evolutions of their running context and users. Therefore, the so-called dynamic reconfiguration has been commonly adopted for modifying some components and/or the architecture at runtime. Traditional approaches typically stop the needed components, apply the changes, and restart the components. However, this scheme is not suitable for critical systems and degrades user experience. This paper proposes to switch from the stop/restart scheme to dynamic software updating (DSU) techniques. Instead of stopping a component, its implementation is replaced by another one specifically built to apply the modifications while maintaining the best quality of service possible. The major contributions of this work are: (i) the integration of DSU techniques in a component model; (ii) a reconfiguration development process including specification, proof of correctness using Coq, and; (iii) a systematic method to produce the executable script. In this perspective, the use of DSU techniques brings higher quality of service when reconfiguring component-based software. Moreover, the formalization allows ensuring the safety and consistency of the reconfiguration process.",Component model | Dynamic reconfiguration | Dynamic software updating | Runtime evolution,Journal of Systems and Software,2016-12-01,Article,"Buisson, Jérémy;Dagnat, Fabien;Leroux, Elena;Martinez, Sébastien",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84949844080,10.1016/j.jss.2015.11.035,A formal framework for context-aware systems specification and verification,"Context-aware applications development is still a challenging issue due to their adaptive behavior complexity and uncertainty features. A conceptual framework, as an ideal reuse technique, is one of the most suitable solutions to simplify the development of such systems and overcome their development complexity. We aim in this paper to design a framework that promotes the ability to specify and verify context-aware systems to assist and facilitate designer's task. The objective is gained here by combining two complementary modeling techniques: Model-driven Engineering (MDE) and Formal Methods. Model-driven technique is adopted to design a modeling framework for context-aware systems. Nevertheless, this technique generally lacks formal semantics and it is unfit for model analysis. Therefore, we define a formal semantics to overcome these drawbacks. Moreover, we show how context-aware system's adaptive behavior can be verified according to their invariants by applying model-checking techniques.",Context-aware adaptive systems | Formal methods | Model-driven engineering,Journal of Systems and Software,2016-12-01,Article,"Djoudi, Brahim;Bouanaka, Chafia;Zeghib, Nadia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84949818790,10.1016/j.jss.2015.09.002,Test automation of a measurement system using a domain-specific modelling language,"The construction of domain-specific modelling languages (DSMLs) is only the first step within the needed toolchain. Models need to be maintained, modified or functional errors searched for. Therefore, tool support is vital for the DSML end-user's efficiency. This paper presents SeTT, a simple but very useful tool for DSML end-users, a testing framework integrated within a DSML Sequencer. This Sequencer, part of the DEWESoft data acquisition system, supports the development of model-based tests using a high-level abstraction. The tests are used during the whole data acquisition process and able to test different systems' parts. This paper shows how high-level specifications can be extended to describe a testing infrastructure for a specific DSML. In this manner, the Sequencer and SeTT were combined at the metamodel level. The contribution of the paper is to show that one can leverage on the DSML to build a testing framework with relatively little effort, by implementing assertions to it.",Domain-specific modelling languages | Test automation | Usage experience,Journal of Systems and Software,2016-01-01,Article,"Kos, Tomaž;Mernik, Marjan;Kosar, Tomaž",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84949883493,10.1016/j.jss.2015.08.047,Evolving models in Model-Driven Engineering: State-of-the-art and future challenges,"The artefacts used in Model-Driven Engineering (MDE) evolve as a matter of course: models are modified and updated as part of the engineering process; metamodels change as a result of domain analysis and standardisation efforts; and the operations applied to models change as engineering requirements change. MDE artefacts are inter-related, and simultaneously constrain each other, making evolution a challenge to manage. We discuss some of the key problems of evolution in MDE, summarise the key state-of-the-art, and look forward to new challenges in research in this area.",Co-evolution | Evolution | Migration,Journal of Systems and Software,2016-01-01,Article,"Paige, Richard F.;Matragkas, Nicholas;Rose, Louis M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84947607270,10.1016/j.jss.2015.10.034,Decision architect - A decision documentation tool for industry,"Architecture decisions are often not explicitly documented in practice, even though explicit capturing and documentation of architecture decisions has been associated with a multitude of benefits. Several decision documentation tools have been proposed but they often do not meet the expectations and needs of the industry. As part of a research collaboration with ABB, we developed a decision documentation tool that aims to ensure industrial applicability. This tool is an add-in for Enterprise Architect and is an implementation of a viewpoint-based decision documentation framework. To validate the add-in, we conducted a case study with architects from ABB. In this study, we assessed to what extent and why software architects intend to use our approach. We therefore investigated the perceived usefulness, perceived ease-of-use, and contextual factors threat influence the architect's intention to use the tool. We found that the tool has a high relevance for software architects, that it increases the quality of decision documentation and that it improves productivity of architects in various ways. With respect to ease-of-use, the study identified the need for better guidelines. With respect to contextual factors, we found several factors that influence the intention to use the add-in in an industrial environment.",Architectural viewpoints | Architecture decisions | Tool-support,Journal of Systems and Software,2016-02-01,Conference Paper,"Manteuffel, Christian;Tofan, Dan;Avgeriou, Paris;Koziolek, Heiko;Goldschmidt, Thomas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84962437221,10.1016/j.jss.2015.11.037,Extracting reusable design decisions for UML-based domain-specific languages: A multi-method study,"When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the DSML itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other DSML projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 DSML projects. These projects were identified - among others - via an extensive systematic literature review (SLR) for the years 2005-2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based DSMLs publicly available. Furthermore, our study offers insights into UML usage (e.g. diagram types) and into the adoption of UML extension techniques (e.g. metamodel extensions, profiles).",Design decision | Design rationale | Domain-specific language | Domain-specific modeling | Model-driven development | Unified modeling language,Journal of Systems and Software,2016-03-01,Article,"Sobernig, Stefan;Hoisl, Bernhard;Strembeck, Mark",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84962339872,10.1016/j.jss.2015.12.003,Cost-effective strategies for the regression testing of database applications: Case study and lessons learned,"Testing and, more specifically, the regression testing of database applications is highly challenging and costly. One can rely on production data or generate synthetic data, for example based on combinatorial techniques or operational profiles. Both approaches have drawbacks and advantages. Automating testing with production data is impractical and combinatorial test suites might not be representative of system operations. In this paper, based on a large scale case study in a representative development environment, we explore the cost and effectiveness of various approaches and their combination for the regression testing of database applications, based on production data and synthetic data generated through classification tree models of the input domain. The results confirm that combinatorial test suite specifications bear little relation to test suite specifications derived from the system operational profile. Nevertheless, combinatorial testing strategies are effective, both in terms of the number of regression faults discovered but also, more surprisingly, in terms of the importance of these faults. However, our study also shows that relying solely on synthesized test data derived from test models could lead to important faults slipping to production. Thus, we recommend that testing on production data and combinatorial testing be combined to achieve optimal results.",Classification tree modeling | Database applications | Regression testing,Journal of Systems and Software,2016-03-01,Article,"Rogstad, Erik;Briand, Lionel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960130505,10.1016/j.jss.2015.10.042,Spot pricing in the Cloud ecosystem: A comparative investigation,"Background: Spot pricing is considered as a significant supplement for building a full-fledged market economy for the Cloud ecosystem. However, it seems that both providers and consumers are still hesitating to enter the Cloud spot market. The relevant academic community also has conflicting opinions about Cloud spot pricing in terms of revenue generation. Aim: This work aims to systematically identify, assess, synthesize and report the published evidence in favor of or against spot-price scheme compared with fixed-price scheme of Cloud computing, so as to help relieve the aforementioned conflict. Method: We employed the systematic literature review (SLR) method to collect and investigate the empirical studies of Cloud spot pricing indexed by major electronic libraries. Results: This SLR identified 61 primary studies that either delivered discussions or conducted experiments to perform comparison between spot pricing and fixed pricing in the Cloud domain. The reported benefits and limitations were summarized to facilitate cost-benefit analysis of being a Cloud spot pricing player, while four types of theories were distinguished to help both researchers and practitioners better understand the Cloud spot market. Conclusions: This SLR shows that the academic community strongly advocates the emerging Cloud spot market. Although there is still a lack of practical and easily deployable market-driven mechanisms, the overall findings of our work indicate that spot pricing plays a promising role in the sustainability of Cloud resource exploitation.",Cloud computing | Cloud spot pricing | Systematic literature review,Journal of Systems and Software,2016-04-01,Article,"Li, Zheng;Zhang, He;O'Brien, Liam;Jiang, Shu;Zhou, You;Kihl, Maria;Ranjan, Rajiv",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960088300,10.1016/j.jss.2015.12.056,A family of experiments to evaluate the understandability of TRiStar and i<sup>∗</sup> for modeling teleo-reactive systems,"The teleo-reactive approach facilitates reactive system development without losing sight of the system goals. Objective To introduce TRiStar as an extension of i∗ notation to specify teleo-reactive systems. To evaluate whether the notational extension is an improvement in terms of effectiveness and efficiency over the original language when it is used to specify teleo-reactive systems. Method A family of experiments was carried out with final-year engineering students and experienced software development professionals in which the participants were asked to fill in a form designed to evaluate the efficiency and effectiveness of each of the languages. Results Both the statistical results of the experiments, analyzed separately, and the meta-analysis of the experiments as a whole, allow us to conclude that TRiStar notation is more effective and efficient than i∗ as a requirements specification language for modeling teleo-reactive systems. Conclusion The extensions made on i∗ have led to TRiStar definition, a more effective and efficient goal-oriented notation than the original i∗ language.",i | Requirements engineering | Teleo-reactive | TRiStar,Journal of Systems and Software,2016-04-01,Article,"Morales, José Miguel;Navarro, Elena;Sánchez, Pedro;Alonso, Diego",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84929591037,10.1016/j.jss.2015.04.081,"Code search with input/output queries: Generalizing, ranking, and assessment","In this work we generalize, improve, and extensively assess our semantic source code search engine through which developers use an input/output query model to specify what behavior they want instead of how it may be implemented. Under this approach a code repository contains programs encoded as constraints and an SMT solver finds encoded programs that match an input/output query. The search engine returns a list of source code snippets that match the specification. The initial instantiation of this approach showed potential but was limited. It only encoded single-path programs, reported just complete matches, did not rank the results, and was only partly assessed. In this work, we explore the use of symbolic execution to address some of these technical shortcomings. We implemented a tool, Satsy, that uses symbolic execution to encode multi-path programs as constraints and a novel ranking algorithm based on the strength of the match between an input/output query and the program paths traversed by symbolic execution. An assessment about the relevance of Satsy's results versus other search engines, Merobase and Google, on eight novice-level programming tasks gathered from StackOverflow, using the opinions of 30 study participants, reveals that Satsy often out-performs the competition in terms of precision, and that matches are found in seconds.",Semantic code search | SMT solvers | Symbolic execution,Journal of Systems and Software,2016-06-01,Article,"Stolee, Kathryn T.;Elbaum, Sebastian;Dwyer, Matthew B.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84941255053,10.1016/j.jss.2015.08.006,ROAR: A QoS-oriented modeling framework for automated cloud resource allocation and optimization,"Cloud computing offers a fast, easy and cost-effective way to configure and allocate computing resources for web applications, such as consoles for smart grid applications, medical records systems, and security management platforms. Although a diverse collection of cloud resources (e.g., servers) is available, choosing the most optimized and cost-effective set of cloud resources for a given web application and set of quality of service (QoS) goals is not a straightforward task. Optimizing cloud resource allocation is a critical task for offering web applications using a software as a service model in the cloud, where minimizing operational cost while ensuring QoS goals are met is critical to meeting customer demands and maximizing profit. Manual load testing with different sets of cloud resources, followed by comparison of test results to QoS goals is tedious and inaccurate due to the limitations of the load testing tools, challenges characterizing resource utilization, significant manual test orchestration effort, and challenges identifying resource bottlenecks. This paper introduces our work using a modeling framework - ROAR (Resource Optimization, Allocation and Recommendation System) to simplify, optimize, and automate cloud resource allocation decisions to meet QoS goals for web applications, including complex multi-tier application distributed in different server groups. ROAR uses a domain-specific language to describe the configuration of the web application, the APIs to benchmark and the expected QoS requirements (e.g., throughput and latency), and the resource optimization engine uses model-based analysis and code generation to automatically deploy and load test the application in multiple resource configurations in order to derive a cost-optimal resource configuration that meets the QoS goals.",Cloud computing | Load testing and benchmarking | Resource optimization,Journal of Systems and Software,2016-06-01,Article,"Sun, Yu;White, Jules;Eade, Sean;Schmidt, Douglas C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84940688730,10.1016/j.jss.2015.07.037,METRIC: METamorphic Relation Identification based on the Category-choice framework,"Metamorphic testing is a promising technique for testing software systems when the oracle problem exists, and has been successfully applied to various application domains and paradigms. An important and essential task in metamorphic testing is the identification of metamorphic relations, which, due to the absence of a systematic and specification-based methodology, has often been done in an ad hoc manner - something which has hindered the applicability and effectiveness of metamorphic testing. To address this, a systematic methodology for identifying metamorphic relations based on the category-choice framework, called metric, is introduced in this paper. A tool implementing this methodology has been developed and examined in an experiment to determine the viability and effectiveness of metric, with the results of the experiment confirming that metric is both effective and efficient at identifying metamorphic relations.",Metamorphic testing | Oracle problem | Test oracle,Journal of Systems and Software,2016-06-01,Article,"Chen, Tsong Yueh;Poon, Pak Lok;Xie, Xiaoyuan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84963624895,10.1016/j.jss.2016.03.064,An approach to modeling and developing teleo-reactive systems considering timing constraints,"Context TeleoR is an extension and implementation of teleo-reactive (TR) language for defining the behavior of reactive systems when the consideration of timing constraints is a matter of interest. Objective This paper analyzes how to consider real-time constraints when a TR approach is followed from modeling to implementation. Method After carrying out a study of the type of timing constraints from the TR perspective, the possibility of using TeleoR for incorporating such constraints was considered. Some extensions on TRiStar notation were then made to represent temporal requirements. A drone-based case study was carried out to demonstrate the usefulness of this approach. Finally, a survey was conducted to validate the approach. Results TeleoR can, to a great extent, support the kind of real-time constraints required for developing real-time systems, offering a direct solution to five of the eight temporal requirements identified, which can be implemented using the basic features of the language. Conclusions Considering real-time requirements should be part of the specification of reactive systems implemented when using the TR approach and should be supported by the implementation platform. In this regard, TeleoR offers reasonable possibilities that should be extended by taking into account the limitations identified here.",Requirements engineering | Teleo-reactive | Timing constraints | TRiStar,Journal of Systems and Software,2016-07-01,Article,"Sánchez, Pedro;Álvarez, Bárbara;Morales, José Miguel;Alonso, Diego;Iborra, Andrés",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84964683434,10.1016/j.jss.2016.04.014,AspectJ code analysis and verification with GASR,"Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.",Aspect oriented programming | Logic program querying | Source code analysis,Journal of Systems and Software,2016-07-01,Article,"Fabry, Johan;De Roover, Coen;Noguera, Carlos;Zschaler, Steffen;Rashid, Awais;Jonckers, Viviane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961279419,10.1016/j.jss.2015.10.024,Grounded requirements engineering: An approach to use case driven requirements engineering,"Requirements engineering produces specifications of the needs or conditions to meet for a software product. These specifications may be vague and ungrounded, i.e. the relation of the requirements to the observations they are derived from may be unclear or not documented. Furthermore, stakeholders may be influenced by solutions of existing software without knowing if these actually suit the software to be developed. To cope with the above issues, it is important to understand the complete task, before designing a software system to support the task. Thus, we developed a method called Grounded Requirements Engineering (GRE) that leverages the Grounded Theory method to observe and analyze processes and user activities in the real world. GRE is an iterative process consisting of two steps. First, Grounded Theory methods are used to analyze user experiments or interviews. Second, the resulting abstract descriptions of the user behavior are transferred into use cases. GRE produces comprehensible and grounded requirements for the software system to be built, i.e. the requirements are traceable back to their origins. In this paper, we provide an elaborate description of the GRE method and illustrate it by applying it to derive requirements for an interactive software tool for model merging. The development of this tool both served as a basis for the design of GRE as well as to test it.",Grounded Theory | Requirements | Software engineering,Journal of Systems and Software,2016-07-01,Article,"Würfel, David;Lutz, Rainer;Diehl, Stephan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84969262130,10.1016/j.jss.2016.05.001,Agile methods in embedded system development: Multiple-case study of three industrial cases,"Agile methods are widely utilized in software development but their usage in embedded system development is often limited to software. A case study of three industrial cases was carried out to understand how to tailor agile methods effectively including also hardware development. Agile practices, mostly derived from Scrum, were tailored to fit the needs of each team and the method development was closely followed. Surveys conducted in the beginning and in the end of the cases were compared and complemented with interviews to understand the new working methods and their effects. Case evidence shows that interdependencies between work of each developer were taken into account better, visibility over the whole product increased and need for internal documentation diminished due to improved communication, but dividing hardware tasks into iterations was experienced difficult. With some tailoring, agile practices are beneficial also in the embedded system development. To successfully adopt agile methods into embedded system development, the team must consist of all the project members, the natural cycle lengths of different disciplines and different knowledge between the developers must be accepted and built upon, and the progress of the product must be presented or visualized in the end of each iteration.",Agile | Agile method | Case study | Embedded system,Journal of Systems and Software,2016-08-01,Article,"Könnölä, Kaisa;Suomi, Samuli;Mäkilä, Tuomas;Jokela, Tero;Rantala, Ville;Lehtonen, Teijo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84973138759,10.1016/j.jss.2016.03.055,Effect of developer collaboration activity on software quality in two large scale projects,"Developers work together during software development and maintenance to resolve issues and implement features in large software projects. The structure of their development collaboration activity may have impact on the quality of the final product in terms of higher number of defects. In this paper, we aim to understand the effect of collaboration on the defect proneness software. We model the collaboration of developers as an undirected network. We extract the centrality of the developers from the collaboration network using different measures that quantifies the importance of the nodes. We analyze the defect inducing and fixing data of the developers in two large software projects. Our findings in this study can be summarized as follows: (a) Centrality and source code change activity of developers in the collaboration network may change their defect induction rates i.e. the defect proneness of their change sets, (b) Contrary to the common perception, more experienced people have relatively higher defect induction rates.",Collaboration networks | Developer collaboration | Human factor in software engineering | Software quality,Journal of Systems and Software,2016-08-01,Article,"Çaglayan, Bora;Bener, Ayşe Başar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84969718661,10.1016/j.infsof.2016.05.004,A metamodel to integrate business processes time perspective in BPMN 2.0,"Context Business Process Management (BPM) is becoming a strategic advantage for organizations to streamline their operations. Most business experts are betting for OMG Business Process Model and Notation (BPMN) as de-facto standard (ISO/IEC 19510:2013) and selected technology to model processes. The temporal dimension underlies in any kind of process however, technicians need to shape this perspective that must also coexist with task control flow aspects, as well as resource and case perspectives. BPMN poorly gathers temporary rules. This is why there are contributions that extend the standard to cover such dimension. BPMN is mainly an imperative language. There are research contributions showing time constraints in BPMN, such as (i) BPMN patterns to express each rule with a combination of artifacts, thus these approaches increase the use of imperative BPMN style, and (ii) new decorators to capture time rules semantics giving clearer and simpler comprehensible specifications. Nevertheless, these extensions cannot yet be found in the present standard. Objective To define a time rule taxonomy easily found in most business processes and look for an approach that applies each rule with current BPMN 2.0 standard in a declarative way. Method A model-driven approach is used to propose a BPMN metamodel extension to address time-perspective. Results We look at a declarative approach where new time specifications may overlie the main control flow of a BPMN process. This proposal is totally supported with current BPMN standard, giving a BPMN metamodel extension with OCL constraints. We also use AQUA-WS as a software project case study which is planned and managed with MS Project. We illustrate business process extraction from project plans. Conclusion This paper suggests to handle business temporal rules with current BPMN standard, along with other business perspectives like resources and cases. This approach can be applied to reverse engineering processes from legacy databases.",BPM | BPMN | Legacy system | MDE | Metamodel | OCL | Time perspective,Information and Software Technology,2016-09-01,Review,"Arevalo, C.;Escalona, M. J.;Ramos, I.;Domínguez-Muñoz, M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960344985,10.1016/j.infsof.2016.01.018,Mining and checking paired functions in device drivers using characteristic fault injection,"Context Device drivers often call specific kernel interface functions in pairs to allocate and release resources, and these functions can be called as paired functions. But due to poor documentation and carelessness, developers sometimes misuse paired functions in drivers, which causes resource-usage violations. Objective Many dynamic approaches have been proposed to mine API rules and check resource usage for user-mode applications, but they are rarely applied to kernel-mode device drivers due to their designs. Meanwhile, most existing dynamic approaches lack systematic mechanisms to cover error handling code, which limits their availability and scalability. Our goal is to improve dynamic analysis to solve these problems. Method In this paper, we propose PairCheck, a novel approach for mining and checking paired functions in device drivers, using three techniques. Firstly, we design a characteristic fault injection framework to generate test cases, which simulates occasional errors and covers most error handling code with little effort. Secondly, complete runtime information is recorded through call interception during test-case execution. Thirdly, we mine and check paired functions based on collected runtime information, name patterns and statistical analysis. Result To validate the availability of PairCheck, we evaluate it on 11 Linux Ethernet card drivers. PairCheck mines 37 and 43 real paired functions in Linux 3.1.1 and 3.17.2, respectively. With these mined paired functions, it finds 10 violations in Linux 3.1.1 which have been fixed in 3.17.2, and 35 new violations in 3.17.2. The replies from developers indicate the false positive rate is low. Compared to normal execution, code coverage increases by 8.3% on average. Conclusion Our work shows that it is possible to precisely mine API rules of resource usage by using characteristic fault injection. The mined rules are useful for improving the reliability of device drivers.",Device drivers | Dynamic analysis | Fault injection | Paired functions | Resource usage,Information and Software Technology,2016-05-01,Article,"Bai, Jia Ju;Wang, Yu Ping;Liu, Hu Qiu;Hu, Shi Min",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978499609,10.1016/j.infsof.2016.01.008,Combining lexical and structural information to reconstruct software layers,"Context The architectures of existing software systems generally lack documentation or have often drifted from their initial design due to repetitive maintenance operations. To evolve such systems, it is mandatory to reconstruct and document their architectures. Many approaches were proposed to support the architecture recovery process but few of these consider the architectural style of the system under analysis. Moreover, most of existing approaches rely on structural dependencies between entities of the system and do not exploit the semantic information hidden in the source code of these entities. Objective We propose an approach that exploits both linguistic and structural information to recover the software architecture of Object Oriented (OO) systems. The focus of this paper is the recovery of architectures that comply with the layered style, which is widely used in software systems. Method In this work, we (i) recover the responsibilities of the system under study and (ii) assign these responsibilities to different abstraction layers. To do so, we use the linguistic information extracted from the source code to recover clusters corresponding to the responsibilities of the system. Then we assign these clusters to layers using the system's structural information and the layered style constraints. We formulate the recovery of the responsibilities and their assignment to layers as optimization problems that we solve using search-based algorithms. Results To assess the effectiveness of our approach we conducted experiments on four open source systems. The so-obtained layering results yielded higher precision and recall than those generated using a structural-based layering approach. Conclusion Our hybrid lexical–structural approach is effective and shows potential for significant improvement over techniques based only on structural information.",Architecture recovery | Hill climbing | Latent Dirichlet Allocation | Layering style | Reverse engineering | Software maintenance,Information and Software Technology,2016-06-01,Article,"Belle, Alvine Boaye;Boussaidi, Ghizlane El;Kpodjedo, Sègla",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84977503162,10.1016/j.infsof.2016.06.003,Documented decision-making strategies and decision knowledge in open source projects: An empirical study on Firefox issue reports,"Context: Decision-making is a vital task during software development. Typically, issue tracking systems are used to document decisions in large open source projects where developers are spread across the world. While most decision documentation approaches assume that developers use rational decision strategies, in practice also naturalistic strategies are employed. However, quantitative studies of the distribution of decision strategies and related knowledge are missing. Objective: Our overall goal is to provide insights and ideas for further research to systematically support and document decision-making during software development in open source projects. In this paper, we analyze decisions documented in comments to issue reports in order to understand the documentation of decision-making in detail. Method: We coded the comments of 260 issue reports of the open source project Firefox for decision-making strategies and knowledge on decisions. Then, we statistically analyzed the coded data with regard to the dominant decision strategy, the distribution of decision strategies and knowledge, and the relations between strategy and knowledge. Results: The vast majority of documented decision-making strategies was naturalistic. Interestingly, for feature requests the percentage of rational decision-making strategies was higher than for bugs. Documented knowledge mostly concerned the decision context. More solutions were documented together with a higher amount of naturalistic decision-making. However, solutions were negatively correlated with the assessment of the situation. So, developers are likely to exploit and document decision problems and solutions in an imbalanced way. Conclusion: Our analysis revealed important insights on how decision-making and its related knowledge is documented during software development in open source projects. For instance, we found naturalistic decision-making to play an important role for development decisions. Our coding tables can be used by other researchers to further investigate our results. The study insights should be reflected in decision support systems to improve their effectiveness and acceptance by developers.",Decision documentation | Decision knowledge | Decision-making strategy | Design decision | Empirical study | Issue tracking system | Naturalistic decision-making | Rational decision-making | Software development decision,Information and Software Technology,2016-11-01,Article,"Hesse, Tom Michael;Lerche, Veronika;Seiler, Marcus;Knoess, Konstantin;Paech, Barbara",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84958637662,10.1016/j.infsof.2015.12.004,Model-based incremental conformance checking to enable interactive product configuration,"Context Model-based product line engineering (PLE) is a paradigm that can enable automated product configuration of large-scale software systems, in which models are used as an abstract specification of commonalities and variabilities of products of a product line. Objective In the context of PLE, providing immediate feedback on the correctness of a manual configuration step to users has a practical impact on whether a configuration process with tool support can be successfully adopted in practice. Method In an existing work, a UML-based variability modeling methodology named as SimPL and an interactive configuration process was proposed. Based on the existing work, we propose an automated, incremental and efficient conformance checking approach to ensure that the manual configuration of a variation point conforms to a set of pre-defined conformance rules specified in the Object Constraint Language (OCL). The proposed approach, named as Zen-CC, has been implemented as an integrated part of our product configuration and derivation tool: Zen-Configurator. Results The performance and scalability of Zen-CC have been evaluated with a real-world case study. Results show that Zen-CC significantly outperformed two baseline engines in terms of performance. Besides, the performance of Zen-CC remains stable during the configuration of all the 10 products of the product line and its efficiency also remains un-impacted even with the growing product complexity, which is not the case for both of the baseline engines. Conclusion The results suggest that Zen-CC performs practically well and is much more scalable than the two baseline engines and is scalable for configuring products with a larger number of variation points.",Incremental conformance checking | Interactive product configuration | Model based engineering | Product line engineering | Variation point,Information and Software Technology,2016-04-01,Article,"Lu, Hong;Yue, Tao;Ali, Shaukat;Zhang, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84988354207,10.1016/j.infsof.2016.09.005,GODA: A goal-oriented requirements engineering framework for runtime dependability analysis,"Context: Many modern software systems must deal with changes and uncertainty. Traditional dependability requirements engineering is not equipped for this since it assumes that the context in which a system operates be stable and deterministic, which often leads to failures and recurrent corrective maintenance. The Contextual Goal Model (CGM), a requirements model that proposes the idea of context-dependent goal fulfillment, mitigates the problem by relating alternative strategies for achieving goals to the space of context changes. Additionally, the Runtime Goal Model (RGM) adds behavioral constraints to the fulfillment of goals that may be checked against system execution traces. Objective: This paper proposes GODA (Goal-Oriented Dependability Analysis) and its supporting framework as concrete means for reasoning about the dependability requirements of systems that operate in dynamic contexts. Method: GODA blends the power of CGM, RGM and probabilistic model checking to provide a formal requirements specification and verification solution. At design time, it can help with design and implementation decisions; at runtime it helps the system self-adapt by analyzing the different alternatives and selecting the one with the highest probability for the system to be dependable. GODA is integrated into TAO4ME, a state-of-the-art tool for goal modeling and analysis. Results: GODA has been evaluated against feasibility and scalability on Mobee: a real-life software system that allows people to share live and updated information about public transportation via mobile devices, and on larger goal models. GODA can verify, at runtime, up to two thousand leaf-tasks in less than 35ms, and requires less than 240 KB of memory. Conclusion: Presented results show GODA's design-time and runtime verification capabilities, even under limited computational resources, and the scalability of the proposed solution.",Dependability | Goal modeling | Probabilistic model checking | Runtime analysis,Information and Software Technology,2016-12-01,Article,"Mendonça, Danilo Filgueira;Nunes Rodrigues, Genaína;Ali, Raian;Alves, Vander;Baresi, Luciano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84949644579,10.1016/j.infsof.2015.11.006,Terminological inconsistency analysis of natural language requirements,"Context: Terminological inconsistencies owing to errors in usage of terms in requirements specifications could result into subtle yet critical problems in interpreting and applying these specifications into various phases of SDLC. Objective: In this paper, we consider special class of terminological inconsistencies arising from term-aliasing, wherein multiple terms spread across a corpus of natural language text requirements may be referring to the same entity. Identification of such alias entity-terms is a difficult problem for manual analysis as well as for developing tool support. Method: We consider the case of syntactic as well as semantic aliasing and propose a systematic approach for identifying these. Identification of syntactic aliasing involves automated generation of patterns for identifying syntactic variances of terms including abbreviations and introduced-aliases. Identification of semantic aliasing involves extracting multidimensional features (linguistic, statistical, and locational) from given requirement text to estimate semantic relatedness among terms. Based upon the estimated relatedness and standard language database based refinement, clusters of potential semantic aliases are generated. Results of these analyses with user refinement lead to generation of entity-term alias glossary and unification of term usage across requirements. Results: A prototype tool was developed to assess the effectiveness of the proposed approach for an automated analysis of term-aliasing in the requirements given as plain English language text. Experimental results suggest that approach is effective in identifying syntactic as well as semantic aliases, however, when aiming for higher recall on larger corpus, user selection is necessary to eliminate false positives. Conclusion: This proposed approach reduces the time-consuming and error-prone task of identifying multiple terms which might be referring to the same entity to a process of tool assisted identification of such term-aliases.",Alias identification | Entity disambiguation | Latent semantic analysis | Requirements analysis | Requirements management | Terminological inconsistency analysis,Information and Software Technology,2016-06-01,Article,"Misra, Janardan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019270101,10.1016/j.infsof.2016.05.002,Analyzing and visualizing information flow in heterogeneous component-based software systems,"Context: Component-based software engineering is aimed at managing the complexity of large-scale software development by composing systems from reusable parts. To understand or validate the behavior of such a system, one needs to understand the components involved in combination with understanding how they are configured and composed. This becomes increasingly difficult when components are implemented in various programming languages, and composition is specified in external artifacts. Moreover, tooling that supports in-depth system-wide analysis of such heterogeneous systems is lacking. Objective: This paper contributes a method to analyze and visualize information flow in a component-based system at various levels of abstraction. These visualizations are designed to support the comprehension needs of both safety domain experts and software developers for, respectively, certification and evolution of safety-critical cyber-physical systems. Method: We build system-wide dependence graphs and use static program slicing to determine all possible end-to-end information flows through and across a system's components. We define a hierarchy of five abstractions over these information flows that reduce visual distraction and cognitive overload, while satisfying the users’ information needs. We improve on our earlier work to provide interconnected views that support both systematic, as well as opportunistic navigation scenarios. Results: We discuss the design and implementation of our approach and the resulting views in a prototype tool called FlowTracker. We summarize the results of a qualitative evaluation study, carried out via two rounds of interview, on the effectiveness and usability of these views. We discuss a number of improvements, such as more selective information presentations, that resulted from the evaluation. Conclusion: The evaluation shows that the proposed approach and views are useful for understanding and validating heterogeneous component-based systems, and address information needs that could earlier only be met by manual inspection of the source code. We discuss lessons learned and directions for future work.",Component-based software systems | Information flow analysis | Model reconstruction | Program comprehension | Software visualization,Information and Software Technology,2016-09-01,Article,"Moonen, Leon;Yazdanshenas, Amir Reza",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84987985171,10.1016/j.infsof.2016.09.004,Towards designing an extendable vulnerability detection method for executable codes,"Context: Software vulnerabilities allow the attackers to harm the computer systems. Timely detection and removal of vulnerabilities help to improve the security of computer systems and avoid the losses from exploiting the vulnerabilities. Objective: Various methods have been proposed to detect the vulnerabilities in the past decades. However, most of these methods are suggested for detecting one or a limited number of vulnerability classes and require fundamental changes to be able to detect other vulnerabilities. In this paper, we present a first step towards designing an extendable vulnerability detection method that is independent from the characteristics of specific vulnerabilities. Method: To do so, we first propose a general model for specifying software vulnerabilities. Based on this model, a general specification method and an extendable algorithm is then presented for detecting the specified vulnerabilities in executable codes. As the first step, single-instruction vulnerabilities–the vulnerabilities that appear in one instruction–are specified and detected. We present a formal definition for single-instruction vulnerabilities. In our method, detection of the specified vulnerabilities is considered as solving a satisfaction problem. The suggested method is implemented as a plug-in for Valgrind binary instrumentation framework and the vulnerabilities are specified by the use of Valgrind intermediate language, called Vex. Results: Three classes of single-instruction vulnerabilities are specified in this paper, i. e. division by zero, integer bugs and NULL pointer dereference. The experiments demonstrate that the presented specification for these vulnerabilities are accurate and the implemented method can detect all the specified vulnerabilities. Conclusion: As we employ a general model for specifying the vulnerabilities and the structure of our vulnerability detection method does not depend on a specific vulnerability, our method can be extended to detect other specified vulnerabilities.",Executable codes | Extendable method | General specification | Software vulnerability,Information and Software Technology,2016-12-01,Article,"Mouzarani, Maryam;Sadeghiyan, Babak",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84946600354,10.1016/j.infsof.2015.09.004,Designing service-based applications in the presence of non-functional properties: A mapping study,"Context The development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. Objective This paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. Method Our analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. Results This paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. Conclusion This systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping-service-based-app-nfr.",Non-functional requirements | Service-based software process | Systematic mapping,Information and Software Technology,2016-01-01,Article,"Souza Neto, Plácido A.;Vargas-Solar, Genoveva;Da Costa, Umberto Souza;Musicante, Martin A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84964334205,10.1016/j.infsof.2016.03.007,EXAF: A search engine for sample applications of object-oriented framework-provided concepts,"Context: Object-oriented application frameworks, such as Java Swing, provide reusable code and design for implementing domain-specific concepts, such as Context Menu, in software applications. Hence, use of such frameworks not only can decrease the time and the cost of developing new software applications, but also can increase their maintainability. However, the main problems of using object-oriented application frameworks are their large and complex APIs, and often incomplete user manuals. To mitigate these problems, developers often try to learn how to implement their desired concepts from available sample applications. Nonetheless, this introduces another hard and time-consuming challenge which is finding proper sample applications. Objective: To address this difficulty, we introduce EXAF (EXample Applications Finder) that helps developers find sample applications which implement their desired framework-provided concepts. Method: The majority of existing framework comprehension approaches can only help programmers to get familiar with the usage of particular fine-grained API elements of the desired framework such as its classes and methods. Nevertheless, our approach is able to find sample applications that implement a particular framework-provided concept. To this end, EXAF benefits from the Latent Semantic Indexing (LSI) information retrieval technique. We evaluated the approach using 24 concepts on top of the Microsoft.Net, Qt, and Java Swing frameworks. Results: Based on our evaluations, the precision of EXAF is more than 79%. Besides, it can find some sample applications that could not be found by common code search engines such as the ones which are used in SourceForge and Google Code. Conclusions: The results of our evaluations indicate that EXAF is effective in practice, and yields better search results because it considers various artifacts of a project like user reviews and bug reports.",Code search engines | Framework-provided concepts | Frameworks comprehension | Object-oriented application frameworks | Sample applications,Information and Software Technology,2016-07-01,Article,"Noei, Ehsan;Heydarnoori, Abbas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84949463306,10.1016/j.infsof.2015.10.007,Hybrid business process modeling for the optimization of outcome data,"Context: Declarative business processes are commonly used to describe permitted and prohibited actions in a business process. However, most current proposals of declarative languages fail in three aspects: (1) they tend to be oriented only towards the execution order of the activities; (2) the optimization is oriented only towards the minimization of the execution time or the resources used in the business process; and (3) there is an absence of capacity of execution of declarative models in commercial Business Process Management Systems. Objective: This contribution aims at taking into account these three aspects, by means of: (1) the formalization of a hybrid model oriented towards obtaining the outcome data optimization by combining a data-oriented declarative specification and a control-flow-oriented imperative specification; and (2) the automatic creation from this hybrid model to an imperative model that is executable in a standard Business Process Management System. Method: An approach, based on the definition of a hybrid business process, which uses a constraint programming paradigm, is presented. This approach enables the optimized outcome data to be obtained at runtime for the various instances. Results: A language capable of defining a hybrid model is provided, and applied to a case study. Likewise, the automatic creation of an executable constraint satisfaction problem is addressed, whose resolution allows us to attain the optimized outcome data. A brief computational study is also shown. Conclusion: A hybrid business process is defined for the specification of the relationships between declarative data and control-flow imperative components of a business process. In addition, the way in which this hybrid model automatically creates an entirely imperative model at design time is also defined. The resulting imperative model, executable in any commercial Business Process Management System, can obtain, at execution time, the optimized outcome data of the process.",Business process | Constraint programming | Data optimization | Hybrid model,Information and Software Technology,2016-01-01,Article,"Parody, Luisa;Gómez-López, María Teresa;Gasca, Rafael M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960395805,10.1016/j.infsof.2016.01.011,Understanding the API usage in Java,"Context Application Programming Interfaces (APIs) facilitate the use of programming languages. They define sets of rules and specifications for software programs to interact with. The design of language API is usually artistic, driven by aesthetic concerns and the intuitions of language architects. Despite recent studies on limited scope of API usage, there is a lack of comprehensive, quantitative analyses that explore and seek to understand how real-world source code uses language APIs. Objective This study aims to understand how APIs are employed in practical development and explore their potential applications based on the results of API usage analysis. Method We conduct a large-scale, comprehensive, empirical analysis of the actual usage of APIs on Java, a modern, mature, and widely-used programming language. Our corpus contains over 5000 open-source Java projects, totaling 150 million source lines of code (SLoC). We study the usage of both core (official) API library and third-party (unofficial) API libraries. We resolve project dependencies automatically, generate accurate resolved abstract syntax trees (ASTs), capture used API entities from over 1.5 million ASTs, and measure the usage based on our defined metrics: frequency, popularity and coverage. Results Our study provides detailed quantitative information and yield insight, particularly, (1) confirms the conventional wisdom that the usage of APIs obeys Zipf distribution; (2) demonstrates that core API is not fully used (many classes, methods and fields have never been used); (3) discovers that deprecated API entities (in which some were deprecated long ago) are still widely used; (4) evaluates that the use of current compact profiles is under-utilized; (5) identifies API library coldspots and hotspots. Conclusions Our findings are suggestive of potential applications across language API design, optimization and restriction, API education, library recommendation and compact profile construction.",API usage | Empirical study | Java,Information and Software Technology,2016-05-01,Article,"Qiu, Dong;Li, Bixin;Leung, Hareton",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84964614622,10.1016/j.infsof.2016.01.001,Specification of behavioral anti-patterns for the verification of block-structured Collaborative Business Processes,"Context: The verification of the control flow of a Collaborative Business Process (CBP) is important when developing cross-organizational systems, since the control flow defines the behavior of the cross-organizational collaboration. Behavioral anti-patterns have been proposed to improve the performance of formal verification methods. However, a systematic approach for the discovery and specification of behavioral anti-patterns of CBPs has not been proposed so far. Objective: The aim of this work is an approach to systematically discover and specify the behavioral anti-patterns of block-structured CBP models. Method: The approach proposes using the metamodel of a CBP language to discover all possible combinations of constructs leading to a problem in the behavior of block-structured CBPs. Each combination is called minimal CBP. The set of all minimal CBPs with behavioral problems defines the unsoundness profile of a CBP language, from which is possible specifying the behavioral anti-patterns of such language. Results: The approach for specification of behavioral anti-patterns was applied to the UP-ColBPIP language. Twelve behavioral anti-patterns were defined, including support to complex control flow such as advanced synchronization, cancellation and exception management, and multiple instances. Anti-patterns were evaluated on a repository of block-structured CBP models and compared with a formal verification method. Results show that the verification based on anti-patterns is as accurate as the formal method, but it clearly improves the performance of the latter. Conclusion: By using the proposed approach, it is possible to systematically specify behavioral anti-patterns for block-structured CBP languages. During the discovery of anti-patterns different formalisms can be used. With this approach, the specification of anti-patterns provides the exact combination of elements that can cause a problem, making error correction and result interpretation easier. Although the proposed approach was defined for the context of CBPs, it could be applied to the context of intra-organizational processes.",Anti-patterns | Behavior | Collaborative Business Processes | Control flow | Cross-organizational collaborations | Formal methods | Verification,Information and Software Technology,2016-07-01,Article,"Roa, J.;Chiotti, O.;Villarreal, P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960145451,10.1016/j.infsof.2016.02.001,Effective algorithms for constructing minimum cost adaptive distinguishing sequences,"Context: Given a Finite State Machine (FSM), a checking sequence is a test sequence that determines whether the system under test is correct as long as certain standard assumptions hold. Many checking sequence generation methods use an adaptive distinguishing sequence (ADS), which is an experiment that distinguishes the states of the specification machine. Furthermore, it has been shown that the use of shorter ADSs yields shorter checking sequences. It is also known, on the other hand, that constructing a minimum cost ADS is an NP-hard problem and it is NP-hard to approximate. This motivates studying and investigating effective ADS construction methods. Objective: The main objective of this paper is to suggest new methods that can compute compact ADSs to be used in the construction of checking sequences. Method: We briefly present the existing ADS construction algorithms. We then propose generalizations of these approaches with a set of heuristics. We also conduct experiments to compare the size of the resultant ADSs and the length of the checking sequences constructed using these ADSs. Results: The results indicate that when the ADSs are constructed with the proposed methods, the length of the checking sequences may reduce up to 54% (40% on the average). Conclusions: In this paper, we present the state of the art ADS construction methods for FSMs and we propose generalizations of these methods. We show that our methods are effective in terms of computation time and ADS quality.",Adaptive distinguishing sequences | Checking sequences | Finite State Machines,Information and Software Technology,2016-06-01,Article,"Türker, Uraz Cengiz;Ünluÿurt, Tonguç;Yenigün, Hüsnü",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84958618379,10.1016/j.infsof.2015.11.008,Model-based specification of safety compliance needs for critical systems: A holistic generic metamodel,"Context: Many critical systems must comply with safety standards as a way of providing assurance that they do not pose undue risks to people, property, or the environment. Safety compliance is a very demanding activity, as the standards can consist of hundreds of pages and practitioners typically have to show the fulfilment of thousands of safety-related criteria. Furthermore, the text of the standards can be ambiguous, inconsistent, and hard to understand, making it difficult to determine how to effectively structure and manage safety compliance information. These issues become even more challenging when a system is intended to be reused in another application domain with different applicable standards. Objective: This paper aims to resolve these issues by providing a metamodel for the specification of safety compliance needs for critical systems. Method: The metamodel is holistic and generic, and abstracts common concepts for demonstrating safety compliance from different standards and application domains. Its application results in the specification of ""reference assurance frameworks"" for safety-critical systems, which correspond to a model of the safety criteria of a given standard. For validating the metamodel with safety standards, parts of several standards have been modelled by both academic and industry personnel, and other standards have been analysed. We further augment this with feedback from practitioners, including feedback during a workshop. Results: The results from the validation show that the metamodel can be used to specify safety compliance needs for aerospace, automotive, avionics, defence, healthcare, machinery, maritime, oil and gas, process industry, railway, and robotics. Practitioners consider that the metamodel can meet their needs and find benefits in its use. Conclusion: The metamodel supports the specification of safety compliance needs for most critical computer-based and software-intensive systems. The resulting models can provide an effective means of structuring and managing safety compliance information.",Reference assurance framework | Safety assurance | Safety certification | Safety compliance | Safety standard | Safety-critical system,Information and Software Technology,2016-04-01,Article,"De La Vara, Jose Luis;Ruiz, Alejandra;Attwood, Katrina;Espinoza, Huáscar;Panesar-Walawege, Rajwinder Kaur;López, Ángel;Del Río, Idoya;Kelly, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960414415,10.1016/j.infsof.2015.12.006,"A low-overhead, value-tracking approach to information flow security","Context: Sensitive information such as passwords often leaks inadvertently because of implementation defects. Objective: Our objective is to use dynamic techniques to prevent information leakage before it occurs. We also aim to develop techniques that incur low overheads, and are safe in the presence of aliasing. Method: We use a dynamic approach to track secret values and safe locations. We assume that programs have annotations which identify values and locations that need to be protected against disclosure. We instrument a program with statements that record relevant values and locations and assertions to relevant assignments to determine if they leak information. At run-time the values being assigned to unsafe locations are analysed. If a particular assignment leads to information leakage an assertion violation is triggered. We evaluate our approach by experimentation which uses our prototype implementation for C programs to analyse security-oriented UNIX utilities and programs chosen from the SPEC CPU datasets. Results: Our experiments show that the overhead to detect problems such as password disclosure in real software does not exceed 1%. The overheads associated with detection of CWE security vulnerabilities in real applications are still acceptable; however, tracking a large number of values incurs higher overheads (over 10 times in certain cases). Conclusion: Our dynamic approach to detecting information leaks can be used in various contexts. For a program that tracks only a limited number of values the overhead is marginal. Thus, our instrumentation can be used in release versions. However, if an application has a large number of secret values, our technique is useful in a testing phase. The overheads in this case are too high for a real use, but still within an acceptable range to be used for detection of potential leaks during testing.",Information leakage | Monitoring | Program instrumentation,Information and Software Technology,2016-05-01,Article,"Vorobyov, Kostyantyn;Krishnan, Padmanabhan;Stocks, Phil",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84961588444,10.1016/j.infsof.2016.03.004,Effective detection of android malware based on the usage of data flow APIs and machine learning,"Context. Android has been ranked as the top smartphone platform nowadays. Studies show that Android malware have increased dramatically and that personal privacy theft has become a major form of attack in recent years. These critical security circumstances have generated a strong interest in developing systems that automatically detect malicious behaviour in Android applications (apps). However, most methods of detecting sensitive data leakage have certain shortcomings, including computational expensiveness and false positives. Objective. This study proposes an Android malware detecting system that provides highly accurate classification and efficient sensitive data transmission analysis. Method. The study adopts a machine learning approach that leverages the use of dataflow application program interfaces (APIs) as classification features to detect Android malware. We conduct a thorough analysis to extract dataflow-related API-level features and improve the k-nearest neighbour classification model. The dataflow-related API list is further optimized through machine learning, which enables us to improve considerably the efficiency of sensitive data transmission analysis, whereas analytical accuracy is approximated to that of the experiment using a full dataflow-related API list. Results. The proposed scheme is evaluated using 1160 benign and 1050 malicious samples. Results show that the system can achieve an accuracy rate of as high as 97.66% in detecting unknown Android malware. Our experiment of static dataflow analysis shows that more than 85% of sensitive data transmission paths can be determined using the refined API subset, whereas time of analysis decreases by nearly 40%. Conclusion. The usage of dataflow-related APIs is a valid feature for identifying Android malware. The proposed scheme provides an efficient approach to detecting Android malware and investigating privacy violations in malicious apps.",Android security | Malware detection | Privacy leakage,Information and Software Technology,2016-07-01,Article,"Wu, Songyang;Wang, Pan;Li, Xun;Zhang, Yong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84960330477,10.1016/j.infsof.2016.01.005,A component recommender for bug reports using Discriminative Probability Latent Semantic Analysis,"Context The component field in a bug report provides important location information required by developers during bug fixes. Research has shown that incorrect component assignment for a bug report often causes problems and delays in bug fixes. A topic model technique, Latent Dirichlet Allocation (LDA), has been developed to create a component recommender for bug reports. Objective We seek to investigate a better way to use topic modeling in creating a component recommender. Method This paper presents a component recommender by using the proposed Discriminative Probability Latent Semantic Analysis (DPLSA) model and Jensen-Shannon divergence (DPLSA-JS). The proposed DPLSA model provides a novel method to initialize the word distributions for different topics. It uses the past assigned bug reports from the same component in the model training step. This results in a correlation between the learned topics and the components. Results We evaluate the proposed approach over five open source projects, Mylyn, Gcc, Platform, Bugzilla and Firefox. The results show that the proposed approach on average outperforms the LDA-KL method by 30.08%, 19.60% and 14.13% for recall @1, recall @3 and recall @5, outperforms the LDA-SVM method by 31.56%, 17.80% and 8.78% for recall @1, recall @3 and recall @5, respectively. Conclusion Our method discovers that using comments in the DPLSA-JS recommender does not always make a contribution to the performance. The vocabulary size does matter in DPLSA-JS. Different projects need to adaptively set the vocabulary size according to an experimental method. In addition, the correspondence between the learned topics and components in DPLSA increases the discriminative power of the topics which is useful for the recommendation task.",Bug reports | Bug triage | Component recommendation | Discriminative topic model,Information and Software Technology,2016-05-01,Article,"Yan, Meng;Zhang, Xiaohong;Yang, Dan;Xu, Ling;Kymer, Jeffrey D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85007165390,10.15439/2016F524,Preliminary report on empirical study of repeated fragments in internal documentation,"In this paper we present preliminary results of an empirical study, in which we used copy/paste detection (PMD CPD implementation) to search for repeating documentation fragments. The study was performed on 5 open source projects, including Java 8 SDK sources. The study shows that there are many occurrences of copy-pasting documentation fragments in the internal documentation, e.g., copy-pasted method parameter description. Besides these, many of the copy-pasted fragments express some domain or design concern, e.g., that the method is obsolete and deprecated. Therefore the study indicates that the cross-cutting concerns are present in the internal documentation in form of documentation phrases.",,"Proceedings of the 2016 Federated Conference on Computer Science and Information Systems, FedCSIS 2016",2016-11-03,Conference Paper,"Nosal, Milan;Poruban, Jaroslav",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85113855758,10.1109/SANER.2016.10,Software-specific named entity recognition in software engineering social content,"Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed rule-based baseline system, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.",,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Ye, Deheng;Xing, Zhenchang;Foo, Chee Yong;Ang, Zi Qun;Li, Jing;Kapre, Nachiket",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/SANER.2016.59,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85113272113,10.1109/SANER.2016.80,RACK: Automatic API Recommendation using Crowdsourced Knowledge,"Traditional code search engines often do not perform well with natural language queries since they mostly apply keyword matching. These engines thus need carefully designed queries containing information about programming APIs for code search. Unfortunately, existing studies suggest that preparing an effective code search query is both challenging and time consuming for the developers. In this paper, we propose a novel API recommendation technique–RACK that recommends a list of relevant APIs for a natural language query for code search by exploiting keyword-API associations from the crowdsourced knowledge of Stack Overflow. We first motivate our technique using an exploratory study with 11 core Java packages and 344K Java posts from Stack Overflow. Experiments using 150 code search queries randomly chosen from three Java tutorial sites show that our technique recommends correct API classes within the top 10 results for about 79% of the queries which is highly promising. Comparison with two variants of the state-of-the-art technique also shows that RACK outperforms both of them not only in Top-K accuracy but also in mean average precision and mean recall by a large margin.",Code search | crowdsourced knowledge | keyword-API association | query reformulation | Stack Overflow,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Rahman, Mohammad Masudur;Roy, Chanchal K.;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85029006268,10.1109/SANER.2016.79,Towards building API usage example metrics,"It is not unreasonable to say that examples are one of the most commonly used knowledge sources when learning the usage and best practices of a new API. That being said, in many cases the examples provided on the APIs' website are lacking in quantity or quality, so developers have to resort to other information sources, namely blogs and coding forums. Moreover, there is no good way for API developers to measure anything concerning the examples they are creating. In order to resolve the problem of lacking examples information and feedback in their creation, our goal is to develop metrics for empirical measurement of examples and to offer support during the APIs example creation steps, and this paper represents the starting point towards that aim. We have analyzed the source code examples provided on the API's website or GitHub directory for seven popular API libraries written in Java and measured certain metrics, such as example coverage, example code to source code ratio, class coverage percentage, and problems that occur with the compilation and execution of existing examples. The purpose of this paper is to investigate the current situation of examples and provide a starting knowledge base for building an automatic tool for source code example metrics analysis.",,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Radevski, Stevche;Hata, Hideaki;Matsumoto, Kenichi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84974593470,10.1109/ICIT.2016.7475028,Towards generation of annotation for package diagrams,"Many kinds of open source software (OSS) are used to develop software or systems. In order to develop software using existing forms of OSS, developers have to understand them. However some OSS lacks adequate documentation for its source code. Program diagrams, many of which have been recently proposed, can enhance the understanding of software or systems but developers cannot properly understand these diagrams without clear notations. In this paper, will discuss the importance of and principles for annotating package diagrams by analyzing case research and several case studies in this field.",,Proceedings of the IEEE International Conference on Industrial Technology,2016-05-19,Conference Paper,"Goto, Takaaki;Tsuchida, Kensei",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84990879559,10.1109/ICWS.2016.48,Inferring service recommendation from natural language API descriptions,"Software reuse through Application Programming Interfaces (APIs) is a common practice in software development. It remains a big challenge to bridge the semantic gap between user expectations and application functionality with the development of Web-based services. This paper proposes a service recommendation approach via extracting semantic relationship from natural language API descriptions and inferring. To validate our approach, large-scale experiments are conducted based on a real-world accessible service repository, ProgrammableWeb. The results show the effectiveness of our proposed approach.",Natural language processing | Semantic graph | Service recommendation | Web-based service,"Proceedings - 2016 IEEE International Conference on Web Services, ICWS 2016",2016-08-31,Conference Paper,"Xiong, Wei;Wu, Zhao;Li, Bing;Gu, Qiong;Yuan, Lei;Hang, Bo",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85000763577,10.1109/VLHCC.2016.7739671,Developing usable APIs with XP and cognitive dimensions,"Developing a usable Application Programming Interface (API) is a complex and expensive task. Two major factors play important roles on the usability of an API: the design and resources (e.g. documentation, tutorials). API Developers typically evaluate the usability of an API after implementation that results in refactoring tasks if an API lacks usability after development. This refactoring could be avoided if evaluation were continuously conducted while development. This paper explores a new combined process for building usable APIs that combines concepts from a usability evaluation method (Cognitive Dimensions Framework) and an Agile development methodology (eXtreme Programming). We explored the effectiveness of this combined process by implementing a web-based API and conducting a user study. The findings from our evaluation indicated that the new process helped in designing and building a usable API, but ignored some concerns related to resources.",APIs | Cognitive Dimension Framework | Usability | XP,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2016-11-08,Conference Paper,"Bhaskar, Rahul Kamal;Anslow, Craig;Brosz, John;Maurer, Frank",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84973449745,10.1145/2897659.2897665,Linking usage tutorials into API client code,"Traceability links between software artifacts have important applications in the development process. This paper concerns a special case of traceability recovery, i.e., the automated integration of API usage tutorials with the API client code. Our solution involves partitioning the client code into multiple semantic groups/snippets and linking each snippet to the best matching tutorials. Evaluation using benchmarks created for two popular APIs reveals that our solution can find the expected tutorial links at the average rank of 1.6 and 1.4 in the top ranked results, for the two API's, respectively, and with good average precision and recall. We also evaluate the impact of both method partitioning and JavaDoc query expansion on tutorial linking performance. Lastly, we conduct a formative user study to pinpoint the scenarios where our solution actually helps a software developer. We conclude that it is a promising approach to speeding up the maintenance of API client code.",Abstract syntax tree | API | Documentation | Information retrieval | Java | Traceability | Vector space model,"Proceedings - 3rd International Workshop on CrowdSourcing in Software Engineering, CSI-SE 2016",2016-05-14,Conference Paper,"Wu, Naihao;Hou, Daqing;Liu, Qingkun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/978-3-319-62386-3_14,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84981719104,10.1142/S0218194016500339,Code Comment Quality Analysis and Improvement Recommendation: An Automated Approach,"Program comprehension is one of the first and most frequently performed activities during software maintenance and evolution. In a program, there are not only source code, but also comments. Comments in a program is one of the main sources of information for program comprehension. If a program has good comments, it will be easier for developers to understand it. Unfortunately, for many software systems, due to developers' poor coding style or hectic work schedule, it is often the case that a number of methods and classes are not written with good comments. This can make it difficult for developers to understand the methods and classes, when they are performing future software maintenance tasks. To deal with this problem, in this paper we propose an approach which assesses the quality of a code comment and generates suggestions to improve comment quality. A user study is conducted to assess the effectiveness of our approach and the results show that our comment quality assessments are similar to the assessments made by our user study participants, the suggestions provided by our approach are useful to improve comment quality, and our approach can improve the accuracy of the previous comment quality analysis approaches.",code comment quality analysis | Program comprehension | user study,International Journal of Software Engineering and Knowledge Engineering,2016-08-01,Article,"Sun, Xiaobing;Geng, Qiang;Lo, David;Duan, Yucong;Liu, Xiangyue;Li, Bin",Include,
10.1016/j.jss.2022.111515,2-s2.0-84958957432,10.1002/smr.1768,Automated feature discovery via sentence selection and source code summarization,"Programs are, in essence, a collection of implemented features. Feature discovery in software engineering is the task of identifying key functionalities that a program implements. Manual feature discovery can be time consuming and expensive, leading to automatic feature discovery tools being developed. However, these approaches typically only describe features using lists of keywords, which can be difficult for readers who are not already familiar with the source code. An alternative to keyword lists is sentence selection, in which one sentence is chosen from among the sentences in a text document to describe that document. Sentence selection has been widely studied in the context of natural language summarization but is only beginning to be explored as a solution to feature discovery. In this paper, we compare four sentence selection strategies for the purpose of feature discovery. Two are off-the-shelf approaches, while two are adaptations we propose. We present our findings as guidelines and recommendations to designers of feature discovery tools.",feature discovery | sentence selection | source code summarization,Journal of Software: Evolution and Process,2016-02-01,Article,"McBurney, Paul W.;Liu, Cheng;McMillan, Collin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84984903636,10.1007/s11390-016-1671-1,Summarizing Software Artifacts: A Literature Review,"This paper presents a literature review in the field of summarizing software artifacts, focusing on bug reports, source code, mailing lists and developer discussions artifacts. From Jan. 2010 to Apr. 2016, numerous summarization techniques, approaches, and tools have been proposed to satisfy the ongoing demand of improving software performance and quality and facilitating developers in understanding the problems at hand. Since aforementioned artifacts contain both structured and unstructured data at the same time, researchers have applied different machine learning and data mining techniques to generate summaries. Therefore, this paper first intends to provide a general perspective on the state of the art, describing the type of artifacts, approaches for summarization, as well as the common portions of experimental procedures shared among these artifacts. Moreover, we discuss the applications of summarization, i.e., what tasks at hand have been achieved through summarization. Next, this paper presents tools that are generated for summarization tasks or employed during summarization tasks. In addition, we present different summarization evaluation methods employed in selected studies as well as other important factors that are used for the evaluation of generated summaries such as adequacy and quality. Moreover, we briefly present modern communication channels and complementarities with commonalities among different software artifacts. Finally, some thoughts about the challenges applicable to the existing studies in general as well as future research directions are also discussed. The survey of existing studies will allow future researchers to have a wide and useful background knowledge on the main and important aspects of this research field.",machine learning | mining software engineering data | mining software repositories | summarizing software artifacts | summarizing source code,Journal of Computer Science and Technology,2016-09-01,Article,"Nazar, Najam;Hu, Yan;Jiang, He",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85003749628,10.1186/s13173-016-0049-0,Redocumenting APIs with crowd knowledge: a coverage analysis based on question types,"Background: Software libraries and frameworks play an important role in software system development. The appropriate usage of their functionalities/components through their APIs, however, is a challenge for developers. Usually, API documentation, when it exists, is insufficient to assist them in their programming tasks. There are few API documentation writers for the many potential readers, resulting in the lack of explanations and examples concerning different scenarios and perspectives. The interaction of developers on the Web, on the other hand, generates content concerning APIs from different perspectives, which can be used to document APIs, also known as crowd documentation. Methods: In this paper, we present a study regarding the knowledge generated by the crowd on the Stack Overflow question-and-answer website. Our main goal is to understand how the crowd can contribute for API documentation on two programming tasks: how to implement a scenario using an API (how-to-do-it), and how to fix domain-independent bugs in an existing code where there was a misunderstanding regarding the usage of an API (debug-corrective). We classified questions available on Stack Overflow by the main concerns of askers, and we used those classified as how-to-do-it and debug-corrective to analyze the coverage of API elements on the discussions related to such questions. Our cases included the well-known and popular Swing and Android APIs. Results: Our main findings showed that the crowd provides more content for debug-corrective tasks than for how-to-do-it tasks, regardless of the API. Android API elements are more discussed by the crowd compared to Swing. Moreover, we observed that some API elements are frequently mentioned together in discussions, and that there is a strong association between API coverage on Stack Overflow and its usage in real software systems. Conclusions: Crowd documentation may not be a complete substitute for official documentation because of its partial coverage, especially for how-to-do-it tasks. However, it can still significantly enhance the existent documentation, especially for the most commonly used API elements, providing code samples and explanations on a large variety of usage nuances. Finally, taking advantage of the high coverage for debug-corrective tasks, a new kind of debugging assistant may be conceived.",API documentation | Coverage analysis | Crowd knowledge | Question classification | Stack Overflow,Journal of the Brazilian Computer Society,2016-12-01,Article,"Delfim, Fernanda Madeiral;Paixão, Klérisson V.R.;Cassou, Damien;de Almeida Maia, Marcelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84959419834,10.1016/j.scico.2015.10.014,How organisation of architecture documentation affects architectural knowledge retrieval,"A common approach to software architecture documentation in industry projects is the use of file-based documents. This approach offers a single-dimensional arrangement of the architectural knowledge. Knowledge retrieval from file-based architecture documentation is efficient if the organisation of knowledge supports the needs of the readers; otherwise it can be difficult. In this paper, we compare the organisation and retrieval of architectural knowledge in a file-based documentation approach and an ontology-based documentation approach. The ontology-based approach offers a multi-dimensional organisation of architectural knowledge by means of a software ontology and semantic wiki, whereas file-based documentation typically uses hierarchical organisation by directory structure and table of content. We conducted case studies in two companies to study the efficiency and effectiveness of retrieving architectural knowledge from the different organisations of knowledge. We found that the use of better knowledge organisation correlates with the efficiency and effectiveness of AK retrieval. Professionals who used the knowledge organisation found this beneficial.",Architectural knowledge retrieval | Ontology-based documentation | Semantic wiki | Software architecture documentation | Software ontologies,Science of Computer Programming,2016-06-01,Article,"De Graaf, K. A.;Liang, P.;Tang, A.;Van Vliet, H.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/s11219-016-9347-1,,,,,,,,Include,
10.1016/j.jss.2022.111515,2-s2.0-84974574980,10.17485/ijst/2016/v9i20/86869,Software documentation management issues and practices: A survey,Background/Objectives: Software Documentation plays a significant role in any project. This paper is mainly focused on summarizing the various problems with documentation and the practices proposed to address them. Findings: The analysis establishes the fact that documentation needs are specific for each project. The documentation management issues relate to either neglecting or overdoing documentation. Application/Improvements: A study of all the issues and practices for documentation management revealed that documentation processes should be tailor made for every project according to the various factors that determine the documentation needs for a project. Implementation of documentation practices should be project specific rather than organization specific.,Artifacts | Maintenance | Software documentation | Software engineering,Indian Journal of Science and Technology,2016-05-01,Article,"Satish, C. J.;Anand, M.",Exclude,A survey paper
10.1016/j.jss.2022.111515,,10.5072/PRISM/24951,,,,,,,,Exclude,Not a peer reviewed paper or is a pre-print
10.1016/j.jss.2022.111515,2-s2.0-85013032590,10.1109/SANER.2016.99,Do developers deprecate APIs with replacement messages? A large-scale analysis on Java systems,"As any other software system, frameworks and libraries evolve over time, and so their APIs. Consequently, client systems should be updated to benefit from improved APIs. To facilitate this task and preserve backward compatibility, API elements should always be deprecated with clear replacement messages. However, in practice, there are evidences that API elements are usually deprecated without such messages. In this paper, we study a set of questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the usage of deprecation messages and to investigate whether a tool is needed to recommend such messages. Thus, we verify (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on such frequency, and (iii) the characteristics of systems which deprecate API elements in a correct way. Our large-scale analysis on 661 real-world Java systems shows that (i) 64% of the API elements are deprecated with replacement messages per system, (ii) there is almost no major effort to improve deprecation messages over time, and (iii) systems that deprecated API elements in a correct way are statistically significantly different from the ones that do not in terms of size and developing community. As a result, we provide the basis for the design of a tool to support client developers on detecting missing deprecation messages.",,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Brito, Gleison;Hora, Andre;Valente, Marco Tulio;Robbes, Romain",Include,
10.1016/j.jss.2022.111515,2-s2.0-85099135104,10.1109/SANER.2016.72,Examining the impact of self-admitted technical debt on software quality,"Technical debt refers to incomplete or temporary workarounds that allow us to speed software development in the short term at the cost of paying a higher price later on. Recently, studies have shown that technical debt can be detected from source code comments, referred to as self-admitted technical debt. Researchers have examined the detection, classification and removal of self-admitted technical debt. However, to date there is no empirical evidence on the impact of self-admitted technical debt on software quality. Therefore, in this paper, we examine the relation between self-admitted technical debt and software quality by investigating whether (i) files with self-admitted technical debt have more defects compared to files without self-admitted technical debt, (ii) whether self-admitted technical debt changes introduce future defects, and (iii) whether self-admitted technical debt-related changes tend to be more difficult. We measured the difficulty of a change using well-known measures proposed in prior work such as the amount of churn, the number of files, the number of modified modules in a change, as well as the entropy of a change. An empirical study using five open source projects, namely Hadoop, Chromium, Cassandra, Spark and Tomcat, showed that: i) there is no clear trend when it comes to defects and self-admitted technical debt, although the defectiveness of the technical debt files increases after the introduction of technical debt, ii) self-admitted technical debt changes induce less future defects than none technical debt changes, however, iii) self-admitted technical debt changes are more difficult to perform, i.e., they are more complex. Our study indicates that although technical debt may have negative effects, its impact is not only related to defects, rather making the system more difficult to change in the future.",,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Wehaibi, Sultan;Shihab, Emad;Guerrouj, Latifa",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85113272113,10.1109/SANER.2016.80,RACK: Automatic API Recommendation using Crowdsourced Knowledge,"Traditional code search engines often do not perform well with natural language queries since they mostly apply keyword matching. These engines thus need carefully designed queries containing information about programming APIs for code search. Unfortunately, existing studies suggest that preparing an effective code search query is both challenging and time consuming for the developers. In this paper, we propose a novel API recommendation technique–RACK that recommends a list of relevant APIs for a natural language query for code search by exploiting keyword-API associations from the crowdsourced knowledge of Stack Overflow. We first motivate our technique using an exploratory study with 11 core Java packages and 344K Java posts from Stack Overflow. Experiments using 150 code search queries randomly chosen from three Java tutorial sites show that our technique recommends correct API classes within the top 10 results for about 79% of the queries which is highly promising. Comparison with two variants of the state-of-the-art technique also shows that RACK outperforms both of them not only in Top-K accuracy but also in mean average precision and mean recall by a large margin.",Code search | crowdsourced knowledge | keyword-API association | query reformulation | Stack Overflow,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Rahman, Mohammad Masudur;Roy, Chanchal K.;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85113855758,10.1109/SANER.2016.10,Software-specific named entity recognition in software engineering social content,"Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed rule-based baseline system, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.",,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",2016-05-20,Conference Paper,"Ye, Deheng;Xing, Zhenchang;Foo, Chee Yong;Ang, Zi Qun;Li, Jing;Kapre, Nachiket",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85012016847,10.18653/v1/p16-1195,Summarizing source code using a neural attention model,"High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely data-driven approach for generating high level summaries of source code. Our model, CODE-NN, uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C# benchmark by a large margin.",,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers",2016-01-01,Conference Paper,"Iyer, Srinivasan;Konstas, Ioannis;Cheung, Alvin;Zettlemoyer, Luke",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84979520531,10.5220/0005914503690378,Investigating the use of a contextualized vocabulary in the identification of technical debt: A controlled experiment,"In order to effectively manage technical debt (TD), a set of indicators has been used by automated approaches to identify TD items. However, some debt may not be directly identified using only metrics collected from the source code. CVM-TD is a model to support the identification of technical debt by considering the developer point of view when identifying TD through code comment analysis. In this paper, we analyze the use of CVM-TD with the purpose of characterizing factors that affect the accuracy of the identification of TD. We performed a controlled experiment investigating the accuracy of CVM-TD and the influence of English skills and developer experience factors. The results indicated that CVM-TD provided promising results considering the accuracy values. English reading skills have an impact on the TD detection process. We could not conclude that the experience level affects this process. Finally, we also observed that many comments suggested by CVM-TD were considered good indicators of TD. The results motivate us continuing to explore code comments in the context of TD identification process in order to improve CVM-TD.",Code comment | Contextualized vocabulary | Controlled experiment | Technical debt,ICEIS 2016 - Proceedings of the 18th International Conference on Enterprise Information Systems,2016-01-01,Conference Paper,"De Freitas Farias, Mário André;Santos, José Amancio;Da Silva, André Batista;Kalinowski, Marcos;Mendonça, Manoel;Spínola, Rodrigo Oliveira",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84984903636,10.1007/s11390-016-1671-1,Summarizing Software Artifacts: A Literature Review,"This paper presents a literature review in the field of summarizing software artifacts, focusing on bug reports, source code, mailing lists and developer discussions artifacts. From Jan. 2010 to Apr. 2016, numerous summarization techniques, approaches, and tools have been proposed to satisfy the ongoing demand of improving software performance and quality and facilitating developers in understanding the problems at hand. Since aforementioned artifacts contain both structured and unstructured data at the same time, researchers have applied different machine learning and data mining techniques to generate summaries. Therefore, this paper first intends to provide a general perspective on the state of the art, describing the type of artifacts, approaches for summarization, as well as the common portions of experimental procedures shared among these artifacts. Moreover, we discuss the applications of summarization, i.e., what tasks at hand have been achieved through summarization. Next, this paper presents tools that are generated for summarization tasks or employed during summarization tasks. In addition, we present different summarization evaluation methods employed in selected studies as well as other important factors that are used for the evaluation of generated summaries such as adequacy and quality. Moreover, we briefly present modern communication channels and complementarities with commonalities among different software artifacts. Finally, some thoughts about the challenges applicable to the existing studies in general as well as future research directions are also discussed. The survey of existing studies will allow future researchers to have a wide and useful background knowledge on the main and important aspects of this research field.",machine learning | mining software engineering data | mining software repositories | summarizing software artifacts | summarizing source code,Journal of Computer Science and Technology,2016-09-01,Article,"Nazar, Najam;Hu, Yan;Jiang, He",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-84975883385,10.1145/2851613.2851761,Impacts of agile requirements documentation debt on software projects: A retrospective study,"Documentation debt is a type of technical debt that describes problems in documentation such as missing, inadequate or incomplete artifacts. Unlike traditional methods, agile methodologies usually employ short iterative cycles and rely on tacit knowledge within a team. In particular, Agile Requirements (AR) (e.g., user stories) tend to reduce the focus on requirements specification activities. This scenario contributes to the occurrence of documentation debt. The goal of this paper is to investigate the impact that this type of debt brings to projects developed by using AR. We address this goal by performing a retrospective study in a real software project that used AR in its development. Our analysis was concentrated on data from 132 maintenance and evolution tasks. Of this total, 65 were related to the presence of documentation debt and were performed within a timeframe of 18 months. The findings indicated an extra maintenance effort of about 47% of the total effort estimated for developing the project and an extra cost of about 48% of the initial cost of the development phase.",Agile Requirements | Documentation debt | Retrospective study | Technical debt | User stories,Proceedings of the ACM Symposium on Applied Computing,2016-04-04,Conference Paper,"Mendes, Thiago Souto;De Farias, Mário André F.;Mendonça, Manoel;Soares, Henrique Frota;Kalinowski, Marcos;Spínola, Rodrigo Oliveira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/s11219-016-9344-4,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.14569/IJACSA.2016.070227,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.14569/IJACSA.2016.071239,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ASE.2017.8115612,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041445715,10.1109/ASE.2017.8115624,Detecting fragile comments,"Refactoring is a common software development practice and many simple refactorings can be performed automatically by tools. Identifier renaming is a widely performed refactoring activity. With tool support, rename refactorings can rely on the program structure to ensure correctness of the code transformation. Unfortunately, the textual references to the renamed identifier present in the unstructured comment text cannot be formally detected through the syntax of the language, and are thus fragile with respect to identifier renaming. We designed a new rule-based approach to detect fragile comments. Our approach, called Fraco, takes into account the type of identifier, its morphology, the scope of the identifier and the location of comments. We evaluated the approach by comparing its precision and recall against hand-annotated benchmarks created for six target Java systems, and compared the results against the performance of Eclipse's automated in-comment identifier replacement feature. Fraco performed with near-optimal precision and recall on most components of our evaluation data set, and generally outperformed the baseline Eclipse feature. As part of our evaluation, we also noted that more than half of the total number of identifiers in our data set had fragile comments after renaming, which further motivates the need for research on automatic comment refactoring.",fragile comments | inconsistent code | refactoring | Software evolution | source code comments,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Ratol, Inderjot Kaur;Robillard, Martin P.",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85041450198,10.1109/ASE.2017.8115625,Improving software text retrieval using conceptual knowledge in source code,"A large software project usually has lots of various textual learning resources about its API, such as tutorials, mailing lists, user forums, etc. Text retrieval technology allows developers to search these API learning resources for related documents using free-text queries, but it suffers from the lexical gap between search queries and documents. In this paper, we propose a novel approach for improving the retrieval of API learning resources through leveraging software-specific conceptual knowledge in software source code. The basic idea behind this approach is that the semantic relatedness between queries and documents could be measured according to software-specific concepts involved in them, and software source code contains a large amount of software-specific conceptual knowledge. In detail, firstly we extract an API graph from software source code and use it as software-specific conceptual knowledge. Then we discover API entities involved in queries and documents, and infer semantic document relatedness through analyzing structural relationships between these API entities. We evaluate our approach in three popular open source software projects. Comparing to the state-of-the-art text retrieval approaches, our approach lead to at least 13.77% improvement with respect to mean average precision (MAP).",API graph | conceptual knowledge | semantic relatedness | Software text retrieval,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Lin, Zeqi;Zou, Yanzhen;Zhao, Junfeng;Xie, Bing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041449577,10.1109/ASE.2017.8115626,Automatically generating commit messages from diffs using neural machine translation,"Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically 'translate' diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.",,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Jiang, Siyuan;Armaly, Ameer;McMillan, Collin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041445342,10.1109/ASE.2017.8115628,APIBot: Question answering bot for API documentation,"As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706.",API Documentation | Bot | Question Answering,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Tian, Yuan;Thung, Ferdian;Sharma, Abhishek;Lo, David",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85041435467,10.1109/ASE.2017.8115652,Mining implicit design templates for actionable code reuse,"In this paper, we propose an approach to detecting project-specific recurring designs in code base and abstracting them into design templates as reuse opportunities. The mined templates allow programmers to make further customization for generating new code. The generated code involves the code skeleton of recurring design as well as the semi-implemented code bodies annotated with comments to remind programmers of necessary modification. We implemented our approach as an Eclipse plugin called MICoDe. We evaluated our approach with a reuse simulation experiment and a user study involving 16 participants. The results of our simulation experiment on 10 open source Java projects show that, to create a new similar feature with a design template, (1) on average 69% of the elements in the template can be reused and (2) on average 60% code of the new feature can be adopted from the template. Our user study further shows that, compared to the participants adopting the copy-paste-modify strategy, the ones using MICoDe are more effective to understand a big design picture and more efficient to accomplish the code reuse task.",,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Lin, Yun;Meng, Guozhu;Xue, Yinxing;Xing, Zhenchang;Sun, Jun;Peng, Xin;Liu, Yang;Zhao, Wenyun;Dong, Jinsong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041431985,10.1109/ASE.2017.8115654,Automatically assessing code understandability: How far are we?,"Program understanding plays a pivotal role in software maintenance and evolution: a deep understanding of code is the stepping stone for most software-related activities, such as bug fixing or testing. Being able to measure the understandability of a piece of code might help in estimating the effort required for a maintenance activity, in comparing the quality of alternative implementations, or even in predicting bugs. Unfortunately, there are no existing metrics specifically designed to assess the understandability of a given code snippet. In this paper, we perform a first step in this direction, by studying the extent to which several types of metrics computed on code, documentation, and developers correlate with code understandability. To perform such an investigation we ran a study with 46 participants who were asked to understand eight code snippets each. We collected a total of 324 evaluations aiming at assessing the perceived understandability, the actual level of understanding, and the time needed to understand a code snippet. Our results demonstrate that none of the (existing and new) metrics we considered is able to capture code understandability, not even the ones assumed to assess quality attributes strongly related with it, such as code readability and complexity.",Code understandability | Empirical study | Negative result | Software metrics,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Scalabrino, Simone;Bavota, Gabriele;Vendome, Christopher;Linares-Vasquez, Mario;Poshyvanyk, Denys;Oliveto, Rocco",Include,
10.1016/j.jss.2022.111515,2-s2.0-85032467228,10.1109/ASE.2017.8115681,AnswerBot: Automated generation of answer summary to developers' technical questions,"The prevalence of questions and answers on domain-specific Q&A sites like Stack Overflow constitutes a core knowledge asset for software engineering domain. Although search engines can return a list of questions relevant to a user query of some technical question, the abundance of relevant posts and the sheer amount of information in them makes it difficult for developers to digest them and find the most needed answers to their questions. In this work, we aim to help developers who want to quickly capture the key points of several answer posts relevant to a technical question before they read the details of the posts. We formulate our task as a query-focused multi-answer-posts summarization task for a given technical question. Our proposed approach AnswerBot contains three main steps : 1) relevant question retrieval, 2) useful answer paragraph selection, 3) diverse answer summary generation. To evaluate our approach, we build a repository of 228,817 Java questions and their corresponding answers from Stack Overflow. We conduct user studies with 100 randomly selected Java questions (not in the question repository) to evaluate the quality of the answer summaries generated by our approach, and the effectiveness of its relevant question retrieval and answer paragraph selection components. The user study results demonstrate that answer summaries generated by our approach are relevant, useful and diverse; moreover, the two components are able to effectively retrieve relevant questions and select salient answer paragraphs for summarization.",question retrieval | Summary generation,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Xu, Bowen;Xing, Zhenchang;Xia, Xin;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041442182,10.1109/ASE.2017.8115687,Understanding and overcoming parallelism bottlenecks in ForkJoin applications,"ForkJoin framework is a widely used parallel programming framework upon which both core concurrency libraries and real-world applications are built. Beneath its simple and user-friendly APIs, ForkJoin is a sophisticated managed parallel runtime unfamiliar to many application programmers: the framework core is a work-stealing scheduler, handles fine-grained tasks, and sustains the pressure from automatic memory management. ForkJoin poses a unique gap in the compute stack between high-level software engineering and low-level system optimization. Understanding and bridging this gap is crucial for the future of parallelism support in JVM-supported applications. This paper describes a comprehensive study on parallelism bottlenecks in ForkJoin applications, with a unique focus on how they interact with underlying system-level features, such as work stealing and memory management. We identify 6 bottlenecks, and found that refactoring them can significantly improve performance and energy efficiency. Our field study includes an in-depth analysis of Akka - a real-world actor framework - and 30 additional open-source ForkJoin projects. We sent our patches to the developers of 15 projects, and 7 out of the 9 projects that replied to our patches have accepted them.",,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Pinto, Gustavo;Canino, Anthony;Castor, Fernando;Xu, Guoqing;Liu, Yu David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041442741,10.1109/ASE.2017.8115693,Mining constraints for event-based monitoring in systems of systems,"The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.",Constraint mining | event-based monitoring | systems of systems,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Krismayer, Thomas;Rabiser, Rick;Grunbacher, Paul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041434429,10.1109/ASE.2017.8115702,TREM: A tool for mining timed regular specifications from system traces,"Software specifications are useful for software validation, model checking, runtime verification, debugging, monitoring, etc. In context of safety-critical real-time systems, temporal properties play an important role. However, temporal properties are rarely present due to the complexity and evolutionary nature of software systems. We propose Timed Regular Expression Mining (TREM) a hosted tool for specification mining using timed regular expressions (TREs). It is designed for easy and robust mining of dominant temporal properties. TREM uses an abstract structure of the property; the framework constructs a finite state machine to serve as an acceptor. TREM is scalable, easy to access/use, and platform independent specification mining framework. The tool is tested on industrial strength software system traces such as the QNX real-time operating system using traces with more than 1.5 Million entries. The tool demonstration video can be accessed here: youtu.be/cSd-aj3-LH8",Real-time systems | Specification Mining | Timed Regular Expressions,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Schmidt, Lukas;Narayan, Apurva;Fischmeister, Sebastian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041436961,10.1109/ASE.2017.8115707,CogniCrypt: Supporting developers in using cryptography,"Previous research suggests that developers often struggle using low-level cryptographic APIs and, as a result, produce insecure code. When asked, developers desire, among other things, more tool support to help them use such APIs. In this paper, we present CogniCrypt, a tool that supports developers with the use of cryptographic APIs. CogniCrypt assists the developer in two ways. First, for a number of common cryptographic tasks, CogniCrypt generates code that implements the respective task in a secure manner. Currently, CogniCrypt supports tasks such as data encryption, communication over secure channels, and long-term archiving. Second, CogniCrypt continuously runs static analyses in the background to ensure a secure integration of the generated code into the developer's workspace. This video demo showcases the main features of CogniCrypt: youtube.com/watch?v=JUq5mRHfAWY.",Code Analysis | Code Generation | Cryptography | Variability Modeling,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Kruger, Stefan;Nadi, Sarah;Reif, Michael;Ali, Karim;Mezini, Mira;Bodden, Eric;Gopfert, Florian;Gunther, Felix;Weinert, Christian;Demmler, Daniel;Kamath, Ram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040583553,10.1109/ASE.2017.8115713,FEMIR: A tool for recommending framework extension examples,"Software frameworks enable developers to reuse existing well tested functionalities instead of taking the burden of implementing everything from scratch. However, to meet application specific requirements, the frameworks need to be customized via extension points. This is often done by passing a framework related object as an argument to an API call. To enable such customizations, the object can be created by extending a framework class, implementing an interface, or changing the properties of the object via API calls. However, it is both a common and non-trivial task to find all the details related to the customizations. In this paper, we present a tool, called FEMIR, that utilizes partial program analysis and graph mining technique to detect, group, and rank framework extension examples. The tool extends existing code completion infrastructure to inform developers about customization choices, enabling them to browse through extension points of a framework, and frequent usages of each point in terms of code examples. A video demo is made available at https://asaduzzamanparvez.wordpress.com/femir.",API | extension | extension point | framework | graph mining | partial program analysis | reuse,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,2017-11-20,Conference Paper,"Asaduzzaman, Muhammad;Roy, Chanchal K.;Schneider, Kevin A.;Hou, Daqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042388336,10.1109/ESEM.2017.23,Change-Aware Build Prediction Model for Stall Avoidance in Continuous Integration,"Continuous Integration(CI) is a widely used development practice where developers integrate their work after submitting code changes at central repository. CI servers usually monitor central repository for code change submission and automatically build software with changed code, perform unit testing, integration testing and provide test summary report. If build or test fails developers fix those issues and submit the code changes. Continuous submission of code modification by developers and build latency time creates stalls at CI server build pipeline and hence developers have to wait long time to get build outcome. In this paper, we proposed build prediction model that uses TravisTorrent data set with build error log clustering and AST level code change modification data to predict whether a build will be successful or not without attempting actual build so that developer can get early build outcome result. With the proposed model we can predict build outcome with an average F-Measure over 87% on all three build systems (Ant, Maven, Gradle) under the cross-project prediction scenario.",Continuous Integration | Software Build Outcome Prediction,International Symposium on Empirical Software Engineering and Measurement,2017-12-07,Conference Paper,"Hassan, Foyzul;Wang, Xiaoyin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042360349,10.1109/ESEM.2017.25,An Ontology-Based Approach to Automate Tagging of Software Artifacts,"Context: Software engineering repositories contain a wealth of textual information such as source code comments, developers' discussions, commit messages and bug reports. These free form text descriptions can contain both direct and implicit references to security concerns. Goal: Derive an approach to extract security concerns from textual information that can yield several benefits, such as bug management (e.g., prioritization), bug triage or capturing zero-day attack. Method: Propose a fully automated classification and tagging approach that can extract security tags from these texts without the need for manual training data. Results: We introduce an ontology based Software Security Tagger Framework that can automatically identify and classify cybersecurity-related entities, and concepts in text of software artifacts. Conclusion: Our preliminary results indicate that the framework can successfully extract and classify cybersecurity knowledge captured in unstructured text found in software artifacts.",automated security concern classification | bug reports | tagging | topic modeling,International Symposium on Empirical Software Engineering and Measurement,2017-12-07,Conference Paper,"Alqahtani, Sultan S.;Rilling, Juergen",Include,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85042366221,10.1109/ESEM.2017.41,The Influence of Requirements in Software Model Development in an Industrial Environment,"Textual description of requirements is a specification technique that is widely used in industry, where time is key for success. How requirements are specified textually greatly depends on human factors. In order to study how requirements processing is affected by the level of detail in textual descriptions, this paper compares enriched textual requirements specifications with non-enriched ones. To do this, we have conducted an experiment in industry with 19 engineers of CAF (Construcciones y Auxiliares de Ferrocarril), which is a supplier of railway solutions. The experiment is a crossover design that analyzes efficiency, effectiveness, and perceived difficulty starting from a written specification of requirements that subjects must process in order to build software models. The results show that effectiveness and efficiency for enriched requirements are better, while non-enriched requirements are slightly more difficult to deal with. Therefore, even though enriched requirements require more time to be specified, the results are more successfully when using them.",Controlled Experiment | Model-Based Software Development | Software Requirements,International Symposium on Empirical Software Engineering and Measurement,2017-12-07,Conference Paper,"Echeverria, Jorge;Perez, Francisca;Panach, Jose Ignacio;Cetina, Carlos;Pastor, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042388214,10.1109/ESEM.2017.56,Mining Version Control System for Automatically Generating Commit Comment,"Commit comments increasingly receive attention as an important complementary component in code change comprehension. To address the comment scarcity issue, a variety of automatic approaches for commit comment generation have been intensively proposed. However, most of these approaches mechanically outline a superficial level summary of the changed software entities, the change intent behind the code changes is lost (e.g., the existing approaches cannot generate such comment: 'fixing null pointer exception'). Considering the comments written by developers often describe the intent behind the code change, we propose a method to automatically generate commit comment by reusing the existing comments in version control system. Specifically, for an input commit, we apply syntax, semantic, pre-syntax, and pre-semantic similarities to discover the similar commits from half a million commits, and recommend the reusable comments to the input commit from the ones of the similar commits. We evaluate our approach on 7 projects. The results show that 9.1% of the generated comments are good, 27.7% of the generated comments need minor fix, and 63.2% are bad, and we also analyze the reasons that make a comment available or unavailable.",Code Change Comprehension | Code Semantic Similarity | Code Syntax Similarity | Commit Comment Generation,International Symposium on Empirical Software Engineering and Measurement,2017-12-07,Conference Paper,"Huang, Yuan;Zheng, Qiaoyang;Chen, Xiangping;Xiong, Yingfei;Liu, Zhiyong;Luo, Xiaonan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030787201,10.1145/3106237.3106299,Modeling and verification of evolving cyber-physical spaces,"We increasingly live in cyber-physical spaces - spaces that are both physical and digital, and where the two aspects are intertwined. Such spaces are highly dynamic and typically undergo continuous change. Software engineering can have a profound impact in this domain, by defining suitable modeling and specification notations as well as supporting design-time formal verification. In this paper, we present a methodology and a technical framework which support modeling of evolving cyber-physical spaces and reasoning about their spatio-temporal properties. We utilize a discrete, graph-based formalism for modeling cyber-physical spaces as well as primitives of change, giving rise to a reactive system consisting of rewriting rules with both local and global application conditions. Formal reasoning facilities are implemented adopting logic-based specification of properties and according model checking procedures, in both spatial and temporal fragments. We evaluate our approach using a case study of a disaster scenario in a smart city.",Cyber-physical spaces | Dependable systems | Formal verification | Modelling and specification | Safety and reliability,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Tsigkanos, Christos;Kehrer, Timo;Ghezzi, Carlo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85028995068,10.1145/3106237.3106272,"The power of ""why"" and ""why not"": Enriching scenario exploration with provenance","Scenario-finding tools like the Alloy Analyzer are widely used in numerous concrete domains like security, network analysis, UML analysis, and so on. They can help to verify properties and, more generally, aid in exploring a system's behavior. While scenario finders are valuable for their ability to produce concrete examples, individual scenarios only give insight into what is possible, leaving the user to make their own conclusions about what might be necessary. This paper enriches scenario finding by allowing users to ask ""why?"" and ""why not?"" questions about the examples they are given. We show how to distinguish parts of an example that cannot be consistently removed (or changed) from those that merely reflect underconstraint in the specification. In the former case we show how to determine which elements of the specification and which other components of the example together explain the presence of such facts. This paper formalizes the act of computing provenance in scenariofinding. We present Amalgam, an extension of the popular Alloy scenario-finder, which implements these foundations and provides interactive exploration of examples. We also evaluate Amalgam's algorithmics on a variety of both textbook and real-world examples.",Alloy analyzer | Formal methods | Model finding | Provenance,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Nelson, Tim;Danas, Natasha;Dougherty, Daniel J.;Krishnamurthi, Shriram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85107630092,10.1145/3106237.3106284,Bayesian specification learning for finding API usage errors,"We present a Bayesian framework for learning probabilistic specifications from large, unstructured code corpora, and then using these specifications to statically detect anomalous, hence likely buggy, program behavior. Our key insight is to build a statistical model that correlates all specifications hidden inside a corpus with the syntax and observed behavior of programs that implement these specifications. During the analysis of a particular program, this model is conditioned into a posterior distribution that prioritizes specifications that are relevant to the program. The problem of finding anomalies is now framed quantitatively, as a problem of computing a distance between a ""reference distribution""over program behaviors that our model expects from the program, and the distribution over behaviors that the program actually produces. We implement our ideas in a system, called Salento, for finding anomalous API usage in Android programs. Salento learns specifications using a combination of a topic model and a neural network model. Our encouraging experimental results show that the system can automatically discover subtle errors in Android applications in the wild, and has high precision and recall compared to competing probabilistic approaches.",Anomaly Detection | APIs | Bug Finding | Specification Learning,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-01-01,Conference Paper,"Murali, Vijayaraghavan;Chaudhuri, Swarat;Jermaine, Chris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030763152,10.1145/3106237.3106263,Synergistic debug-repair of heap manipulations,"We present Wolverine, an integrated Debug-Repair environment for heap manipulating programs. Wolverine facilitates stepping through a concrete program execution, provides visualizations of the abstract program states (as box-and-arrow diagrams) and integrates a novel, proof-directed repair algorithm to synthesize repair patches. To provide a seamless environment, Wolverine supports ""hot-patching"" of the generated repair patches, enabling the programmer to continue the debug session without requiring an abort-compile-debug cycle. We also propose new debug-repair possibilities, specification refinement and specification slicing made possible by Wolverine. We evaluate our framework on 1600 buggy programs (generated using fault injection) on a variety of data-structures like singly, doubly and circular linked-lists, Binary Search Trees, AVL trees, Red-Black trees and Splay trees; Wolverine could repair all the buggy instances within reasonable time (less than 5 sec in most cases). We also evaluate Wolverine on 247 (buggy) student submissions; Wolverine could repair more than 80% of programs where the student had made a reasonable attempt.",Heap manipulations | Program debugging | Program repair,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Verma, Sahil;Roy, Subhajit",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030774214,10.1145/3106237.3106283,Kmax: Finding all configurations of Kbuild makefiles statically,"Feature-oriented software design is a useful paradigm for building and reasoning about highly-configurable software. By making variability explicit, feature-oriented tools and languages make program analysis tasks easier, such as bug-finding, maintenance, and more. But critical software, such as Linux, coreboot, and BusyBox rely instead on brittle tools, such as Makefiles, to encode variability, impeding variability-aware tool development. Summarizing Makefile behavior for all configurations is difficult, because Makefiles have unusual semantics, and exhaustive enumeration of all configurations is intractable in practice. Existing approaches use ad-hoc heuristics, missing much of the encoded variability in Makefiles.We present Kmax, a new static analysis algorithm and tool for Kbuild Makefiles. It is a family-based variability analysis algorithm, where paths are Boolean expressions of configuration options, called reaching configurations, and its abstract state enumerates string values for all configurations. Kmax localizes configuration explosion to the statement level, making precise analysis tractable. The implementation analyzes Makefiles from the Kbuild build system used by several low-level systems projects. Evaluation of Kmax on the Linux and BusyBox build systems shows it to be accurate, precise, and fast. It is the first tool to collect all source files and their configurations from Linux. Compared to previous approaches, Kmax is far more accurate and precise, performs with little overhead, and scales better.",Configuration | Kbuild | Kmax | Makefiles | Static analysis | Variability,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Gazzillo, Paul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030763637,10.1145/3106237.3106282,ARTINALI: Dynamic invariant detection for Cyber-Physical System security,"Cyber-Physical Systems (CPSes) are being widely deployed in security- critical scenarios such as smart homes and medical devices. Unfortunately, the connectedness of these systems and their relative lack of security measures makes them ripe targets for attacks. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing CPSs. Unfortunately, deriving invariants for capturing the specifications of CPS systems is a tedious and error-prone process. Therefore, it is important to dynamically monitor the CPS system to learn its common behaviors and formulate invariants for detecting security attacks. Existing techniques for invariant mining only incorporate data and events, but not time. However, time is central to most CPS systems, and hence incorporating time in addition to data and events, is essential for achieving low false positives and false negatives. This paper proposes ARTINALI, which mines dynamic system properties by incorporating time as a first-class property of the system. We build ARTINALI-based Intrusion Detection Systems (IDSes) for two CPSes, namely smart meters and smart medical devices, and measure their efficacy. We find that the ARTINALIbased IDSes significantly reduce the ratio of false positives and false negatives by 16 to 48% (average 30.75%) and 89 to 95% (average 93.4%) respectively over other dynamic invariant detection tools.",CPS | Cyber Physical System | Multi-dimensional model | Security | Software engineering,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Aliabadi, Maryam Raiyat;Kamath, Amita Ajith;Gascon-Samson, Julien;Pattabiraman, Karthik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030757304,10.1145/3106237.3106309,S3: Syntax- and semantic-guided repair synthesis via programming by examples,"A notable class of techniques for automatic program repair is known as semantics-based. Such techniques, e.g., Angelix, infer semantic specifications via symbolic execution, and then use program synthesis to construct new code that satisfies those inferred specifications. However, the obtained specifications are naturally incomplete, leaving the synthesis engine with a difficult task of synthesizing a general solution from a sparse space of many possible solutions that are consistent with the provided specifications but that do not necessarily generalize. We present S3, a new repair synthesis engine that leverages programming-by-examples methodology to synthesize high-quality bug repairs. The novelty in S3 that allows it to tackle the sparse search space to create more general repairs is three-fold: (1) A systematic way to customize and constrain the syntactic search space via a domain-specific language, (2) An efficient enumerationbased search strategy over the constrained search space, and (3) A number of ranking features based on measures of the syntactic and semantic distances between candidate solutions and the original buggy program.We compare S3's repair effectiveness with state-ofthe- art synthesis engines Angelix, Enumerative, and CVC4. S3 can successfully and correctly fix at least three times more bugs than the best baseline on datasets of 52 bugs in small programs, and 100 bugs in real-world large programs.",Inductive synthesis | Program repair | Programming by examples | Symbolic execution,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Le, Xuan Bach D.;Chu, Duc Hiep;Lo, David;Le Goues, Claire;Visser, Willem",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030762571,10.1145/3106237.3106279,Discovering relational specifications,"Formal specifications of library functions play a critical role in a number of program analysis and development tasks. We present Bach, a technique for discovering likely relational specifications from data describing input-output behavior of a set of functions comprising a library or a program. Relational specifications correlate different executions of different functions; for instance, commutativity, transitivity, equivalence of two functions, etc. Bach combines novel insights from program synthesis and databases to discover a rich array of specifications. We apply Bach to learn specifications from data generated for a number of standard libraries. Our experimental evaluation demonstrates Bach's ability to learn useful and deep specifications in a small amount of time.",Datalog | Hyperproperties | Specification mining,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Smith, Calvin;Ferns, Gabriel;Albarghouthi, Aws",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030759213,10.1145/3106237.3117773,Reference architectures and scrum: Friends or foes?,"Software reference architectures provide templates and guidelines for designing systems in a particular domain. Companies use them to achieve interoperability of (parts of) their software, standardization, and faster development. In contrast to system-specific software architectures that ""emerge"" during development, reference architectures dictate significant parts of the software design early on. Agile software development frameworks (such as Scrum) acknowledge changing software requirements and the need to adapt the software design accordingly. In this paper, we present lessons learned about how reference architectures interact with Scrum (the most frequently used agile process framework). These lessons are based on observing software development projects in five companies. We found that reference architectures can support good practice in Scrum: They provide enough design upfront without too much effort, reduce documentation activities, facilitate knowledge sharing, and contribute to ""architectural thinking"" of developers. However, reference architectures can impose risks or even threats to the success of Scrum (e.g., to self-organizing and motivated teams).",Experience report | Lessons learned | Reference architectures | Scrum,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Galster, Matthias;Angelov, Samuil;Martínez-Fernández, Silverio;Tofan, Dan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030774723,10.1145/3106237.3117772,Improving understanding of dynamically typed software developed by agile practitioners,"Agile Development values working software over documentation. Therefore, in maintenance stages of existing software, the source code is the sole software artifact that developers have for analyzing the viability and impact of a new user story. Since functionality is often spread in hundreds of lines of code, it is hard for the developer to understand the system, which may lead to under-/overestimation of the new feature cost and rework/delays in the subsequent phases of development. In a previous work, we proposed a Model-Driven Reverse Engineering approach for obtaining software visualizations from source code. Two case studies of comprehension of applications written in statically typed languages have shown the applicability of this approach. A recent experience with an industrial partner, where the systems are developed on dynamically typed languages, has motivated us to adapt the previous proposal to take as input not only the source code but also the application data schema to complete the information that is missing in the code, and then automatically generate more meaningful diagrams that help developers in maintenance tasks. In this article, we present the adaptation of the general approach to support data schema as an additional input and its instrumentation in an industrial case study where the technology is Ruby on Rails. The paper ends by explaining the precision and performance of the instrumentation when used in a Colombian company as well as lessons learned.",Model Transformation Grammars | Model-Driven Reverse Engineering | Ruby On Rails | Software visualization | Xtext,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"García, Jair;Garcés, Kelly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030749834,10.1145/3106237.3122826,DynAlloy analyzer: A tool for the specification and analysis of alloy models with dynamic behaviour,"We describe DynAlloy Analyzer, a tool that extends Alloy Analyzer with support for dynamic elements in Alloy models. The tool builds upon Alloy Analyzer in a way that makes it fully compatible with Alloy models, and extends their syntax with a particular idiom, inspired in dynamic logic, for the description of dynamic behaviours, understood as sequences of states over standard Alloy models, in terms of programs. The syntax is broad enough to accommodate abstract dynamic behaviours, e.g., using nondeterministic choice and finite unbounded iteration, as well as more concrete ones, using standard sequential programming constructions. The analysis of DynAlloy models resorts to the analysis of Alloy models, through an optimized translation that often makes the analysis more efficient than that of typical ad-hoc constructions to capture dynamism in Alloy. Tool screencast, binaries and further details available in: http://dc.exa.unrc.edu.ar/tools/dynalloy.",Alloy | Dynamic logic | Software specification | Software validation,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Regis, Germán;Cornejo, César;Brida, Simón Gutiérrez;Politano, Mariano;Raverta, Fernando;Ponzio, Pablo;Aguirre, Nazareno;Galeotti, Juan Pablo;Frias, Marcelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025149120,10.1109/ICPC.2017.20,Software Engineers' Information Seeking Behavior in Change Impact Analysis - An Interview Study,"Software engineers working in large projects must navigate complex information landscapes. Change Impact Analysis (CIA) is a task that relies on engineers' successful information seeking in databases storing, e.g., source code, requirements, design descriptions, and test case specifications. Several previous approaches to support information seeking are task-specific, thus understanding engineers' seeking behavior in specific tasks is fundamental. We present an industrial case study on how engineers seek information in CIA, with a particular focus on traceability and development artifacts that are not source code. We show that engineers have different information seeking behavior, and that some do not consider traceability particularly useful when conducting CIA. Furthermore, we observe a tendency for engineers to prefer less rigid types of support rather than formal approaches, i.e., engineers value support that allows flexibility in how to practically conduct CIA. Finally, due to diverse information seeking behavior, we argue that future CIA support should embrace individual preferences to identify change impact by empowering several seeking alternatives, including searching, browsing, and tracing.",case study | change impact analysis | information seeking | safety-critical systems | traceability,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Borg, Markus;Alegroth, Emil;Runeson, Per",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025173013,10.1109/ICPC.2017.30,How Developers Document Pull Requests with External References,"Online resources of formal and informal documentation-such as reference manuals, forum discussions and tutorials-have become an asset to software developers, as they allow them to tackle problems and to learn about new tools, libraries, and technologies. This study investigates to what extent and for which purpose developers refer to external online resources when they contribute changes to a repository by raising a pull request. Our study involved (i) a quantitative analysis of over 150k URLs occurring in pull requests posted in GitHub, (ii) a manual coding of the kinds of software evolution activities performed in commits related to a statistically significant sample of 2,130 pull requests referencing external documentation resources, (iii) a survey with 69 participants, who provided feedback on how they use online resources and how they refer to them when filing a pull request. Results of the study indicate that, on the one hand, developers find external resources useful to learn something new or to solve specific problems, and they perceive useful referring such resources to better document changes. On the other hand, both interviews and repository mining suggest that external resources are still rarely referred in document changes.",Documenting Changes | Empirical Study | Online Resources,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Zampetti, Fiorella;Ponzanelli, Luca;Bavota, Gabriele;Mocci, Andrea;Di Penta, Massimiliano;Lanza, Michele",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025134739,10.1109/ICPC.2017.33,Exploiting Type Hints in Method Argument Names to Improve Lightweight Type Inference,"The lack of static type information is one of the main obstacles to program comprehension in dynamically-typed languages. While static type inference algorithms try to remedy this problem, they usually suffer from the problem of false positives or false negatives. In order to partially compensate for the lack of static type information, a common practice in dynamically-typed languages is to name or annotate method arguments in such a way that they reveal their expected type, e.g., aString, anInt, or string: String. Recent studies confirmed that these type annotations are indeed frequently used by developers in dynamically-typed languages. We propose a lightweight heuristic that uses these hints from method argument names to augment the performance of a static type inference algorithm. The evaluation through a proof-of-concept prototype implemented in Pharo Smalltalk shows that the augmented algorithm outperforms the basic algorithm, and correctly infers types for 81% more method arguments.",dynamically-typed languages | heuristic | type hints | type-inference,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Milojkovic, Nevena;Ghafari, Mohammad;Nierstrasz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025155680,10.1109/ICPC.2017.2,How Professional Hackers Understand Protected Code while Performing Attack Tasks,"Code protections aim at blocking (or at least delaying) reverse engineering and tampering attacks to critical assets within programs. Knowing the way hackers understand protected code and perform attacks is important to achieve a stronger protection of the software assets, based on realistic assumptions about the hackers' behaviour. However, building such knowledge is difficult because hackers can hardly be involved in controlled experiments and empirical studies. The FP7 European project Aspire has given the authors of this paper the unique opportunity to have access to the professional penetration testers employed by the three industrial partners. In particular, we have been able to perform a qualitative analysis of three reports of professional penetration test performed on protected industrial code. Our qualitative analysis of the reports consists of open coding, carried out by 7 annotators and resulting in 459 annotations, followed by concept extraction and model inference. We identified the main activities: understanding, building attack, choosing and customizing tools, and working around or defeating protections. We built a model of how such activities take place. We used such models to identify a set of research directions for the creation of stronger code protections.",,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Ceccato, M.;Tonella, P.;Basile, C.;Coppens, B.;De Sutter, B.;Falcarin, P.;Torchiano, M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025139989,10.1109/ICPC.2017.3,NetDroid: Summarizing Network Behavior of Android Apps for Network Code Maintenance,"Network access is one of the most common featuresof Android applications. Statistics show that almost 80% ofAndroid apps ask for network permission and thus may havesome network-related features. Android apps may access multipleservers to retrieve or post various types of data, and the codeto handle such network features often needs to change as aresult of server API evolution or the content change of datatransferred. Since various network code is used by multiplefeatures, maintenance of network-related code is often difficultbecause the code may scatter in different places in the codebase, and it may not be easy to predict the impact of a codechange to the network behavior of an Android app. In thispaper, we present an approach to statically summarize networkbehavior from the byte code of Android apps. Our approachis based on string taint analysis, and generates a summary ofnetwork requests by statically estimating the possible values ofnetwork API arguments. To evaluate our technique, we appliedour technique to top 500 android apps from the official GooglePlay market, and the result shows that our approach is able tosummarize network behavior for most apps efficiently (averagelyless than 50 second for an app). Furthermore, we performed anempirical evaluation on 8 real-world maintenance tasks extractedfrom bug reports of open-source Android projects on Github. The empirical evaluation shows that our technique is effective inlocating relevant network code.",Android Apps | Network Summary | String Analysis,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Mostafa, Shaikh;Rodriguez, Rodney;Wang, Xiaoyin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025141671,10.1109/ICPC.2017.26,Analyzing User Comments on YouTube Coding Tutorial Videos,"Video coding tutorials enable expert and noviceprogrammers to visually observe real developers write, debug, and execute code. Previous research in this domain has focusedon helping programmers find relevant content in coding tutorialvideos as well as understanding the motivation and needs ofcontent creators. In this paper, we focus on the link connectingprogrammers creating coding videos with their audience. Morespecifically, we analyze user comments on YouTube codingtutorial videos. Our main objective is to help content creators toeffectively understand the needs and concerns of their viewers, thus respond faster to these concerns and deliver higher-qualitycontent. A dataset of 6000 comments sampled from 12 YouTubecoding videos is used to conduct our analysis. Important userquestions and concerns are then automatically classified andsummarized. The results show that Support Vector Machinescan detect useful viewers' comments on coding videos with anaverage accuracy of 77%. The results also show that SumBasic, an extractive frequency-based summarization technique withredundancy control, can sufficiently capture the main concernspresent in viewers' comments.",classification | coding tutorial | comments | summarization | YouTube,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Poche, Elizabeth;Jha, Nishant;Williams, Grant;Staten, Jazmine;Vesper, Miles;Mahmoud, Anas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025116103,10.1109/ICPC.2017.40,The Effect of Delocalized Plans on Spreadsheet Comprehension: A Controlled Experiment,"Spreadsheets are widely used in industry. Spreadsheets also suffer from typical software engineering issues. Previous research shows that they contain code smells, lack documentation and tests, and have a long live span during which they are transferred multiple times among users. These transfers highlight the importance of spreadsheet comprehension. Therefore, in this paper, we analyze the effect of the organization of formulas on spreadsheet comprehension. To that end, we conduct a controlled experiment with 107 spreadsheet users, divided into two groups. One group receives a model where the formulas are organized such that all related components are grouped closely together, while the other group receives a model where the components are spread far and wide across the spreadsheet. All subjects perform the same set of comprehension tasks on their spreadsheet. The results indicate that the way formulas are located relative to each other in a spreadsheet, influences the performance of the subjects in their ability to comprehend and adapt the spreadsheet. Especially for the comprehension tasks, the subjects perform better on the model where the formulas were grouped closely together. For the adaptation tasks, we found that the length of the calculation chain influences the performance of the subjects more than the location of the formulas itself.",comprehension | controlled experiment | delocalized plans | end user programming | Feature Envy | locality | maintenance | spreadsheets,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Jansen, Bas;Hermans, Felienne",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025173309,10.1109/ICPC.2017.41,The Discipline of Preprocessor-Based Annotations - Does #ifdef TAG n't #endif Matter,"The C preprocessor is a simple, effective, and language-independent tool. Developers use the preprocessor in practice to deal with portability and variability issues. Despite the widespread usage, the C preprocessor suffers from severe criticism, such as negative effects on code understandability and maintainability. In particular, these problems may get worse when using undisciplined annotations, i.e., when a preprocessor directive encompasses only parts of C syntactical units. Nevertheless, despite the criticism and guidelines found in systems like Linux to avoid undisciplined annotations, the results of a previous controlled experiment indicated that the discipline of annotations has no influence on program comprehension and maintenance. To better understand whether developers care about the discipline of preprocessor-based annotations and whether they can really influence on maintenance tasks, in this paper we conduct a mixed-method research involving two studies. In the first one, we identify undisciplined annotations in 110 open-source C/C++ systems of different domains, sizes, and popularity GitHub metrics. We then refactor the identified undisciplined annotations to make them disciplined. Right away, we submit pull requests with our code changes. Our results show that almost two thirds of our pull requests have been accepted and are now merged. In the second study, we conduct a controlled experiment. We have several differences with respect to the aforementioned one, such as blocking of cofounding effects and more replicas. We have evidences that maintaining undisciplined annotations is more time consuming and error prone, representing a different result when compared to the previous experiment. Overall, we conclude that undisciplined annotations should not be neglected.",,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Malaquias, Romero;Ribeiro, Marcio;Bonifacio, Rodrigo;Monteiro, Eduardo;Medeiros, Flavio;Garcia, Alessandro;Gheyi, Rohit",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025129989,10.1109/ICPC.2017.11,Replicating Parser Behavior Using Neural Machine Translation,"More than other machine learning techniques, neural networks have been shown to excel at tasks where humans traditionally outperform computers: recognizing objects in images, distinguishing spoken words from background noise or playing 'Go'. These are hard problems, where hand-crafting solutions is rarely feasible due to their inherent complexity. Higher level program comprehension is not dissimilar in nature: while a compiler or program analysis tool can extract certain facts from (correctly written) code, it has no intrinsic 'understanding' of the data and for the majority of real-world problems, a human developer is needed - for example to find and fix a bug or to summarize the bahavior of a method. We perform a pilot study to determine the suitability of neural machine translation (NMT) for processing plain-text source code. We find that, on one hand, NMT is too fragile to accurately tokenize code, while on the other hand, it can precisely recognize different types of tokens and make accurate guesses regarding their relative position in the local syntax tree. Our results suggest that NMT may be exploited for annotating and enriching out-of-context code snippets to support automated tooling for code comprehension problems. We also identify several challenges in applying neural networks to learning from source code and determine key differences between the application of existing neural network models to source code instead of natural language.",code analysis | deep learning | neural machine translation | parsing | source code,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Alexandru, Carol V.;Panichella, Sebastiano;Gall, Harald C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025144337,10.1109/ICPC.2017.12,Towards Automatic Generation of Short Summaries of Commits,"Committing to a version control system means submitting a software change to the system. Each commit can have a message to describe the submission. Several approaches have been proposed to automatically generate the content of such messages. However, the quality of the automatically generated messages falls far short of what humans write. In studying the differences between auto-generated and human-written messages, we found that 82% of the human-written messages have only one sentence, while the automatically generated messages often have multiple lines. Furthermore, we found that the commit messages often begin with a verb followed by an direct object. This finding inspired us to use a 'verb+object' format in this paper to generate short commit summaries. We split the approach into two parts: verb generation and object generation. As our first try, we trained a classifier to classify a diff to a verb. We are seeking feedback from the community before we continue to work on generating direct objects for the commits.",code differencing | commit log | commit message | natural language processing | program comprehension | version control system,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Jiang, Siyuan;McMillan, Collin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025129947,10.1109/ICPC.2017.14,Android Repository Mining for Detecting Publicly Accessible Functions Missing Permission Checks,"Android has become the most popular mobile operating system. Millions of applications, including many malware, haven been developed for it. Even though its overall system architecture and many APIs are documented, many other methods and implementation details are not, not to mention potential bugs and vulnerabilities that may be exploited. Manual documentation may also be easily outdated as Android evolves constantly with changing features and higher complexities. Techniques and tool supports are thus needed to automatically extract information from different versions of Android to facilitate whole-system analysis of undocumented code. This paper presents an approach for alleviating the challenges associated with whole-system analysis. It performs usual program analysis for different versions of Android by control-flow and data-flow analyses. More importantly, it integrates information retrieval and query heuristics to customize the graphs for purposes related to the queries and make whole-system analyses more efficient. In particular, we use the approach to identify functions in Android that can be invoked by applications in either benign or malicious way, which are referred to as publicly accessible functions in this paper, and with the queries we provided, identify functions that may access sensitive system and/or user data and should be protected by certain permission checks. Based on such information, we can detect some publicly accessible functions in the system that may miss sufficient permission checks. As a proof of concept, this paper has analyzed six Android versions and shows basic statistics about the publicly accessible functions in the Android versions, and detects and verifies several system functions that miss permission checks and may have security implications.",android | call graph | dependency | information retrieval | program analysis | program comprehension,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Nguyen, Hoang H.;Jiang, Lingxiao;Quan, Tho",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027703564,10.1109/ICSE.2017.11,Analyzing APIs Documentation and Code to Detect Directive Defects,"Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility.",API documentation | natural language processing | static analysis,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Zhou, Yu;Gu, Ruihang;Chen, Taolue;Huang, Zhiqiu;Panichella, Sebastiano;Gall, Harald",Include,
10.1016/j.jss.2022.111515,2-s2.0-85027687742,10.1109/ICSE.2017.12,An Unsupervised Approach for Discovering Relevant Tutorial Fragments for APIs,"Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely Fragment Recommender for APIs with PageRank and Topic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-The-Art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.",Application Programming Interface | PageRank Algorithm | Topic Model | Unsupervised Approaches,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Jiang, He;Zhang, Jingxuan;Ren, Zhilei;Zhang, Tao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027679282,10.1109/ICSE.2017.13,Detecting User Story Information in Developer-Client Conversations to Generate Extractive Summaries,"User stories are descriptions of functionality that a software user needs. They play an important role in determining which software requirements and bug fixes should be handled and in what order. Developers elicit user stories through meetings with customers. But user story elicitation is complex, and involves many passes to accommodate shifting and unclear customer needs. The result is that developers must take detailed notes during meetings or risk missing important information. Ideally, developers would be freed of the need to take notes themselves, and instead speak naturally with their customers. This paper is a step towards that ideal. We present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers. We perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically. From this, we found that roughly 10.2% of these conversations contained user story information. Then, we test our technique in a quantitative study to determine the degree to which our technique can extract user story information. In our experiment, our process obtained about 70.8% precision and 18.3% recall on the information.",developer communication | productivity | software engineering | transcripts | user story generation,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Rodeghero, Paige;Jiang, Siyuan;Armaly, Ameer;McMillan, Collin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026763563,10.1109/ICSE.2017.30,Statically Checking Web API Requests in JavaScript,"Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-Time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.",JavaScript | Static analysis | Web APIs,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Wittern, Erik;Ying, Annie T.T.;Zheng, Yunhui;Dolby, Julian;Laredo, Jim A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027708416,10.1109/ICSE.2017.43,Feedback-based debugging,"Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible. In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8% of the bugs and 65% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8% less time to locate the bugs.",Approximation | Debugging | Feedback | Path Pattern | Slicing,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Lin, Yun;Sun, Jun;Xue, Yinxing;Liu, Yang;Dong, Jinsong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026675138,10.1109/ICSE.2017.45,Precise condition synthesis for program repair,"Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an 'if' condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.",,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Xiong, Yingfei;Wang, Jie;Yan, Runfa;Zhang, Jiachen;Han, Shi;Huang, Gang;Zhang, Lu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027719957,10.1109/ICSE.2017.47,Exploring API embedding for API usages and applications,"Word2Vec is a class of neural network models that as being trainedfrom a large corpus of texts, they can produce for each unique word acorresponding vector in a continuous space in which linguisticcontexts of words can be observed. In this work, we study thecharacteristics of Word2Vec vectors, called API2VEC or API embeddings, for the API elements within the API sequences in source code. Ourempirical study shows that the close proximity of the API2VEC vectorsfor API elements reflects the similar usage contexts containing thesurrounding APIs of those API elements. Moreover, API2VEC can captureseveral similar semantic relations between API elements in API usagesvia vector offsets. We demonstrate the usefulness of API2VEC vectorsfor API elements in three applications. First, we build a tool thatmines the pairs of API elements that share the same usage relationsamong them. The other applications are in the code migrationdomain. We develop API2API, a tool to automatically learn the APImappings between Java and C# using a characteristic of the API2VECvectors for API elements in the two languages: semantic relationsamong API elements in their usages are observed in the two vectorspaces for the two languages as similar geometric arrangements amongtheir API2VEC vectors. Our empirical evaluation shows that API2APIrelatively improves 22.6% and 40.1% top-1 and top-5 accuracy over astate-of-The-Art mining approach for API mappings. Finally, as anotherapplication in code migration, we are able to migrate equivalent APIusages from Java to C# with up to 90.6% recall and 87.2% precision.",API embedding | API usages | migration | Word2Vec,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Nguyen, Trong Duc;Nguyen, Anh Tuan;Phan, Hung Dang;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027711067,10.1109/ICSE.2017.49,SPAIN: Security patch analysis for binaries towards understanding the pain and pills,"Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities.",,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Xu, Zhengzi;Chen, Bihuan;Chandramohan, Mahinthan;Liu, Yang;Song, Fu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027721960,10.1109/ICSE.2017.53,Challenges for static analysis of Java reflection-literature review and empirical study,"The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-The-Art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.",Empirical Study | Java | Reflection | Static Analysis | Systematic Literature Review,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",2017-07-19,Conference Paper,"Landman, Davy;Serebrenik, Alexander;Vinju, Jurgen J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040613801,10.1109/ICSME.2017.11,Detecting DOM-sourced cross-site scripting in browser extensions,"In recent years, with the advances in JavaScript engines and the adoption of HTML5 APIs, web applications begin to show a tendency to shift their functionality from the server side towards the client side, resulting in dense and complex interactions with HTML documents using the Document Object Model (DOM). As a consequence, client-side vulnerabilities become more and more prevalent. In this paper, we focus on DOM-sourced Cross-site Scripting (XSS), which is a kind of severe but not well-studied vulnerability appearing in browser extensions. Comparing with conventional DOM-based XSS, a new attack surface is introduced by DOM-sourced XSS where the DOM could become a vulnerable source as well besides common sources such as URLs and form inputs. To discover such vulnerability, we propose a detecting framework employing hybrid analysis with two phases. The first phase is the lightweight static analysis consisting of a text filter and an abstract syntax tree parser, which produces potential vulnerable candidates. The second phase is the dynamic symbolic execution with an additional component named shadow DOM, generating a document as a proof-of-concept exploit. In our large-scale real-world experiment, 58 previously unknown DOM-sourced XSS vulnerabilities were discovered in user scripts of the popular browser extension Greasemonkey.",Browser extension vulnerability | DOM-sourced XSS | Dynamic symbolic execution | JavaScript | Shadow DOM | Web security,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Pan, Jinkun;Mao, Xiaoguang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040591802,10.1109/ICSME.2017.44,Recommending when design technical debt should be self-admitted,"Previous research has shown how developers ""self-admit"" technical debt introduced in the source code, commenting why such code represents a workaround or a temporary, incomplete solution. This paper investigates the extent to which previously self-admitted technical debt can be used to provide recommendations to developers when they write new source code, suggesting them when to ""self-admit"" design technical debt, or possibly when to improve the code being written. To achieve this goal, we have developed a machine learning approach named TEDIOUS (TEchnical Debt IdentificatiOn System), which leverages various kinds of method-level features as independent variables, including source code structural metrics, readability metrics and, last but not least, warnings raised by static analysis tools. We assessed TEDIOUS on data from nine open source projects for which there are available tagged self-admitted technical debt instances, also comparing the performances of different machine learners. Results of the study indicate that TEDIOUS achieves, when recommending self-admitted technical debts within a single project, an average precision of about 50% and a recall of 52%. When predicting cross-projects, TEDIOUS improves, achieving an average precision of 67% and a recall of 55%. Last, but not least, we noticed how TEDIOUS leverages readability, size and complexity metrics, as well as some warnings raised by static analysis tools.",Recommender systems | Self-admitted technical debt | Static analysis tools,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Zampetti, Fiorella;Noiseux, Cedric;Antoniol, Giuliano;Khomh, Foutse;Di Penta, Massimiliano",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85040559968,10.1109/ICSME.2017.8,An empirical study on the removal of Self-Admitted Technical Debt,"Technical debt refers to the phenomena of taking shortcuts to achieve short term gain at the cost of higher maintenance efforts in the future. Recently, approaches were developed to detect technical debt through code comments, referred to as Self-Admitted Technical Debt (SATD). Due to its importance, several studies have focused on the detection of SATD and examined its impact on software quality. However, preliminary findings showed that in some cases SATD may live in a project for a long time, i.e., more than 10 years. These findings clearly show that not all SATD may be regarded as 'bad' and some SATD needs to be removed, while other SATD may be fine to take on. Therefore, in this paper, we study the removal of SATD. In an empirical study on five open source projects, we examine how much SATD is removed and who removes SATD? We also investigate for how long SATD lives in a project and what activities lead to the removal of SATD? Our findings indicate that the majority of SATD is removed and that the majority is self-removed (i.e., removed by the same person that introduced it). Moreover, we find that SATD can last between approx. 18-172 days, on median. Finally, through a developer survey, we find that developers mostly use SATD to track future bugs and areas of the code that need improvements. Also, developers mostly remove SATD when they are fixing bugs or adding new features. Our findings contribute to the body of empirical evidence on SATD, in particular evidence pertaining to its removal.",Mining software repositories | Self-Admitted Technical Debt | Source code quality,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Da Maldonado, Everton S.;Abdalkareem, Rabe;Shihab, Emad;Serebrenik, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040581486,10.1109/ICSME.2017.76,The evaluation of an approach for automatic generated documentation,"Two studies are conducted to evaluate an approach to automatically generate natural language documentation summaries for C++ methods. The documentation approach relies on a method's stereotype information. First, each method is automatically assigned a stereotype(s) based on static analysis and a set of heuristics. Then, the approach uses the stereotype information, static analysis, and predefined templates to generate a natural-language summary/documentation for each method. This documentation is automatically added to the code base as a comment for each method. The result of the first study reveals that the generated documentation is accurate, does not include unnecessary information, and does a reasonable job describing what the method does. Based on statistical analysis of the second study, the most important part of the documentation is the short description as it describes the intended behavior of a method.",Method stereotypes | Program comprehension | Source-code summarization | Static analysis,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Abid, Nahla;Dragan, Natalia;Collard, Michael L.;Maletic, Jonathan I.",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85030760681,10.1109/ICSME.2017.27,"Continuous, evolutionary and large-scale: A new perspective for automated mobile app testing","Mobile app development involves a unique set of challenges including device fragmentation and rapidly evolving platforms, making testing a difficult task. The design space for a comprehensive mobile testing strategy includes features, inputs, potential contextual app states, and large combinations of devices and underlying platforms. Therefore, automated testing is an essential activity of the development process. However, current state of the art of automated testing tools for mobile apps posses limitations that has driven a preference for manual testing in practice. As of today, there is no comprehensive automated solution for mobile testing that overcomes fundamental issues such as automated oracles, history awareness in test cases, or automated evolution of test cases. In this perspective paper we survey the current state of the art in terms of the frameworks, tools, and services available to developers to aid in mobile testing, highlighting present shortcomings. Next, we provide commentary on current key challenges that restrict the possibility of a comprehensive, effective, and practical automated testing solution. Finally, we offer our vision of a comprehensive mobile app testing framework, complete with research agenda, that is succinctly summarized along three principles: Continuous, Evolutionary and Large-scale (CEL).",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Linares-Vásquez, Mario;Moran, Kevin;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040571715,10.1109/ICSME.2017.75,An empirical study of local database usage in android applications,"Local databases have become an important component within mobile applications. Developers use local databases to provide mobile users with a responsive and secure service for data storage and access. However, using local databases comes with a cost Studies have shown that they are one of the most energy consuming components on mobile devices and misuse of their APIs can lead to performance and security problems. In this paper, we report the results of a large scale empirical study on 1,000 top ranked apps from the Google Play app store. Our results present a detailed look into the practices, costs, and potential problems associated with local database usage in deployed apps. We distill our findings into actionable guidance for developers and motivate future areas of research related to techniques to support mobile app developers.",Database | Empirical study | Energy | Mobile applications | Performance | Security,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Lyu, Yingjun;Gui, Jiaping;Wan, Mian;Halfond, William G.J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040543168,10.1109/ICSME.2017.80,Recommending framework extension examples,"The use of software frameworks enables the delivery of common functionality but with significantly less effort than when developing from scratch. To meet application specific requirements, the behavior of a framework needs to be customized via extension points. A common way of customizing framework behavior is by passing a framework related object as an argument to an API call. Such an object can be created by subclassing an existing framework class or interface, or by directly customizing an existing framework object. However, to do this effectively requires developers to have extensive knowledge of the framework's extension points and their interactions. To aid the developers in this regard, we propose and evaluate a graph mining approach for extension point management. Specifically, we propose a taxonomy of extension patterns to categorize the various ways an extension point has been used in the code examples. Our approach mines a large amount of code examples to discover all extension points and patterns for each framework class. Given a framework class that is being used, our approach AIDS the developer by following a two-step recommendation process. First, it recommends all the extension points that are available in the class. Once the developer chooses an extension point, our approach then discovers all of its usage patterns and recommends the best code examples for each pattern. Using five frameworks, we evaluate the performance of our two-step recommendation, in terms of precision, recall, and F-measure. We also report several statistics related to framework extension points.",API | Code example | Extension pattern | Extension point | Framework | Graph mining | Recommender | Reuse,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Asaduzzaman, Muhammad;Roy, Chanchal K.;Schneider, Kevin A.;Hou, Daqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040628967,10.1109/ICSME.2017.17,On-demand developer documentation,"We advocate for a paradigm shift in supporting the information needs of developers, centered around the concept of automated on-demand developer documentation. Currently, developer information needs are fulfilled by asking experts or consulting documentation. Unfortunately, traditional documentation practices are inefficient because of, among others, the manual nature of its creation and the gap between the creators and consumers. We discuss the major challenges we face in realizing such a paradigm shift, highlight existing research that can be leveraged to this end, and promote opportunities for increased convergence in research on software documentation.",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Robillard, Martin P.;Marcus, Andrian;Treude, Christoph;Bavota, Gabriele;Chaparro, Oscar;Ernst, Neil;Gerosall, Marco Aurélio;Godfrey, Michael;Lanza, Michele;Linares-Vásquez, Mario;Murphy, Gail C.;Moreno, Laura;Shepherd, David;Wong, Edmund",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85040639267,10.1109/ICSME.2017.65,Flattening code for metrics measurement and analysis,"When we measure code metrics or analyze source code, code normalization is occasionally performed as a preprocessing. Code normalization means removing untargeted program elements, formatting source code, or transforming source code with specific rules. Code normalization makes measurement and analysis results more significant. Existing code normalization mainly targets program elements not influencing program behavior (e.g., code comments and blank lines) or program tokens (e.g., variable names and literals). In this paper, we propose a new code normalization technique targeting program structure. Our proposed technique transforms a complex program statement to simple ones. We call this transformation flattening. By flattening code, we can obtain source code including only simple program statements. As applications of the code flattening, we report how it changes LOC metric and clone detection results.",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Higo, Yoshiki;Kusumoto, Shinji",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040611701,10.1109/ICSME.2017.78,KOWALSKI: Collecting API clients in easy mode,"Understanding API usage is important for upstream and downstream developers. However, compiling a dataset of API clients is often a tedious task, especially since one needs many clients to draw a representative picture of the API usage. In this paper, we present KOWALSKI, a tool that takes the name of an API, then finds and downloads client binaries by exploiting the Maven dependency management system. As a case study, we collect clients of Apache Lucene, the de facto standard for full-text search, analyze the binaries, and create a typed call graph that allows developers to identify hotspots in the API. A video demonstrating how KOWALSKI is used for this experiment can be found at https://youtu.be/zdx28GnoSRQ.",API client collection | API usage analysis | Dependency management systems | Repository mining,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Leuenberger, Manuel;Osman, Haidar;Ghafari, Mohammad;Nierstrasz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040634957,10.1109/ICSME.2017.73,Behavior-informed algorithms for automatic documentation generation,"Programmers rely on source code documentation to quickly understand what the source code does and how they would use it. Unfortunately, many programmers do not have the time to write and maintain source code documentation. A solution to this problem is to document and summarize source code automatically. Unfortunately, research efforts to automatically generate documentation have stalled recently because the research community does not know exactly what a summary of source code should include. To solve this problem, my overall strategy is to study programmer behavior in order to write algorithms that mimic that behavior. I have four key areas of work in which I execute that strategy: First, I determine what areas of code programmers read when they create documentation. Second, I find patterns in programmers' eye movements when they reading code. Third, I use recordings of developer-client meetings to extract user story information. Finally, I propose to conduct a grounded theory study at a medium sized software company to determine whether factors outside the code influence source code summarization. This paper discusses the foundation for my career in the software engineering community, and I seek the community's advice.",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Rodeghero, Paige",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85040608482,10.1109/ICSME.2017.49,Mining AndroZoo: A retrospect,"This paper presents a retrospect of an Android app collection named AndroZoo and some research works conducted on top of the collection. AndroZoo is a growing collection of Android apps from various markets including the official Google Play. At the moment, over five million Android apps have been collected. Based on AndroZoo, we have explored several directions that mine Android apps for resolving various challenges. In this work, we summarize those resolved mining challenges in three research dimensions, including code analysis, app evolution analysis, malware analysis, and present in each dimension several case studies that experimentally demonstrate the usefulness of AndroZoo.",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Li, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026811762,10.1109/MiSE.2017.15,9th Workshop on Modelling in Software Engineering (MiSE 2017),,,"Proceedings - 2017 IEEE/ACM 9th International Workshop on Modelling in Software Engineering, MiSE 2017",2017-06-29,Editorial,"Di Ruscio, Davide;Chechik, Marsha;Rumpe, Bernhard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026832060,10.1109/MiSE.2017.9,A Restricted Natural Language Based Use Case Modeling Methodology for Real-Time Systems,"Time-related properties are a critical type of extrafunctionalrequirements for designing real-time systems. Modeling and validating time-related properties at therequirements specification and analysis phases is important forthe successful development of real-time systems in terms of cost, quality and productivity. In the literature and practice, timinganalyses (e.g., Worst Case Execution Time) are often performedto ensure that the design of a real-time system fully conforms toits time-related constraints. However, such analyses are mostlyperformed at the design and implementation stages, but not atthe requirements level. This paper presents a restricted, naturallanguage based, use case modeling methodology (named asRUCM4RT) to specify functional requirements of real-timesystems as use case models, along with associated time-relatedconstraints. RUCM4RT was proposed based on the UMLprofile for Modeling and Analysis of Real-Time and EmbeddedSystems (MARTE). In addition, in this paper, we also propose ametamodel-based formalization mechanism named asUCMeta4RT to automatically formalize use case models. Wehave conducted two real-world case studies to evaluate oursolution and 40 use cases were modeled, among which 27 realtimeuse cases, 118 time-related constraints and 47 other extrafunctional(also commonly called non-functional) constraintswere specified. Results show that RUCM4RT was able to handleall the real-time related elements (e.g., time-related constraints)of the use case models.",MARTE | Time-related Constraints | Use Case Modeling,"Proceedings - 2017 IEEE/ACM 9th International Workshop on Modelling in Software Engineering, MiSE 2017",2017-06-29,Conference Paper,"Zhang, Huihui;Yue, Tao;Ali, Shaukat;Wu, Ji;Liu, Chao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026833071,10.1109/MiSE.2017.10,Specifying Evolving Requirements Models with TimedURN,"The User Requirements Notation (URN) supports the elicitation, specification, and analysis of integrated goal and scenario models. The analysis of the goal and scenario models focuses on one snapshot in time and does not allow the model to change over time. While several models may be created that represent different stages of a system, managing several, slightly different model copies is a space-consuming, time-consuming, and error-prone task that makes it difficult to maintain consistency across the model copies. This paper introduces TimedURN, an extension of the URN standard, which enables the modeling and analysis of a comprehensive set of changes to a goal and scenario model over time. The changes to the model are captured in one base model, which eases system evolution. The metamodel for TimedURN is presented and it is argued that it can also be applied to other modeling languages. Furthermore, the usefulness of TimedURN is illustrated with an example from the sustainability domain and the comprehensiveness of the supported types of changes is assessed.",analysis | evaluation mechanism | evolution | goal modeling | Goal-oriented Requirement Language | GRL | scenario modeling | traversal mechanism | UCM | URN | Use Case Maps | User Requirements Notation,"Proceedings - 2017 IEEE/ACM 9th International Workshop on Modelling in Software Engineering, MiSE 2017",2017-06-29,Conference Paper,"Aprajita, ;Luthra, Sahil;Mussbacher, Gunter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026847945,10.1109/MiSE.2017.5,Transforming Workflow Models into Automated End-to-End Acceptance Test Cases,"The User Requirements Notation is a standard published by the International Telecommunication Union that contains two complementary notations for goal and scenario/workflow modeling. Use Case Maps (UCM)-the workflow notation-focuses on the causal relationships of the steps in a workflow without requiring the specification of detailed message exchanges and data. A UCM model captures the interactions between actors and the system and typically integrates several use cases into a combined system view. This results in a high-level description of the system and its end-to-end usage scenarios. At the UCM level, scenario definitions create a regression test suite for the UCM model. This paper investigates the transformation of such workflow models into end-to-end acceptance test cases that can be automated with the JUnit testing framework. For that purpose, the UCM model is enriched with (i) input data types and expected results, (ii) a code-level description of system behavior as needed for the workflow, and (iii) testing logic including assertions. Based on this specification, the proposed approach uses boundary value analysis of the input data and Myer's test selection heuristics to determine a set of test cases for the described workflow. Coverage criteria may be specified at the UCM model level. Results from a case study of a small data management system indicate a reduction of the number of lines of code that need to be specified in the workflow model vs. the test implementation by an order of magnitude.",acceptance testing | test case generation | traversal mechanism | UCM | URN | Use Case Maps | User Requirements Notation,"Proceedings - 2017 IEEE/ACM 9th International Workshop on Modelling in Software Engineering, MiSE 2017",2017-06-29,Conference Paper,"Boucher, Mathieu;Mussbacher, Gunter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026528397,10.1109/MSR.2017.16,Mining social web service repositories for social relationships to aid service discovery,"The Service Oriented Computing (SOC) paradigm promotes building new applications by discovering and then invoking services, i.e., software components accessible through the Internet. Discovering services means inspecting registries where textual descriptions of services functional capabilities are stored. To automate this, existing approaches index descriptions and associate users' queries to relevant services. However, the massive adoption of Web-exposed API development practices, specially in large service ecosystems such as the IoT, is leading to evergrowing registries which challenge the accuracy and speed of such approaches. The recent notion of Social Web Services (SWS), where registries not only store service information but also sociallike relationships between users and services opens the door to new discovery schemes. We investigate an approach to discover SWSs that operates on graphs with user-service relationships and employs lightweight topological metrics to assess service similarity. Then, 'socially' similar services, which are determined exploiting explicit relationships and mining implicit relationships in the graph, are clustered via exemplar-based clustering to ultimately aid discovery. Experiments performed with the ProgrammableWeb.com registry, which is at present the largest SWS repository with over 15k services and 140k user-service relationships, show that pure topology-based clustering may represent a promising complement to content-based approaches, which in fact are more time-consuming due to text processing operations.",Exemplar-based clustering | Service discovery | Social recommender systems | Social Web Service,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Corbellini, Alejandro;Godoy, Daniela;Mateos, Cristian;Zunino, Alejandro;Lizarralde, Ignacio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026525638,10.1109/MSR.2017.35,Spencer: Interactive heap analysis for the masses,"Programming language-design and run-Time-implementation require detailed knowledge about the programs that users want to implement. Acquiring this knowledge is hard, and there is little tool support to effectively estimate whether a proposed tradeoff actually makes sense in the context of real world applications. Ideally, knowledge about behaviour of 'typical' programs is 1) easily obtainable, 2) easily reproducible, and 3) easily sharable. We present Spencer, an open source web service and APIframework for dynamic analysis of a continuously growing set of traces of standard program corpora. Users do not obtain traces on their own, but can instead send queries to the web service that will be executed on a set of program traces. Queries are built in terms of a set of query combinators that present a high level interface for working with trace data. Since the framework is high level, and there is a hosted collection of recorded traces, queries are easy to implement. Since the data sets are shared by the research community, results are reproducible. Since the actual queries run on one (or many) servers that provide analysis as a service, obtaining results is possible on commodity hardware. Data in Spencer is meant to be obtained once, and analysed often, making the overhead of data collection mostly irrelevant. This allows Spencer to collect more data than traditional tracing tools can afford within their performance budget. Results in Spencer are cached, making complicated analyses that build on cached primitive queries speedy.",Dynamic Analysis | Heap Analysis | Tracing,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Brandauer, Stephan;Wrigstad, Tobias",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026548340,10.1109/MSR.2017.53,Predicting likelihood of requirement implementation within the planned iteration: An empirical study at IBM,"There has been a significant interest in the estimation of time and effort in fixing defects among both software practitioners and researchers over the past two decades. However, most of the focus has been on prediction of time and effort in resolving bugs, without much regard to predicting time needed to complete high-level requirements, a critical step in release planning. In this paper, we describe a mixed-method empirical study on three large IBM projects in which we developed and evaluated a process of training a predictive model constituting a set of 29 features in nine categories in order to predict if a requirement will be completed within its planned iteration. We conducted feature engineering through iterative interviews with IBM practitioners as well as analysis of large development repositories of these three projects. Using machine learning techniques, we were able to make predictions on completion time of requirements at four different stages of their lifetime. Using our industrial partner's interest in high precision over recall, we then adopted a cost sensitive learning method and maximized precision of predictions (ranging from 0.8 to 0.97) while maintaining an acceptable recall. We also ranked the features based on their relative importance to the optimized predictive model. We show that although satisfying predictions can be made at early stages, performance of predictions improves over time by taking advantage of requirements' progress data. Furthermore, feature importance ranking results show that although importance of features are highly dependent on project and prediction stage, there are certain features (e.g. requirement creator, time remained to the end of iteration, time since last requirement summary change and number of times requirement has been replanned for a new iteration) that emerge as important across most projects and stages, implying future worthwhile research directions for both researchers and practitioners.",Completion Time Prediction | Machine Learning | Mining Software Repositories | Release Planning,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Dehghan, Ali;Neal, Adam;Blincoe, Kelly;Linaker, Johan;Damian, Daniela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026506794,10.1109/MSR.2017.42,Choosing an NLP Library for Analyzing Software Documentation: A Systematic Literature Review and a Series of Experiments,"To uncover interesting and actionable information from natural language documents authored by software developers, many researchers rely on 'out-of-The-box' NLP libraries. However, software artifacts written in natural language are different from other textual documents due to the technical language used. In this paper, we first analyze the state of the art through a systematic literature review in which we find that only a small minority of papers justify their choice of an NLP library. We then report on a series of experiments in which we applied four state-of-The-Art NLP libraries to publicly available software artifacts from three different sources. Our results show low agreement between different libraries (only between 60% and 71% of tokens were assigned the same part-of-speech tag by all four libraries) as well as differences in accuracy depending on source: For example, spaCy achieved the best accuracy on Stack Overflow data with nearly 90% of tokens tagged correctly, while it was clearly outperformed by Google's SyntaxNet when parsing GitHub ReadMe files. Our work implies that researchers should make an informed decision about the particular NLP library they choose and that customizations to libraries might be necessary to achieve good results when analyzing software artifacts written in natural language.",Natural language processing | NLP libraries | Part-of-Speech tagging | Software documentation,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Al Omran, Fouad Nasser A.;Treude, Christoph",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85026547310,10.1109/MSR.2017.9,Leveraging Automated Sentiment Analysis in Software Engineering,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-The-Art sentiment analysis tool we compare with.",,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Islam, Md Rakibul;Zibran, Minhaz F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026557759,10.1109/MSR.2017.63,Classifying Code Comments in Java Open-Source Software Systems,"Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments, subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning, initial results are promising and suggest that an accurate classification is within reach.",comment taxonomy | software quality | source code comments,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Pascarella, Luca;Bacchelli, Alberto",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85026557876,10.1109/MSR.2017.24,TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration,"Continuous Integration (CI) has become a best practice of modern software development. Thanks in part to its tight integration with GitHub, Travis CI has emerged as arguably the most widely used CI platform for Open-Source Software (OSS) development. However, despite its prominent role in Software Engineering in practice, the benefits, costs, and implications of doing CI are all but clear from an academic standpoint. Little research has been done, and even less was of quantitative nature. In order to lay the groundwork for data-driven research on CI, we built TravisTorrent, travistorrent.testroots.org, a freely available data set based on Travis CI and GitHub that provides easy access to hundreds of thousands of analyzed builds from more than 1,000 projects. Unique to TravisTorrent is that each of its 2,640,825 Travis builds is synthesized with meta data from Travis CI's API, the results of analyzing its textual build log, a link to the GitHub commit which triggered the build, and dynamically aggregated project data from the time of commit extracted through GHTorrent.",,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Beller, Moritz;Gousios, Georgios;Zaidman, Andy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026534210,10.1109/MSR.2017.31,An empirical study of the personnel overhead of continuous integration,"Continuous Integration (CI) is a software development practice where changes to the codebase are compiled and automatically checked for software quality issues. Like any software artifact (e.g., production code, build specifications), CI systems require an investment of development resources in order to keep them running smoothly. In this paper, we examine the human resources that are associated with developing and maintaining CI systems. Through the analysis of 1,279 GitHub repositories that adopt Travis CI (a popular CI service provider), we observe that: (i) there are 0 to 6 unique contributors to CI-related development in any 30-day period, regardless of project size, and (ii) the total number of CI developers has an upper bound of 15 for 99.2% of the studied projects, regardless of overall team size. These results indicate that service-based CI systems only require a small proportion of the development team to contribute. These costs are almost certainly outweighed by the reported benefits of CI (e.g., team communication and time-To-market for new content).",Continuous Integration | Empirical Study | Software Engineering,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Manglaviti, Marco;Coronado-Montoya, Eduardo;Gallaba, Keheliya;McIntosh, Shane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3062341.3062378,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3062341.3062367,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047181379,10.1109/SCAM.2017.16,Revisiting Exception Handling Practices with Exception Flow Analysis,"Modern programming languages, such as Java and C#, typically provide features that handle exceptions. These features separate error-handling code from regular source code and aim to assist in the practice of software comprehension and maintenance. Having acknowledged the advantages of exception handling features, their misuse can still cause reliability degradation or even catastrophic software failures. Prior studies on exception handling aim to understand the practices of exception handling in its different components, such as the origin of the exceptions and the handling code of the exceptions. Yet, the observed findings were scattered and diverse. In this paper, to complement prior research findings on exception handling, we study its features by enriching the knowledge of handling code with a flow analysis of exceptions. Our case study is conducted with over 10K exception handling blocks, and over 77K related exception flows from 16 open-source Java and C# (.NET) libraries and applications. Our case study results show that each try block has up to 12 possible potentially recoverable yet propagated exceptions. More importantly, 22% of the distinct possible exceptions can be traced back to multiple methods (average of 1.39 and max of 34). Such results highlight the additional challenge of composing quality exception handling code. To make it worse, we confirm that there is a lack of documentation of the possible exceptions and their sources. However, such critical information can be identified by exception flow analysis on well-documented API calls (e.g., JRE and.NET documentation). Finally, we observe different strategies in exception handling code between Java and C#. Our findings highlight the opportunities of leveraging automated software analysis to assist in exception handling practices and signify the need of more further in-depth studies on exception handling practice.",exception flow analysis | exception handling | software engineering | source code analysis,"Proceedings - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation, SCAM 2017",2017-10-30,Conference Paper,"Padua, Guilherme B.De;Shang, Weiyi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047197551,10.1109/SCAM.2017.25,Does the Choice of Configuration Framework Matter for Developers? Empirical Study on 11 Java Configuration Frameworks,"Configuration frameworks are routinely used in software systems to change application behavior without recompilation. Selecting a suitable configuration framework among the vast variety of existing choices is a crucial decision for developers, as it can impact project reliability and its maintenance profile. In this paper, we analyze almost 2,000 Java projects on GitHub to investigate the features and properties of 11 major Java configuration frameworks. We analyze the popularity of the frameworks and try to identify links between the maintenance effort involved with the usage of these frameworks and the frameworks' properties. More basic frameworks turn out to be the most popular, but in half of the cases are complemented by more complex frameworks. Furthermore, younger, more active frameworks with more detailed documentation, support for hierarchical configuration models and/or more data formats seem to require more maintenance by client developers.",,"Proceedings - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation, SCAM 2017",2017-10-30,Conference Paper,"Sayagh, Mohammed;Dong, Zhen;Andrzejak, Artur;Adams, Bram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040622329,10.1109/SCAM.2017.22,Harvesting the Wisdom of the Crowd to Infer Method Nullness in Java,"Null pointer exceptions are common bugs in Java projects. Previous research has shown that dereferencing the results of method calls is the main source of these bugs, as developers do not anticipate that some methods return null. To make matters worse, we find that whether a method returns null or not (nullness), is rarely documented. We argue that method nullness is a vital piece of information that can help developers avoid this category of bugs. This is especially important for external APIs where developers may not even have access to the code.,In this paper, we study the method nullness of Apache Lucene, the de facto standard library for text processing in Java. Particularly, we investigate how often the result of each Lucene method is checked against null in Lucene clients. We call this measure method nullability, which can serve as a proxy for method nullness. Analyzing Lucene internal and external usage, we find that most methods are never checked for null. External clients check more methods than Lucene checks internally. Manually inspecting our dataset reveals that some null checks are unnecessary. We present an IDE plugin that complements existing documentation and makes up for missing documentation regarding method nullness and generates nullness annotations, so that static analysis can pinpoint potentially missing or unnecessary null checks.",API usage analysis | dependency management systems | null pointer exceptions | static analysis,"Proceedings - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation, SCAM 2017",2017-10-30,Conference Paper,"Leuenberger, Manuel;Osman, Haidar;Ghafari, Mohammad;Nierstrasz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047189561,10.1109/VISSOFT.2017.11,Method Execution Reports: Generating Text and Visualization to Describe Program Behavior,"To obtain an accurate understanding of program behavior, developers use a set of tools and techniques such as logging outputs, debuggers, profilers, and visualizations. These support an in-depth analysis of the program behavior, each approach focusing on a different aspect. What is missing, however, is an approach to get an overview of a program execution. As a first step to fill this gap, this paper presents an approach to generate Method Execution Reports. Each report summarizes the execution of a selected method for a specific execution of the program using natural-language text and embedded visualizations. A report provides an overview of the dynamic calls and time consumption related to the selected method. We present a framework to generate these reports and discuss the specific instantiation and phrasing we have chosen. Our results comprise feedback from developers discussing the understandability and usefulness of our approach and a task-based comparison to state-of-the-art solutions.",documentation | natural language generation | program execution | sparklines,"Proceedings - 2017 IEEE Working Conference on Software Visualization, VISSOFT 2017",2017-10-31,Conference Paper,"Beck, Fabian;Siddiqui, Hafiz Ammar;Bergel, Alexandre;Weiskopf, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027727858,10.1109/ICGSE.2017.14,Preliminary evaluation of a tag-based knowledge condensation tool in agile and distributed teams,"An important challenge in Agile Global Software Development (AGSD) is architectural knowledge vaporization, i.e., the loss of technical knowledge due to a lack of documentation. In a previous work we identified that technical knowledge is usually available in unstructured textual electronic means (e.g. chat, mail, blogs, etc.-also known as UTEMs) used by AGSD workers during development, although not necessarily easily accessible. Also, we proposed the concept of Knowledge Condensation as a means to recover knowledge from UTEMs, by introducing a structure to their contents along with means to retrieve knowledge. In this work we present our first step towards a Knowledge Condensation tool, through a prototype tagging mechanism to structure architectural knowledge during UTEM interactions. We evaluated the prototype and obtained the following results: (1) a high perception of usefulness and a high projected intention of use, and (2) the identification of 6 tagging behavior profiles from participants. These tagging behaviors can be grouped as (i) participants who tag as expected, (ii) participants who tag more than expected, and (ii) participants who tag less than expected. These results suggest that it is important to cope with 'little or no tagging' and 'excessive tagging' behaviors to implement knowledge condensation based on UTEM tagging. Furthermore, we identified that tagging could be a good option to structure UTEM interactions, and to retrieve architectural knowledge from UTEM logs. Future work includes improving the tagging mechanism prototype based on the participants' suggestions and overall evaluation results, and implementing a searcher prototype to further evaluate the concept of knowledge condensation based on tagging of UTEM interactions.",Agile global software development | Architectural knowledge | Knowledge condensing | Knowledge vaporization | Unstructured and textual electronic media,"Proceedings - 2017 IEEE 12th International Conference on Global Software Engineering, ICGSE 2017",2017-07-12,Conference Paper,"Borrego, Gilberto;Moran, Alberto L.;Palacio, Ramon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027689021,10.1109/ICGSE.2017.11,"Developer turnover in global, industrial open source projects: Insights from applying survival analysis","Large open source software projects often have a globally distributed development team. Studies have shown developer turnover has a significant impact on the project success. Frequent developer turnover may lead to loss of productivity due to lacking relevant knowledge and spending extra time learning how projects work. Thus, lots of attention has been paid to which factors are related to developer retention, however, few of them focus on the impact of activities of individual developers. In this paper, we study five open source projects from different organizations and examine whether developer turnover is affected by when they start contributing and what types of contributions they are making. Our study reveals that developers have higher chances to survive in software projects when they 1) start contributing to the project earlier, 2) mainly modify instead of creating files, 3) mainly code instead of dealing with documentations. Our results also shed lights on the potential approaches to improving developer retention.",,"Proceedings - 2017 IEEE 12th International Conference on Global Software Engineering, ICGSE 2017",2017-07-12,Conference Paper,"Lin, Bin;Robles, Gregorio;Serebrenik, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026774524,10.1007/978-3-319-63387-9_11,Stlinspector: Stl validation with guarantees,"STLInspector is a tool for systematic validation of Signal Temporal Logic (STL) specifications against informal textual requirements. Its goal is to identify typical faults that occur in the process of formalizing requirements by mutating a candidate specification. STLInspector computes a series of representative signals that enables a requirements engineer to validate a candidate specification against all its mutated variants, thus achieving full mutation coverage. By visual inspection of the signals via a web-based GUI, an engineer can obtain high confidence in the correctness of the formalization – even if she is not familiar with STL. STLInspector makes the assessment of formal specifications accessible to a wide range of developers in industry, hence contributes to leveraging the use of formal specifications and computer-aided verification in industrial practice. We apply the tool to several collections of STL formulas and show its effectiveness.",MTL | SMT Mutation testing | Specification validation | STL | Temporal logic,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Roehm, Hendrik;Heinz, Thomas;Mayer, Eva Charlotte",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026750370,10.1007/978-3-319-63387-9_16,Montre: A tool for monitoring timed regular expressions,"We present Montre, a monitoring tool to search patterns specified by timed regular expressions over real-time behaviors. We use timed regular expressions as a compact, natural, and highly-expressive pattern specification language for monitoring applications involving quantitative timing constraints. Our tool essentially incorporates online and offline timed pattern matching algorithms so it is capable of finding all occurrences of a given pattern over both logged and streaming behaviors. Furthermore, Montre is designed to work with other tools via standard interfaces to perform more complex and versatile tasks for analyzing and reasoning about cyber-physical systems. As the first of its kind, we believe Montre will enable a new line of inquiries and techniques in these fields.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Ulus, Dogan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026733414,10.1007/978-3-319-63387-9_21,Mightyl: A compositional translation from mitl to timed automata,"Metric Interval Temporal Logic (MITL) was first proposed in the early 1990s as a specification formalism for real-time systems. Apart from its appealing intuitive syntax, there are also theoretical evidences that make MITL a prime real-time counterpart of Linear Temporal Logic (LTL). Unfortunately, the tool support for MITL verification is still lacking to this day. In this paper, we propose a new construction from MITL to timed automata via very-weak one-clock alternating timed automata. Our construction subsumes the well-known construction from LTL to Büchi automata by Gastin and Oddoux and yet has the additional benefits of being compositional and integrating easily with existing tools. We implement the construction in our new tool MightyL and report on experiments using Uppaal and LTSmin as back-ends.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Brihaye, Thomas;Geeraerts, Gilles;Ho, Hsi Ming;Monmege, Benjamin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026766106,10.1007/978-3-319-63387-9_24,Classification and coverage-based falsification for embedded control systems,"Many industrial cyber-physical system (CPS) designs are too complex to formally verify system-level properties. A practical approach for testing and debugging these system designs is falsification, wherein the user provides a temporal logic specification of correct system behaviors, and some technique for selecting test cases is used to identify behaviors that demonstrate that the specification does not hold for the system. While coverage metrics are often used to measure the exhaustiveness of this kind of testing approach for software systems, existing falsification approaches for CPS designs do not consider coverage for the signal variables. We present a new coverage measure for continuous signals and a new falsification technique that leverages the measure to efficiently identify falsifying traces. This falsification algorithm combines global and local search methods and uses a classification technique based on support vector machines to identify regions of the search space on which to focus effort. We use an industrial example from an automotive fuel cell application and other benchmark models to compare the new approach against existing falsification tools.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Adimoolam, Arvind;Dang, Thao;Donzé, Alexandre;Kapinski, James;Jin, Xiaoqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026771315,10.1007/978-3-319-63387-9_29,Pithya: A parallel tool for parameter synthesis of piecewise multi-affine dynamical systems,"We present a novel tool for parameter synthesis of piecewise multi-affine dynamical systems from specifications expressed in a hybrid branching-time temporal logic. The tool is based on the algorithm of parallel semi-symbolic coloured model checking that extends standard model checking methods to cope with parametrised Kripke structures. The tool implements state-of-the-art techniques developed in our previous research and is primarily intended to be used for the analysis of dynamical systems with uncertain parameters that frequently arise in computational systems biology. However, it can be employed for any dynamical system where the non-linear equations can be sufficiently well approximated by piecewise multi-affine equations.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Beneš, Nikola;Brim, Luboš;Demko, Martin;Pastva, Samuel;Šafránek, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026765118,10.1007/978-3-319-63390-9_19,Quantitative assume guarantee synthesis,"In assume-guarantee synthesis, we are given a specification <A, G>, describing an assumption on the environment and a guarantee for the system, and we construct a system that interacts with an environment and is guaranteed to satisfy G whenever the environment satisfies A. While assume-guarantee synthesis is 2EXPTIME-complete for specifications in LTL, researchers have identified the GR(1) fragment of LTL, which supports assume-guarantee reasoning and for which synthesis has an efficient symbolic solution. In recent years we see a transition to quantitative synthesis, in which the specification formalism is multi-valued and the goal is to generate high-quality systems, namely ones that maximize the satisfaction value of the specification. We study quantitative assume-guarantee synthesis. We start with specifications in LTL[F], an extension of LTL by quality operators. The satisfaction value of an LTL[F] formula is a real value in [0, 1], where the higher the value is, the higher is the quality in which the computation satisfies the specification. We define the quantitative extension GR(1)[F] of GR(1). We show that the implication relation, which is at the heart of assume-guarantee reasoning, has two natural semantics in the quantitative setting. Indeed, in addition to max{1 − A, G}, which is the multi-valued counterpart of Boolean implication, there are settings in which maximizing the ratio G/A is more appropriate. We show that GR(1)[F] formulas in both semantics are hard to synthesize. Still, in the implication semantics, we can reduce GR(1)[F] synthesis to GR(1) synthesis and apply its efficient symbolic algorithm. For the ratio semantics, we present a sound approximation, which can also be solved efficiently. Our experimental results show that our approach can successfully synthesize GR(1)[F] specifications with over a million of concrete states.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Almagor, Shaull;Kupferman, Orna;Ringert, Jan Oliver;Velner, Yaron",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026737173,10.1007/978-3-319-63390-9_20,Syntax-guided optimal synthesis for chemical reaction networks,"We study the problem of optimal syntax-guided synthesis of stochastic Chemical Reaction Networks (CRNs) that plays a fundamental role in design automation of molecular devices and in the construction of predictive biochemical models. We propose a sketching language for CRNs that concisely captures syntactic constraints on the network topology and allows its under-specification. Given a sketch, a correctness specification, and a cost function defined over the CRN syntax, our goal is to find a CRN that simultaneously meets the constraints, satisfies the specification and minimizes the cost function. To ensure computational feasibility of the synthesis process, we employ the Linear Noise Approximation allowing us to encode the synthesis problem as a satisfiability modulo theories problem over a set of parametric Ordinary Differential Equations (ODEs). We design and implement a novel algorithm for the optimal synthesis of CRNs that employs almost complete refutation procedure for SMT over reals and ODEs, and exploits a meta-sketching abstraction controlling the search strategy. Through relevant case studies we demonstrate that our approach significantly improves the capability of existing methods for synthesis of biochemical systems and paves the way towards their automated and provably-correct design.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Cardelli, Luca;Češka, Milan;Fränzle, Martin;Kwiatkowska, Marta;Laurenti, Luca;Paoletti, Nicola;Whitby, Max",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026766593,10.1007/978-3-319-63390-9_29,"Eahyper: Satisfiability, implication, and equivalence checking of hyperproperties","We introduce EAHyper, the first tool for the automatic checking of satisfiability, implication, and equivalence of hyperproperties. Hyperproperties are system properties that relate multiple computation traces. A typical example is an information flow policy that compares the observations made by an external observer on execution traces that result from different values of a secret variable. EAHyper analyzes hyperproperties that are specified in HyperLTL, a recently introduced extension of linear-time temporal logic (LTL). HyperLTL uses trace variables and trace quantifiers to refer to multiple execution traces simultaneously. Applications of EAHyper include the automatic detection of specifications that are inconsistent or vacuously true, as well as the comparison of multiple formalizations of the same policy, such as different notions of observational determinism.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Finkbeiner, Bernd;Hahn, Christopher;Stenger, Marvin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026787579,10.1007/978-3-319-63390-9_30,Automating induction for solving horn clauses,"Verification problems of programs in various paradigms can be reduced to problems of solving Horn clause constraints on predicate variables that represent unknown inductive invariants. This paper presents a novel Horn constraint solving method based on inductive theorem proving: the method reduces Horn constraint solving to validity checking of first-order formulas with inductively defined predicates, which are then checked by induction on the derivation of the predicates. To automate inductive proofs, we introduce a novel proof system tailored to Horn constraint solving, and use a PDR-based Horn constraint solver as well as an SMT solver to discharge proof obligations arising in the proof search. We prove that our proof system satisfies the soundness and relative completeness with respect to ordinary Horn constraint solving schemes. The two main advantages of the proposed method are that (1) it can deal with constraints over any background theories supported by the underlying SMT solver, including nonlinear arithmetic and algebraic data structures, and (2) the method can verify relational specifications across programs in various paradigms where multiple function calls need to be analyzed simultaneously. The class of specifications includes practically important ones such as functional equivalence, asso-ciativity, commutativity, distributivity, monotonicity, idempotency, and non-interference. Our novel combination of Horn clause constraints with inductive theorem proving enables us to naturally and automatically axiomatize recursive functions that are possibly non-terminating, nondeterministic, higher-order, exception-raising, and over non-inductively defined data types. We have implemented a relational verification tool for the OCaml functional language based on the proposed method and obtained promising results in preliminary experiments.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Unno, Hiroshi;Torii, Sho;Sakamoto, Hiroki",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026765560,10.1007/978-3-319-63390-9_31,A s<inf>torm</inf> is coming: A modern probabilistic model checker,"We launch the new probabilistic model checker Storm. It features the analysis of discrete- and continuous-time variants of both Markov chains and MDPs. It supports the Prism and JANI modeling languages, probabilistic programs, dynamic fault trees and generalized stochastic Petri nets. It has a modular set-up in which solvers and symbolic engines can easily be exchanged. It offers a Python API for rapid prototyping by encapsulating Storm’s fast and scalable algorithms. Experiments on a variety of benchmarks show its competitive performance.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Dehnert, Christian;Junges, Sebastian;Katoen, Joost Pieter;Volk, Matthias",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025435472,10.1145/3084226.3084276,Easier said than done: Diagnosing misconfiguration via configuration constraints analysis,"Misconfigurations have drawn tremendous attention for their increasing prevalence and severity, and the main causes are the complexity of configurations as well as the lack of domain knowledge for software. To diagnose misconfigurations, one typical approach is to find out the conditions that configuration options should satisfy, which we refer to as configuration constraints. Current researches only handled part of the situations of configuration constraints in source code, which provide only limited help for misconfiguration diagnosis. To better extract configuration constraints, we conduct a comprehensive manual study on the existence and variance of the configuration constraints in source code from five pieces of popular open-source software. We summarized several findings from different aspects, including the general statistics about configuration constraints, the general features for specific configurations, and the obstacles in extraction of configuration constraints. Based on the findings, we propose several suggestions to maximize the automation of constraints extraction.",Configuration constraints | Misconfiguration | Misconfiguration diagnosis,ACM International Conference Proceeding Series,2017-06-15,Conference Paper,"Zhou, Shulin;Li, Shanshan;Liu, Xiaodong;Xu, Xiangyang;Zheng, Si;Liao, Xiangke;Xiong, Yun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025438153,10.1145/3084226.3084286,An exploratory study of functionality and learning resources of web APIS on programmableweb,"Web APIs provide various functionalities that can be leveraged by developers in building their applications. ProgrammableWeb, which is the largest and most active web API and mashup collection, provides a record of thousands of web APIs and mashups. However, important properties about these large number of web APIs, such as their functionality and support/resources for learning, have never been studied by the existing research work. In this study, we perform an exploratory analysis on functionality and learning resources of 9,883 web APIs and 4,315 mashups listed on ProgrammableWeb, and find that: (1) web APIs provide a wide range of functionalities related to business solution, text analysis, data source, etc.; many of them are substitutable; only a minority have been used with other APIs; (2) a majority of web APIs on ProgrammableWeb have provided resources to support developers in learning how to use the APIs.",,ACM International Conference Proceeding Series,2017-06-15,Conference Paper,"Tian, Yuan;Kochhar, Pavneet Singh;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027727858,10.1109/ICGSE.2017.14,Preliminary evaluation of a tag-based knowledge condensation tool in agile and distributed teams,"An important challenge in Agile Global Software Development (AGSD) is architectural knowledge vaporization, i.e., the loss of technical knowledge due to a lack of documentation. In a previous work we identified that technical knowledge is usually available in unstructured textual electronic means (e.g. chat, mail, blogs, etc.-also known as UTEMs) used by AGSD workers during development, although not necessarily easily accessible. Also, we proposed the concept of Knowledge Condensation as a means to recover knowledge from UTEMs, by introducing a structure to their contents along with means to retrieve knowledge. In this work we present our first step towards a Knowledge Condensation tool, through a prototype tagging mechanism to structure architectural knowledge during UTEM interactions. We evaluated the prototype and obtained the following results: (1) a high perception of usefulness and a high projected intention of use, and (2) the identification of 6 tagging behavior profiles from participants. These tagging behaviors can be grouped as (i) participants who tag as expected, (ii) participants who tag more than expected, and (ii) participants who tag less than expected. These results suggest that it is important to cope with 'little or no tagging' and 'excessive tagging' behaviors to implement knowledge condensation based on UTEM tagging. Furthermore, we identified that tagging could be a good option to structure UTEM interactions, and to retrieve architectural knowledge from UTEM logs. Future work includes improving the tagging mechanism prototype based on the participants' suggestions and overall evaluation results, and implementing a searcher prototype to further evaluate the concept of knowledge condensation based on tagging of UTEM interactions.",Agile global software development | Architectural knowledge | Knowledge condensing | Knowledge vaporization | Unstructured and textual electronic media,"Proceedings - 2017 IEEE 12th International Conference on Global Software Engineering, ICGSE 2017",2017-07-12,Conference Paper,"Borrego, Gilberto;Moran, Alberto L.;Palacio, Ramon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027689021,10.1109/ICGSE.2017.11,"Developer turnover in global, industrial open source projects: Insights from applying survival analysis","Large open source software projects often have a globally distributed development team. Studies have shown developer turnover has a significant impact on the project success. Frequent developer turnover may lead to loss of productivity due to lacking relevant knowledge and spending extra time learning how projects work. Thus, lots of attention has been paid to which factors are related to developer retention, however, few of them focus on the impact of activities of individual developers. In this paper, we study five open source projects from different organizations and examine whether developer turnover is affected by when they start contributing and what types of contributions they are making. Our study reveals that developers have higher chances to survive in software projects when they 1) start contributing to the project earlier, 2) mainly modify instead of creating files, 3) mainly code instead of dealing with documentations. Our results also shed lights on the potential approaches to improving developer retention.",,"Proceedings - 2017 IEEE 12th International Conference on Global Software Engineering, ICGSE 2017",2017-07-12,Conference Paper,"Lin, Bin;Robles, Gregorio;Serebrenik, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3162625,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3176643,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85009895253,10.1109/TSE.2016.2576451,Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft,"Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.",Code review | commercial projects | open source | OSS | peer impressions | survey,IEEE Transactions on Software Engineering,2017-01-01,Article,"Bosu, Amiangshu;Carver, Jeffrey C.;Bird, Christian;Orbeck, Jonathan;Chockley, Christopher",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85013119887,10.1109/TSE.2016.2591536,ARENA: An Approach for the Automated Generation of Release Notes,"Release notes document corrections, enhancements, and, in general, changes that were implemented in a new release of a software project. They are usually created manually and may include hundreds of different items, such as descriptions of new features, bug fixes, structural changes, new or deprecated APIs, and changes to software licenses. Thus, producing them can be a time-consuming and daunting task. This paper describes ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. ARENA was designed based on the manual analysis of 990 existing release notes. In order to evaluate the quality of the release notes automatically generated by ARENA, we performed four empirical studies involving a total of 56 participants (48 professional developers and eight students). The obtained results indicate that the generated release notes are very good approximations of the ones manually produced by developers and often include important information that is missing in the manually created release notes.",Release notes | software documentation | software evolution,IEEE Transactions on Software Engineering,2017-02-01,Article,"Moreno, Laura;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;Marcus, Andrian;Canfora, Gerardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85013031980,10.1109/TSE.2016.2587646,Automated Synthesis and Dynamic Analysis of Tradeoff Spaces for Object-Relational Mapping,"Producing software systems that achieve acceptable tradeoffs among multiple non-functional properties remains a significant engineering problem. We propose an approach to solving this problem that combines synthesis of spaces of design alternatives from logical specifications and dynamic analysis of each point in the resulting spaces. We hypothesize that this approach has potential to help engineers understand important tradeoffs among dynamically measurable properties of system components at meaningful scales within reach of existing synthesis tools. To test this hypothesis, we developed tools to enable, and we conducted, a set of experiments in the domain of relational databases for object-oriented data models. For each of several data models, we used our approach to empirically test the accuracy of a published suite of metrics to predict tradeoffs based on the static schema structure alone. The results show that exhaustive synthesis and analysis provides a superior view of the tradeoff spaces for such designs. This work creates a path forward toward systems that achieve significantly better tradeoffs among important system properties.",dynamic analysis | ORM | relational logic | Specification-driven synthesis | static analysis | tradespace analysis,IEEE Transactions on Software Engineering,2017-02-01,Article,"Bagheri, Hamid;Tang, Chong;Sullivan, Kevin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85013078272,10.1109/TSE.2016.2589242,DECAF: A Platform-Neutral Whole-System Dynamic Binary Analysis Platform,"Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, a semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, and lacking APIs. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection and a plugin architecture with a simple-to-use event-driven programming interface. DECAF implements a new instruction-level taint tracking engine at bit granularity, which exercises fine control over the QEMU Tiny Code Generator (TCG) intermediate representation to accomplish on-the-fly optimizations while ensuring that the taint propagation is sound and highly precise. We perform a formal analysis of DECAF's taint propagation rules to verify that most instructions introduce neither false positives nor false negatives. We also present three platform-neutral plugins - Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9,550 lines of C++ code and 10,270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605 percent for system wide tainting and 12 percent for VMI.",Dynamic binary analysis | dynamic taint analysis | virtual machine introspection,IEEE Transactions on Software Engineering,2017-02-01,Article,"Henderson, Andrew;Yan, Lok Kwong;Hu, Xunchao;Prakash, Aravind;Yin, Heng;McCamant, Stephen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040309554,10.1109/TSE.2017.2653778,Language Inclusion Checking of Timed Automata with Non-Zenoness,"Given a timed automaton P modeling an implementation and a timed automaton S as a specification, the problem of language inclusion checking is to decide whether the language of P is a subset of that of S. It is known to be undecidable. The problem gets more complicated if non-Zenoness is taken into consideration. A run is Zeno if it permits infinitely many actions within finite time. Otherwise it is non-Zeno. Zeno runs might present in both P and S. It is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples of language inclusion checking. In this work, we propose a zone-based semi-Algorithm for language inclusion checking with non-Zenoness. It is further improved with simulation reduction based on LU-simulation. Though our approach is not guaranteed to terminate, we show that it does in many cases through empirical study. Our approach has been incorporated into the PAT model checker, and applied to multiple systems to show its usefulness.",language inclusion | non-Zenoness | Timed automata,IEEE Transactions on Software Engineering,2017-11-01,Article,"Wang, Xinyu;Sun, Jun;Wang, Ting;Qin, Shengchao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025122019,10.1109/TSE.2017.2654244,Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt,"The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-Admitted technical debt), and that the most common types of self-Admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-Admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-Admitted technical debt, significantly outperforming the current state-of-The-Art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-Admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-Admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.",empirical study | natural language processing | source code comments | Technical debt,IEEE Transactions on Software Engineering,2017-11-01,Article,"Maldonado, Everton Da Silva;Shihab, Emad;Tsantalis, Nikolaos",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1109/TSE.2017.2664836,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,,10.1109/TSE.2017.2654251,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/TSE.2017.2655056,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030247631,10.1145/3121113.3121125,Determining the usability of Bitcoin for beginners using ChangeTip and Coinbase,"The purpose of this study is to investigate to what degree usability and user experience are factors in the uptake and use of bitcoin. This paper investigates whether usability affects bitcoin adoption by beginners. To ascertain whether this is true, a pilot study was designed to gather rich qualitative data. Participants in this study were asked to provide commentary while completing an assigned task that was designed to emulate a common case that a person unfamiliar with bitcoin might encounter when dealing with the currency for the first time.",Bitcoin | Cryptocurrency | Digital wallets | Usability | User experience,SIGDOC 2017 - 35th ACM International Conference on the Design of Communication,2017-08-11,Conference Paper,"Kazerani, Ali;Rosati, Domenic;Lesser, Brian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030221800,10.1145/3121113.3121201,Users as makers: User experience design for and by the crowd,"This project explores some key implications for how overlapping user roles and user experiences are affected by the design of public, crowdsourced projects. The growing prevalence of crowdsourcing is creating new and complex contexts for communication, prompting a need to more fully understand how these arrangements can best be sustained for and by their participants and stakeholders. I draw on an ethnographic study of LibriVox.org-a volunteer audiobook publishing project-to argue that such large communities of makers necessitate a unique understanding of usability: one where, instead of flawless consumer experiences, a more important, long-term goal is to cultivate good, empowering user experiences for makers. Through surveying four central sites of user-generated LibriVox documentation, I learn how this decentralized maker community offers its volunteers multiple and flexible paths toward valuable instances of user-as-maker agency.",Crowdsourcing | Digital publishing | Ethnography | Makers | Online communities | User experience | Volunteer organizations,SIGDOC 2017 - 35th ACM International Conference on the Design of Communication,2017-08-11,Conference Paper,"Chesley, Amelia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030213250,10.1145/3121113.3121227,Content audit for the assessment of digital information space: Definitions and exploratory typology,"Content audit is an evaluation method used to identify, describe, quantify, and assess content quality of a website or of a larger information ecosystem (social media, web application, newsletter, intranet, etc.). Despite the growing popularity of the method in the last years, very little research has been conducted on this topic. However, it is extensively described and commented in a large body of literature, mostly written by information architecture (IA), content strategy and UX professionals. Hence, this is what led to further examine, in this research, a corpus of 200 publications (books, web pages, blog articles, journal articles) addressing content audit. This study attempts to take stock and establish a realistic picture of the current knowledge about the method, particularly concerning content audit definition components and content audit types, as well as to suggest possible means to further develop this evaluation method.",Content assessment | Content audit | Digital information space | Information architecture | Typology,SIGDOC 2017 - 35th ACM International Conference on the Design of Communication,2017-08-11,Conference Paper,"Sperano, I.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030230869,10.1145/3121113.3121229,Learn by doing...It wrong: Lessons from seven years of industry-led service learning,"The iFixit Technical Writing Project (iTWP) is an industry-led, service-learning project for university technical communication classes, developed to give students real-world experience in process documentation while helping to address the global ewaste crisis. Over the past seven years, the iTWP has positioned over 10, 000 student-authors as writers, engagers, and advocates in a community of 90 million users. This experience report will discuss our lessons-learned in crafting an impactful industry partnership or service learning project including the use of data, balancing stakeholder needs, and communication/feedback structures focusing around implementing UX and technical communication practices into iterative program design.",,SIGDOC 2017 - 35th ACM International Conference on the Design of Communication,2017-08-11,Conference Paper,"McCrigler, B.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85030223196,10.1145/3121113.3121241,Leveraging industry onboarding materials in the curriculum,"Delivers1 a resource-sharing project between undergraduate technical writing programs and industry technical documentation groups that provides: 1) Crisp descriptions of the top five content development skills, 2) Industry college-hire and new-employee training materials (conceptuals), 3) Real-world, sample technical content in substantial breadth and depth, 4) Multiple content markup formats (Word, Markdown, XML DITA), 5) Detailed writing exercises based on the sample content and assessment guidelines, and 6) Community-sourced content development, maintenance, and curation (GitHub).",DITA | Experiential learning | GitHub | Markdown | New hires | Onboarding | Technical communication,SIGDOC 2017 - 35th ACM International Conference on the Design of Communication,2017-08-11,Conference Paper,"Doherty, Stanley",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84973896642,10.1145/2906151,Understanding graph-based trust evaluation in online social networks: Methodologies and challenges,"Online Social Networks (OSNs) are becoming a popular method of meeting people and keeping in touch with friends. OSNs resort to trust evaluation models and algorithms to improve service quality and enhance user experiences. Much research has been done to evaluate trust and predict the trustworthiness of a target, usually from the view of a source. Graph-based approaches make up a major portion of the existing works, in which the trust value is calculated through a trusted graph (or trusted network, web of trust, or multiple trust chains). In this article, we focus on graph-based trust evaluation models in OSNs, particularly in the computer science literature. We first summarize the features of OSNs and the properties of trust. Then we comparatively review two categories of graph-simplification-based and graph-analogy-based approaches and discuss their individual problems and challenges. We also analyze the common challenges of all graphbased models. To provide an integrated view of trust evaluation, we conduct a brief review of its preand postprocesses (i.e., the preparation and validation of trust models, including information collection, performance evaluation, and related applications). Finally, we identify some open challenges that all trust models are facing.",A.1 [introductory and survey] | Analogy | C.2.4 [computer-communication networks]: distributed systems | Design | Management | Online social networks (OSNs) | Reliability | Simplification | Trust evaluation | Trust models | Trusted graph,ACM Computing Surveys,2017-03-31,Review,"Jiang, Wenjun;Wang, Guojun;Bhuiyan, Md Zakirul Alam;Wu, Jie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84975077478,10.1145/2932707,Computational health informatics in the big data age: A survey,"The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.",,ACM Computing Surveys,2017-03-31,Review,"Fang, Ruogu;Pouyanfar, Samira;Yang, Yimin;Chen, Shu Ching;Iyengar, S. S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84975078911,10.1145/2906153,Ensuring security and privacy preservation for cloud data services,"With the rapid development of cloud computing, more and more enterprises/individuals are starting to outsource local data to the cloud servers. However, under open networks and not fully trusted cloud environments, they face enormous security and privacy risks (e.g., data leakage or disclosure, data corruption or loss, and user privacy breach) when outsourcing their data to a public cloud or using their outsourced data. Recently, several studies were conducted to address these risks, and a series of solutions were proposed to enable data and privacy protection in untrusted cloud environments. To fully understand the advances and discover the research trends of this area, this survey summarizes and analyzes the state-of-the-art protection technologies. We first present security threats and requirements of an outsourcing data service to a cloud, and follow that with a high-level overview of the corresponding security technologies. We then dwell on existing protection solutions to achieve secure, dependable, and privacy-assured cloud data services including data search, data computation, data sharing, data storage, and data access. Finally, we propose open challenges and potential research directions in each category of solutions.",,ACM Computing Surveys,2017-03-31,Article,"Tang, Jun;Cui, Yong;Li, Qi;Ren, Kui;Liu, Jiangchuan;Buyya, Rajkumar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978086505,10.1145/2926966,Survey on software design-pattern specification languages,"A design pattern is a well-defined solution to a recurrent problem. Over the years, the number of patterns and domains of design patterns have expanded, as the patterns are the experiences of the experts of the domain captured in a higher-level abstraction. This led others to work on languages for design patterns to systematically document abstraction detailed in the design pattern rather than capture algorithms and data. These design-pattern specification languages come in different flavors, targeting different aspects of design patterns. Some design-pattern specification languages tried to capture the description of the design pattern in graphical or textual format, others tried to discover design patterns in code or design diagrams, and still other design-pattern specification languages have other objectives. However, so far, no effort has been made to compare these design-pattern specification languages and identify their strengths and weaknesses. This article provides a survey and a comparison between existing design-pattern specification languages using a design-pattern specification language evaluation framework. Analysis is done by grouping the design-pattern specification languages into different categories. In addition, a brief description is provided regarding the tools available for the design-pattern specification languages. Finally, we identify some open research issues that still need to be resolved.",Design pattern specification languages | Domain specific languages,ACM Computing Surveys,2017-03-31,Article,"Khwaja, Salman;Alshayeb, Mohammad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978239660,10.1145/2938369,A survey on thread-level speculation techniques,"Thread-Level Speculation (TLS) is a promising technique that allows the parallel execution of sequential code without relying on a prior, compile-time-dependence analysis. In this work, we introduce the technique, present a taxonomy of TLS solutions, and summarize and put into perspective the most relevant advances in this field.",Runtime parallelization,ACM Computing Surveys,2017-06-30,Review,"Estebanez, Alvaro;Llanos, Diego R.;Gonzalez-Escribano, Arturo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978280222,10.1145/2933241,Biometric recognition in automated border control: A survey,"The increasing demand for traveler clearance at international border crossing points (BCPs) has motivated research for finding more efficient solutions. Automated border control (ABC) is emerging as a solution to enhance the convenience of travelers, the throughput of BCPs, and national security. This is the first comprehensive survey on the biometric techniques and systems that enable automatic identity verification in ABC.We survey the biometric literature relevant to identity verification and summarize the best practices and biometric techniques applicable to ABC, relying on real experience collected in the field. Furthermore, we select some of the major biometric issues raised and highlight the open research areas.",Authentication | Automated border control | Biometric recognition | e-Gate | Security,ACM Computing Surveys,2017-06-30,Review,"Labati, Ruggero Donida;Genovese, Angelo;Muñoz, Enrique;Piuri, Vincenzo;Scotti, Fabio;Sforza, Gianluca",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84978221329,10.1145/2931098,Heap abstractions for static analysis,"Heap data is potentially unbounded and seemingly arbitrary. Hence, unlike stack and static data, heap data cannot be abstracted in terms of a fixed set of program variables. This makes it an interesting topic of study and there is an abundance of literature employing heap abstractions. Although most studies have addressed similar concerns, insights gained in one description of heap abstraction may not directly carry over to some other description. In our search of a unified theme, we view heap abstraction as consisting of two steps: (a) heap modelling, which is the process of representing a heap memory (i.e., an unbounded set of concrete locations) as a heap model (i.e., an unbounded set of abstract locations), and (b) summarization, which is the process of bounding the heap model by merging multiple abstract locations into summary locations. We classify the heap models as storeless, store based, and hybrid. We describe various summarization techniques based on k-limiting, allocation sites, patterns, variables, other generic instrumentation predicates, and higher-order logics. This approach allows us to compare the insights of a large number of seemingly dissimilar heap abstractions and also paves the way for creating new abstractions bymix andmatch of models and summarization techniques.",Algorithms | Design | Languages | Verification,ACM Computing Surveys,2017-06-30,Article,"Kanvar, Vini;Khedker, Uday P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84983748443,10.1145/2907070,A survey of predictive modeling on imbalanced domains,"Many real-world data-mining applications involve obtaining predictive models using datasets with strongly imbalanced distributions of the target variable. Frequently, the least-common values of this target variable are associated with events that are highly relevant for end users (e.g., fraud detection, unusual returns on stock markets, anticipation of catastrophes, etc.). Moreover, the events may have different costs and benefits, which, when associated with the rarity of some of them on the available training data, creates serious problems to predictive modeling techniques. This article presents a survey of existing techniques for handling these important applications of predictive analytics. Although most of the existing work addresses classification tasks (nominal target variables), we also describe methods designed to handle similar problems within regression tasks (numeric target variables). In this survey, we discuss the main challenges raised by imbalanced domains, propose a definition of the problem, describe the main approaches to these tasks, propose a taxonomy of the methods, summarize the conclusions of existing comparative studies as well as some theoretical analyses of some methods, and refer to some related problems within predictive modeling.",Classification | Imbalanced domains | Performance metrics | Rare cases | Regression,ACM Computing Surveys,2017-06-30,Review,"Branco, Paula;Torgo, Luís;Ribeiro, Rita P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84997755028,10.1145/2938727,"Knuckle print biometrics and fusion schemes - Overview, challenges, and solutions","Numerous behavioral or physiological biometric traits, including iris, signature, hand geometry, speech, palm print, face, etc. have been used to discriminate individuals in a number of security applications over the last 30 years. Among these, hand-based biometric systems have come to the attention of researchers worldwide who utilize them for low- to medium-security applications such as financial transactions, access control, law enforcement, border control, computer security, time and attendance systems, dormitory meal plan access, etc. Several approaches for biometric recognition have been summarized in the literature. The survey in this article focuses on the interface between various hand modalities, summary of inner- and dorsal-knuckle print recognition, and fusion techniques. First, an overview of various feature extraction and classification approaches for knuckle print, a new entrant in the hand biometrics family with a higher user acceptance and invariance to emotions, is presented. Next, knuckle print fusion schemes with possible integration scenarios, and traditional capturing devices have been discussed. The economic relevance of various biometric traits, including knuckle print for commercial and forensic applications is debated. Finally, conclusions related to the scope of knuckle print as a biometric trait are drawn and some recommendations for the development of hand-based multimodal biometrics have been presented.",A.2 [general and reference]: surveysand overviews | Algorithms | CRR (correct recognition rate) | ERR (equal error rate) | FAR (false acceptance rate) | FKP (finger knuckle print) | Fusion | Hand biometrics | I.3.1 [security and privacy]: authentication | IKP (inner knuckle print) | K.3.8 [computing methodologies]: object recognition | MCP (metacarpophalangeal joint) | Performance | ROC (reciever operating characteristic) | ROI (region of interest) | Security | Verification/identification,ACM Computing Surveys,2017-06-30,Review,"Jaswal, Gaurav;Kaul, Amit;Nath, Ravinder",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994220073,10.1145/2980052,Gaussian random number generation: A survey on hardware architectures,"Some excellent surveys of the Gaussian random number generators (GRNGs) from the algorithmic perspective exist in the published literature to date (e.g., Thomas et al. [2007]). In the last decade, however, advancements in digital hardware have resulted in an ever-decreasing hardware cost and increased design flexibility. Additionally, recent advances in applications like gaming, weather forecasting, and simulations in physics and astronomy require faster, cheaper, and statistically accurate GRNGs. These two trends have contributed toward the development of a number of novel GRNG architectures optimized for hardware design. A detailed comparative study of these hardware architectures has been somewhat missing in the published literature. This work provides the potential user a capsulization of the published hardware GRNG architectures. We have provided the method and theory, pros and cons, and a comparative summary of the speed, statistical accuracy, and hardware resource utilization of these architectures. Finally, we have complemented this work by describing two novel hardware GRNG architectures, namely, the CLT-inversion and the multihat algorithm, respectively. These new architectures provide high tail accuracy (6σ and 8σ, respectively) at a low hardware cost.",Algorithms | AWGN | Gaussian | Hardware accelerators | Normal | Random number generator,ACM Computing Surveys,2017-09-30,Article,"Malik, Jamshaid Sarwar;Hemani, Ahmed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85005979595,10.1145/2856821,"Inner source definition, benefits, and challenges","Inner Source (IS) is the use of open source software development practices and the establishment of an open source-like culture within organizations. The organization may still develop proprietary software but internally opens up its development. A steady stream of scientific literature and practitioner reports indicates the interest in this research area. However, the research area lacks a systematic assessment of known research work: No model exists that defines IS thoroughly. Various case studies provide insights into IS programs in the context of specific organizations but only few publications apply a broader perspective. To resolve this, we performed an extensive literature survey and analyzed 43 IS related publications plus additional background literature. Using qualitative data analysis methods, we developed a model of the elements that constitute IS. We present a classification framework for IS programs and projects and apply it to lay out a map of known IS endeavors. Further, we present qualitative models summarizing the benefits and challenges of IS adoption. The survey provides the first broad review of IS literature and systematic arrangement of IS research results.",Inner source | Internal open source | Open collaboration | Software development efficiency | Software development methods | Software development productivity | Software engineering | Taxonomy,ACM Computing Surveys,2017-12-31,Article,"Capraro, Maximilian;Riehle, Dirk",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84975113024,10.1007/s10664-016-9437-5,Robust Statistical Methods for Empirical Software Engineering,"There have been many changes in statistical theory in the past 30 years, including increased evidence that non-robust methods may fail to detect important results. The statistical advice available to software engineering researchers needs to be updated to address these issues. This paper aims both to explain the new results in the area of robust analysis methods and to provide a large-scale worked example of the new methods. We summarise the results of analyses of the Type 1 error efficiency and power of standard parametric and non-parametric statistical tests when applied to non-normal data sets. We identify parametric and non-parametric methods that are robust to non-normality. We present an analysis of a large-scale software engineering experiment to illustrate their use. We illustrate the use of kernel density plots, and parametric and non-parametric methods using four different software engineering data sets. We explain why the methods are necessary and the rationale for selecting a specific analysis. We suggest using kernel density plots rather than box plots to visualise data distributions. For parametric analysis, we recommend trimmed means, which can support reliable tests of the differences between the central location of two or more samples. When the distribution of the data differs among groups, or we have ordinal scale data, we recommend non-parametric methods such as Cliff’s δ or a robust rank-based ANOVA-like method.",Empirical software engineering | Robust methods | Robust statistical methods | Statistical methods,Empirical Software Engineering,2017-04-01,Article,"Kitchenham, Barbara;Madeyski, Lech;Budgen, David;Keung, Jacky;Brereton, Pearl;Charters, Stuart;Gibbs, Shirley;Pohthong, Amnart",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84954357117,10.1007/s10664-015-9422-4,Generating valid grammar-based test inputs by means of genetic programming and annotated grammars,"Automated generation of system level tests for grammar based systems requires the generation of complex and highly structured inputs, which must typically satisfy some formal grammar. In our previous work, we showed that genetic programming combined with probabilities learned from corpora gives significantly better results over the baseline (random) strategy. In this work, we extend our previous work by introducing grammar annotations as an alternative to learned probabilities, to be used when finding and preparing the corpus required for learning is not affordable. Experimental results carried out on six grammar based systems of varying levels of complexity show that grammar annotations produce a higher number of valid sentences and achieve similar levels of coverage and fault detection as learned probabilities.",Genetic programming | Grammar annotations | Grammar based testing,Empirical Software Engineering,2017-04-01,Article,"Kifetew, Fitsum Meshesha;Tiella, Roberto;Tonella, Paolo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994779547,10.1007/s10664-016-9476-y,Automated training-set creation for software architecture traceability problem,"Automated trace retrieval methods based on machine-learning algorithms can significantly reduce the cost and effort needed to create and maintain traceability links between requirements, architecture and source code. However, there is always an upfront cost to train such algorithms to detect relevant architectural information for each quality attribute in the code. In practice, training supervised or semi-supervised algorithms requires the expert to collect several files of architectural tactics that implement a quality requirement and train a learning method. Establishing such a training set can take weeks to months to complete. Furthermore, the effectiveness of this approach is largely dependent upon the knowledge of the expert. In this paper, we present three baseline approaches for the creation of training data. These approaches are (i) Manual Expert-Based, (ii) Automated Web-Mining, which generates training sets by automatically mining tactic’s APIs from technical programming websites, and lastly (iii) Automated Big-Data Analysis, which mines ultra-large scale code repositories to generate training sets. We compare the trace-link creation accuracy achieved using each of these three baseline approaches and discuss the costs and benefits associated with them. Additionally, in a separate study, we investigate the impact of training set size on the accuracy of recovering trace links. The results indicate that automated techniques can create a reliable training set for the problem of tracing architectural tactics.",Architecturally significant requirements | Architecture traceability | Automation | Dataset generation,Empirical Software Engineering,2017-06-01,Article,"Zogaan, Waleed;Mujhid, Ibrahim;Joanna, Joanna C.;Gonzalez, Danielle;Mirakhorli, Mehdi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84996841758,10.1007/s10664-016-9479-8,Tackling the term-mismatch problem in automated trace retrieval,,Query augmentation | Requirements engineering | Semantic traceability | Traceability,Empirical Software Engineering,2017-06-01,Article,"Guo, Jin;Gibiec, Marek;Cleland-Huang, Jane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85001114526,10.1007/s10664-016-9487-8,Analysis of license inconsistency in large collections of open source projects,"Free and open source software (FOSS) plays an important role in source code reuse practice. They usually come with one or more software licenses written in the header part of source files, stating the requirements and conditions which should be followed when been reused. Removing or modifying the license statement by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. In this paper, we describe and categorize different types of license inconsistencies and propose a method to detect them. Then we applied this method to Debian 7.5 and a collection of 10,514 Java projects on GitHub and present the license inconsistency cases found in these systems. With a manual analysis, we summarized various reasons behind these license inconsistency cases, some of which imply potential license infringement and require attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.",Code clone | License inconsistency | Software license,Empirical Software Engineering,2017-06-01,Article,"Wu, Yuhao;Manabe, Yuki;Kanda, Tetsuya;German, Daniel M.;Inoue, Katsuro",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84982947678,10.1007/s10664-016-9444-6,fine-GRAPE: fine-grained APi usage extractor – an approach and dataset to investigate API usage,"An Application Programming Interface (API) provides a set of functionalities to a developer with the aim of enabling reuse. APIs have been investigated from different angles such as popularity usage and evolution to get a better understanding of their various characteristics. For such studies, software repositories are mined for API usage examples. However, many of the mining algorithms used for such purposes do not take type information into account. Thus making the results unreliable. In this paper, we aim to rectify this by introducing fine-GRAPE, an approach that produces fine-grained API usage information by taking advantage of type information while mining API method invocations and annotation. By means of fine-GRAPE, we investigate API usages from Java projects hosted on GitHub. We select five of the most popular APIs across GitHub Java projects and collect historical API usage information by mining both the release history of these APIs and the code history of every project that uses them. We perform two case studies on the resulting dataset. The first measures the lag time of each client. The second investigates the percentage of used API features. In the first case we find that for APIs that release more frequently clients are far less likely to upgrade to a more recent version of the API as opposed to clients of APIs that release infrequently. The second case study shows us that for most APIs there is a small number of features that is actually used and most of these features relate to those that have been introduced early in the APIs lifecycle.",API popularity | API usage | Application programming interface | Dataset,Empirical Software Engineering,2017-06-01,Article,"Sawant, Anand Ashok;Bacchelli, Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019738483,10.1007/s10664-017-9525-1,Guest editorial: Program comprehension,,,Empirical Software Engineering,2017-06-01,Editorial,"Oliveto, Rocco;Bird, Christian",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85016546260,10.1007/s10664-017-9501-9,Documenting and sharing software knowledge using screencasts,"Screencasts are used to capture a developer’s screen while they narrate how a piece of software works or how the software can be extended. They have recently become a popular alternative to traditional text-based documentation. This paper describes our investigation into how developers produce and share developer-focused screencasts. In this study, we identified and analyzed a set of development screencasts from YouTube to explore what kinds of software knowledge are shared in video walkthroughs of code and what techniques are used for sharing software knowledge. We also interviewed YouTube screencast producers to understand their motivations for creating screencasts as well as to discover the challenges they face while producing code-focused videos. Finally, we compared YouTube screencasts to videos hosted on the professional RailsCasts website to better understand the differences and practices of this more curated ecosystem with the YouTube platform. Our three-phase study showed that video is a useful medium for communicating program knowledge between developers and that developers build their online persona and reputation by sharing videos through social channels. These findings led to a number of best practices for future screencast creators.",Screencasting | Social media | Software engineering,Empirical Software Engineering,2017-06-01,Article,"MacLeod, Laura;Bergen, Andreas;Storey, Margaret Anne",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994709940,10.1007/s10664-016-9469-x,Are delayed issues harder to resolve? Revisiting cost-to-fix of defects throughout the lifecycle,"Many practitioners and academics believe in a delayed issue effect (DIE); i.e. the longer an issue lingers in the system, the more effort it requires to resolve. This belief is often used to justify major investments in new development processes that promise to retire more issues sooner. This paper tests for the delayed issue effect in 171 software projects conducted around the world in the period from 2006–2014. To the best of our knowledge, this is the largest study yet published on this effect. We found no evidence for the delayed issue effect; i.e. the effort to resolve issues in a later phase was not consistently or substantially greater than when issues were resolved soon after their introduction. This paper documents the above study and explores reasons for this mismatch between this common rule of thumb and empirical data. In summary, DIE is not some constant across all projects. Rather, DIE might be an historical relic that occurs intermittently only in certain kinds of projects. This is a significant result since it predicts that new development processes that promise to faster retire more issues will not have a guaranteed return on investment (depending on the context where applied), and that a long-held truth in software engineering should not be considered a global truism.",Cost to fix | Phase delay | Software economics,Empirical Software Engineering,2017-08-01,Article,"Menzies, Tim;Nichols, William;Shull, Forrest;Layman, Lucas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84996550774,10.1007/s10664-016-9473-1,Semantic topic models for source code analysis,"Topic modeling techniques have been recently applied to analyze and model source code. Such techniques exploit the textual content of source code to provide automated support for several basic software engineering activities. Despite these advances, applications of topic modeling in software engineering are frequently suboptimal. This can be attributed to the fact that current state-of-the-art topic modeling techniques tend to be data intensive. However, the textual content of source code, embedded in its identifiers, comments, and string literals, tends to be sparse in nature. This prevents classical topic modeling techniques, typically used to model natural language texts, to generate proper models when applied to source code. Furthermore, the operational complexity and multi-parameter calibration often associated with conventional topic modeling techniques raise important concerns about their feasibility as data analysis models in software engineering. Motivated by these observations, in this paper we propose a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. Ten software systems from different application domains are used to empirically calibrate and configure the proposed approach. The usefulness of generated topics is empirically validated using human judgment. Furthermore, a case study that demonstrates thet operation of the proposed approach in analyzing code evolution is reported. The results show that our approach produces stable, more interpretable, and more expressive topics than classical topic modeling techniques without the necessity for extensive parameter calibration.",Clustering | Information theory | Topic modeling,Empirical Software Engineering,2017-08-01,Article,"Mahmoud, Anas;Bradshaw, Gary",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-84996560163,10.1007/s10664-016-9483-z,Effectiveness and efficiency of a domain-specific language for high-performance marine ecosystem simulation: a controlled experiment,"It is a long-standing hypothesis that the concise and customized notation of a DSL improves the performance of developers when compared with a GPL. For non-technical domains—e.g., science—, this hypothesis lacks empirical evidence. Given this lack of empirical evidence, we evaluate a DSL for ecological modeling designed and implemented by us with regard to performance improvements of developers as compared to a GPL. We conduct an online survey with embedded controlled experiments among ecologists to assess the correctness and time spent of the participants when using a DSL for ecosystem simulation specifications compared with a GPL-based solution. We observe that (1) solving tasks with the DSL, the participants’ correctness point score was —depending on the task— on average 61 % up to 63 % higher than with the GPL-based solution and their average time spent per task was reduced by 31 % up to 56 %; (2) the participants subjectively find it easier to work with the DSL, and (3) more than 90 % of the subjects are able to carry out basic maintenance tasks concerning the infrastructure of the DSL used in our treatment, which is based on another internal DSL embedded into Java. The tasks of our experiments are simplified and our web-based editor components do not offer full IDE-support. Our findings indicate that the development of further DSL for the specific needs of the ecological modeling community should be a worthwhile investment to increase its members’ productivity and to enhance the reliability of their scientific results.",Computational science | Domain-specific languages (DSLs) | Program comprehension | Scientific software development,Empirical Software Engineering,2017-08-01,Article,"Johanson, Arne N.;Hasselbring, Wilhelm",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84992420701,10.1016/j.jss.2016.09.044,An assessment of extended finite state machine test selection criteria,"Extended finite state machines (EFSMs) provide a rigorous model for the derivation of functional tests for software systems and protocols. Various types of data-flow, control-flow, graph-based, and state machine based test selection criteria can be used for deriving tests from a given EFSM specification. Also, traditional types of state machine based notions of faults, such as transfer and output parameter faults, and common types of assignment faults can be used to describe the fault domains of EFSMs. We present an assessment of the most known types of EFSM test selection criteria such as test suites that cover single transfer faults, double transfer faults, single output parameter faults, and many types of single assignment faults of a given EFSM specification. Also, test suites that cover edge-pair, prime path, prime path with side trip, and all-uses criterion are derived from the graph and flow-graph representations of the specification. We also consider transition tour and random test suites. The assessment ranks the considered test suites in terms of their length and their coverage of single transfer, double transfer, and different type of single assignment faults. Dispersion of the obtained results is assessed and results are summarized.",Extended finite state machines | Fault coverage assessment | Model based testing | Mutation testing | Test derivation,Journal of Systems and Software,2017-01-01,Article,"El-Fakih, Khaled;Simao, Adenilso;Jadoon, Noshad;Maldonado, Jose Carlos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85022032339,10.1016/j.jss.2017.06.095,"Evolution of the R software ecosystem: Metrics, relationships, and their impact on qualities","Software ecosystems are an important new concept for collaborative software development, and empirical studies on their development are important towards understanding the underlying dynamics and modelling their behaviour. We conducted an explorative analysis of the R ecosystem as an exemplar on high-level, ecosystem-wide assessment. Based principally on the documentation metadata of the R packages, we generated a variety of metrics that allow the quantification of the R ecosystem. We also categorized the ecosystem participants, both in the software marketplace and in the developer community, by characteristics that measure their activity and impact. By viewing our metrics across the ecosystem's lifecycle for the various participant categories, we discovered interrelationships between them and determined the contribution of each category to the ecosystem as a whole.",Empirical study | Evolution | Quantitative analysis | R | Software ecosystems,Journal of Systems and Software,2017-10-01,Article,"Plakidas, Konstantinos;Schall, Daniel;Zdun, Uwe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85009477583,10.1016/j.jss.2017.01.002,User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps,"User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customer's needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session.",Agile development | Test automation | User acceptance testing,Journal of Systems and Software,2017-11-01,Article,"Otaduy, I.;Diaz, O.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85029575478,10.1016/j.jss.2017.09.007,An industrial case study on an architectural assumption documentation framework,"As an important type of architectural knowledge, documenting architectural assumptions (AAs) is critical to the success of projects. In this work, we proposed and validated an Architectural Assumption Documentation Framework (AADF), which is composed of four viewpoints (i.e., the Detail, Relationship, Tracing, and Evolution viewpoint), to document AAs in projects. One case study with two cases was conducted at two companies from different domains and countries. The main findings are: (1) AADF can be understood by architects in a short time (i.e., a half day workshop); (2) the AA Evolution view requires the least time to create, followed by the AA Detail view and the AA Relationship view; (3) AADF can help stakeholders to identify risks and understand AAs documented by other stakeholders; and (4) understanding and applying AADF is related to various factors, including factors regarding the framework per se (e.g., tutorial, examples, concepts, and terms), personal experience, resources (e.g., time), tool support, and project context (e.g., project size and number of AAs). Adjusting these factors in an appropriate way can facilitate the usage of AADF and further benefit the projects.",Architectural assumption | Case study | Documentation framework | Software architecture,Journal of Systems and Software,2017-12-01,Article,"Yang, Chen;Liang, Peng;Avgeriou, Paris;Eliasson, Ulf;Heldal, Rogardt;Pelliccione, Patrizio;Bi, Tingting",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994005147,10.1016/j.jss.2016.07.006,"An improved genetic algorithm for task scheduling in the cloud environments using the priority queues: Formal verification, simulation, and statistical testing","Cloud computing is a new platform to manage and provide services on the internet. Lately, researchers have paid attention a lot to this new subject. One of the reasons to have high performance in a cloud environment is the task scheduling. Since the task scheduling is an NP-Complete problem, in many cases, meta-heuristics scheduling algorithms are used. In this paper to optimize the task scheduling solutions, a powerful and improved genetic algorithm is proposed. The proposed algorithm uses the advantages of evolutionary genetic algorithm along with heuristic approaches. For analyzing the correctness of the proposed algorithm, we have presented a behavioral modeling approach based on model checking techniques. Then, the expected specifications of the proposed algorithm is extracted in the form of Linear Temporal Logic (LTL) formulas. To achieve the best performance in verification of the proposed algorithm, we use the Labeled Transition System (LTS) method. Also, the proposed behavioral models are verified using NuSMV and PAT model checkers. Then, the correctness of the proposed algorithm is analyzed according to the verification results in terms of some expected specifications, reachability, fairness, and deadlock-free. The simulation and statistical results revealed that the proposed algorithm outperformed the makespans of the three well-known heuristic algorithms and also the execution time of our recently meta-heuristics algorithm.",Cloud computing | Directed acyclic graph | Formal verification | Genetic algorithm | Model checking | Task scheduling,Journal of Systems and Software,2017-02-01,Article,"Keshanchi, Bahman;Souri, Alireza;Navimipour, Nima Jafari",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84996593342,10.1016/j.jss.2016.11.018,Automated extraction of product comparison matrices from informal product descriptions,"Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM – despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users.",Feature mining | Product comparison matrices | Reverse engineering | Software product lines | Variability mining,Journal of Systems and Software,2017-02-01,Article,"Nasr, Sana Ben;Bécan, Guillaume;Acher, Mathieu;Ferreira Filho, João Bosco;Sannier, Nicolas;Baudry, Benoit;Davril, Jean Marc",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84955621963,10.1016/j.jss.2015.12.031,A cybernetics Social Cloud,"This paper proposes a Social Cloud, which presents the system design, development and analysis. The technology is based on the BOINC open source software, our hybrid Cloud, Facebook Graph API and our development in a new Facebook API, SocialMedia. The creation of SocialMedia API with its four functions can ensure a smooth delivery of Big Data processing in the Social Cloud, with four selected examples provided. The proposed solution is focused on processing the contacts who click like or comment on the author's posts. Outputs result in visualization with their core syntax being demonstrated. Four functions in the SocialMedia API have evaluation test and each client-server API processing can be completed efficiently and effectively within 1.36 s. We demonstrate large scale simulations involved with 50,000 simulations and all the execution time can be completed within 70,000 s. Cybernetics functions are created to ensure that 100% job completion rate for Big Data processing. Results support our case for Big Data processing on Social Cloud with no costs involved. All the steps involved have closely followed system design, implementation, experiments and validation for Cybernetics to ensure a high quality of outputs and services at all times. This offers a unique contribution for Cybernetics to meet Big Data research challenges.",Big Data cybernetics | Data visualization | SocialMedia API,Journal of Systems and Software,2017-02-01,Article,"Chang, Victor",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85000399255,10.1016/j.jss.2016.11.031,Integration between requirements engineering and safety analysis: A systematic literature review,"Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying SCS by RE, we investigate the approaches proposed to improve the communication or integration between RE and safety engineering in SCS development. We analyze the activities that should be performed by RE during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help RE during specification of SCS that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to RE in the specification of SCS that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners.",Communication | Integration | Requirements engineering | Safety analysis | Safety-critical systems | Systematic literature review,Journal of Systems and Software,2017-03-01,Article,"Vilela, Jéssyka;Castro, Jaelson;Martins, Luiz Eduardo G.;Gorschek, Tony",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84955312939,10.1016/j.jss.2015.12.050,Critical-blame analysis for OpenMP 4.0 offloading on Intel Xeon Phi,"Recent supercomputers rated in the TOP 500 list increasingly utilize accelerator or co-processor devices to improve performance and energy efficiency. Since version 4.0 of the specification OpenMP addresses this heterogeneity in computing with the target directives, which enable programmers to offload portions of the code to massively-parallel target devices. Due to this new complexity in hardware and software design, performance optimization of large-scale parallel programs becomes more and more challenging. As manual performance analysis is getting infeasible for complex high performance computing (HPC) codes, we propose an approach to automatically detect bottlenecks such as load imbalances in heterogeneous OpenMP applications. We developed a method to perform critical-path and root-cause analysis for the OpenMP 4.0 offloading model and integrated it into the tool CASITA. The post-mortem analysis is based on execution traces that are generated with an implementation of the evolving OpenMP Tools Interface into the measurement system Score-P. To validate the implementation of our method we ported several existing codes to OpenMP 4.0 , executed them with an Intel Xeon Phi as the target device and analyzed the resulting trace files.",Offloading | OpenMP | Performance analysis,Journal of Systems and Software,2017-03-01,Article,"Dietrich, Robert;Schmitt, Felix;Grund, Alexander;Stolle, Jonas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85011411870,10.1016/j.jss.2016.08.069,A parallelization approach for resource-restricted embedded heterogeneous MPSoCs inspired by OpenMP,"Future low-end embedded systems will make an increased use of heterogeneous MPSoCs. To utilize these systems efficiently, methods and tools are required that support the extraction and implementation of parallelism typically found in embedded applications. Ideally, large amounts of existing legacy code should be reused and ported to these new systems. Existing parallelization infrastructures, however, mostly support parallelization according to the requirements of HPEC systems. For resource-restricted embedded systems, different parallelization strategies are necessary to achieve additional non-functional objectives such as the reduction of energy consumption. HPC-focused parallelization also assumes processor, memory and communication structures different from low-end embedded systems and therefore wastes optimization opportunities essential for improving the performance of resource-constrained embedded systems. This paper describes a new approach and infrastructure inspired by the OpenMP API to support the extraction and implementation of pipeline parallelism, which is commonly found in complex embedded applications. In addition, advanced techniques to extract parallelism from legacy applications requiring only minimal code modifications are presented. Further, the resulting toolflow combines advanced parallelization, mapping and communication optimization tools leading to a more efficient approach to exploit parallelism for typical embedded applications on heterogeneous MPSoCs running distributed real-time operating systems.",Embedded systems | Heterogeneous multiprocessor system-on-chip | Parallelization,Journal of Systems and Software,2017-03-01,Article,"Neugebauer, Olaf;Engel, Michael;Marwedel, Peter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85009186306,10.1016/j.jss.2017.01.001,Understanding cloud-native applications after 10 years of cloud computing - A systematic mapping study,"It is common sense that cloud-native applications (CNA) are intentionally designed for the cloud. Although this understanding can be broadly used it does not guide and explain what a cloud-native application exactly is. The term “cloud-native” was used quite frequently in birthday times of cloud computing (2006) which seems somehow obvious nowadays. But the term disappeared almost completely. Suddenly and in the last years the term is used again more and more frequently and shows increasing momentum. This paper summarizes the outcomes of a systematic mapping study analyzing research papers covering “cloud-native” topics, research questions and engineering methodologies. We summarize research focuses and trends dealing with cloud-native application engineering approaches. Furthermore, we provide a definition for the term “cloud-native application” which takes all findings, insights of analyzed publications and already existing and well-defined terminology into account.",Cloud-native application | CNA | Elastic platform | Microservice | Pattern | Self service | Softwareization | Systematic mapping study,Journal of Systems and Software,2017-04-01,Article,"Kratzke, Nane;Quint, Peter Christian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85009268752,10.1016/j.jss.2016.12.036,Design annotations to improve API discoverability,"User studies have revealed that programmers face several obstacles when learning application programming interfaces (APIs). A considerable part of such difficulties relate to discovery of API elements and the relationships among them. To address discoverability problems, we show how to complement APIs with design annotations, which document design decisions in a program-processable form for types, methods, and parameters. The information provided by the annotations is consumed by the integrated development environment (IDE) in order to assist API users with useful code completion proposals regarding object creation and manipulation, which facilitate API exploration and learning. As a proof of concept, we developed Dacite, a tool which comprises a set of Java annotations and an accompanying plugin for the Eclipse IDE. A user study revealed that Dacite is usable and effective, and Dacite's proposals enable programmers to be more successful in solving programming tasks involving unfamiliar APIs.",Annotations | API usability | Code completion | Eclipse | IDE,Journal of Systems and Software,2017-04-01,Article,"Santos, André L.;Myers, Brad A.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-84995569607,10.1016/j.jss.2016.09.015,A survey of the use of crowdsourcing in software engineering,"The term ‘crowdsourcing’ was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced Software Engineering.",Crowdsourced software engineering | Crowdsourcing | Literature survey | Software crowdsourcing,Journal of Systems and Software,2017-04-01,Article,"Mao, Ke;Capra, Licia;Harman, Mark;Jia, Yue",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85006368896,10.1016/j.jss.2016.06.012,"Large scale opinion mining for social, news and blog data","Companies that collect and analyze data from social media, news and other data streams are faced with several challenges that concern storage and processing of huge amounts of data. When they want to serve the processed information to their customers and moreover, when they want to cover different information needs for each customer, they need solutions that process data in near real time in order to gain insights on the data in motion. The volume and volatility of opinionated data that is published in social media, in combination with the variety of data sources has created a demanding ecosystem for stream processing. Although, there are several solutions that can handle information of static nature and small volume quite efficiently, they usually do not scale up properly because of their high complexity. Moreover, such solutions have been designed to run once or to run in a fixed dataset and they are not sufficient for processing huge volumes of streamed data. To address this problem, a platform for real-time opinion mining is proposed. Based on prior research and real application services that have been developed, a new platform called “PaloPro” is presented to cover the needs for brand monitoring.",News streams | Opinion mining | Social media,Journal of Systems and Software,2017-05-01,Article,"Tsirakis, Nikos;Poulopoulos, Vasilis;Tsantilas, Panagiotis;Varlamis, Iraklis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85002530328,10.1016/j.jss.2016.07.004,HB<sup>2</sup>DS: A behavior-driven high-bandwidth network mining system,"This paper proposes a behavior detection system, HB2DS, to address the behavior-detection challenges in high-bandwidth networks. In HB2DS, a summarization of network traffic is represented through some meta-events. The relationships amongst meta-events are used to mine end-user behaviors. HB2DS satisfies the main constraints exist in analyzing of high-bandwidth networks, namely online learning and outlier handling, as well as one-pass processing, delay, and memory limitations. Our evaluation indicates significant improvement in big data stream analyzing in terms of accuracy and efficiency.",Behavior detection | big data stream | Data stream clustering | Network analysis,Journal of Systems and Software,2017-05-01,Article,"Noferesti, Morteza;Jalili, Rasool",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85016506115,10.1016/j.jss.2017.03.014,Source code analysis and manipulation,,,Journal of Systems and Software,2017-07-01,Editorial,"Oliveto, Rocco;Hindle, Abram;Lawrie, Dawn J.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-84979030646,10.1016/j.jss.2016.07.026,API usage pattern recommendation for software development,"Application Programming Interfaces (APIs) facilitate pragmatic reuse and improve the productivity of software development. An API usage pattern documents a set of method calls from multiple API classes to achieve a reusable functionality. Existing approaches often use frequent-sequence mining to extract API usage patterns. However, as reported by earlier studies, frequent-sequence mining may not produce a complete set of usage patterns. In this paper, we explore the possibility of mining API usage patterns without relying on frequent-pattern mining. Our approach represents the source code as a network of object usages where an object usage is a set of method calls invoked on a single API class. We automatically extract usage patterns by clustering the data based on the co-existence relations between object usages. We conduct an empirical study using a corpus of 11,510 Android applications. The results demonstrate that our approach can effectively mine API usage patterns with high completeness and low redundancy. We observe 18% and 38% improvement on F-measure and response time respectively comparing to usage pattern extraction using frequent-sequence mining.",Clustering | Object usage | Usage pattern,Journal of Systems and Software,2017-07-01,Article,"Niu, Haoran;Keivanloo, Iman;Zou, Ying",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85007591252,10.1016/j.jss.2016.04.008,Semantic versioning and impact of breaking changes in the Maven repository,"Systems that depend on third-party libraries may have to be updated when updates to these libraries become available in order to benefit from new functionality, security patches, bug fixes, or API improvements. However, often such changes come with changes to the existing interfaces of these libraries, possibly causing rework on the client system. In this paper, we investigate versioning practices in a set of more than 100,000 jar files from Maven Central, spanning over 7 years of history of more than 22,000 different libraries. We investigate to what degree versioning conventions are followed in this repository. Semantic versioning provides strict rules regarding major (breaking changes allowed), minor (no breaking changes allowed), and patch releases (only backward-compatible bug fixes allowed). We find that around one third of all releases introduce at least one breaking change. We perform an empirical study on potential rework caused by breaking changes in library releases and find that breaking changes have a significant impact on client libraries using the changed functionality. We find out that minor releases generally have larger release intervals than major releases. We also investigate the use of deprecation tags and find out that these tags are applied improperly in our dataset.",Breaking changes | Semantic versioning | Software libraries,Journal of Systems and Software,2017-07-01,Article,"Raemaekers, S.;van Deursen, A.;Visser, J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019638976,10.1016/j.jss.2017.05.039,Identification multi-level frequent usage patterns from APIs,"Software developers increasingly rely on application programming interfaces (APIs) of frameworks to increase productivity. An API method is generally used within code snippets along with other methods of the API of interest. When developers invoke API methods in a framework, they often encounter difficulty to determine which methods to call due to the huge number of included methods in that API. Developers usually exploit a source code search tool searching for code snippets that use the API methods of interest. However, the number of returned code snippets is very large which hinders the developer to locate useful ones. Moreover, co-usage relationships between API methods are often not documented. This article presents an approach to identify multi-level frequent usage patterns (IML-FUP) to help developers understand API usage and facilitate the development tasks when they use new APIs. An identified pattern represents a set of API methods that are frequently called together across interfering usage scenarios. In order to investigate the efficiency of the proposed approach, an experimental evaluation is conducted using four APIs and 89 client programs. For all studied APIs, the experimental results show that the proposed approach identifies usage patterns that are always strongly cohesive and highly consistent.",API documentation | API usage | Formal concept analysis | Identification | Usage patterns,Journal of Systems and Software,2017-08-01,Article,"Eyal Salman, Hamzeh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019753039,10.1016/j.jss.2017.05.081,Embedding statecharts into Teleo-Reactive programs to model interactions between agents,"Context The Teleo-Reactive (TR) approach offers many possibilities for goal-oriented modeling of reactive systems, but it also has drawbacks when the number of interactions among agents is high, leading to barely legible specifications and losing the original benefits of the approach. Objective This work combines the TR paradigm with statecharts and provides advantages for modeling reactive systems and removing the shortcomings detected. Method A basic example is adopted to reveal the problem that appears when agents are modeled only with the TR approach and have frequent interactions with others. This paper proposes an extension to the TR approach that integrates the modeling using statecharts. A transformation procedure from statecharts to TR programs makes it possible to continue using the infrastructure of existing execution platforms such as TeleoR. The approach has been validated for a particular domain by considering a more complex case study in which traditionally there have been no results on the application of the TR paradigm. A survey was carried out on students to verify the benefits of the approach. Results A method to consider statecharts when modeling TR programs. Conclusions Statecharts can facilitate the adoption of the TR approach.",Modeling reactive-systems | Statecharts | Teleo-Reactive,Journal of Systems and Software,2017-09-01,Article,"Sánchez, Pedro;Álvarez, Bárbara;Martínez, Ramón;Iborra, Andrés",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85021148728,10.1016/j.jss.2017.05.098,A framework for automatically ensuring the conformance of agent designs,"Multi-agent systems are increasingly being used in complex applications due to features such as autonomy, pro-activity, flexibility, robustness and social ability. These very features also make verifying multi-agent systems a challenging task. In this article, we propose a mechanism, including automated tool support, for early phase defect detection by comparing the plan structures of a Belief-Desire-Intention agent design against the requirements models and interaction protocols. The basic intuition of our approach is to extract sets of possible behaviour runs from the agents’ behaviour models and to verify whether these runs conform to the specifications of the system-to-be or not. This approach is applicable at design time, not requiring source code, thus enabling detection and removal of some defects at an early phase of the software development lifecycle. We followed an experimental approach for evaluating the proposed verification framework. Our evaluation shows that even simple system's specifications developed by relatively experienced developers are prone to defects, and our approach is successful in uncovering most of these defects. In addition, we conducted a scalability analysis on the approach, and the outcomes show that our approach can scale when designs grow in size.",Agent-oriented software engineering | Multi-agent systems | Verification,Journal of Systems and Software,2017-09-01,Article,"Abushark, Yoosef;Thangarajah, John;Harland, James;Miller, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85003510868,10.1016/j.jss.2016.06.064,Automating the license compatibility process in open source software with SPDX,"Free and Open Source Software (FOSS) promotes software reuse and distribution at different levels for both creator and users, but at the same time imposes some challenges in terms of FOSS licenses that can be selected and combined. The main problem linked to this selection is the presence of a large set of licenses that define different rights and obligations in software use. The problem becomes more evident in cases of complex combinations of software that carries different – often conflicting – licenses. In this paper we are presenting our work on automating license compatibility by proposing a process that examines the structure of Software Package Data Exchange (SPDX) for license compatibility issues assisting in their correct use and combination. We are offering the possibility to detect license violations in existing software projects and make suggestions on appropriate combinations of different software packages. We are also elaborating on the complexity and ambiguity of licensing detection in software products through representative case studies. Our work constitutes a useful process towards automating the analysis of software systems in terms of license use and compatibilities.",License compatibility | License violations | Open Source Software | Software Package Data Exchange,Journal of Systems and Software,2017-09-01,Article,"Kapitsaki, Georgia M.;Kramer, Frederik;Tselikas, Nikolaos D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85002957064,10.1016/j.jss.2016.06.101,Reverse engineering reusable software components from object-oriented APIs,"Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs.",API | Frequent usage pattern | Object-oriented | Reverse engineering | Software component | Software reuse,Journal of Systems and Software,2017-09-01,Article,"Shatnawi, Anas;Seriai, Abdelhak Djamel;Sahraoui, Houari;Alshara, Zakarea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85003443942,10.1016/j.jss.2016.06.063,Stepwise API usage assistance using n-gram language models,"Reusing software involves learning third-party APIs, a process that is often time-consuming and error-prone. Recommendation systems for API usage assistance based on statistical models built from source code corpora are capable of assisting API users through code completion mechanisms in IDEs. A valid sequence of API calls involving different types may be regarded as a well-formed sentence of tokens from the API vocabulary. In this article we describe an approach for recommending subsequent tokens to complete API sentences using n-gram language models built from source code corpora. The provided system was integrated in the code completion facilities of the Eclipse IDE, providing contextualized completion proposals for Java taking into account the nearest lines of code. The approach was evaluated against existing client code of four widely used APIs, revealing that in more than 90% of the cases the expected subsequent token is within the 10-top-most proposals of our models. The high score provides evidence that the recommendations could help on API learning and exploration, namely through the assistance on writing valid API sentences.",API | Code completion | IDE | N-grams | Usability,Journal of Systems and Software,2017-09-01,Article,"Santos, André L.;Prendi, Gonçalo;Sousa, Hugo;Ribeiro, Ricardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84981736041,10.1016/j.jss.2016.07.033,A Formal Approach to implement java exceptions in cooperative systems,"The increasing number of systems that work on the top of cooperating elements have required new techniques to control cooperation on both normal and abnormal behaviors of systems. The controllability of the normal behaviors has received more attention because they are concerned with the users expectations, while for the abnormal behaviors it is left to designers and programmers. However, for cooperative systems, the abnormal behaviors, mostly represented by exceptions at programming level, become an important issue in software development because they can affect the overall system behavior. If an exception is raised and not handled accordingly, the system may collapse. To avoid such situation, certain concepts and models have been proposed to coordinate propagation and recovering of exceptional behaviors, including the Coordinated Atomic Actions (CAA). Regardless of the effort in creating these conceptual models, an actual implementation of them in real systems is not very straightforward. This article provides a reliable framework for the implementation of Java exceptions propagation and recovery using CAA concepts. To do this, a Java framework (based on a formal specification) is presented, together with a set of properties to be preserved and proved with the Java Pathfinder (JPF) model checker. In practice, to develop new systems based on the given coordination concepts, designers/programmers can instantiate the framework to implement the exceptional behavior and then verify the correctness of the resulting code using JPF. Therefore, by using the framework, designers/programmers can reuse the provided CAA implementation and instantiate fault-tolerant Java systems.",concurrent exception handling | Coordinated atomic actions model | java framework | program verification,Journal of Systems and Software,2017-09-01,Article,"Hanazumi, Simone;de Melo, Ana C.V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84979497676,10.1016/j.jss.2016.07.010,An exploratory study on the usage of common interface elements in android applications,"The number of mobile applications has increased drastically in the past few years. A recent study has shown that reusing source code is a common practice for Android application development. However, reuse in mobile applications is not necessarily limited to the source code (i.e., program logic). User interface (UI) design plays a vital role in constructing the user-perceived quality of a mobile application. The user-perceived quality reflects the users’ opinions of a product. For mobile applications, it can be quantified by the number of downloads and raters. In this study, we extract commonly used UI elements, denoted as Common Element Sets (CESs), from user interfaces of applications. Moreover, we highlight the characteristics of CESs that can result in a high user-perceived quality by proposing various metrics. Through an empirical study on 1292 mobile applications, we observe that (i) CESs of mobile applications widely occur among and across different categories; (ii) certain characteristics of CESs can provide a high user-perceived quality; and (iii) through a manual analysis, we recommend UI templates that are extracted and summarized from CESs for developers. Developers and quality assurance personnel can use our guidelines to improve the quality of mobile applications.",Common UI elements | Mobile applications | User-perceived quality,Journal of Systems and Software,2017-09-01,Article,"Taba, Seyyed Ehsan Salamati;Keivanloo, Iman;Zou, Ying;Wang, Shaohua",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85028270241,10.1016/j.infsof.2017.02.005,"A study on the statistical convertibility of IFPUG Function Point, COSMIC Function Point and Simple Function Point","Background Several functional size measurement methods have been proposed. A few ones –like IFPUG and COSMIC methods– are widely used, while others –like Simple Function Points method– are interesting new proposals, which promise to deliver functional size measures via a faster and cheaper measurement process. Objectives Since all functional size measurement methods address the measurement of the same property of software (namely, the size of functional specifications), it is expected that measures provided in a given measurement unit can be converted into a different measurement unit. In this paper, convertibility of IFPUG Function Points, COSMIC Function Points, and Simple Function Points is studied. Method Convertibility is analyzed statistically via regression techniques. Seven datasets, each one containing measures of a set of software applications expressed in IFPUG Function Points, COSMIC Function Points and Simple Function Points, were analyzed. The components of functional size measures (usually known as Base Functional Components) were also involved in the analysis. Results All the analyzed measures appear well correlated to each other. Statistically significant quantitative models were found for all the combinations of measures, for all the analyzed datasets. Several models involving Base Functional Components were found as well. Conclusions From a practical point of view, the paper shows that converting measures from a given functional size unit into another one is viable. The magnitude of the conversion errors is reported, so that practitioners can evaluate if the expected conversion error is acceptable for their specific purposes. From a conceptual point of view, the paper shows that Base Functional Components of a given method can be used to estimate measures expressed in a different measurement unit: this seems to imply that different functional size measurement methods are ‘structurally’ strongly correlated.",Base Functional Components (BFC) | Convertibility | COSMIC Function Points | Functional size measurement | IFPUG Function Points | Simple Function Points,Information and Software Technology,2017-06-01,Article,"Abualkishik, Abedallah Zaid;Ferrucci, Filomena;Gravino, Carmine;Lavazza, Luigi;Liu, Geng;Meli, Roberto;Robiolo, Gabriela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84958213157,10.1016/j.infsof.2016.01.019,A novel use of equivalent mutants for static anomaly detection in software artifacts,"Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis. Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed. Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement. Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level. Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome.",Equivalent mutant | Quality measure | Static anomaly,Information and Software Technology,2017-01-01,Article,"Arcaini, Paolo;Gargantini, Angelo;Riccobene, Elvinia;Vavassori, Paolo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85000799299,10.1016/j.infsof.2016.09.006,Automated triaging of very large bug repositories,"Context: Bug tracking systems play an important role in software maintenance. They allow both developers and users to submit problem reports on observed failures. However, by allowing anyone to submit problem reports, it is likely that more than one reporter will report on the same issue. Research in open source repositories has focused on two broad areas: determining the original report associated with each known duplicate, and assigning a developer to fix a particular problem. Objective: Limited research has been done in developing a fully automated triager, one that can first ascertain if a problem report is original or duplicate, and then provide a list of 20 potential matches for a duplicate report. We address this limitation by developing an automated triaging system that can be used to assist human triagers in bug tracking systems. Method: Our automated triaging system automatically assigns a label of original or duplicate to each incoming problem report, and provides a list of 20 suggestions for reports classified as duplicate. The system uses 24 document similarity measures and associated summary statistics, along with a suite of document property and user metrics. We perform our research on a lifetime of problem reports from the Eclipse, Firefox and Open Office repositories. Results: Our system can be used as a filtration aide, with high original recall exceeding 95% and low duplicate recall, or as a triaging guide, with balanced recall of approximately 70% for both originals and duplicates. Furthermore, the system reduces the workload on the triager by over 90%. Conclusions: Our work represents the first full scale effort at automatically triaging problem reports in open source repositories. By utilizing multiple similarity measures, we reduce the potential of false matches caused by the diversity of human language.",Automated triaging | Big data analytics | Bug tracking | Software problem repositories,Information and Software Technology,2017-09-01,Article,"Banerjee, Sean;Syed, Zahid;Helmick, Jordan;Culp, Mark;Ryan, Kenneth;Cukic, Bojan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019052872,10.1016/j.infsof.2017.05.001,Domain-aware Mashup service clustering based on LDA topic model from multiple data sources,"Context Mashup is emerging as a promising software development method for allowing software developers to compose existing Web APIs to create new or value-added composite Web services. However, the rapid growth in the number of available Mashup services makes it difficult for software developers to select a suitable Mashup service to satisfy their requirements. Even though clustering based Mashup discovery technique shows a promise of improving the quality of Mashup service discovery, Mashup service clustering with high accuracy and good efficiency is still a challenge problem. Objective This paper proposes a novel domain-aware Mashup service clustering method with high accuracy and good efficiency by exploiting LDA topic model built from multiple data sources, to improve the quality of Mashup service discovery. Method The proposed method firstly designs a domain-aware Mashup service feature selection and reduction process by refining characterization of their domains to consolidate domain relevance. Then, it presents an extended LDA topic model built from multiple data sources (include Mashup description text, Web APIs and tags) to infer topic probability distribution of Mashup services, which serves as a basis of Mashup service similarity computation. Finally, K-means and Agnes algorithm are used to perform Mashup service clustering in terms of their similarities. Results Compared with other existing Mashup service clustering methods, experimental results show that the proposed method achieves a significant improvement in terms of precision, recall, F-measure, purity and entropy. Conclusion The results of the proposed method help software developers to improve the quality of Mashup service discovery and Mashup-based software development. In the future, there will be a need to extend the method by considering heterogeneous network information among Mashup, Web APIs, tags, users, and applying it to Mashup discovery for software developers.",Domain feature selection and reduction | LDA | Mashup service | Multiple data sources | Service clustering,Information and Software Technology,2017-10-01,Article,"Cao, Buqing;Frank Liu, Xiaoqing;Liu, Jianxun;Tang, Mingdong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85018634750,10.1016/j.infsof.2017.04.003,"A historical, textual analysis approach to feature location","Context Feature location is the task of finding the source code that implements specific functionality in software systems. A common approach is to leverage textual information in source code against a query, using Information Retrieval (IR) techniques. To address the paucity of meaningful terms in source code, alternative, relevant source-code descriptions, like change-sets could be leveraged for these IR techniques. However, the extent to which these descriptions are useful has not been thoroughly studied. Objective This work rigorously characterizes the efficacy of source-code lexical annotation by change-sets (ACIR), in terms of its best-performing configuration. Method A tool, implementing ACIR, was used to study different configurations of the approach and to compare them to a baseline approach (thus allowing comparison against other techniques going forward). This large-scale evaluation employs eight subject systems and 600 features. Results It was found that, for ACIR: (1) method level granularity demands less search effort; (2) using more recent change-sets improves effectiveness; (3) aggregation of recent change-sets by change request, decreases effectiveness; (4) naive, text-classification-based filtering of “management” change-sets also decreases the effectiveness. In addition, a strongly pronounced dichotomy of subject systems emerged, where one set recorded better feature location using ACIR and the other recorded better feature location using the baseline approach. Finally, merging ACIR and the baseline approach significantly improved performance over both standalone approaches for all systems. Conclusion The most fundamental finding is the importance of rigorously characterizing proposed feature location techniques, to identify their optimal configurations. The results also suggest it is important to characterize the software systems under study when selecting the appropriate feature location technique. In the past, configuration of the techniques and characterization of subject systems have not been considered first-class entities in research papers, whereas the results presented here suggests these factors can have a big impact.",Dataset expansion | Feature location | Search effort | Software systems’ characterization | Version histories,Information and Software Technology,2017-08-01,Article,"Chochlov, Muslim;English, Michael;Buckley, Jim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84994257635,10.1016/j.infsof.2016.07.002,Assessment of class mutation operators for C++ with the MuCPP mutation system,"Context: Mutation testing has been mainly analyzed regarding traditional mutation operators involving structured programming constructs common in mainstream languages, but mutations at the class level have not been assessed to the same extent. This fact is noteworthy in the case of C++, despite being one of the most relevant languages including object-oriented features. Objective: This paper provides a complete evaluation of class operators for the C++ programming language. MuCPP, a new system devoted to the application of mutation testing to this language, was developed to this end. This mutation system implements class mutation operators in a robust way, dealing with the inherent complexity of the language. Method: MuCPP generates the mutants by traversing the abstract syntax tree of each translation unit with the Clang API, and stores mutants as branches in the Git version control system. The tool is able to detect duplicate mutants, avoid system headers, and drive the compilation process. Then, MuCPP is used to conduct experiments with several open-source C++ programs. Results: The improvement rules listed in this paper to reduce unproductive class mutants have a significant impact in the computational cost of the technique. We also calculate the quantity and distribution of mutants generated with class operators, which generate far fewer mutants than their traditional counterparts. Conclusions: We show that the tests accompanying these programs cannot detect faults related to particular object-oriented features of C++. In order to increase the mutation score, we create new test scenarios to kill the surviving class mutants for all the applications. The results confirm that, while traditional mutation operators are still needed, class operators can complement them and help testers further improve the test suite.",C++ | Class mutation operators | Mutation system | Mutation testing | Object-oriented programming,Information and Software Technology,2017-01-01,Article,"Delgado-Pérez, Pedro;Medina-Bulo, Inmaculada;Palomo-Lozano, Francisco;García-Domínguez, Antonio;Domínguez-Jiménez, Juan José",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85025476436,10.1016/j.infsof.2017.07.007,Open source software ecosystems: A Systematic mapping,"Context: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together. Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented. Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible. Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs. Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs.",Literature review | Open source software | OSS | OSSECO | SECO | Software ecosystem | Systematic mapping,Information and Software Technology,2017-11-01,Article,"Franco-Bedoya, Oscar;Ameller, David;Costal, Dolors;Franch, Xavier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027512992,10.1016/j.infsof.2017.07.008,Reusable and generic design decisions for developing UML-based domain-specific languages,"Context: In recent years, UML-based domain-specific model languages (DSMLs) have become a popular option in model-driven development projects. However, making informed design decisions for such DSMLs involves a large number of non-trivial and inter-related options. These options concern the language-model specification, UML extension techniques, concrete-syntax language design, and modeling-tool support. Objective: In order to make the corresponding knowledge on design decisions reusable, proven design rationale from existing DSML projects must be collected, systematized, and documented using an agreed upon documentation format. Method: We applied a sequential multi-method approach to identify and to document reusable design decisions for UML-based DSMLs. The approach included a Web-based survey with 80 participants. Moreover, 80 DSML projects1 which have been identified through a prior systematic literature review, were analyzed in detail in order to identify reusable design decisions for such DSMLs. Results: We present insights on the current state of practice in documenting UML-based DSMLs (e.g., perceived barriers, documentation techniques, reuse potential) and a publicly available collection of reusable design decisions, including 35 decision options on different DSML development concerns (especially concerning the language model, concrete-syntax language design, and modeling tools). The reusable design decisions are documented using a structured documentation format (decision record). Conclusion: Our results are both, scientifically relevant (e.g. for design-space analyses or for creating classification schemas for further research on UML-based DSML development) and important for actual software engineering projects (e.g. by providing best-practice guidelines and pointers to common pitfalls).",Design decision | Design rationale | Domain-specific language | Model-driven software development | Survey | Unified modeling language,Information and Software Technology,2017-12-01,Article,"Hoisl, Bernhard;Sobernig, Stefan;Strembeck, Mark",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85008601796,10.1016/j.infsof.2016.10.006,Who should comment on this pull request? Analyzing attributes for more accurate commenter recommendation in pull-based development,"Context: The pull-based software development helps developers make contributions flexibly and efficiently. Commenters freely discuss code changes and provide suggestions. Core members make decision of pull requests. Both commenters and core members are reviewers in the evaluation of pull requests. Since some popular projects receive many pull requests, commenters may not notice new pull requests in time, and even ignore appropriate pull requests. Objective: Our objective in this paper is to analyze attributes that affect the precision and recall of commenter prediction, and choose appropriate attributes to build commenter recommendation approach. Method: We collect 19,543 pull requests, 206,664 comments and 4817 commenters from 8 popular projects in GitHub. We build approaches based on different attributes, including activeness, text similarity, file similarity and social relation. We also build composite approaches, including time-based text similarity, time-based file similarity and time-based social relation. The time-based social relation approach is the state-of-the-art approach proposed by Yu et al. Then we compare precision and recall of different approaches. Results: We find that for 8 projects, the activeness based approach achieves the top-3 precision of 0.276, 0.386, 0.389, 0.516, 0.322, 0.572, 0.428, 0.402, and achieves the top-3 recall of 0.475, 0.593, 0.613, 0.66, 0.644, 0.791, 0.714, 0.65, which outperforms approaches based on text similarity, file similarity or social relation by a substantial margin. Moreover, the activeness based approach achieves better precision and recall than composite approaches. In comparison with the state-of-the-art approach, the activeness based approach improves the top-3 precision by 178.788%, 30.41%, 25.08%, 41.76%, 49.07%, 32.71%, 25.15%, 78.67%, and improves the top-3 recall by 196.875%, 36.32%, 29.05%, 46.02%, 43.43%, 27.79%, 25.483%, 79.06% for 8 projects. Conclusion: The activeness is the most important attribute in the commenter prediction. The activeness based approach can be used to improve the commenter recommendation in code review.",Attribute selection | Commenter recommendation | Pull-based software development | Reviewer recommendation,Information and Software Technology,2017-04-01,Article,"Jiang, Jing;Yang, Yun;He, Jiahuan;Blanc, Xavier;Zhang, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85023638446,10.1016/j.infsof.2017.07.001,A method for generation and design of business processes with business rules,"Context: Business Processes provide a universal method of describing operational aspects of business. Business Rules, in turn, support declarative specification of business domain knowledge. Although there is a difference in abstraction levels between these both modeling techniques, rules can be complementary to processes. Rules can be efficiently used to specify process low-level logic, while processes can serve as a procedural specification of the workflow, including the inference control. Objective: One of the research problems in this area is supporting business analytics in the modeling of processes integrated with rules. Such a support can take advantage of new design method for such models. Method: We describe a model of procedural Business Process as well as the model and method of creating Attribute Relationship Diagrams. Based on these two representations, we provide a formalized model combining a process model with rules. Using these models, we introduce an algorithm that generates an executable process model along with decision table schemas for rules (rule templates for rule sets grouped in decision tables). Results: The paper provides an automated approach for generation of Business Process models from Attribute Relationship Diagrams. The approach was evaluated based on the selected benchmark cases, which were deployed and tested in the provided modeling and execution environment for such integrated models. Conclusion: The paper presents an efficient and formalized method for design of processes with rules that allows for generating BPMN models integrated with the rules from the Semantic Knowledge Engineering approach. Such a model can be treated as a structured rule base that provides explicit inference flow determined by the process control flow.",Business process modeling | Business rules | Process with rules integration,Information and Software Technology,2017-11-01,Article,"Kluza, Krzysztof;Nalepa, Grzegorz J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85021928111,10.1016/j.infsof.2017.07.002,A dedicated approach for model composition traceability,"Context: Software systems are often too complex to be expressed by a single model. Recognizing this, the Model Driven Engineering (MDE) proposes multi-modeling approaches to allow developers to describe a system from different perspectives. In this context, model composition has become important since the combination of those partial representations is inevitable. Nevertheless, no approach has been defined for keeping track of the composition effects, and this operation has been overshadowed by model transformations. Objective This paper presents a traceability approach dedicated to the composition of models. Two aspects of quality are considered: producing relevant traces; and dealing with scalability. Method The composition of softgoal trees has been selected to motivate the need for tracing the composition of models and to illustrate our approach. The base principle is to augment the specification of the composition with the behavior needed to generate the expected composed model accompanied with a trace model. This latter includes traces of the execution details. For that, traceability is considered as a crosscutting concern and encapsulated in an aspect. As part of the proposal, an Eclipse plug-in has been implemented as a tool support. Besides, a comparative experiment has been conducted to assess the traces relevance. We also used the regression method to validate the scalability of the tool support. Results Our experiments show that the proposed approach allows generating relevant traces. In addition, the obtained results reveal that tracing a growing number of elements causes an acceptable increase of response time. Conclusion This paper presents a traceability approach dedicated to the composition of models and its application to softgoal trees. The experiment results reveal that our proposal considers the composition specificities for producing valuable traceability information while supporting scalability.",Aspect-oriented modeling | Graph transformations | Model composition | Model traceability | NFR framework,Information and Software Technology,2017-11-01,Article,"Laghouaouta, Youness;Anwar, Adil;Nassar, Mahmoud;Coulette, Bernard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019033585,10.1016/j.infsof.2017.03.013,A systematic literature review on methods that handle multiple quality attributes in architecture-based self-adaptive systems,"Context Handling multiple quality attributes (QAs) in the domain of self-adaptive systems is an understudied research area. One well-known approach to engineer adaptive software systems and fulfill QAs of the system is architecture-based self-adaptation. In order to develop models that capture the required knowledge of the QAs of interest, and to investigate how these models can be employed at runtime to handle multiple quality attributes, we need to first examine current architecture-based self-adaptive methods. Objective In this paper we review the state-of-the-art of architecture-based methods for handling multiple QAs in self-adaptive systems. We also provide a descriptive analysis of the collected data from the literature. Method We conducted a systematic literature review by performing an automatic search on 28 selected venues and books in the domain of self-adaptive systems. As a result, we selected 54 primary studies which we used for data extraction and analysis. Results Performance and cost are the most frequently addressed set of QAs. Current self-adaptive systems dealing with multiple QAs mostly belong to the domain of robotics and web-based systems paradigm. The most widely used mechanisms/models to measure and quantify QAs sets are QA data variables. After QA data variables, utility functions and Markov chain models are the most common models which are also used for decision making process and selection of the best solution in presence of many alternatives. The most widely used tools to deal with multiple QAs are PRISM and IBM's autonomic computing toolkit. KLAPER is the only language that has been specifically developed to deal with quality properties analysis. Conclusions Our results help researchers to understand the current state of research regarding architecture-based methods for handling multiple QAs in self-adaptive systems, and to identity areas for improvement in the future. To summarize, further research is required to improve existing methods performing tradeoff analysis and preemption, and in particular, new methods may be proposed to make use of models to handle multiple QAs and to enhance and facilitate the tradeoffs analysis and decision making mechanism at runtime.",,Information and Software Technology,2017-10-01,Article,"Mahdavi-Hezavehi, Sara;Durelli, Vinicius H.S.;Weyns, Danny;Avgeriou, Paris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85006269717,10.1016/j.infsof.2016.11.008,A model-driven approach to catch performance antipatterns in ADL specifications,"Context: While the performance analysis of a software architecture is a quite well-assessed task nowadays, the issue of interpreting the performance results for providing feedback to software architects is still very critical. Performance antipatterns represent effective instruments to tackle this issue, because they document common mistakes leading to performance problems as well as their solutions. Objective: Up today performance antipatterns have been only studied in the context of software modeling languages like UML, whereas in this manuscript our objective is to catch them in the context of ADL-based software architectures to investigate their effectiveness. Method: We have implemented a model-driven approach that allows the automatic detection of four performance antipatterns in Æmilia, that is a stochastic process algebraic ADL for performance-aware component-oriented modeling of software systems. Results: We evaluate the approach by applying it to three case studies in different application domains. Experimental results demonstrate the effectiveness of our approach to support the performance improvement of ADL-based software architectures. Conclusion: We can conclude that the detection of performance antipatterns, from the earliest stages of software development, represents an effective instrument to tackle the issue of identifying flaws and improving system performance.",Architecture description languages | Model-driven engineering | Performance antipatterns | Software performance analysis | Æmilia ADL,Information and Software Technology,2017-03-01,Article,"De Sanctis, Martina;Trubiani, Catia;Cortellessa, Vittorio;Di Marco, Antinisca;Flamminj, Mirko",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85020853907,10.1016/j.infsof.2017.06.002,Research patterns and trends in software effort estimation,"Context Software effort estimation (SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset. Objective To identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016. Method A generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE. Results As many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas. Conclusions This study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas.",Latent Dirichlet allocation | Research trends | Software effort estimation,Information and Software Technology,2017-11-01,Review,"Sehra, Sumeet Kaur;Brar, Yadwinder Singh;Kaur, Navdeep;Sehra, Sukhjit Singh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027938830,10.1016/j.infsof.2017.01.003,Software teams and their knowledge networks in large-scale software development,"Context Large software development projects involve multiple interconnected teams, often spread around the world, developing complex products for a growing number of customers and users. Succeeding with large-scale software development requires access to an enormous amount of knowledge and skills. Since neither individuals nor teams can possibly possess all the needed expertise, the resource availability in a team's knowledge network, also known as social capital, and effective knowledge coordination become paramount. Objective In this paper, we explore the role of social capital in terms of knowledge networks and networking behavior in large-scale software development projects. Method We conducted a multi-case study in two organizations, Ericsson and ABB, with software development teams as embedded units of analysis. We organized focus groups with ten software teams and surveyed 61 members from these teams to characterize and visualize the teams’ knowledge networks. To complement the team perspective, we conducted individual interviews with representatives of supporting and coordination roles. Based on survey data, data obtained from focus groups, and individual interviews, we compared the different network characteristics and mechanisms that support knowledge networks. We used social network analysis to construct the team networks, thematic coding to identify network characteristics and context factors, and tabular summaries to identify the trends. Results Our findings indicate that social capital and networking are essential for both novice and mature teams when solving complex, unfamiliar, or interdependent tasks. Network size and networking behavior depend on company experience, employee turnover, team culture, need for networking, and organizational support. A number of mechanisms can support the development of knowledge networks and social capital, for example, introduction of formal technical experts, facilitation of communities of practice and adequate communication infrastructure. Conclusions Our study emphasizes the importance of social capital and knowledge networks. Therefore, we suggest that, along with investments into training programs, software companies should also cultivate a networking culture to strengthen their social capital, a known driver of better performance.",Agile | Case study | Cross-functional | Empirical | Feature teams | Intellectual capital | Knowledge networks | Large-scale software development | Social capital | Teams,Information and Software Technology,2017-06-01,Article,"Šmite, Darja;Moe, Nils Brede;Šāblis, Aivars;Wohlin, Claes",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85006293265,10.1016/j.infsof.2016.11.006,Cinders: The continuous integration and delivery architecture framework,"Context: The popular agile practices of continuous integration and delivery have become an essential part of the software development process in many companies, yet effective methods and tools to support design, description and communication of continuous integration and delivery systems are lacking. Objective: The work reported on in this paper addresses that lack by presenting Cinders — an architecture framework designed specifically to meet the needs of such systems, influenced both by prominent enterprise and software architecture frameworks as well as experiences from continuous integration and delivery modeling in industry. Method: The state of the art for systematic design and description of continuous integration and delivery systems is established through review of literature, whereupon a proposal for an architecture framework addressing requirements derived from continuous integration and delivery modeling experiences is proposed. This framework is subsequently evaluated through interviews and workshops with engineers in varying roles in three independent companies. Results: Cinders, an architecture framework designed specifically for the purpose of describing continuous integration and delivery systems is proposed and confirmed to constitute an improvement over previous methods. This work presents software professionals with a demonstrably effective method for describing their continuous integration and delivery systems from multiple points of view and supporting multiple use-cases, including system design, communication and documentation. Conclusion: It is concluded that an architecture framework for the continuous integration and delivery domain has value; at the same time potential for further improvement is identified, particularly in the area of tool support for data collection as well as for manual modeling.",Architecture framework | Cinders | Continuous delivery | Continuous integration | Software integration | Software testing,Information and Software Technology,2017-03-01,Article,"Ståhl, Daniel;Bosch, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85021818457,10.1016/j.infsof.2017.06.008,Choreography in the embedded systems domain: A systematic literature review,"Context Software companies that develop their products on a basis of service-oriented architecture can expect various improvements as a result of choreography. Current choreography practices, however, are not yet used extensively in the embedded systems domain even though service-oriented architecture is increasingly used in this domain. Objective The objective of this study is to identify current features of the use of choreography in the embedded systems domain for practitioners and researchers by systematically analysing current developments in the scientific literature, strategies for choreography adaption, choreography specification and execution types, and implicit assumptions about choreography. Method To fulfil this objective, a systematic literature review of scientific publications that focus on the use of choreography in the embedded systems domain was carried out. After a systematic screening of 6823 publications, 48 were selected as primary studies and analysed using thematic synthesis. Results The main results of the study showed that there are differences in how choreography is used in embedded and non-embedded systems domain. In the embedded systems domain, it is used to capture the service interactions of a single organisation, while, for example, in the enterprise systems domain it captures the service interactions among multiple organisations. Additionally, the results indicate that the use of choreography can lead to improvements in system performance and that the languages that are used for choreography modelling in the embedded systems domain are insufficiently expressive to capture the complexities that are typical in this domain. Conclusion The selection of the key information resources and the identified gaps in the existing literature offer researchers a foundation for further investigations and contribute to the advancement of the use of choreography in the embedded systems domain. The study results facilitate the work of practitioners by allowing them to make informed decisions about the applicability of choreography in their organisations.",Choreography | Embedded systems | Service-oriented architecture | Systematic literature review,Information and Software Technology,2017-11-01,Review,"Taušan, Nebojša;Markkula, Jouni;Kuvaja, Pasi;Oivo, Markku",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85021212048,10.1016/j.infsof.2017.06.003,Investigating comprehension and learnability aspects of use cases for software specification problems,"Context: Availability of multiple use case templates to document software requirements inevitably requires their characterization in terms of their relevance, usefulness, and the degree of the formality of the expressions. Objective: This paper reports two experimental studies that separately investigate two usability aspects, namely the comprehension and the learnability of use case templates for software specification problems. Method: We judged the comprehension aspect by evaluating the subjects’ understanding of the requirements, specified in eight different use case templates, and the ease with which the changes were made by them in the requirement specifications. The learnability aspect was judged by assessing the completeness, the correctness, and the redundancy of the use case specifications developed by the subjects using these eight use case templates for three software specification problems. Results: Our results suggested that the Kettenis's use case template was found to be significantly more understandable, and the templates by Tiwari, Yue and Somé were found to be significantly more flexible to adapt to the changes. On the learnability aspect, the way we formulated it, we found different templates to be more complete (Kettenis), correct (Somé), and non-redundant (Tiwari). Conclusion: The specifications documented using a more detailed use case template with an intermediate degree of formality can be more comprehensible and flexible to adapt to the required changes to be made in the specification. A more formal template seems to enhance the learnability as well.",Comprehension | Experimental study | Learnability | Software specification problem | Usability aspects | Use case templates | Use cases,Information and Software Technology,2017-11-01,Article,"Tiwari, Saurabh;Gupta, Atul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84991790848,10.1016/j.infsof.2016.09.009,"Twenty years of object-relational mapping: A survey on patterns, solutions, and their implications on application design","Context Almost twenty years after the first release of TopLink for Java, Object-Relational Mapping Solutions (ORMSs) are available at every popular development platform, providing useful tools for developers to deal with the impedance mismatch problem. However, no matter how ubiquitous these solutions are, this essential problem remains as challenging as ever. Different solutions, each with a particular vocabulary, are difficult to learn, and make the impedance problem looks deceptively simpler than it really is. Objective The objective of this paper is to identify, discuss, and organize the knowledge concerning ORMSs, helping designers towards making better informed decisions about designing and implementing their models, focusing at the static view of persistence mapping. Method This paper presents a survey with nine ORMSs, selected from the top ten development platforms in popularity. Each ORMS was assessed, by documentation review and experience, in relation to architectural and structural patterns, selected from literature, and its characteristics and implementation options, including platform specific particularities. Results We found out that all studied ORMSs followed architectural and structural patterns in the literature, but often with distinct nomenclature, and some singularities. Many decisions, depending on how patterns are implemented and configured, affect how class models should be adapted, in order to create practical mappings to the database. Conclusion This survey identified what structural patterns each ORMS followed, highlighting major structural decisions a designer must take, and its consequences, in order to turn analysis models into object oriented systems. It also offers a pattern based set of characteristics that developers can use as a baseline to make their own assessments of ORMSs.",Class models | Design patterns | Enterprise patterns | Impedance mismatch problem | Object-relational mapping,Information and Software Technology,2017-02-01,Article,"Torres, Alexandre;Galante, Renata;Pimenta, Marcelo S.;Martins, Alexandre Jonatan B.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-84995673130,10.1016/j.infsof.2016.11.002,Improved bug localization based on code change histories and bug reports,"Context Several issues or defects in released software during the maintenance phase are reported to the development team. It is costly and time-consuming for developers to precisely localize bugs. Bug reports and the code change history are frequently used and provide information for identifying fault locations during the software maintenance phase. Objective It is difficult to standardize the style of bug reports written in natural languages to improve the accuracy of bug localization. The objective of this paper is to propose an effective information retrieval-based bug localization method to find suspicious files and methods for resolving bugs. Method In this paper, we propose a novel information retrieval-based bug localization approach, termed Bug Localization using Integrated Analysis (BLIA). Our proposed BLIA integrates analyzed data by utilizing texts, stack traces and comments in bug reports, structured information of source files, and the source code change history. We improved the granularity of bug localization from the file level to the method level by extending previous bug repository data. Results We evaluated the effectiveness of our approach based on experiments using three open-source projects, namely AspectJ, SWT, and ZXing. In terms of the mean average precision, on average our approach improves the metric of BugLocator, BLUiR, BRTracer, AmaLgam and the preliminary version of BLIA by 54%, 42%, 30%, 25% and 15%, respectively, at the file level of bug localization. Conclusion Compared with prior tools, the results showed that BLIA outperforms these other methods. We analyzed the influence of each score of BLIA from various combinations based on the analyzed information. Our proposed enhancement significantly improved the accuracy. To improve the granularity level of bug localization, a new approach at the method level is proposed and its potential is evaluated.",Bug localization | Bug reports | Code change history | Information retrieval | Method analysis | Stack traces,Information and Software Technology,2017-02-01,Article,"Youm, Klaus Changsun;Ahn, June;Lee, Eunseok",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85008516012,10.1016/j.infsof.2016.12.003,Towards comprehending the non-functional requirements through Developers’ eyes: An exploration of Stack Overflow using topic analysis,"Context As a vital role for the quality of software, non-functional requirements (NFRs) are attracting greater attention from developers. The programming question and answer (Q&A) websites like Stack Overflow gathered the knowledge and expertise of developers worldwide which reflects some insight into the development activities (e.g., NFRs), but the NFRs in the Q&A site are rarely investigated. Objective Our research aims to aid comprehension on the actual thoughts and needs of the developers by analyzing the NFRs on Stack Overflow. Method We extracted the textual content of Stack Overflow discussions, and then we applied the topic modeling technique called latent Dirichlet allocation (LDA) helping us to discover the main topics of the corpus. Next, we labelled the topics with NFRs by the wordlists to analyze the hot, unresolved, difficult NFRs, and the evolutionary trends which involves the trends of the NFRs focus and NFRs difficulty. Results Our findings show that (1) The developers mostly discuss usability and reliability while discussing less on maintainability and efficiency. (2) The most unresolved problems also occurred in usability and reliability. (3) The visualization of the NFR evolutions over time shows the functionality and reliability attract more and more attention from developers and usability remains hot. (4) The NFRs investigation in specific technologies indicates the quality is a similar concern among different technologies and some NFRs are of more interest as time progresses. (5) The research on NFRs difficulty in specific technologies shows the maintainability is the most difficult NFR. In addition, the trends of the NFRs difficulty over time in the seven categories signal that we should focus more on usability to address them. Conclusion We present an empirical study on 21.7 million posts and 32.5 million comments of Stack Overflow, and our research provides some guide to understand the NFRs through developers’ eyes.",Latent Dirichlet allocation (LDA) | Non-functional requirements (NFRs) | Stack Overflow | Topic model,Information and Software Technology,2017-04-01,Article,"Zou, Jie;Xu, Ling;Yang, Mengning;Zhang, Xiaohong;Yang, Dan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045880841,10.1109/APSEC.2017.26,Method Level Text Summarization for Java Code Using Nano-Patterns,"Rapid growth in providing automated solutions resulted in large code bases to get quickly developed and consumed. However, maintaining code and its subsequent reuse pose some challenges here. One of the best practices used to handle such issues is also to provide suitable text summary of the code to allow the human developers to comprehend the code easily, but this can be quite time-consuming and costly affair. A few efforts have been made in this direction where the text summary of the code either generated from the method signature or its body. In this paper, we propose a text summarization approach for Java code that makes use of identification of code level nano-patterns to obtain text summary. The approach also looks for associations between these nano-patterns in a Java method code and then use a template based text generation to obtain the final text summary of the Java method. We evaluated the summary generated by the proposed approach using a controlled experiment with other three existing approaches. Our results suggested that the summary generated by our approach was better on the part of completeness and correctness criteria. The feedback obtained during the experimental validation suggested additional inputs to improve the generated text summary on the other two accounts as well.",Java Methods | Nano-Patterns | Natural Language Generation | Source Code | Text Summarization,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2017-07-02,Conference Paper,"Rai, Sawan;Gaikwad, Tejaswini;Jain, Sparshi;Gupta, Atul",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85045907841,10.1109/APSEC.2017.43,Mining API Type Specifications for JavaScript,"API specifications play an important role in software development. However, API specifications are often not well documented, especially for JavaScript. Many JavaScript API specifications lack of precise type information for API parameters and return values. In this paper, we propose a static approach for mining JavaScript type specifications automatically. We gather the usage information of return values and parameters statically, and infer types of return values based their usages, by identifying a known type which they are used most likely to be, and infer parameters by identifying the most used parameters. We evaluate the approach on the homepages of Alexa top 1000 websites, the experimental results show that our approach can gain high precision. Our case study on jQuery shows that our approach gains high precision and reasonable recall on jQuery, and we can use our inferred API type specifications to detect 2 jQuery misusage errors in real-world web sites, and 1 missing type error in jQuery documentations.",API | JavaScript | Type Specification,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2017-07-02,Conference Paper,"Wang, Shuai;Dou, Wensheng;Gao, Chushu;Wei, Jun;Huang, Tao",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85034415043,10.1109/SEAA.2017.23,Empirical analysis of words in comments written for Java methods,"This paper focuses on comments written in source programs. While comments can work for improving the readability of code, i.e., the quality of programs, there have also been concerns thatcomments can be added for complicated source code in order to compensate for a lack of readability. That is to say, well-written comments might be associated with problematic parts to be refactored. This paper collected Java methods (programs) from six popular open source products, and performs analyses on words which appear in their comments. Then, the paper shows that a method having a longer comments (more words)tends to be more change-prone and would be required more fixes after their releases.",Change-proneness | Comment | Java | Word,"Proceedings - 43rd Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2017",2017-09-26,Conference Paper,"Aman, Hirohisa;Amasaki, Sousuke;Yokogawa, Tomoyuki;Kawahara, Minoru",Include,
10.1016/j.jss.2022.111515,2-s2.0-85071044499,10.1109/ACIT-CSII-BCD.2017.73,An empirical study on relationships between comments and design properties,"Recent studies empirically revealed a relationship between source code comments and code quality. Those studies considered activity-based metrics as quality indicators. Project management tasks often used source code-based metrics, and analysis of such metrics can also give a useful insight into the relationship. Objective: To clarify the relationship between comments and code quality with CK metrics. Method: Compare distributions of CK metrics between commented files and noncommented files over four OSS projects. Results: Files with comments tend to be more complex than those without any comments regarding CBO, LCOM, RFC, and WMC. The differences in such metrics were statistically significant. Conclusions: Commented files were prone to be complex and thus less maintainable. The results suggest that developers should take notice of comments and code beside them.",,"Proceedings - 2017 5th International Conference on Applied Computing and Information Technology, 2017 4th International Conference on Computational Science/Intelligence and Applied Informatics and 2017 1st International Conference on Big Data, Cloud Computing, Data Science and Engineering, ACIT-CSII-BCD 2017",2017-07-01,Conference Paper,"Miyake, Yuto;Amasaki, Sousuke;Yokogawa, Tomoyuki;Aman, Hirohisa",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85040548444,10.1109/MODELS.2017.9,A Model-Driven Approach to Trace Checking of Pattern-Based Temporal Properties,"Trace checking is a procedure for evaluating requirements over a log of events produced by a system. This paper deals with the problem of performing trace checking of temporal properties expressed in TemPsy, a pattern-based specification language. The goal of the paper is to present a scalable and practical solution for trace checking, which can be used in contexts where relying on model-driven engineering standards and tools for property checking is a fundamental prerequisite.The main contributions of the paper are: a model-driven trace checking procedure, which relies on the efficient mapping of temporal requirements written in TemPsy into OCL constraints on a conceptual model of execution traces; the implementation of this trace checking procedure in the TEMPSY-CHECK tool; the evaluation of the scalability of TEMPSY-CHECK, applied to the verification of real properties derived from a case study of our industrial partner, including a comparison with a state-of-the-art alternative technology based on temporal logic. The results of the evaluation show the feasibility of applying our model-driven approach for trace checking in realistic settings: TEMPSY-CHECK scales linearly with respect to the length of the input trace and can analyze traces with one million events in about two seconds.",model-driven engineering | OCL | temporal properties | Trace checking,"Proceedings - ACM/IEEE 20th International Conference on Model Driven Engineering Languages and Systems, MODELS 2017",2017-11-07,Conference Paper,"Dou, Wei;Bianculli, Domenico;Briand, Lionel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85018428699,10.1109/SANER.2017.7884622,Automatically generating natural language descriptions for object-related statement sequences,"Current source code analyses driving software maintenance tools treat methods as either a single unit or a set of individual statements or words. They often leverage method names and any existing internal comments. However, internal comments are rare, and method names do not typically capture the method's multiple high-level algorithmic steps that are too small to be a single method, but require more than one statement to implement. Previous work demonstrated feasibility of identifying high level actions automatically for loops; however, many high level actions remain unaddressed and undocumented, particularly sequences of consecutive statements that are associated with each other primarily by object references. We call these object-related action units. In this paper, we present an approach to automatically generate natural language descriptions of object-related action units within methods. We leverage the available, large source of high-quality open source projects to learn the templates of object-related actions, identify the statement that can represent the main action, and generate natural language descriptions for these actions. Our evaluation study of a set of 100 object-related statement sequences showed promise of our approach to automatically identify the action and arguments and generate natural language descriptions.",abstraction | documentation generation | mining code patterns,"SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",2017-03-21,Conference Paper,"Wang, Xiaoran;Pollock, Lori;Vijay-Shanker, K.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85049220022,10.1109/KBEI.2017.8324908,A conceptual framework for clone detection using machine learning,"Code clones can happen in any software project. One of the challenges is that, code clones come in various forms which makes them hard to detect using standard templates. Due to this variety in structure and form of semantically similar clones, machine learning techniques are required to detect them. Recently in many domains, e.g., natural language processing, deep neural networks drew a lot of attention due to their accuracy. In this paper, we exploit the results of some convolutional neural networks for code summarization to find the code clones. We use the generated descriptions for two code snippets as a metric to measure the similarities between them. We propose a vector similarity measure to calculate a similarity indicator between these measures which can decide which code snippets are clones.",Clone detection | Convolutional attention neural networks | extractive software product lines | refactoring | reusability | summarization,"2017 IEEE 4th International Conference on Knowledge-Based Engineering and Innovation, KBEI 2017",2017-07-02,Conference Paper,"Ghofrani, Javad;Mohseni, Mahdi;Bozorgmehr, Arezoo",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85032394842,10.1109/ICWS.2017.10,CARP: Context-Aware Reliability Prediction of Black-Box Web Services,"Reliability prediction is an important task in software reliability engineering, which has been widely studied in the last decades. However, modelling and predicting user-perceived reliability of black-box services remain an open research problem. Software services, such as Web services and Web APIs, generally provide black-box functionalities to users through the Internet, thus leading to a lack of their internal information for reliability analysis. Furthermore, the user-perceived service reliability depends not only on the service itself, but also heavily on the invocation context (e.g., service workloads, network conditions), whereby traditional reliability models become ineffective and inappropriate. To address these new challenges posed by blackbox services, in this paper, we propose CARP, a new contextaware reliability prediction approach, which leverages historical usage data from users to construct context-aware reliability models and further provides online reliability prediction results to users. Through context-aware reliability modelling, CARP is able to alleviate the data sparsity problem that heavily limits the prediction accuracy of other existing approaches. The preliminary evaluation results show that CARP can make a significant improvement in reliability prediction accuracy, e.g., about 41% in MAE and 38% in RMSE when only 5% of the data are available.",Black-box services | context awareness | matrix factorization | reliability prediction,"Proceedings - 2017 IEEE 24th International Conference on Web Services, ICWS 2017",2017-09-07,Conference Paper,"Zhu, Jieming;He, Pinjia;Xie, Qi;Zheng, Zibin;Lyu, Michael R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85024481648,10.1109/SP.2017.52,Comparing the Usability of Cryptographic APIs,"Potentially dangerous cryptography errors are well-documented in many applications. Conventional wisdom suggests that many of these errors are caused by cryptographic Application Programming Interfaces (APIs) that are too complicated, have insecure defaults, or are poorly documented. To address this problem, researchers have created several cryptographic libraries that they claim are more usable, however, none of these libraries have been empirically evaluated for their ability to promote more secure development. This paper is the first to examine both how and why the design and resulting usability of different cryptographic libraries affects the security of code written with them, with the goal of understanding how to build effective future libraries. We conducted a controlled experiment in which 256 Python developers recruited from GitHub attempt common tasks involving symmetric and asymmetric cryptography using one of five different APIs. We examine their resulting code for functional correctness and security, and compare their results to their self-reported sentiment about their assigned library. Our results suggest that while APIs designed for simplicity can provide security benefits - reducing the decision space, as expected, prevents choice of insecure parameters - simplicity is not enough. Poor documentation, missing code examples, and a lack of auxiliary features such as secure key storage, caused even participants assigned to simplified libraries to struggle with both basic functional correctness and security. Surprisingly, the availability of comprehensive documentation and easy-to-use code examples seems to compensate for more complicated APIs in terms of functionally correct results and participant reactions, however, this did not extend to security results. We find it particularly concerning that for about 20% of functionally correct tasks, across libraries, participants believed their code was secure when it was not. Our results suggest that while new cryptographic libraries that want to promote effective security should offer a simple, convenient interface, this is not enough: they should also, and perhaps more importantly, ensure support for a broad range of common tasks and provide accessible documentation with secure, easy-to-use code examples.",API usability | Controlled experiment | Cryptography | Usable security,Proceedings - IEEE Symposium on Security and Privacy,2017-06-23,Conference Paper,"Acar, Yasemin;Backes, Michael;Fahl, Sascha;Garfinkel, Simson;Kim, Doowon;Mazurek, Michelle L.;Stransky, Christian",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85040987624,10.1109/VLHCC.2017.8103450,A study of the effectiveness of usage examples in REST API documentation,"Generating and maintaining REST API documentation with usage examples can be a time consuming and expensive process for evolving APIs. Most REST API documentation tools focus on automating the documentation of the API objects, but require manual effort for capturing usage examples. Consequently, REST API developers need to know the cost vs. benefit of providing usage examples in the documentation to prioritize the documentation efforts. To this end, we have performed a controlled study with 26 experienced software engineers to understand problems that REST API client developers face while using an API without usage examples. We found that REST API client developers face productivity problems with using correct data types, data formats, required HTTP headers and request body when documentation lacks usage examples. By following the REST API documentation suggestions from this paper, REST API developers can reduce the errors, improve success rate and satisfaction of API client developers.",API | Controlled Study | Documentation | Empirical Study | Productivity | REST | Usage Examples,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2017-11-09,Conference Paper,"Sohan, S. M.;Maurer, Frank;Anslow, Craig;Robillard, Martin P.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85040996371,10.1109/VLHCC.2017.8103466,An exploratory study of the usage of different educational resources in an independent context,"There are a variety of learning resources with the potential to support children in learning programming independently. While many of them have been evaluated in laboratory settings, we know little about how children choose to use these resources on their own. We conducted a study organized around a film festival to explore children's open-ended use of four different learning supports: Tutorials, code puzzles, in-Application documentation and code suggestions. The study began with a workshop to introduce the programming environment and available tools, continued through two weeks of home use, and culminated in a film festival. Results suggest that participants leveraged in-context forms of help most frequently, but valued documentation for question-Answering and suggestions for opportunistic learning.",code puzzles | documentation | examples | novice programming | programming support | tutorials,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2017-11-09,Conference Paper,"Hnin, Wint;Ichinco, Michelle;Kelleher, Caitlin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85026740228,10.1109/WAPI.2017.5,Type Checking for Reliable APIs,"In this paper, we propose to configure at compiletime the checking associated with Application Programming Interfaces' methods that can receive possibly malformed values (e.g. erroneous user inputs and problematic retrieved recordsfrom databases) and thus cause application execution failures. To achieve this, we design a type system for implementing apluggable checker on the Java's compiler and find at compile timeinsufficient checking bugs that can lead to application crashesdue to malformed inputs. Our goal is to wrap methods whenthey receive external inputs so that the former generate checkedinstead of unchecked exceptions. We believe that our approachcan improve Java developers' productivity, by using exceptionhandling only when it is required, and ensure client applications'stability. We want to evaluate our checker by using it to verifythe source code of Java projects from the Apache ecosystem. Also, we want to analyze stack traces to validate the identifiedfailures by our checker.",application programming interfaces | exceptions | type systems,"Proceedings - 2017 IEEE/ACM 1st International Workshop on API Usage and Evolution, WAPI 2017",2017-06-30,Conference Paper,"Kechagia, Maria;Spinellis, Diomidis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85026757933,10.1109/WAPI.2017.3,API Usage in Descriptions of Source Code Functionality,"In this paper, we present a study exploring the use of API keywords within method summaries. We conducted a web-based study where we asked participants to rank Java method summaries based on five levels of detail, from low level to high level. We found that programmers widely use API in both high and low level summaries. Specifically, we found that 76.78% of higher level summaries contain Java API keywords. Additionally, we found that 93.75% of lower level summaries also contain them. This also shows that, in general, as the detail level decreases, the number of API keywords within the summary increases. It is our hope that this line of research will spark a discussion about API usage outside of source code. It is possible that method summaries are not the only form of documentation that API usage plays an important role. We believe these may be important results that could lead to an improvement for API usability design.",API usage | Levels of detail | Source code summaries,"Proceedings - 2017 IEEE/ACM 1st International Workshop on API Usage and Evolution, WAPI 2017",2017-06-30,Conference Paper,"Rodeghero, Paige;McMillan, Collin;Shirey, Abigail",Exclude,Not a full paper
10.1016/j.jss.2022.111515,,10.1145/3068698.3068702,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040990699,10.1145/3025453.3025827,Suggesting API usage to novice programmers with the Example Guru,"Programmers, especially novices, often have difficulty learning new APIs (Application Programming Interfaces). Existing research has not fully addressed novice programmers' un-awareness of all available API methods. To help novices discover new and appropriate uses for API methods, we designed a system called the Example Guru. The Example Guru suggests context-relevant API methods based on each programmer's code. The suggestions provide contrasting examples to demonstrate how to use the API methods. To evaluate the effectiveness of the Example Guru, we ran a study comparing novice programmers' use of the Example Guru and documentation-inspired API information. We found that twice as many participants accessed the Example Guru suggestions compared to documentation and that participants used more than twice as many new API methods after accessing suggestions than documentation.",APIs | Examples | Novice programming | Programming support,Conference on Human Factors in Computing Systems - Proceedings,2017-05-02,Conference Paper,"Ichinco, Michelle;Hnin, Wint Yee;Kelleher, Caitlin L.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85022092862,10.2298/CSIS161022009R,REST API example generation using Javadoc,"Formatting and editing documentation can be a tedious process regardless of how well your documentation templates are made. Especially, keeping the code examples up-to-date can be time-consuming and error-prone. The research presented in this article describes a Javadoc extension that can be used to produce example data in combination with automatically generated API method call examples, and explains how the APIs in our implementation are organized to further ease the automatic documentation process. The primary goal is to make generating method call examples for (RESTful) web services easier. The method has been used in the implementation of a media content analysis service, and the experiences, advantages of using the described approach are discussed in this article. The method allows easier validation and maintenance for the documentation of method usage examples with a downside of an increased workload in the implementation of software components required for the automatic documentation process.",Documentation | Examples | Programming techniques | REST,Computer Science and Information Systems,2017-06-01,Article,"Rantanen, Petri",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85025141268,10.1007/978-3-319-58460-7_11,A generic cognitive dimensions questionnaire to evaluate the usability of security APIs,"Programmers use security APIs to embed security into the applications they develop. Security vulnerabilities get introduced into those applications, due to the usability issues that exist in the security APIs. Improving usability of security APIs would contribute to improve the security of applications that programmers develop. However, currently there is no methodology to evaluate the usability of security APIs. In this study, we attempt to improve the Cognitive Dimensions framework based API usability evaluation methodology, to evaluate the usability of security APIs.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Wijayarathna, Chamila;Arachchilage, Nalin A.G.;Slay, Jill",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85019252348,10.1007/978-3-319-56856-0_2,Documentation reuse: Hot or not? an empirical study,"Having available a high quality documentation is critical for software projects. This is why documentation tools such as Javadoc are so popular. As for code, documentation should be reused when possible to increase developer productivity and simplify maintenance. In this paper, we perform an empirical study of duplications in JavaDoc documentation on a corpus of seven famous Java APIs. Our results show that copy-pastes of JavaDoc documentation tags are abundant in our corpus. We also show that these copy-pastes are caused by four different kinds of relations in the underlying source code. In addition, we show that popular documentation tools do not provide any reuse mechanism to cope with these relations. Finally, we make a proposal for a simple but efficient automatic reuse mechanism.",Documentation | Empirical study | Reuse,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Oumaziz, Mohamed A.;Charpentier, Alan;Falleri, Jean Rémy;Blanc, Xavier",Include,
10.1016/j.jss.2022.111515,2-s2.0-85017170567,10.1109/MS.2017.45,CrowdSummarizer: Automated Generation of Code Summaries for Java Programs through Crowdsourcing,"To perform software maintenance, developers must have a relatively good understanding of the program's source code, which is often written by other developers. Code summaries, which describe a program's entities (for example, its methods), help developers better comprehend code more quickly. However, generating code summaries can be challenging. To mitigate this problem, CrowdSummarizer exploits crowdsourcing, gamification, and natural-language processing to automatically generate high-level summaries of Java program methods. Researchers have implemented it as an Eclipse plug-in together with a Web-based code summarization game that can be played by the crowd. Two empirical studies determined that CrowdSummarizer generates quality results. This article is part of a special issue on Crowdsourcing for Software Engineering.",crowdsourcing | CrowdSummarizer | gamification | program comprehension | software development | software engineering | source code summarization,IEEE Software,2017-03-01,Article,"Badihi, Sahar;Heydarnoori, Abbas",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85017595863,10.1109/TVCG.2017.2657098,Semantic Entity-Component State Management Techniques to Enhance Software Quality for Multimodal VR-Systems,"Modularity, modifiability, reusability, and API usability are important software qualities that determine the maintainability of software architectures. Virtual, Augmented, and Mixed Reality (VR, AR, MR) systems, modern computer games, as well as interactive human-robot systems often include various dedicated input-, output-, and processing subsystems. These subsystems collectively maintain a real-time simulation of a coherent application state. The resulting interdependencies between individual state representations, mutual state access, overall synchronization, and flow of control implies a conceptual close coupling whereas software quality asks for a decoupling to develop maintainable solutions. This article presents five semantics-based software techniques that address this contradiction: Semantic grounding, code from semantics, grounded actions, semantic queries, and decoupling by semantics. These techniques are applied to extend the well-established entity-component-system (ECS) pattern to overcome some of this pattern's deficits with respect to the implied state access. A walk-through of central implementation aspects of a multimodal (speech and gesture) VR-interface is used to highlight the techniques' benefits. This use-case is chosen as a prototypical example of complex architectures with multiple interacting subsystems found in many VR, AR and MR architectures. Finally, implementation hints are given, lessons learned regarding maintainability pointed-out, and performance implications discussed.",multimodal processing | Real-time interactive systems | software architecture | virtual reality systems,IEEE Transactions on Visualization and Computer Graphics,2017-04-01,Article,"Fischbach, Martin;Wiebusch, Dennis;Latoschik, Marc Erich",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019007478,10.1587/transinf.2016EDL8224,Change-prone Java method prediction by focusing on individual differences in comment density,"This paper focuses on differences in comment densities among individual programmers, and proposes to adjust the conventional code complexity metric (the cyclomatic complexity) by using the abnormality of the comment density. An empirical study with nine popular open source Java products (including 103,246 methods) shows that the proposed metric performs better than the conventional one in predicting change-prone methods; the proposed metric improves the area under the ROC curve (AUC) by about 3.4% on average.",AUC | Change-prone method | Comment density | Individual difference | ROC curve,IEICE Transactions on Information and Systems,2017-05-01,Article,"Burhandenny, Aji Ery;Aman, Hirohisa;Kawahara, Minoru",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85036477048,10.3390/info8040148,Source code documentation generation using program execution,"Automated source code documentation approaches often describe methods in abstract terms, using the words contained in the static source code or code excerpts from repositories. In this paper, we describe DynamiDoc: a simple automated documentation generator based on dynamic analysis. Our representation-based approach traces the program being executed and records string representations of concrete argument values, a return value and a target object state before and after each method execution. Then, for each method, it generates documentation sentences with examples, such as ""When called on [3, 1.2] with element = 3, the object changed to [1.2]"". Advantages and shortcomings of the approach are listed. We also found out that the generated sentences are substantially shorter than the methods they describe. According to our small-scale study, the majority of objects in the generated documentation have their string representations overridden, which further confirms the potential usefulness of our approach. Finally, we propose an alternative, variable-based approach that describes the values of individual member variables, rather than the state of an object as a whole.",Documentation generation | Dynamic analysis | Examples | Method documentation | Source code summarization,Information (Switzerland),2017-11-17,Article,"Sulír, Matúš;Porubän, Jaroslav",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85019261120,10.1142/S0218843017420011,Semantic Analysis of RESTful APIs for the Detection of Linguistic Patterns and Antipatterns,"Identifier lexicon may have a direct impact on software understandability and reusability and, thus, on the quality of the final software product. Understandability and reusability are two important characteristics of software quality. REpresentational State Transfer (REST) style is becoming a de facto standard adopted by software organizations to build their Web applications. Understandable and reusable Uniform Resource Identifers (URIs) are important to attract client developers of RESTful APIs because good URIs support the client developers to understand and reuse the APIs. Consequently, the use of proper lexicon in RESTful APIs has also a direct impact on the quality of Web applications that integrate these APIs. Linguistic antipatterns represent poor practices in the naming, documentation, and choice of identifiers in the APIs as opposed to linguistic patterns that represent the corresponding best practices. In this paper, we present the Semantic Analysis of RESTful APIs (SARA) approach that employs both syntactic and semantic analyses for the detection of linguistic patterns and antipatterns in RESTful APIs. We provide detailed definitions of 12 linguistic patterns and antipatterns and define and apply their detection algorithms on 18 widely-used RESTful APIs, including Facebook, Twitter, and Dropbox. Our detection results show that linguistic patterns and antipatterns do occur in major RESTful APIs in particular in the form of poor documentation practices. Those results also show that SARA can detect linguistic patterns and antipatterns with higher accuracy compared to its state-of-the-art approach - DOLAR.",detection | documentation | Latent Dirichlet Allocation (LDA) | linguistic antipatterns | patterns | RESTful APIs | second-order similarity | semantic analysis,International Journal of Cooperative Information Systems,2017-06-01,Article,"Palma, Francis;Gonzalez-Huerta, Javier;Founi, Mohamed;Moha, Naouel;Tremblay, Guy;Guéhéneuc, Yann Gaël",Include,
10.1016/j.jss.2022.111515,2-s2.0-85021095186,10.1142/S0218194017500267,Learning Frameworks in a Social-Intensive Knowledge Environment-An Empirical Study,"Application frameworks are a powerful technique for large-scale reuse, but require a considerable effort to understand them. Good documentation is costly, as it needs to address different audiences with disparate learning needs. When code and documentation prove insufficient, developers turn to their network of experts. Nevertheless, this proves difficult, mainly due to the lack of expertise awareness (who to ask), wasteful interruptions of the wrong people and unavailability (either due to intrusion or time constraints). The DRIVER platform is a collaborative learning environment where framework users can, in a non-intrusive way, store and share their learning knowledge while following the best practices of framework understanding (patterns). Developed by the authors, it provides a framework documentation repository, mounted on a wiki, where the learning paths of the community of learners can be captured, shared, rated, and recommended. Combining these social activities, the DRIVER platform promotes collaborative learning, mitigating intrusiveness, unavailability of experts and loss of tacit knowledge. This paper presents the assessment of DRIVER using a controlled academic experiment that measured the performance, effectiveness and framework knowledge intake of MSc students. The study concluded that, especially for novice learners, the platform allows for a faster and more effective learning process.",Collaborative learning | frameworks | tools,International Journal of Software Engineering and Knowledge Engineering,2017-06-01,Article,"Flores, Nuno;Aguiar, Ademar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85032484430,10.1145/3131704.3131714,Automatically generating task-oriented API learning guide,"Learning and reusing open source API libraries remain a time consuming process due to the documentation quality and the knowledge gap between API providers and users. Some researchers and API providers have found that the development tasks would narrow the knowledge gap and meet the needs of busy developers. To our knowledge, there is no existing work to generating task oriented API documents. In this paper, we propose an automatic approach to generating task oriented API learning guide. The guide is organized by a hierarchical task list. We integrate the natural language processing techniques with an evidence-based filtering pipeline in our approach. We also employ a graph-based clustering procedure to generate a three-layer task list. Furthermore, we define the normal form of the task phrases as the metadata in our approach. The approach has been implemented as a tool, APITasks. We used it to generate the API documents for four libraries. In an empirical study, we evaluate the accuracy and completeness of our approach with the manually created benchmarks. The results affirm the capability of our approach.",API Documentation | Development Tasks | Natural Language Processing | Software Reuse | Stack Overflow,ACM International Conference Proceeding Series,2017-09-23,Conference Paper,"Zhu, Zixiao;Hua, Chenyan;Zou, Yanzhen;Xie, Bing;Zhao, Junfeng",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85013387683,10.1002/smr.1845,TMAP: Discovering relevant API methods through text mining of API documentation,"Developers often migrate their applications to support various platform/programming-language application programming interfaces (APIs) to retain existing users and to attract new users. To migrate an application written using 1 API (source) to another API (target), a developer must know how the methods in the source API map to the methods in the target API. Given that a typical platform or language exposes a large number of API methods, manually discovering API mappings is prohibitively resource-intensive and may be error prone. The goal of this research is to support software developers in migrating an application from a source API to a target API by automatically discovering relevant method mappings across APIs using text mining on the natural language API method descriptions. This paper proposes text mining based approach (TMAP) to discover relevant API mappings. To evaluate our approach, we used TMAP to discover API mappings for 15 classes across (1) Java and C# API; and (2) Java ME and Android API. We compared the discovered mappings with state-of-the-art source code analysis-based approaches: Rosetta and StaMiner. Our results indicate that TMAP on average found relevant mappings for 56% and 57% more methods compared to the Rosetta and the StaMiner approaches, respectively.",API documents | API mappings | text mining,Journal of Software: Evolution and Process,2017-12-01,Conference Paper,"Pandita, Rahul;Jetley, Raoul;Sudarsan, Sithu;Menzies, Timothy;Williams, Laurie",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85020700418,10.1016/j.jvlc.2016.07.005,Designing an API at an appropriate abstraction level for programming social robot applications,"Whilst robots are increasingly being deployed as social agents, it is still difficult to program them to interact socially. To create usable tools for programming these robots, tool developers need to know what abstraction levels are appropriate for programming social robot applications. We explore this through the iterative design and evaluation of an API for programming social robots. The results show that high level primitives, with a close mapping to social interaction, are suitable for programming social robot applications. However, the abstraction level should not be so high that it takes away too much control from programmers. This has the potential to enable programmers to produce high quality social robot applications with less programming effort.",API | Application programming interfaces | Cognitive dimensions | Design | Human robot interaction | Humanoid robot | Social robot interaction | Usability,Journal of Visual Languages and Computing,2017-04-01,Article,"Diprose, James;MacDonald, Bruce;Hosking, John;Plimmer, Beryl",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85055501596,10.1145/3131151.3131173,Comprehensibility of Heterogeneous Configuration Knowledge: A User Study,"An essential part of configurable software systems is the configuration knowledge (CK), which is used by developers to derive customized products. Proper comprehension of the CK is key to achieve correct product customization. However, each configurable system is often built with heterogeneous software technologies, such as diverse types of frameworks. These technologies impose the use of additional concepts on the definition of the CK, thereby making it harder to comprehend. This paper presents a domain specific modeling technique aimed at improving the comprehensibility of the CK in framework-based configurable systems. We also present the results of a user study that compares domain-specific modeling with two other state-of-the-art CK techniques, namely annotation-based and general-purpose modeling. We analyzed the impact of these techniques on the CK comprehensibility across three configurable systems. The results strongly suggest that: (i) domain-specific models improve the CK comprehensibility; (ii) the use of general purpose models imposes significant obstacles to the CK comprehension; and (iii) participants who worked with domain-specific models rated the comprehension tasks as easier than those that worked without these models.",Configurable Software Systems | Configuration Knowledge,ACM International Conference Proceeding Series,2017-09-20,Conference Paper,"Cirilo, Elder;Cafeo, Bruno;Nunes, Ingrid;Garcia, Alessandro;Kulesza, Uirá;Lucena, Carlos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/s11432-017-9402-3,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85032632180,10.4230/OASIcs.SLATE.2017.3,Generating method documentation using concrete values from executions,"There exist multiple automated approaches of source code documentation generation. They often describe methods in abstract terms, using the words contained in the static source code or code excerpts from repositories. In this paper, we introduce DynamiDoc – a simple yet e ective automated documentation approach based on dynamic analysis. It traces the program being executed and records string representations of concrete argument values, a return value, and a target object state before and after each method execution. Then for every concerned method, it generates documentation sentences containing examples, such as “When called on [3, 1.2] with element = 3, the object changed to [1.2]”. A qualitative evaluation is performed, listing advantages and shortcomings of the approach.",Documentation generation | Dynamic analysis | Examples | Methods | Source code summarization,OpenAccess Series in Informatics,2017-10-01,Conference Paper,"Sulír, Matúš;Porubän, Jaroslav",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1109/WAPI.2017..5,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/WAPI.2017..3,,,,,,,,Exclude,Not a full paper
10.1016/j.jss.2022.111515,,10.14361/dcs-2017-0211,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/978-3-030-32861-0_6,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85015804869,10.1007/978-3-319-51472-7_2,A replicated study on relationship between code quality and method comments,"Context: Recent studies empirically revealed a relationship between source code comments and code quality. Some studies showed well-written source code comments could be a sign of problematic methods. Other studies also show that source code files with comments confessing a technical debt (called self-admitted technical debt, SATD) could be fixed more times. The former studies only considered the amount of comments, and their findings might be due to a specific type of comments, namely, SATD comments used in the latter studies. Objective: Toclarify the relationship between comments other than SATD comments and code quality. Method: Replicate a part of the latter studies with such comments of methods on four OSS projects. Results: At both the file-level and the method-level, the presence of comments could be related to more code fixings even if the comments were not SATD comments. However, SATD comments were more effective to spot fix-prone files and methods than the non-SATD comments. Conclusions: Source code comments other than SATD comments could still be a sign of problematic code. This study demonstrates a need for further analysis on the contents of comments and its relation to code quality.",Self-admitted technical debt | Software quality | Source code comment,Studies in Computational Intelligence,2017-01-01,Conference Paper,"Miyake, Yuto;Amasaki, Sousuke;Aman, Hirohisa;Yokogawa, Tomoyuki",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85018418614,10.1109/SANER.2017.7884616,Historical and impact analysis of API breaking changes: A large-scale study,"Change is a routine in software development. Like any system, libraries also evolve over time. As a consequence, clients are compelled to update and, thus, benefit from the available API improvements. However, some of these API changes may break contracts previously established, resulting in compilation errors and behavioral changes. In this paper, we study a set of questions regarding API breaking changes. Our goal is to measure the amount of breaking changes on real-world libraries and its impact on clients at a large-scale level. We assess (i) the frequency of breaking changes, (ii) the behavior of these changes over time, (iii) the impact on clients, and (iv) the characteristics of libraries with high frequency of breaking changes. Our large-scale analysis on 317 real-world Java libraries, 9K releases, and 260K client applications shows that (i) 14.78% of the API changes break compatibility with previous versions, (ii) the frequency of breaking changes increases over time, (iii) 2.54% of their clients are impacted, and (iv) systems with higher frequency of breaking changes are larger, more popular, and more active. Based on these results, we provide a set of lessons to better support library and client developers in their maintenance tasks.",API Stability | API Usage | Backwards Compatibility | Software Evolution,"SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",2017-03-21,Conference Paper,"Xavier, Laerte;Brito, Aline;Hora, Andre;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,Not a peer reviewed paper or is a pre-print
10.1016/j.jss.2022.111515,2-s2.0-85034607519,10.1007/978-3-319-69926-4_41,Non-functional requirements documentation in agile software development: Challenges and solution proposal,"Non-functional requirements (NFRs) are determinant for the success of software projects. However, they are characterized as hard to define, and in agile software development (ASD), are often given less priority and usually not documented. In this paper, we present the findings of the documentation practices and challenges of NFRs in companies utilizing ASD and propose guidelines for enhancing NFRs documentation in ASD. We interviewed practitioners from four companies and identified that epics, features, user stories, acceptance criteria, Definition of Done (DoD), product and sprint backlogs are used for documenting NFRs. Wikis, word documents, mockups and spreadsheets are also used for documenting NFRs. In smaller companies, NFRs are communicated through white board and flip chart discussions and developers’ tacit knowledge is prioritized over documentation. However, loss of traceability of NFRs, the difficulty in comprehending NFRs by new developers joining the team and limitations of documentation practices for NFRs are challenges in ASD. In this regard, we propose guidelines for documenting NFRs in ASD. The proposed guidelines consider the diversity of the NFRs to document and suggest different representation artefacts depending on the NFRs scope and level of detail. The representation artefacts suggested are among those currently used in ASD in order not to introduce new specific ones that might hamper actual adoption by practitioners.",Agile software development | NFR | Non-functional requirements | Non-functional requirements documentation | Quality requirements,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Behutiye, Woubshet;Karhapää, Pertti;Costal, Dolors;Oivo, Markku;Franch, Xavier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056560584,10.1145/3238147.3238212,Is this class thread-safe? Inferring documentation using graph-based learning,"Thread-safe classes are pervasive in concurrent, object-oriented software. However, many classes lack documentation regarding their safety guarantees under multi-threaded usage. This lack of documentation forces developers who use a class in a concurrent program to either carefully inspect the implementation of the class, to conservatively synchronize all accesses to it, or to optimistically assume that the class is thread-safe. To overcome the lack of documentation, we present TSFinder, an approach to automatically classify classes as supposedly thread-safe or thread-unsafe. The key idea is to combine a lightweight static analysis that extracts a graph representation from classes with a graph-based classifier. After training the classifier with classes known to be thread-safe and thread-unsafe, it achieves an accuracy of 94.5% on previously unseen classes, enabling the approach to infer thread safety documentation with high confidence. The classifier takes about 3 seconds per class, i.e., it is efficient enough to infer documentation for many classes.",Graph-based learning | Inferring documentation | Thread-safe class,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Habib, Andrew;Pradel, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056534298,10.1145/3238147.3238201,Tackling combinatorial explosion: A study of industrial needs and practices for analyzing highly configurable systems,"Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and followup interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.",Analysis | Highly configurable systems | Software product lines,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Mukelabai, Mukelabai;Nešić, Damir;Maro, Salome;Berger, Thorsten;Steghöfer, Jan Philipp",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056528510,10.1145/3238147.3238185,Understanding and detecting evolution-induced compatibility issues in android apps,"The frequent release of Android OS and its various versions bring many compatibility issues to Android Apps. This paper studies and addresses such evolution-induced compatibility problems. We conduct an extensive empirical study over 11 different Android versions and 4,936 Android Apps. Our study shows that there are drastic API changes between adjacent Android versions, with averagely 140.8 new types, 1,505.6 new methods, and 979.2 new fields being introduced in each release. However, the Android Support Library (provided by the Android OS) only supports less than 23% of the newly added methods, with much less support for new types and fields. As a result, 91.84% of Android Apps write additional code to support different OS versions. Furthermore, 88.65% of the supporting codes share a common pattern, which directly compares variable android.os.Build.VERSION.SDK_INT with a constant version number, to use an API of particular versions. Based on our findings, we develop a new tool called IctApiFinder, to detect incompatible API usages in Android applications. IctApiFinder effectively computes the OS versions on which an API may be invoked, using an inter-procedural data-flow analysis framework. It detects numerous incompatible API usages in 361 out of 1,425 Apps. Compared to Android Lint, IctApiFinder is sound and able to reduce the false positives by 82.1%. We have reported the issues to 13 Apps developers. At present, 5 of them have already been confirmed by the original developers and 3 of them have already been fixed.",,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"He, Dongjie;Zheng, Hengjie;Li, Lian;Li, Guangwei;Wang, Lei;Xue, Jingling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056511900,10.1145/3238147.3238193,Characterizing the natural language descriptions in software logging statements,"Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released.",Empirical study | Logging | Natural language processing,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"He, Pinjia;He, Shilin;Chen, Zhuangbin;Lyu, Michael R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056532155,10.1145/3238147.3238173,Assessing the type annotation burden,"Type annotations provide a link between program variables and domain-specific types. When combined with a type system, these annotations can enable early fault detection. For type annotations to be cost-effective in practice, they need to be both accurate and affordable for developers. We lack, however, an understanding of how burdensome type annotation is for developers. Hence, this work explores three fundamental questions: 1) how accurately do developers make type annotations; 2) how long does a single annotation take; and, 3) if a system could automatically suggest a type annotation, how beneficial to accuracy are correct suggestions and how detrimental are incorrect suggestions? We present results of a study of 71 programmers using 20 random code artifacts that contain variables with physical unit types that must be annotated. Subjects choose a correct type annotation only 51% of the time and take an average of 136 seconds to make a single correct annotation. Our qualitative analysis reveals that variable names and reasoning over mathematical operations are the leading clues for type selection. We find that suggesting the correct type boosts accuracy to 73%, while making a poor suggestion decreases accuracy to 28%. We also explore what state-of-the-art automated type annotation systems can and cannot do to help developers with type annotations, and identify implications for tool developers.",Abstract type inference | Dimensional analysis | Physical units | Program analysis | Robotic systems | Static analysis | Type checking | Unit consistency,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Ore, John Paul;Detweiler, Carrick;Elbaum, Sebastian;Karkazis, Lambros",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056546946,10.1145/3238147.3238177,ContractFuzzer: Fuzzing smart contracts for vulnerability detection,"Decentralized cryptocurrencies feature the use of blockchain to transfer values among peers on networks without central agency. Smart contracts are programs running on top of the blockchain consensus protocol to enable people make agreements while minimizing trusts. Millions of smart contracts have been deployed in various decentralized applications. The security vulnerabilities within those smart contracts pose significant threats to their applications. Indeed, many critical security vulnerabilities within smart contracts on Ethereum platform have caused huge financial losses to their users. In this work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs based on the ABI specifications of smart contracts, defines test oracles to detect security vulnerabilities, instruments the EVM to log smart contracts runtime behaviors, and analyzes these logs to report security vulnerabilities. Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities with high precision. In particular, our fuzzing tool successfully detects the vulnerability of the DAO contract that leads to $60 million loss and the vulnerabilities of Parity Wallet that have led to the loss of $30 million and the freezing of $150 million worth of Ether.",Blockchain | Ethereum | Fuzzer | Fuzzing | Smart contract | Test oracle | Vulnerability,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Jiang, Bo;Liu, Ye;Chan, W. K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056537700,10.1145/3238147.3238191,API method recommendation without worrying about the TASK-API knowledge gap,"Developers often need to search for appropriate APIs for their programming tasks. Although most libraries have API reference documentation, it is not easy to find appropriate APIs due to the lexical gap and knowledge gap between the natural language description of the programming task and the API description in API documentation. Here, the lexical gap refers to the fact that the same semantic meaning can be expressed by different words, and the knowledge gap refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes, which are usually the key information in the task description. In this paper, we propose an API recommendation approach named BIKER (Bi-Information source based KnowledgE Recommendation) to tackle these two gaps. To bridge the lexical gap, BIKER uses word embedding technique to calculate the similarity score between two text descriptions. Inspired by our survey findings that developers incorporate Stack Overflow posts and API documentation for bridging the knowledge gap, BIKER leverages Stack Overflow posts to extract candidate APIs for a program task, and ranks candidate APIs by considering the query's similarity with both Stack Overflow posts and API documentation. It also summarizes supplementary information (e.g., API description, code examples in Stack Overflow posts) for each API to help developers select the APIs that are most relevant to their tasks. Our evaluation with 413 API-related questions confirms the effectiveness of BIKER for both class- and method-level API recommendation, compared with state-of-the-art baselines. Our user study with 28 Java developers further demonstrates the practicality of BIKER for API search.",API documentation | API recommendation | Stack overflow | Word embedding,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Huang, Qiao;Xia, Xin;Xing, Zhenchang;Lo, David;Wang, Xinyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056517495,10.1145/3238147.3238226,A unified lattice model and framework for purity analyses,"Analyzing methods in object-oriented programs whether they are side-effect free and also deterministic, i.e., mathematically pure, has been the target of extensive research. Identifying such methods helps to find code smells and security related issues, and also helps analyses detecting concurrency bugs. Pure methods are also used by formal verification approaches as the foundations for specifications and proving the pureness is necessary to ensure correct specifications. However, so far no common terminology exists which describes the purity of methods. Furthermore, some terms (e.g., pure or side-effect free) are also used inconsistently. Further, all current approaches only report selected purity information making them only suitable for a smaller subset of the potential use cases. In this paper, we present a fine-grained unified lattice model which puts the purity levels found in the literature into relation and which adds a new level that generalizes existing definitions. We have also implemented a scalable, modularized purity analysis which produces significantly more precise results for real-world programs than the best-performing related work. The analysis shows that all defined levels are found in real-world projects.",Java | Lattice | Purity | Side-effects | Static analysis,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Helm, Dominik;Kübler, Florian;Eichberg, Michael;Reif, Michael;Mezini, Mira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056544271,10.1145/3238147.3238190,Neural-machine-translation-based commit message generation: How far are we?,"Commit messages can be regarded as the documentation of software changes. These messages describe the content and purposes of changes, hence are useful for program comprehension and software maintenance. However, due to the lack of time and direct motivation, commit messages sometimes are neglected by developers. To address this problem, Jiang et al. proposed an approach (we refer to it as NMT), which leverages a neural machine translation algorithm to automatically generate short commit messages from code. The reported performance of their approach is promising, however, they did not explore why their approach performs well. Thus, in this paper, we first perform an in-depth analysis of their experimental results. We find that (1) Most of the test diffs from which NMT can generate high-quality messages are similar to one or more training diffs at the token level. (2) About 16% of the commit messages in Jiang et al.'s dataset are noisy due to being automatically generated or due to them describing repetitive trivial changes. (3) The performance of NMT declines by a large amount after removing such noisy commit messages. In addition, NMT is complicated and time-consuming. Inspired by our first finding, we proposed a simpler and faster approach, named NNGen (Nearest Neighbor Generator), to generate concise commit messages using the nearest neighbor algorithm. Our experimental results show that NNGen is over 2,600 times faster than NMT, and outperforms NMT in terms of BLEU (an accuracy measure that is widely used to evaluate machine translation systems) by 21%. Finally, we also discuss some observations for the road ahead for automated commit message generation to inspire other researchers.",Commit message generation | Nearest neighbor algorithm | Neural machine translation,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Liu, Zhongxin;Lo, David;Xia, Xin;Xing, Zhenchang;Hassan, Ahmed E.;Wang, Xinyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056535193,10.1145/3238147.3238206,Improving automatic source code summarization via deep reinforcement learning,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.",Code summarization | Comment generation | Deep learning | Reinforcement learning,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Wan, Yao;Zhao, Zhou;Yang, Min;Xu, Guandong;Ying, Haochao;Wu, Jian;Yu, Philip S.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85056493694,10.1145/3238147.3238220,A genetic algorithm for goal-conflict identification,"Goal-conflict analysis has been widely used as an abstraction for risk analysis in goal-oriented requirements engineering approaches. In this context, where the expected behaviour of the system-to-be is captured in terms of domain properties and goals, identifying combinations of circumstances that may make the goals diverge, i.e., not to be satisfied as a whole, is of most importance. Various approaches have been proposed in order to automatically identify boundary conditions, i.e., formulas capturing goal-divergent situations, but they either apply only to some specific goal expressions, or are affected by scalability issues that make them applicable only to relatively small specifications. In this paper, we present a novel approach to automatically identify boundary conditions, using evolutionary computation. More precisely, we develop a genetic algorithm that, given the LTL formulation of the domain properties and the goals, it searches for formulas that capture divergences in the specification. We exploit a modern LTL satisfiability checker to successfully guide our genetic algorithm to the solutions. We assess our technique on a set of case studies, and show that our genetic algorithm is able to find boundary conditions that cannot be generated by related approaches, and is able to efficiently scale to LTL specifications that other approaches are unable to deal with.",Genetic algorithms | Goal conflicts | LTL satisfiability,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Degiovanni, Renzo;Regis, Germán;Molina, Facundo;Aguirre, Nazareno",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056518167,10.1145/3238147.3238181,Understanding and detecting callback compatibility issues for android applications,"The control flows of Android apps are largely driven by the protocols that govern how callback APIs are invoked in response to various events. When these callback APIs evolve along with the Android framework, the changes in their invocation protocols can induce unexpected control flows to existing Android apps, causing various compatibility issues. We refer to these issues as callback compatibility issues. While Android framework updates have received due attention, little is known about their impacts on app control flows and the callback compatibility issues thus induced. To bridge the gap, we examined Android documentations and conducted an empirical study on 100 real-world callback compatibility issues to investigate how these issues were induced by callback API evolutions. Based on our empirical findings, we propose a graph-based model to capture the control flow inconsistencies caused by API evolutions and devise a static analysis technique, Cider, to detect callback compatibility issues. Our evaluation of Cider on 20 popular open-source Android apps shows that Cider is effective. It detected 13 new callback compatibility issues in these apps, among which 12 issues were confirmed and 9 issues were fixed.",Android API | Callback compatibility | Empirical study | Static analysis,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Huang, Huaxun;Liu, Yepang;Wei, Lili;Cheung, Shing Chi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056529776,10.1145/3238147.3238203,Detecting and summarizing GUI changes in evolving mobile apps,"Mobile applications have become a popular software development domain in recent years due in part to a large user base, capable hardware, and accessible platforms. However, mobile developers also face unique challenges, including pressure for frequent releases to keep pace with rapid platform evolution, hardware iteration, and user feedback. Due to this rapid pace of evolution, developers need automated support for documenting the changes made to their apps in order to aid in program comprehension. One of the more challenging types of changes to document in mobile apps are those made to the graphical user interface (GUI) due to its abstract, pixel-based representation. In this paper, we present a fully automated approach, called Gcat, for detecting and summarizing GUI changes during the evolution of mobile apps. Gcat leverages computer vision techniques and natural language generation to accurately and concisely summarize changes made to the GUI of a mobile app between successive commits or releases. We evaluate the performance of our approach in terms of its precision and recall in detecting GUI changes compared to developer specified changes, and investigate the utility of the generated change reports in a controlled user study. Our results indicate that Gcat is capable of accurately detecting and classifying GUI changes - outperforming developers - while providing useful documentation.",Android | GUI changes | Mobile apps | Software evolution,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Moran, Kevin;Watson, Cody;Hoskins, John;Purnell, George;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056554616,10.1145/3238147.3238174,Safe stream-based programming with refinement types,"In stream-based programming, data sources are abstracted as a stream of values that can be manipulated via callback functions. Stream-based programming is exploding in popularity, as it provides a powerful and expressive paradigm for handling asynchronous data sources in interactive software. However, high-level stream abstractions can also make it difficult for developers to reason about control- and data-flow relationships in their programs. This is particularly impactful when asynchronous stream-based code interacts with thread-limited features such as UI frameworks that restrict UI access to a single thread, since the threading behavior of streaming constructs is often non-intuitive and insufficiently documented. In this paper, we present a type-based approach that can statically prove the thread-safety of UI accesses in stream-based software. Our key insight is that the fluent APIs of stream-processing frameworks enable the tracking of threads via type-refinement, making it possible to reason automatically about what thread a piece of code runs on - a difficult problem in general. We implement the system as an annotation-based Java typechecker for Android programs built upon the popular ReactiveX framework and evaluate its efficacy by annotating and analyzing 8 open-source apps, where we find 33 instances of unsafe UI access while incurring an annotation burden of only one annotation per 186 source lines of code. We also report on our experience applying the typechecker to two much larger apps from the Uber Technologies, Inc. codebase, where it currently runs on every code change and blocks changes that introduce potential threading bugs.",Mobile applications | Refinement types | Stream-based programming,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Stein, Benno;Sridharan, Manu;Clapp, Lazaro;Chang, Bor Yuh Evan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056457238,10.1145/3238147.3238195,Domain-independent multi-threaded software model checking,"Recent development of software aims at massively parallel execution, because of the trend to increase the number of processing units per CPU socket. But many approaches for program analysis are not designed to benefit from a multi-threaded execution and lack support to utilize multi-core computers. Rewriting existing algorithms is difficult and error-prone, and the design of new parallel algorithms also has limitations. An orthogonal problem is the granularity: computing each successor state in parallel seems too fine-grained, so the open question is to find the right structural level for parallel execution. We propose an elegant solution to these problems: Block summaries should be computed in parallel. Many successful approaches to software verification are based on summaries of control-flow blocks, large blocks, or function bodies. Block-abstraction memoization is a successful domain-independent approach for summary-based program analysis. We redesigned the verification approach of block-abstraction memoization starting from its original recursive definition, such that it can run in a parallel manner for utilizing the available computation resources without losing its advantages of being independent from a certain abstract domain. We present an implementation of our new approach for multi-core shared-memory machines. The experimental evaluation shows that our summary-based approach has no significant overhead compared to the existing sequential approach and that it has a significant speedup when using multi-threading.",Block-abstraction memoization | Multi-threading | Parallel algorithm | Program analysis | Software verification,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Beyer, Dirk;Friedberger, Karlheinz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056521780,10.1145/3238147.3238219,Cldiff: Generating concise linked code differences,"Analyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks. In this paper, we propose a code differencing approach, named ClDiff, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of ClDiff is to generate more easily understandable code differences. ClDiff takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declarations from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of ClDiff.",AST | Code differencing | Program comprehension,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Huang, Kaifeng;Zhou, Daihong;Chen, Bihuan;Wang, Ying;Zhao, Wenyun;Peng, Xin;Liu, Yang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056522800,10.1145/3238147.3238164,Characterizing and identifying misexposed activities in android applications,"Exported Activity (EA), a kind of activities in Android apps that can be launched by external components, is one of the most important inter-component communication (ICC) mechanisms to realize the interaction and cooperation among multiple apps. Existing works have pointed out that, once exposed, an activity will be vulnerable to malicious ICC attacks, such as permission leakage attack. Unfortunately, it is observed that a considerable number of activities in commercial apps are exposed inadvertently, while few works have studied the necessity and reasonability of such exposure. This work takes the first step to systematically study the exposing behavior of EAs through analyzing 13,873 Android apps. It utilizes the EA associated call relationships extracted from byte-code via data-flow analysis, as well as the launch conditions obtained from the manifest files, to guide the study on the usage and misexposure of EAs. The empirical findings are that the EA mechanism is widely adopted in development and the activities are liable to be misexposed due to the developers' misunderstanding or carelessness. Further study on subsets of apps selected according to different criteria indicates that the misexposed EAs have specific characteristics, which are manually summarized into six typical misuse patterns. As a consequence, ten heuristics are designed to decide whether an activity should be exposed or not and are implemented into an automatic tool called Mist. Experiments on the collected apps show that around one fifth EAs are unnecessarily exposed and there are more than one third EAs whose exposure may not be suggested.",Android apps | Exported activity | Program analysis,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Yan, Jiwei;Wu, Tianyong;Wang, Ping;Zhang, Jian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056514377,10.1145/3238147.3240468,An evolutionary approach for analyzing alloy specifications,"Formal methods use mathematical notations and logical reasoning to precisely define a program's specifications, from which we can instantiate valid instances of a system. With these techniques we can perform a multitude of tasks to check system dependability. Despite the existence of many automated tools including ones considered lightweight, they still lack a strong adoption in practice. At the crux of this problem, is scalability and applicability to large real world applications. In this paper we show how to relax the completeness guarantee without much loss, since soundness is maintained. We have extended a popular lightweight analysis, Alloy, with a genetic algorithm. Our new tool, EvoAlloy, works at the level of finite relations generated by Kodkod and evolves the chromosomes based on the failed constraints. In a feasibility study we demonstrate that we can find solutions to a set of specifications beyond the scope where traditional Alloy fails. While small specifications take longer with EvoAlloy, the scalability means we can handle larger specifications. Our future vision is that when specifications are small we can maintain both soundness and completeness, but when this fails, EvoAlloy can switch to its genetic algorithm.",,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Wang, Jianghao;Bagheri, Hamid;Cohen, Myra B.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056522575,10.1145/3238147.3240475,The electrum analyzer: Model checking relational first-order temporal specifications,"This paper presents the Electrum Analyzer, a free-software tool to validate and perform model checking of Electrum specifications. Electrum is an extension of Alloy that enriches its relational logic with LTL operators, thus simplifying the specification of dynamic systems. The Analyzer supports both automatic bounded model checking, with an encoding into SAT, and unbounded model checking, with an encoding into SMV. Instance, or counter-example, traces are presented back to the user in a unified visualizer. Features to speed up model checking are offered, including a decomposed parallel solving strategy and the extraction of symbolic bounds. Source code: https://github.com/haslab/ElectrumVideo: https://youtu.be/FbjlpvjgMDA.",Formal specification language | Model checking | Model validation,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Brunel, Julien;Chemouil, David;Cunha, Alcino;Macedo, Nuno",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056560750,10.1145/3238147.3240481,ESBMC 5.0,"ESBMC is a mature, permissively licensed open-source context-bounded model checker for the verification of single- and multi-threaded C programs. It can verify both predefined safety properties (e.g., bounds check, pointer safety, overflow) and user-defined program assertions automatically. ESBMC provides C++ and Python APIs to access internal data structures, allowing inspection and extension at any stage of the verification process. We discuss improvements over previous versions of ESBMC, including the description of new front- and back-ends, IEEE floating-point support, and an improved k-induction algorithm. A demonstration is available at https://www.youtube.com/watch?v=YcJjXHlN1v8.",Bug detection | K-induction | Software model checking,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,2018-09-03,Conference Paper,"Gadelha, Mikhail R.;Cordeiro, Lucas C.;Monteiro, Felipe R.;Fischer, Bernd;Checker, C. Model;Morse, Jeremy;Nicole, Denis A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3173162.3173196,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3173162.3173189,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85061511138,10.1145/3239235.3239239,Comparing techniques for aggregating interrelated replications in software engineering,"Context: Researchers from different groups and institutions are collaborating towards the construction of groups of interrelated replications. Applying unsuitable techniques to aggregate interrelated replications' results may impact the reliability of joint conclusions. Objectives: Comparing the advantages and disadvantages of the techniques applied to aggregate interrelated replications' results in Software Engineering (SE). Method: We conducted a literature review to identify the techniques applied to aggregate interrelated replications' results in SE. We analyze a prototypical group of interrelated replications in SE with the techniques that we identified. We check whether the advantages and disadvantages of each technique - -according to mature experimental disciplines such as medicine - -materialize in the SE context. Results: Narrative synthesis and Aggregation of p-values do not take advantage of all the information contained within the raw-data for providing joint conclusions. Aggregated Data (AD) meta-analysis provides visual summaries of results and allows assessing experiment-level moderators. Individual Participant Data (IPD) meta-analysis allows interpreting results in natural units and assessing experiment-level and participant-level moderators. Conclusion: All the information contained within the raw-data should be used to provide joint conclusions. AD and IPD, when used in tandem, seem suitable to analyze groups of interrelated replications in SE.",AD | Experimentation | IPD | Meta-analysis | Replication,International Symposium on Empirical Software Engineering and Measurement,2018-10-11,Conference Paper,"Santos, Adrian;Juristo, Natalia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85061483075,10.1145/3239235.3267434,On the use of emoticons in open source software development,"Background: Using sentiment analysis to study software developers' behavior comes with challenges such as the presence of a large amount of technical discussion unlikely to express any positive or negative sentiment. However, emoticons provide information about developer sentiments that can easily be extracted from software repositories. Aim: We investigate how software developers use emoticons differently in issue trackers in order to better understand the differences between developers and determine to which extent emoticons can be used as in place of sentiment analysis. Method: We extract emoticons from 1.3M comments from Apache's issue tracker and 4.5M from Mozilla's issue tracker using regular expressions built from a list of emoticons used by SentiStrength and Wikipedia. We check for statistical differences using Mann-Whitney U tests and determine the effect size with Cliff's Δ. Results: Overall Mozilla developers rely more on emoticons than Apache developers. While the overall rate of comments with emoticons is of 1% and 3% for Apache and Mozilla, some individual developers can have a rate up to 21%. Looking specifically at Mozilla developers, we find that western developers use significantly more emoticons (with medium size effect) than eastern developers. While the majority of emoticons are used to express joy, we find that Mozilla developers use emoticons more frequently to express sadness and surprise than Apache developers. Finally, we find that Apache developers use overall more emoticons during weekends than during weekdays, with the share of sad and surprised emoticons increasing during weekends. Conclusions: While emoticons are primarily used to express joy, the more occasional use of sad and surprised emoticons can potentially be utilized to detect frustration in place of sentiment analysis among developers using emoticons frequently enough.",Emoticon usage | Emoticons | Emotions | Open source software development | Repository mining | Sentiment analysis,International Symposium on Empirical Software Engineering and Measurement,2018-10-11,Conference Paper,"Claes, Maëlick;Mäntylä, Mika;Farooq, Umar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058303104,10.1145/3236024.3236054,Be careful of when: An empirical study on time-related misuse of issue tracking data,"Issue tracking data have been used extensively to aid in predicting or recommending software development practices. Issue attributes typically change over time, but users may use data from a separate time of data collection rather than the time of their application scenarios. We, therefore, investigate data leakage, which results from ignoring the chronological order in which the data were produced. Information leaked from the ""future"" makes prediction models misleadingly optimistic. We examine existing literature to confirm the existence of data leakage and reproduce three typical studies (detecting duplicate issues, localizing issues, and predicting issue-fix time) adjusted for appropriate data to quantify the impact of the data leakage. We confirm that 11 out of 58 studies have leakage problem, while 44 are suspected. We observe biased results caused by data leakage while the extent is not striking. Attributes of summary, component, and assignee have the largest impact on the results. Our findings suggest that data users are often unaware of the context of the data being produced. We recommend researchers and practitioners who attempt to utilize issue tracking data to address software development problems to have a full understanding of their application scenarios, the origin and change of the data, and the influential issue attributes to manage data leakage risks.",data leakage | data misuse | Issue tracking data,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Tu, Feifei;Zhu, Jiaxin;Zheng, Qimu;Zhou, Minghui",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058263272,10.1145/3236024.3236059,Path-based function embedding and its application to error-handling specification mining,"Identifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2vec, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2vec at identifying function synonyms in the Linux kernel. Finally, we apply Func2vec to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2vec result in error-handling specifications with high support.",error handling | program analysis | program comprehension | program embeddings | specification mining,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"DeFreez, Daniel;Thakur, Aditya V.;Rubio-Gonzalez, Cindy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058285282,10.1145/3236024.3236036,Complementing global and local contexts in representing api descriptions to improve API retrieval tasks,"When being trained on API documentation and tutorials,Word2vec produces vector representations to estimate the relevance between texts and API elements. However, existing Word2vec-based approaches to measure document similarities aggregateWord2vec vectors of individual words or APIs to build the representation of a document as if the words are independent. Thus, the semantics of API descriptions or code fragments are not well represented. In this work, we introduce D2Vec, a new model that fits with API documentation better than Word2vec. D2Vec is a neural network model that considers two complementary contexts to better capture the semantics of API documentation. We first connect the global context of the current API topic under description to all text phrases within the description of that API. Second, the local orders of words and APIs in the text phrases are maintained in computing the vector representations for the APIs.We conducted an experiment to verify two intrinsic properties of D2Vec's vectors: 1) similar words and relevant API elements are projected into nearby locations; and 2) some vector operations carry semantics. We demonstrate the usefulness and good performance of D2Vec in three applications: API code search (text-to-code retrieval), API tutorial fragment search (code-to-text retrieval), and mining API mappings between software libraries (code-to-code retrieval). Finally, we provide actionable insights and implications for researchers in using our model in other applications with other types of documents.",API documents | API Mappings | Big Code | Code Search | Word2vec,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Nguyen, Thanh;Tran, Ngoc;Phan, Hung;Nguyen, Trong;Truong, Linh;Nguyen, Anh Tuan;Nguyen, Hoan Anh;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058342342,10.1145/3236024.3236076,Stochastic energy optimization for mobile GPS applications,"Mobile applications regularly interact with their noisy and everchanging physical environment. The fundamentally uncertain nature of such interactions leads to significant challenges in energy optimization, a crucial goal of software engineering on mobile devices. This paper presents Aeneas, a novel energy optimization framework for Android in the presence of uncertainty. Aeneas provides a minimalistic programming model where acceptable program behavioral settings are abstracted as knobs and application-specific optimization goals - such as meeting an energy budget - are crystallized as rewards, both of which are directly programmable. At its heart, Aeneas is endowed with a stochastic optimizer to adaptively and intelligently select the reward-optimal knob setting through a form of reinforcement learning. We evaluate Aeneas on mobile GPS applications built over Google LocationService API. Through an in-field case study that covers approximately 6500 miles and 150 hours of driving as well as 20 hours of biking and hiking, we find that Aeneas can effectively and resiliently meet programmerspecified energy budgets in uncertain physical environments where individual GPS readings undergo significant fluctuation. Compared with non-stochastic approaches such as profile-guided optimization, Aeneas produces significantly more stable results across runs.",Energy Management | Stochastic Optimization | Uncertainty,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Canino, Anthony;Liu, Yu David;Masuhara, Hidehiko",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058270875,10.1145/3236024.3264836,Towards data-driven vulnerability prediction for requirements,"Due to the abundance of security breaches we continue to see, the software development community is recently paying attention to a more proactive approach towards security. This includes predicting vulnerability before exploitation employing static code analysis and machine learning techniques. Such mechanisms, however, are designed to detect post-implementation vulnerabilities. As the root of a vulnerability can often be traced back to the requirement specification, and vulnerability discovered later in the development life cycle is more expensive to fix, we need additional preventive mechanisms capable of predicting vulnerability at a much earlier stage. In this paper, we propose a novel framework providing an automated support to predict vulnerabilities for a requirement as early as during requirement engineering. We further present a preliminary demonstration of our framework and the promising results we observe clearly indicate the value of this new research idea.",information retrieval | software requirements | software security | software vulnerability | Traceability | vulnerability prediction,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Imtiaz, Sayem Mohammad;Bhowmik, Tanmay",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058325982,10.1145/3236024.3264838,Software fairness,"A goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem.",software bias | Software fairness | software process,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Brun, Yuriy;Meliou, Alexandra",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058317239,10.1145/3236024.3275535,Efficient static checking of library updates,"Software engineering practices have evolved to the point where a developer writing a new application today doesn't start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries.",api diffs | automated remediation | call graphs | library upgrades | semantic versioning,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Foo, Darius;Chua, Hendy;Yeo, Jason;Ang, Ming Yi;Sharma, Asankhaya",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058345090,10.1145/3236024.3264585,Augmenting stack overflow with api usage patterns mined from GitHub,"Programmers often consult q&A websites such as Stack Overflow (SO) to learn new APIs. However, online code snippets are not always complete or reliable in terms of API usage. To assess online code snippets, we build a Chrome extension, ExampleCheck that detects API usage violations in SO posts using API usage patterns mined from 380K GitHub projects. It quantifies how many GitHub examples follow common API usage and illustrates how to remedy the detected violation in a given SO snippet. With ExampleCheck, programmers can easily identify the pitfalls of a given SO snippet and learn how much it deviates from common API usage patterns in GitHub.",API usage pattern | code assessment | online q&A forum,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Reinhardt, Anastasia;Zhang, Tianyi;Mathur, Mihir;Kim, Miryung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058342937,10.1145/3236024.3264597,DSM: A specification mining tool using recurrent neural network based language model,"Formal specifications are important but often unavailable. Furthermore, writing these specifications is time-consuming and requires skills from developers. In this work, we present Deep Specification Miner (DSM), an automated tool that applies deep learning to mine finite-state automaton (FSA) based specifications. DSM accepts as input a set of execution traces to train a Recurrent Neural Network Language Model (RNNLM). From the input traces, DSM creates a Prefix Tree Acceptor (PTA) and leverages the inferred RNNLM to extract many features. These features are then forwarded to clustering algorithms for merging similar automata states in the PTA for assembling a number of FSAs. Next, our tool performs a model selection heuristic to approximate F-measure of FSAs, and outputs the one with the highest estimated F-measure. Noticeably, our implementation of DSM provides several options that allows users to optimize quality of resultant FSAs.",Deep Learning | Specification Mining,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Le, Tien Duy B.;Bao, Lingfeng;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058282876,10.1145/3236024.3264595,INFAR: Insight extraction from app reviews,"App reviews play an essential role for users to convey their feedback about using the app. The critical information contained in app reviews can assist app developers for maintaining and updating mobile apps. However, the noisy nature and large-quantity of daily generated app reviews make it difficult to understand essential information carried in app reviews. Several prior studies have proposed methods that can automatically classify or cluster user reviews into a few app topics (e.g., security). These methods usually act on a static collection of user reviews. However, due to the dynamic nature of user feedback (i.e., reviews keep coming as new users register or new app versions being released) and multiple analysis dimensions (e.g., review quantity and user rating), developers still need to spend substantial effort in extracting contrastive information that can only be teased out by comparing data from multiple time periods or analysis dimensions. This is needed to answer questions such as: what kind of issues users are experiencing most? is there an unexpected rise in a particular kind of issue? etc. To address this need, in this paper, we introduce INFAR, a tool that automatically extracts INsights From App Reviews across time periods and analysis dimensions, and presents them in natural language supported by an interactive chart. The insights INFAR extracts include several perspectives: (1) salient topics (i.e., issue topics with significantly lower ratings), (2) abnormal topics (i.e., issue topics that experience a rapid rise in volume during a time period), (3) correlations between two topics, and (4) causal factors to rating or review quantity changes. To evaluate our tool, we conduct an empirical evaluation by involving six popular apps and 12 industrial practitioners, and 92% (11/12) of them approve the practical usefulness of the insights summarized by INFAR.",App review | insight extraction | review topic,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Gao, Cuiyun;Zeng, Jichuan;Lo, David;Lin, Chin Yew;Lyu, Michael R.;King, Irwin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058298293,10.1145/3236024.3275426,Moving towards objective measures of program comprehension,"Traditionally, program comprehension research relies heavily on indirect measures of comprehension, where subjects report on their own comprehension levels or summarize part of an artifact so that researchers can instead deduce the level of comprehension. However, there are several potential issues that can result from using these indirect measures because they are prone to participant biases and implicitly deduce comprehension based on various factors. The proposed research presents a framework to move towards more objective measures of program comprehension through the use of brain imaging and eye tracking technology. We aim to shed light on how the human brain processes comprehension tasks, specifically what aspects of the source code cause measurable increases in the cognitive load of developers in both bug localization tasks, as well as code reviews. We discuss the proposed methodology, preliminary results, and overall contributions of the work.",Biometrics | Cognitive Load | Eyetracking | fNIRS | Program Comprehension | Source Code Lexicon,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"Fakhoury, Sarah",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058320416,10.1145/3236024.3275440,Mining error-handling specifications for systems software,"This paper presents a technique for mining error-handling specifications from systems software. It presents a static analysis for detecting error handlers in low-level code, and it shows how function synonyms can be used to mine for error-handling specifications with only a few supporting examples.",error handling | program analysis | program comprehension | program embeddings | specification mining,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2018-10-26,Conference Paper,"DeFreez, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051634711,10.1145/3196321.3196350,Un-break my build: Assisting developers with build repair hints,"Continuous integration is an agile software development practice. Instead of integrating features right before a release, they are constantly being integrated in an automated build process. This shortens the release cycle, improves software quality, and reduces time to market. However, the whole process will come to a halt when a commit breaks the build, which can happen for several reasons, e.g., compilation errors or test failures, and fixing the build suddenly becomes a top priority. Developers not only have to find the cause of the build break and fix it, but they have to be quick in all of it to avoid a delay for others. Unfortunately, these steps require deep knowledge and are often time consuming. To support developers in fixing a build break, we propose Bart, a tool that summarizes the reasons of the build failure and suggests possible solutions found on the Internet. We will show in a case study with eight participants that developersfind Bart useful to understand build breaks and that using Bart substantially reduces the time to fix a build break, on average by 41%.",agile software development | build break | error recovery | software development tools | software engineering | summarization,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Vassallo, Carmine;Proksch, Sebastian;Zemp, Timothy;Gall, Harald C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051633907,10.1145/3196321.3196343,Hierarchical abstraction of execution traces for program comprehension,"Understanding the dynamic behavior of a software system is one of the most important and time-consuming tasks for today's software maintainers. In practice, understanding the inner workings of software requires studying the source code and documentation and inserting logging code in order to map high-level descriptions of the program behavior with low-level implementation, i.e., the source code. Unfortunately, for large codebases and large log files, such cognitive mapping can be quite challenging. To bridge the cognitive gap between the source code and detailed models of program behavior, we propose a fully automatic approach to present a semantic abstraction with different levels of functional granularity from full execution traces. Our approach builds multi-level abstractions and identifies frequent behaviors at each level based on a number of execution traces, and then, it labels phases within individual execution traces according to the identified major functional behaviors of the system. To validate our approach, we conducted a case study on a large-scale subject program, Javac, to demonstrate the effectiveness of the mining result. Furthermore, the results of a user study demonstrate that our approach is capable of presenting users a high-level comprehensible abstraction of execution behavior. Based on a real world subject program the participants in our user study were able to achieve a mean accuracy of 70%.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Feng, Yang;Dreef, Kaj;Jones, James A.;Van Deursen, Arie",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051680607,10.1145/3196321.3196335,Recognizing software bug-specific named entity in software bug repository,"Software bug issues are unavoidable in software development and maintenance. In order to manage bugs effectively, bug tracking systems are developed to help to record, manage and track the bugs of each project. The rich information in the bug repository provides the possibility of establishment of entity-centric knowledge bases to help understand and fix the bugs. However, existing named entity recognition (NER) systems deal with text that is structured, formal, well written, with a good grammatical structure and few spelling errors, which cannot be directly used for bug-specific named entity recognition. For bug data, they are free-form texts, which include a mixed language studded with code, abbreviations and software-specific vocabularies. In this paper, we summarize the characteristics of bug entities, propose a classification method for bug entities, and build a baseline corpus on two open source projects (Mozilla and Eclipse). On this basis, we propose an approach for bug-specific entity recognition called BNER with the Conditional Random Fields (CRF) model and word embedding technique. An empirical study is conducted to evaluate the accuracy of our BNER technique, and the results show that the two designed baseline corpus are suitable for bug-specific named entity recognition, and our BNER approach is effective on cross-projects NER.",CRF model | named entity recognition | software bug | software bug corpus | word embedding,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Zhou, Cheng;Li, Bin;Sun, Xiaobing;Guo, Hongjing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051622992,10.1145/3196321.3196326,Unsupervised deep bug report summarization,"Bug report summarization is an effective way to reduce the considerable time in wading through numerous bug reports. Although some supervised and unsupervised algorithms have been proposed for this task, their performance is still limited, due to the particular characteristics of bug reports, including the evaluation behaviours in bug reports, the diverse sentences in software language and natural language, and the domain-specific predefined fields. In this study, we conduct the first exploration of the deep learning network on bug report summarization. Our approach, called DeepSum, is a novel stepped auto-encoder network with evaluation enhancement and predefined fields enhancement modules, which successfully integrates the bug report characteristics into a deep neural network. DeepSum is unsupervised. It significantly reduces the efforts on labeling huge training sets. Extensive experiments show that DeepSum outperforms the comparative algorithms by up to 13.2% and 9.2% in terms of F-score and Rouge-n metrics respectively over the public datasets, and achieves the state-of-the-art performance. Our work shows promising prospects for deep learning to summarize millions of bug reports.",bug report summarization | deep learning | mining software repositories | unsupervised learning,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Li, Xiaochen;Jiang, He;Liu, Dong;Ren, Zhilei;Li, Ge",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051637000,10.1145/3196321.3196328,Logtracker: Learning log revision behaviors proactively from software evolution history,"Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted.",failure diagnose | log revision | software evolution,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Li, Shanshan;Niu, Xu;Jia, Zhouyang;Wang, Ji;He, Haochen;Wang, Teng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051656437,10.1145/3196321.3196349,Identifying software components from object-oriented APIs based on dynamic analysis,"The reuse at the component level is generally more effective than the one at the object-oriented class level. This is due to the granularity level where components expose their functionalities at an abstract level compared to the fine-grained object-oriented classes. Moreover, components clearly define their dependencies through their provided and required interfaces in an explicit way that facilitates the understanding of how to reuse these components. Therefore, several component identification approaches have been proposed to identify components based on the analysis object-oriented software applications. Nevertheless, most of the existing component identification approaches did not consider co-usage dependencies between API classes to identify classes/methods that can be reused to implement a specific scenario. In this paper, we propose an approach to identify reusable software components in object-oriented APIs, based on the interactions between client applications and the targeted API. As we are dealing with actual clients using the API, dynamic analysis allows to better capture the instances of API usage. Approaches using static analysis are usually limited by the difficulty of handling dynamic features such as polymorphism and class loading. We evaluate our approach by applying it to three Java APIs with eight client applications from the DaCapo benchmark. DaCapo provides a set of pre-defined usage scenarios. The results show that our component identification approach has a very high precision.",dynamic analysis | object-oriented APIs | reuse | reverse engineering | software components | source code | understandability,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Shatnawi, Anas;Shatnawi, Hudhaifa;Saied, Mohamed Aymen;Shara, Zakarea Al;Sahraoui, Houari;Seriai, Abdelhak",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051643976,10.1145/3196321.3196334,Deep code comment generation,"During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.",comment generation | deep learning | program comprehension,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Hu, Xing;Li, Ge;Xia, Xin;Lo, David;Jin, Zhi",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85051626965,10.1145/3196321.3196344,Classification of APIs by hierarchical clustering,"APIs can be classified according to the programming domains (e.g., GUIs, databases, collections, or security) that they address. Such classification is vital in searching repositories (e.g., the Maven Central Repository for Java) and for understanding the technology stack used in software projects. We apply hierarchical clustering to a curated suite of Java APIs to compare the computed API clusters with preexisting API classifications. Clustering entails various parameters (e.g., the choice of IDF versus LSI versus LDA). We describe the corresponding variability in terms of a feature model. We exercise all possible configurations to determine the maximum correlation with respect to two baselines: i) a smaller suite of APIs manually classified in previous research; ii) a larger suite of APIs from the Maven Central Repository, thereby taking advantage of crowd-sourced classification while relying on a threshold-based approach for identifying important APIs and versions thereof, subject to an API dependency analysis on GitHub. We discuss the configurations found in this way and we examine the influence of particular features on the correlation between computed clusters and baselines. To this end, we also leverage interactive exploration of the parameter space and the resulting dendrograms. In this manner, we can also identify issues with the use of classifiers (e.g., missing classifiers) in the baselines and limitations of the clustering approach.",APIs | clustering exploration | feature modeling | github | hierarchical clustering | maven central repository,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Härtel, Johannes;Aksu, Hakan;Lämmel, Ralf",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051652360,10.1145/3196321.3196347,The effect of poor source code lexicon and readability on developers' cognitive load,"It has been well documented that a large portion of the cost of any software lies in the time spent by developers in understanding a program's source code before any changes can be undertaken. One of the main contributors to software comprehension, by subsequent developers or by the authors themselves, has to do with the quality of the lexicon, (i.e., the identifiers and comments) that is used by developers to embed domain concepts and to communicate with their teammates. In fact, previous research shows that there is a positive correlation between the quality of identifiers and the quality of a software project. Results suggest that poor quality lexicon impairs program comprehension and consequently increases the effort that developers must spend to maintain the software. However, we do not yet know or have any empirical evidence, of the relationship between the quality of the lexicon and the cognitive load that developers experience when trying to understand a piece of software. Given the associated costs, there is a critical need to empirically characterize the impact of the quality of the lexicon on developers' ability to comprehend a program. In this study, we explore the effect of poor source code lexicon and readability on developers' cognitive load as measured by a cutting-edge and minimally invasive functional brain imaging technique called functional Near Infrared Spectroscopy (fNIRS). Additionally, while developers perform software comprehension tasks, we map cognitive load data to source code identifiers using an eye tracking device. Our results show that the presence of linguistic antipatterns in source code significantly increases the developers' cognitive load.",biometrics | cognitive load | eyetracking | fNIRS | program comprehension | source code lexicon,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Fakhoury, Sarah;Ma, Yuzhan;Arnaoudova, Venera;Adesope, Olusola",Include,
10.1016/j.jss.2022.111515,2-s2.0-85051532094,10.1145/3196321.3196360,Replicomment: Identifying clones in code comments,"Code comments are the primary means to document implementation and ease program comprehension. Thus, their quality should be a primary concern to improve program maintenance. While a lot of effort has been dedicated to detect bad smell in code, little work focuses on comments. In this paper we start working in this direction by detecting clones in comments. Our initial investigation shows that even well known projects have several comment clones, and just as clones are bad smell in code, they may be for comments. A manual analysis of the clones we identified revealed several issues in real Java projects.",bad smell | clones | code comments | software quality,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Blasi, Arianna;Gorla, Alessandra",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85051658780,10.1145/3196321.3196356,On the naturalness of auto-generated code: Can we identify auto-generated code automatically?,"Recently, a variety of studies have been conducted on source code analysis. If auto-generated code is included in the target source code, it is usually removed in a preprocessing phase because the presence of auto-generated code may have negative effects on source code analysis. A straightforward way to remove auto-generated code is searching special comments that are included in the files of auto-generated code. However, it becomes impossible to identify auto-generated code with the way if such special comments have disappeared for some reasons. It is obvious that it takes too much effort to see source files one by one manually. In this paper, we propose a new technique to identify auto-generated code by using the naturalness of auto-generated code. We used a golden set that includes thousands of hand-made source files and source files generated by four kinds of compiler-compilers. Through the evaluation with the dataset, we confirmed that our technique was able to identify auto-generated code with over 99% precision and recall for all the cases.",auto-generated code | N-gram language model | source code analysis,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Doi, Masayuki;Higo, Yoshiki;Arima, Ryo;Shimonaka, Kento;Kusumoto, Shinji",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051663532,10.1145/3196321.3196367,CoBOT: Static C/C++ bug detection in the presence of incomplete code,"To obtain precise and sound results, most of existing static analyzers require whole program analysis with complete source code. However, in reality, the source code of an application always interacts with many third-party libraries, which are often not easily accessible to static analyzers. Worse still, more than 30% of legacy projects [1] cannot be compiled easily due to complicated configuration environments (e.g., third-party libraries, compiler options and macros), making ideal ""whole-program analysis"" unavailable in practice. This paper presents CoBOT [2], a static analysis tool that can detect bugs in the presence of incomplete code. It analyzes function APIs unavailable in application code by either using function summarization or automatically downloading and analyzing the corresponding library code as inferred from the application code and its configuration files. The experiments show that CoBOT is not only easy to use, but also effective in detecting bugs in real-world programs with incomplete code. Our demonstration video is at: https://youtu.be/bhjJp3e7LPM.",bug detection | incomplete code | static analysis,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Gao, Qing;Ma, Sen;Shao, Sihao;Sui, Yulei;Zhao, G.;Ma, Luyao;Ma, Xiao;Duan, Fuyao;Deng, Xiao;Zhang, Shikun;Chen, Xianglong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182533,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85132032270,10.1145/3180155.3180242,Enlightened Debugging,"Numerous automated techniques have been proposed to reduce the cost of software debugging, a notoriously time-consuming and human-intensive activity. Among these techniques, Statistical Fault Localization (SFL) is particularly popular. One issue with SFL is that it is based on strong, often unrealistic assumptions on how developers behave when debugging. To address this problem, we propose Enlighten, an interactive, feedback-driven fault localization technique. Given a failing test, Enlighten (1) leverages SFL and dynamic dependence analysis to identify suspicious method invocations and corresponding data values, (2) presents the developer with a query about the most suspicious invocation expressed in terms of inputs and outputs, (3) encodes the developer feedback on the correctness of individual data values as extra program specifications, and (4) repeats these steps until the fault is found. We evaluated Enlighten in two ways. First, we applied Enlighten to 1,807 real and seeded faults in 3 open source programs using an automated oracle as a simulated user; for over 96% of these faults, Enlighten required less than 10 interactions with the simulated user to localize the fault, and a sensitivity analysis showed that the results were robust to erroneous responses. Second, we performed an actual user study on 4 faults with 24 participants and found that participants who used Enlighten performed significantly better than those not using our tool, in terms of both number of faults localized and time needed to localize the faults.",debugging | dynamic analysis | fault localization,Proceedings - International Conference on Software Engineering,2018-01-01,Conference Paper,"Li, Xiangyu;Zhu, Shaowei;d’Amorim, Marcelo;Orso, Alessandro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182519,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058288935,10.1145/3180155.3180247,Semantic program repair using a reference implementation,"Automated program repair has been studied via the use of techniques involving search, semantic analysis and artificial intelligence. Most of these techniques rely on tests as the correctness criteria, which causes the test overfitting problem. Although various approaches such as learning from code corpus have been proposed to address this problem, they are unable to guarantee that the generated patches generalize beyond the given tests. This work studies automated repair of errors using a reference implementation. The reference implementation is symbolically analyzed to automatically infer a specification of the intended behavior. This specification is then used to synthesize a patch that enforces conditional equivalence of the patched and the reference programs. The use of the reference implementation as an implicit correctness criterion alleviates overfitting in test-based repair. Besides, since we generate patches by semantic analysis, the reference program may have a substantially different implementation from the patched program, which distinguishes our approach from existing techniques for regression repair like Relifix. Our experiments in repairing the embedded Linux Busybox with GNU Coreutils as reference (and vice-versa) revealed that the proposed approach scales to real-world programs and enables the generation of more correct patches.",Debugging | Program repair | Verification,Proceedings - International Conference on Software Engineering,2018-08-28,Conference Paper,"Mechtaev, Sergey;Nguyen, Manh Dung;Noller, Yannic;Grunske, Lars;Roychoudhury, Abhik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3180246,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182518,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099880007,10.1145/3180155.3180190,Program Splicing,"We introduce program splicing, a programming methodology that aims to automate the workflow of copying, pasting, and modifying code available online. Here, the programmer starts by writing a ""draft""that mixes unfinished code, natural language comments, and correctness requirements. A program synthesizer that interacts with a large, searchable database of program snippets is used to automatically complete the draft into a program that meets the requirements. The synthesis process happens in two stages. First, the synthesizer identifies a small number of programs in the database that are relevant to the synthesis task. Next it uses an enumerative search to systematically fill the draft with expressions and statements from these relevant programs. The resulting program is returned to the programmer, who can modify it and possibly invoke additional rounds of synthesis. We present an implementation of program splicing, called Splicer, for the Java programming language. Splicer uses a corpus of over 3.5 million procedures from an open-source software repository. Our evaluation uses the system in a suite of everyday programming tasks, and includes a comparison with a state-of-the-art competing approach as well as a user study. The results point to the broad scope and scalability of program splicing and indicate that the approach can significantly boost programmer productivity.",,Proceedings - International Conference on Software Engineering,2018-01-01,Conference Paper,"Lu, Yanxin;Chaudhuri, Swarat;Jermaine, Chris;Melski, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85098275280,10.1145/3180155.3180251,Chopped Symbolic Execution,"Symbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations. In this paper, we propose chopped symbolic execution, a novel form of symbolic execution that allows users to specify uninteresting parts of the code to exclude during the analysis, thus only targeting the exploration to paths of importance. However, the excluded parts are not summarily ignored, as this may lead to both false positives and false negatives. Instead, they are executed lazily, when their effect may be observable by code under analysis. Chopped symbolic execution leverages various on-demand static analyses at runtime to automatically exclude code fragments while resolving their side effects, thus avoiding expensive manual annotations and imprecision. Our preliminary results show that the approach can effectively improve the effectiveness of symbolic execution in several different scenarios, including failure reproduction and test suite augmentation.",Program slicing | Static analysis | Symbolic execution,Proceedings - International Conference on Software Engineering,2018-01-01,Conference Paper,"Trabish, David;Mattavelli, Andrea;Rinetzky, Noam;Cadar, Cristian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3180201,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182556,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85133561580,10.1145/3180155.3180206,Accurate and Efficient Refactoring Detection in Commit History,"Refactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios. To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art.",Abstract Syntax Tree | Accuracy | Commit | Git | Oracle | Refactoring,Proceedings - International Conference on Software Engineering,2018-01-01,Conference Paper,"Tsantalis, Nikolaos;Mansouri, Matin;Eshkevari, Laleh M.;Mazinanian, Davood;Dig, Danny",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182551,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182543,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051868290,10.1145/3180155.3180170,Understanding Developers' Needs on Deprecation as a Language Feature,"Deprecation is a language feature that allows API producers to mark a feature as obsolete. We aim to gain a deep understanding of the needs of API producers and consumers alike regarding deprecation. To that end, we investigate why API producers deprecate features, whether they remove deprecated features, how they expect consumers to react, and what prompts an API consumer to react to deprecation. To achieve this goal we conduct semi-structured interviews with 17 third-party Java API producers and survey 170 Java developers. We observe that the current deprecation mechanism in Java and the proposal to enhance it does not address all the needs of a developer. This leads us to propose and evaluate three further enhancements to the deprecation mechanism.",API | deprecation | Java,Proceedings - International Conference on Software Engineering,2018-01-01,Conference Paper,"Sawant, Anand Ashok;Aniche, Maurício;Van Deursen, Arie;Bacchelli, Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85116208037,10.1145/3180155.3180171,Dataflow Tunneling Mining Inter-request Data Dependencies for Request-based Applications,"Request-based applications, e.g., most server-side applications, expose services to users in a request-based paradigm, in which requests are served by request-handler methods. An important task for request-based applications is inter-request analysis, which analyzes request-handler methods that are related by inter-request data dependencies together. However, in the request-based paradigm, data dependencies between related request-handler methods are implicitly established by the underlying frameworks that execute these methods. As a result, existing analysis tools are usually limited to the scope of each single method without the knowledge of dependencies between different methods. In this paper, we design an approach called dataflow tunneling to capture inter-request data dependencies from concrete application executions and produce data-dependency specifications. Our approach answers two key questions: (1) what request-handler methods have data dependencies and (2) what these data dependencies are. Our evaluation using applications developed with two representative and popular frameworks shows that our approach is general and accurate. We also present a characteristic study and a use case of cache tuning based on the mined specifications. We envision that our approach can provide key information to enable future inter-request analysis techniques.",inter-request analysis | request-based applications | tracing | Web applications | web frameworks,Proceedings - International Conference on Software Engineering,2018-01-01,Conference Paper,"Yu, Xiao;Jin, Guoliang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049385596,10.1145/3180155.3180176,When not to comment: Questions and tradeoffs with API documentation for C++ projects,"Without usable and accurate documentation of how to use an API, developers can find themselves deterred from reusing relevant code. In C++, one place developers can find documentation is in a header file. When information is missing, they may look at the corresponding implementation code. To understand what's missing from C++ API documentation and the factors influencing whether it will be fixed, we conducted a mixed-methods study involving two experience sampling surveys with hundreds of developers at the moment they visited implementation code, interviews with 18 of those developers, and interviews with 8 API maintainers. In many cases, updating documentation may provide only limited value for developers, while requiring effort maintainers don't want to invest. We identify a set of questions maintainers and tool developers should consider when improving API-level documentation.",,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Head, Andrew;Sadowski, Caitlin;Murphy-Hill, Emerson;Knight, Andrea",Include,
10.1016/j.jss.2022.111515,2-s2.0-85049388864,10.1145/3180155.3180184,ConflictJS: Finding and understanding conflicts between JavaScript libraries,"It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects.",,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Patra, Jibesh;Dixit, Pooja N.;Pradel, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046979029,10.1145/3180155.3180260,Are code examples on an online Q&A forum reliable?: A study of API misuse on stack overflow,"Programmers often consult an online Q&A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons - -missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples.",API usage pattern | Code example assessment | Online Q&A forum,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Zhang, Tianyi;Upadhyaya, Ganesha;Reinhardt, Anastasia;Rajan, Hridesh;Kim, Miryung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049399305,10.1145/3180155.3180239,Identifying design problems in the source code: A grounded theory,"The prevalence of design problems may cause re-engineering or even discontinuation of the system. Due to missing, informal or outdated design documentation, developers often have to rely on the source code to identify design problems. Therefore, developers have to analyze different symptoms that manifest in several code elements, which may quickly turn into a complex task. Although researchers have been investigating techniques to help developers in identifying design problems, there is little knowledge on how developers actually proceed to identify design problems. In order to tackle this problem, we conducted a multi-trial industrial experiment with professionals from 5 software companies to build a grounded theory. The resulting theory offers explanations on how developers identify design problems in practice. For instance, it reveals the characteristics of symptoms that developers consider helpful. Moreover, developers often combine different types of symptoms to identify a single design problem. This knowledge serves as a basis to further understand the phenomena and advance towards more effective identification techniques.",Design problem | Grounded theory | Software design | Symptoms,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Sousa, Leonardo;Oliveira, Anderson;Oizumi, Willian;Barbosa, Simone;Garcia, Alessandro;Lee, Jaejoon;Kalinowski, Marcos;De Mello, Rafael;Fonseca, Baldoino;Oliveira, Roberto;Lucena, Carlos;Paes, Rodrigo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3180155.3182513,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049398609,10.1145/3180155.3180211,Generalized data structure synthesis,"Data structure synthesis is the task of generating data structure implementations from high-level specifications. Recent work in this area has shown potential to save programmer time and reduce the risk of defects. Existing techniques focus on data structures for manipulating subsets of a single collection, but real-world programs often track multiple related collections and aggregate properties such as sums, counts, minimums, and maximums. This paper shows how to synthesize data structures that track subsets and aggregations of multiple related collections. Our technique decomposes the synthesis task into alternating steps of query synthesis and incrementalization. The query synthesis step implements pure operations over the data structure state by leveraging existing enumerative synthesis techniques, specialized to the data structures domain. The incrementalization step implements imperative state modifications by re-framing them as fresh queries that determine what to change, coupled with a small amount of code to apply the change. As an added benefit of this approach over previous work, the synthesized data structure is optimized for not only the queries in the specification but also the required update operations. We have evaluated our approach in four large case studies, demonstrating that these extensions are broadly applicable.",Automatic programming | Data structures | Program synthesis,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Loncaric, Calvin;Ernst, Michael D.;Torlak, Emina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045666745,10.1145/3180155.3180186,A graph solver for the automated generation of consistent domain-specific models,"Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.",,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Semeráth, Oszkár;Nagy, András Szabolcs;Varró, Dániel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049395416,10.1145/3180155.3180231,Automatically finding bugs in a commercial cyber-physical system development tool chain with SLforge,"Cyber-physical system (CPS) development tool chains are widely used in the design, simulation, and verification of CPS data-flow models. Commercial CPS tool chains such as MathWorks' Simulink generate artifacts such as code binaries that are widely deployed in embedded systems. Hardening such tool chains by testing is crucial since formally verifying them is currently infeasible. Existing differential testing frameworks such as CyFuzz can not generate models rich in language features, partly because these tool chains do not leverage the available informal Simulink specifications. Furthermore, no study of existing Simulink models is available, which could guide CyFuzz to generate realistic models. To address these shortcomings, we created the first large collection of public Simulink models and used the collected models' properties to guide random model generation. To further guide model generation we systematically collected semi-formal Simulink specifications. In our experiments on several hundred models, the resulting SLforge generator was more effective and efficient than the state-of-the-art tool CyFuzz. SLforge also found 8 new confirmed bugs in Simulink.",Cyber-physical systems | Differential testing | Tool chain bugs,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Chowdhury, Shafiul Azam;Mohian, Soumik;Mehra, Sidharth;Gawsane, Siddhant;Johnson, Taylor T.;Csallner, Christoph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058272239,10.1109/ICSME.2018.00011,Why are features deprecated? an investigation into the motivation behind deprecation,"In this study, we investigate why API producers deprecate features. Previous work has shown us that knowing the rationale behind deprecation of an API AIDS a consumer in deciding to react, thus hinting at a diversity of deprecation reasons. We manually analyze the Javadoc of 374 deprecated methods pertaining four mainstream Java APIs to see whether the reason behind deprecation is mentioned. We find that understanding the rationale from just the Javadoc is insufficient; hence we add other data sources such as the source code, issue tracker data and commit history. We observe 12 reasons that trigger API producers to deprecate a feature. We evaluate an automated approach to classify these motivations.",API | Deprecation | Documentation,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Sawant, Anand Ashok;Huang, Guangzhe;Vilen, Gabriel;Stojkovski, Stefan;Bacchelli, Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058342190,10.1109/ICSME.2018.00012,A Large-scale empirical study on linguistic antipatterns affecting apis,"The concept of monolithic stand-Alone software systems developed completely from scratch has become obsolete, as modern systems nowadays leverage the abundant presence of Application Programming Interfaces (APIs) developed by third parties, which leads on the one hand to accelerated development, but on the other hand introduces potentially fragile dependencies on external resources. In this context, the design of any API strongly influences how developers write code utilizing it. A wrong design decision like a poorly chosen method name can lead to a steeper learning curve, due to misunderstandings, misuse and eventually bug-prone code in the client projects using the API. It is not unfrequent to find APIs with poorly expressive or misleading names, possibly lacking appropriate documentation. Such issues can manifest in what have been defined in the literature as Linguistic Antipatterns (LAs), i.e., inconsistencies among the naming, documentation, and implementation of a code entity. While previous studies showed the relevance of LAs for software developers, their impact on (developers of) client projects using APIs affected by LAs has not been investigated. This paper fills this gap by presenting a large-scale study conducted on 1.6k releases of popular Maven libraries, 14k open-source Java projects using these libraries, and 4.4k questions related to the investigated APIs asked on Stack Overflow. In particular, we investigate whether developers of client projects have higher chances of introducing bugs when using APIs affected by LAs and if these trigger more questions on Stack Overflow as compared to non-Affected APIs.",Application Programming Interfaces (APIs) | Empirical Study | Linguistic Antipatterns,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Aghajani, Emad;Nagy, Csaba;Bavota, Gabriele;Lanza, Michele",Include,
10.1016/j.jss.2022.111515,2-s2.0-85058343497,10.1109/ICSME.2018.00024,On state machine mining from embedded control software,"Program understanding is a time-consuming and tedious activity for software developers. Manually building abstractions from source code requires in-depth analysis of the code in the first place. Model mining can support program comprehension by semi-Automatically extracting high-level models from code. One potentially helpful model is a state machine, which is an established formalism for specifying the behavior of a software component. There exist only few approaches for state machine mining, and they either deal with object-oriented systems or expect specific state implementation patterns. Both preconditions are usually not met by real-world embedded control software written in procedural languages. Other approaches extract only API protocols instead of the component's behavior. In this paper, we propose and evaluate several techniques that enable state machine mining from embedded control code: 1) We define criteria for state variables in procedural code based on an empirical study. This enables adaptation of an existing approach for extracting state machines from object-oriented software to embedded control code. 2) We present a refinement of the transition extraction process of that approach by removing infeasible transitions, which on average leads to more than 50% reduction of the number of transitions. 3) We evaluate two approaches to reduce the complexity of transition conditions. 4) An empirical study examines the limits of transition conditions' complexity that can still be understood by humans. These techniques and studies constitute major building blocks towards mining understandable state machines from embedded control software.",Boolean minimization | Embedded software | legacy code | Model mining | Program understanding | software maintenance | State machines | Static analysis,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Said, Wasim;Quante, Jochen;Koschke, Rainer",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058287370,10.1109/ICSME.2018.00028,Improving api caveats accessibility by mining api caveats knowledge graph,"API documentation provides important knowledge about the functionality and usage of APIs. In this paper, we focus on API caveats that developers should be aware of in order to avoid unintended use of an API. Our formative study of Stack Overflow questions suggests that API caveats are often scattered in multiple API documents, and are buried in lengthy textual descriptions. These characteristics make the API caveats less discoverable. When developers fail to notice API caveats, it is very likely to cause some unexpected programming errors. In this paper, we propose natural language processing(NLP) techniques to extract ten subcategories of API caveat sentences from API documentation and link these sentences to API entities in an API caveats knowledge graph. The API caveats knowledge graph can support information retrieval based or entity-centric search of API caveats. As a proof-of-concept, we construct an API caveats knowledge graph for Android APIs from the API documentation on the Android Developers website. We study the abundance of different subcategories of API caveats and use a sampling method to manually evaluate the quality of the API caveats knowledge graph. We also conduct a user study to validate whether and how the API caveats knowledge graph may improve the accessibility of API caveats in API documentation.",API caveats | Coreference Resolution | Entity Linking | Knowledge Graph,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Li, Hongwei;Li, Sirui;Sun, Jiamou;Xing, Zhenchang;Peng, Xin;Liu, Mingwei;Zhao, Xuejiao",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85058346659,10.1109/ICSME.2018.00029,StATISTICAL TRANSLATION of ENGLISH TEXTS to API CODE TEMPLATES,"We develop T2API, a context-sensitive, graph-based statistical translation approach that takes as input an English description of a programming task and synthesizes the corresponding API code template for the task. We train T2API to statistically learn the alignments between English and API elements and determine the relevant API elements. The training is done on StackOverflow, a bilingual corpus on which developers discuss programming problems in two types of language: English and programming language. T2API considers both the context of the words in the input query and the context of API elements that often go together in the corpus. The derived API elements with their relevance scores are assembled into an API usage by GraSyn, a novel graph-based API synthesis algorithm that generates a graph representing an API usage from a large code corpus. Importantly, it is capable of generating new API usages from previously seen sub-usages. We curate a test benchmark of 250 real-world StackOverflow posts. Across the benchmark, T2API's synthesized snippets have the correct API elements with a median top-1 precision and recall of 67% and 100%, respectively. Four professional developers and five graduate students judged that 77% of our top synthesized API code templates are useful to solve the problem presented in the StackOverflow posts.",API Usage Synthesis | Graph based Statistical Machine Translation | Text to Code Translation,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Nguyen, Anh;Rigby, Peter;Nguyen, Thanh;Palani, Dharani;Karanfil, Mark;Nguyen, Tien",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85058315859,10.1109/ICSME.2018.00039,A conceptual replication study on bugs that get fixed in open source software,"Bugs dominate the corrective maintenance and evolutionary changes in large-scale software systems. The topic of bugs has been extensively investigated and reported in the literature. Unfortunately, the existential question of all 'whether a reported bug will be fixed or not' has not received much attention. The paper presents an empirical study on four open source projects to examine the factors that influence the likelihood of a bug getting fixed or not. Overall, our study can be contextualized as a conceptual replication of a previous study on Microsoft systems from a commercial domain. The similarities and differences in terms of the design, execution, and results between the two studies are discussed. It was observed from these systems that the reputations of the reporter and assigned developer to fix it, and the number of comments on a bug have the most substantial impact on its probability to get fixed. Moreover, we formulated a predictive model from features available as soon as a bug is reported to estimate whether it will be fixed or not. Intra and inter (cross) project validations were performed. Precision and Recall metrics were used to assess the predictive model. Their values were recorded in the 60% to 70% range.",Bug Prediction Studies | Conceptual Replication,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Wang, Haoren;Kagdi, Huzefa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058326448,10.1109/ICSME.2018.00045,Automatic traceability maintenance via machine learning classification,"Previous studies have shown that software traceability, the ability to link together related artifacts from different sources within a project (e.g., source code, use cases, documentation, etc.), improves project outcomes by assisting developers and other stakeholders with common tasks such as impact analysis, concept location, etc. Establishing traceability links in a software system is an important and costly task, but only half the struggle. As the project undergoes maintenance and evolution, new artifacts are added and existing ones are changed, resulting in outdated traceability information. Therefore, specific steps need to be taken to make sure that traceability links are maintained in tandem with the rest of the project. In this paper we address this problem and propose a novel approach called TRAIL for maintaining traceability information in a system. The novelty of TRAIL stands in the fact that it leverages previously captured knowledge about project traceability to train a machine learning classifier which can then be used to derive new traceability links and update existing ones. We evaluated TRAIL on 11 commonly used traceability datasets from six software systems and compared it to seven popular Information Retrieval (IR) techniques including the most common approaches used in previous work. The results indicate that TRAIL outperforms all IR approaches in terms of precision, recall, and F-score.",Classification | Information retrieval | Machine learning | Query quality | Traceability link recovery | Traceability maintenance,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Mills, Chris;Escobar-Avila, Javier;Haiduc, Sonia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058279506,10.1109/ICSME.2018.00053,On the impact of tokenizer and parameters on n-gram based code analysis,"Recent research shows that language models, such as n-gram models, are useful at a wide variety of software engineering tasks, e.g., code completion, bug identification, code summarisation, etc. However, such models require the appropriate set of numerous parameters. Moreover, the different ways one can read code essentially yield different models (based on the different sequences of tokens). In this paper, we focus on n-gram models and evaluate how the use of tokenizers, smoothing, unknown threshold and n values impact the predicting ability of these models. Thus, we compare the use of multiple tokenizers and sets of different parameters (smoothing, unknown threshold and n values) with the aim of identifying the most appropriate combinations. Our results show that the Modified Kneser-Ney smoothing technique performs best, while n values are depended on the choice of the tokenizer, with values 4 or 5 offering a good trade-off between entropy and computation time. Interestingly, we find that tokenizers treating the code as simple text are the most robust ones. Finally, we demonstrate that the differences between the tokenizers are of practical importance and have the potential of changing the conclusions of a given experiment.",N Grams | Naturalness | Source Code Analysis,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Jimenez, Matthieu;Maxime, Cordy;Traon, Yves Le;Papadakis, Mike",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058331190,10.1109/ICSME.2018.00057,Effective reformulation of query for code search using crowdsourced knowledge and extra-large data analytics,"Software developers frequently issue generic natural language queries for code search while using code search engines (e.g., GitHub native search, Krugle). Such queries often do not lead to any relevant results due to vocabulary mismatch problems. In this paper, we propose a novel technique that automatically identifies relevant and specific API classes from Stack Overflow Q & A site for a programming task written as a natural language query, and then reformulates the query for improved code search. We first collect candidate API classes from Stack Overflow using pseudo-relevance feedback and two term weighting algorithms, and then rank the candidates using Borda count and semantic proximity between query keywords and the API classes. The semantic proximity has been determined by an analysis of 1.3 million questions and answers of Stack Overflow. Experiments using 310 code search queries report that our technique suggests relevant API classes with 48% precision and 58% recall which are 32% and 48% higher respectively than those of the state-of-The-Art. Comparisons with two state-of-The-Art studies and three popular search engines (e.g., Google, Stack Overflow, and GitHub native search) report that our reformulated queries (1) outperform the queries of the state-of-The-Art, and (2) significantly improve the code search results provided by these contemporary search engines.",Borda count | Code search | Crowd-sourced knowledge | Extra-large data analytics | PageRank algorithm | Query reformulation | Semantic similarity | Stack Overflow,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Rahman, Mohammad Masudur;Roy, Chanchal",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058332833,10.1109/ICSME.2018.00048,On the value of bug reports for retrieval-based bug localization,"Software engineering researchers have been applying tools and techniques from information retrieval (IR) to problems such as bug localization to lower the manual effort required to perform maintenance tasks. The central challenge when using an IR-based tool is the formation of a high-quality query. When performing bug localization, one easily accessible source of query words is the bug report. A recent paper investigated the sufficiency of this source by using a genetic algorithm (GA) to build high quality queries. Unfortunately, the GA in essence 'cheats' as it makes use of query performance when evolving a good query. This raises the question, is it feasible to attain similar results without 'cheating?' One approach to providing cheat-free queries is to employ automatic summarization. The performance of the resulting summaries calls into question the sufficiency of the bug reports as a source of query words. To better understand the situation, Information Need Analysis (INA) is applied to quantify both how well the GA is performing and, perhaps more importantly, how well a bug report captures the vocabulary needed to perform IR-based bug localization. The results find that summarization shows potential to produce high-quality queries, but it requires more training data. Furthermore, while bug reports provide a useful source of query words, they are rather limited and thus query expansion techniques, perhaps in combination with summarization, will likely produce higher-quality queries.",Bug localization | Information need | Information retrieval | Query quality,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Lawrie, Dawn;Binkley, Dave",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058298663,10.1109/ICSME.2018.00049,Semi-automated feature traceability with embedded annotations,"Engineering software amounts to implementing and evolving features. While some engineering approaches advocate the explicit use of features, developers usually do not record feature locations in software artifacts. However, when evolving or maintaining features-especially in long-living or variant-rich software with many developers-The knowledge about features and their locations quickly fades and needs to be recovered. While automated or semi-Automated feature-location techniques have been proposed, their accuracy is usually too low to be useful in practice. We propose a semi-Automated, machine-learning-Assisted feature-Traceability technique that allows developers to continuously record feature-Traceability information while being supported by recommendations about missed locations. We show the accuracy of our proposed technique in a preliminary evaluation, simulating the engineering of an open-source web application that evolved in different, cloned variants.",Clone&own | Feature annotations | Feature location | Feature traceability | Machine | Recommendation system | Software evolution | Variability,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Abukwaik, Hadil;Burger, Andreas;Andam, Berima Kweku;Berger, Thorsten",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058280452,10.1109/ICSME.2018.00063,Toward automatic summarization of arbitrary Java statements for novice programmers,"Novice programmers sometimes need to understand code written by others. Unfortunately, most software projects lack comments suitable for novices. The lack of comments have been addressed through automated techniques of generating comments based on program statements. However, these techniques lacked the context of how these statements function since they were aimed toward experienced programmers. In this paper, we present a novel technique towards automatically generating comments for Java statements suitable for novice programmers. Our technique not only goes beyond existing approaches to method summarization to meet the needs of novices, it also leverages API documentation when available. In an experimental study of 30 computer science undergraduate students, we observed explanations based on our technique to be preferred over an existing approach.",Automatic Summarization | Java | Novice Program Comprehension,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Hassan, Mohammed;Hill, Emily",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85058320661,10.1109/ICSME.2018.00071,A qualitative study of variability management of control software for industrial automation systems,"Software product line engineering (SPLE) provides a systematic approach to manage variants and versions arising throughout the development of software systems. While SPLE is successfully applied for variant management in the domain of software engineering, the approach is still not widely spread in industrial automated production systems (aPS). Previous studies highlight the interdisciplinary nature of aPS as a reason for not applying SPLE, since control software variants and versions also result from changes in other disciplines such as the mechanical engineering department (i.e. exchange of a sensor). Additionally, the software may evolve over decades at the customer site. In order to gain a better understanding of the challenges in the development of aPS and the constraints hindering the use of SPLE, we conducted several interviews with software development engineers from the domain of aPS. The interviews main aim was to get an overview of the current state of variability management and applied planned and unplanned software reuse strategies. Based on these insights, we summarize the main results useable for a transition from currently deployed variability management concepts in aPS to the SPLE approach.",automated production systems | reuse strategy | state-of-The-practice study | variability management,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Fischer, Juliane;Bougouffa, Safa;Schlie, Alexander;Schaefer, Ina;Vogel-Heuser, Birgit",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058312386,10.1109/ICSME.2018.00073,Relational database schema evolution: An industrial case study,"Modern relational database management systems provide advanced features allowing, for example, to include behaviour directly inside the database (stored procedures). These features raise new difficulties when a database needs to evolve (e.g. adding a new table). To get a better understanding of these difficulties, we recorded and studied the actions of a database architect during a complex evolution of the database at the core of a software system. From our analysis, problems faced by the database architect are extracted, generalized and explored through the prism of software engineering. Six problems are identified: (1) difficulty in analysing and visualising dependencies between database's entities, (2) difficulty in evaluating the impact of a modification on the database, (3) replicating the evolution of the database schema on other instances of the database, (4) difficulty in testing database's functionalities, (5) lack of synchronization between the IDE's internal model of the database and the database actual state and (6) absence of an integrated tool enabling the architect to search for dependencies between entities, generate a patch or access an up to date PostgreSQL documentation. We suggest that techniques developed by the software engineering community could be adapted to help in the development and evolution of relational databases.",Evolution | Relational database | Software engineering,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Delplanque, Julien;Etien, Anne;Anquetil, Nicolas;Auverlot, Olivier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058297009,10.1109/ICSME.2018.00076,Software process analysis methodology-a methodology based on lessons learned in embracing legacy software,"Over the last decades, the complexity of high-Tech systems, and the software systems controlling them, has increased considerably. In practice, it is hard to keep knowledge and documentation of these ever-evolving software systems up-To-date with their actual realization; we are dealing with legacy software. Clearly, this lack of knowledge, insight, and understanding is more and more becoming a critical issue. Process mining provides an interesting opportunity to improve understanding and analyze software behavior based on observations from the system on the run. However, a concrete software process analysis methodology was lacking. This paper 1) discusses a software process analysis case study at ASML, a large high-Tech company, and, based on the lessons learned, 2) presents a concrete methodology for analyzing software processes. The presented methodology actively includes the system under analysis and is based on practical experiences in applying process mining on industrial-scale legacy software.",Case Study | Methodology | Model-Driven Analysis | Passive Learning | Performance Analysis | Process Mining | Program Analysis | Reverse Engineering,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Leemans, Maikel;Van Der Aalst, Wil M.P.;Van Den Brand, Mark G.J.;Schiffelers, Ramon R.H.;Lensink, Leonard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058305372,10.1109/ICSME.2018.00093,Integrating runtime values with source code to facilitate program comprehension,"An inherently abstract nature of source code makes programs difficult to understand. In our research, we designed three techniques utilizing concrete values of variables and other expressions during program execution. RuntimeSearch is a debugger extension searching for a given string in all expressions at runtime. DynamiDoc generates documentation sentences containing examples of arguments, return values and state changes. RuntimeSamp augments source code lines in the IDE (integrated development environment) with sample variable values. In this post-doctoral article, we briefly describe these three approaches and related motivational studies, surveys and evaluations. We also reflect on the PhD study, providing advice for current students. Finally, short-Term and long-Term future work is described.",Debugging | Documentation | Dynamic analysis | Integrated development environment | Variables,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Sulir, Matus",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85058319516,10.1109/ICSME.2018.00094,Automating software development for mobile computing platforms,"Mobile devices such as smartphones and tablets have become ubiquitous in today's modern computing landscape. The applications that run on these mobile devices (often referred to as 'apps') have become a primary means of computing for millions of users and, as such, have garnered immense developer interest. These apps allow for unique, personal software experiences through touch-based UIs and a complex assortment of sensors. However designing and implementing high quality mobile apps can be a difficult process. This is primarily due to challenges unique to mobile development including change-prone APIs and platform fragmentation, just to name a few. This paper presents the motivation and an overview of a dissertation which presents new approaches for automating and improving mobile app design and development practices. Additionally, this paper discusses potential avenues for future research based upon the work conducted, as well as general lessons learned during the author's tenure as a doctoral student in the general areas of software engineering, maintenance, and evolution.",Android | apps | Machine Learning | Mobile | prototyping | software development | testing,"Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",2018-11-09,Conference Paper,"Moran, Kevin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,https://hank.feild.org/publications/sea06.pdf,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85059621336,10.1109/ISSRE.2018.00033,You Are Where You App: An Assessment on Location Privacy of Social Applications,"The development of positioning technologies has digitalized people's mobility traces for the first time in history. GPS sensors resided in people's mobile devices allow smart apps to access location data. This large amount of mobility data can help to build appealing applications. Meanwhile, location privacy has become a major concern. In this paper, we design a general system to assess whether an app is vulnerable to location inference attacks. We utilize a series of automatic testing mechanisms including UI match and API analysis to extract the location information an app provides. According to different characteristics of these apps, we classify them into two categories corresponding to two kinds of attacks, namely attack with distance limitation (AWDL) and attack without distance limitation (AWODL). After evaluating 800 apps, of which 109 passed automated testing, we found that 24.7% of the passing apps are vulnerable to AWDL and 11.0% to AWODL. Moreover, some apps even allow us to modify the parameters in http requests which largely increases the scope of the attacks. Our system demonstrates the severity of location privacy leakage to mobile devices and can serve as an auditing tool for future smart apps.","Android Applications, Location Privacy, Automatic Testing, Relative Distance, Trilateration","Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2018-11-16,Conference Paper,"Zhao, Fanghua;Gao, Linan;Zhang, Yang;Wang, Zeyu;Wang, Bo;Guo, Shanqing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85054823678,10.1145/3193954.3193959,Visualizing evolving requirements models with timedURN,"Many modern systems are built to be in use for a long time, sometimes spanning several decades. Furthermore, considering the cost of replacing an existing system, existing systems are usually adapted to evolving requirements, some of which may be anticipated. Such systems need to be specified and analyzed in terms of whether the changes introduced into the system still address evolving requirements and continue to satisfy the needs of its stakeholders. Recently, the User Requirements Notation (URN) - a requirements engineering notation standardized by the International Telecommunication Union for the elicitation, specification, and analysis of integrated goal and scenario models - has been extended with support for the modeling and analysis of evolving requirements models by capturing a comprehensive set of changes to a URN model as first-class model concepts in URN. The extension is called TimedURN and this paper builds on TimedURN to analyze and visualize not just an individual time point in the past, present, or future but a time range consisting of a set of time points. Various visualization options are discussed, including their proof-of-concept implementation in the jUCMNav requirements engineering tool.",evaluation mechanism | evolution | goals | scenarios | traversal mechanism | URN | user requirements notation | visualization,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Luthra, Sahil;Aprajita, ;Mussbacher, Gunter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3193954.3193956,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85054864904,10.1145/3193954.3193957,UML diagram synthesis techniques: A systematic mapping study,"Context: UML software development relies on different types of UML diagrams, which must be consistent with one another. UML Synthesis techniques suggest to generate diagram(s) from other diagram(s), thereby implicitly suggesting that input and output diagrams of the synthesis process be consistent with one another. Objective: Our aim is to provide a comprehensive summary of UML synthesis techniques as they have been described in the literature to date to then collect UML consistency rules, which can then be used to verify UML models. Method: We performed a Systematic Mapping Study by following well-known guidelines. We selected 14 studies by means of a search with seven search engines executed until January, 2018. Results: Researchers have not frequently published papers concerning UML synthesis techniques since 2004. We present a set of 47 UML consistency rules collected from the different synthesis techniques analyzed. Conclusion: Although UML diagrams synthesis doesn't seem to be an active line of research, it is relevant since synthesis techniques rely on enforcing diagram consistency, which is an active line of research. We collected consistency rules which can be used to check UML models, specifically to verify if the diagrams of a model are consistent with one another.",model consistency checking | systematic mapping study | UML | UML consistency rules | UML synthesis techniques,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Torre, Damiano;Labiche, Yvan;Genero, Marcela;Baldassarre, Maria Teresa;Elaasar, Maged",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051634093,10.1145/3196398.3196453,A gold standard for emotion annotation in stack overflow,"Software developers experience and share a wide range of emotions throughout a rich ecosystem of communication channels. A recent trend that has emerged in empirical software engineering studies is leveraging sentiment analysis of developers' communication traces. We release a dataset of 4,800 questions, answers, and comments from Stack Overflow, manually annotated for emotions. Our dataset contributes to the building of a shared corpus of annotated resources to support research on emotion awareness in software development.",communication channels | emotion mining | sentiment analysis | social software engineering | stack overflow,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Novielli, Nicole;Calefato, Fabio;Lanubile, Filippo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051645958,10.1145/3196398.3196457,Developer interaction traces backed by IDE screen recordings from think aloud sessions,"There are two well-known difficulties to test and interpret methodologies for mining developer interaction traces: first, the lack of enough large datasets needed by mining or machine learning approaches to provide reliable results; and second, the lack of ""ground truth"" or empirical evidence that can be used to triangulate the results, or to verify their accuracy and correctness. Moreover, relying solely on interaction traces limits our ability to take into account contextual factors that can affect the applicability of mining techniques in other contexts, as well hinders our ability to fully understand the mechanics behind observed phenomena. The data presented in this paper attempts to alleviate these challenges by providing 600+ hours of developer interaction traces, from which 26+ hours are backed with video recordings of the IDE screen and developer's comments. This data set is relevant to researchers interested in investigating program comprehension, and those who are developing techniques for interaction traces analysis and mining.",empirical study | industrial data | interaction traces | log mining | program comprehension | programming flow,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Yamashita, Aiko;Petrillo, Fabio;Khomh, Foutse;Guéhéneuc, Yann Gaël",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051652310,10.1145/3196398.3196462,Mining and extraction of personal software process measures through IDE interaction logs,"The Personal Software Process (PSP) is an effective software process improvement method that heavily relies on manual collection of software development data. This paper describes a semi-automated method that reduces the burden of PSP data collection by extracting the required time and size of PSP measurements from IDE interaction logs. The tool mines enriched event data streams so can be easily generalized to other developing environment also. In addition, the proposed method is adaptable to phase definition changes and creates activity visualizations and summarizations that are helpful for software project management. Tools and processed data used for this paper are available on GitHub at: https://github.com/unknowngithubuser1/data.",IDE | personal software process,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Joonbakhsh, Alireza;Sami, Ashkan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051634090,10.1145/3196398.3196471,Do software engineers use autocompletion features differently than other developers?,"Autocomplete is a common workspace feature that is used to recommend code snippets as developers type in their IDEs. Users of autocomplete features no longer need to remember programming syntax and the names and details of the API methods that are needed to accomplish tasks. Moreover, autocompletion of code snippets may have an accelerating effect, lowering the number of keystrokes that are needed to type the code. However, like any tool, implicit tendencies of users may emerge. Knowledge of how developers in different roles use autocompletion features may help to guide future autocompletion development, research, and training material. In this paper, we set out to better understand how usage of autocompletion varies among software engineers and other developers (i.e., academic researchers, industry researchers, hobby programmers, and students). Analysis of autocompletion events in the Mining Software Repositories (MSR) challenge dataset reveals that: (1) rates of autocompletion usage among software engineers and other developers are not significantly different; and (2) although several non-negligible effect sizes of autocompletion targets (e.g., local variables, method names) are detected between the two groups, the rates at which these targets appear do not vary to a significant degree. These inconclusive results are likely due to the small sample size (n = 35); however, they do provide an interesting insight for future studies to build upon.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Amlekar, Rahul;Gamboa, Andrés Felipe Rincón;Gallaba, Keheliya;McIntosh, Shane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051629455,10.1145/3196398.3196407,An evaluation of open-source software microbenchmark suites for continuous performance assessment,"Continuous integration (CI) emphasizes quick feedback to developers. This is at odds with current practice of performance testing, which predominantely focuses on long-running tests against entire systems in production-like environments. Alternatively, software microbenchmarking attempts to establish a performance baseline for small code fragments in short time. This paper investigates the quality of microbenchmark suites with a focus on suitability to deliver quick performance feedback and CI integration. We study ten open-source libraries written in Java and Go with benchmark suite sizes ranging from 16 to 983 tests, and runtimes between 11 minutes and 8.75 hours. We show that our study subjects include benchmarks with result variability of 50% or higher, indicating that not all benchmarks are useful for reliable discovery of slowdowns. We further artificially inject actual slowdowns into public API methods of the study subjects and test whether test suites are able to discover them. We introduce a performance-test quality metric called the API benchmarking score (ABS). ABS represents a benchmark suite's ability to find slowdowns among a set of defined core API methods. Resulting benchmarking scores (i.e., fraction of discovered slowdowns) vary between 10% and 100% for the study subjects. This paper's methodology and results can be used to (1) assess the quality of existing microbenchmark suites, (2) select a set of tests to be run as part of CI, and (3) suggest or generate benchmarks for currently untested parts of an API.",continuous integration | empirical study | Go | Java | microbenchmarking | software performance testing,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Laaber, Christoph;Leitner, Philipp",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051634576,10.1145/3196398.3196449,What did really change with the new release of the app?,"The mobile app market is evolving at a very fast pace. In order to stay in the market and fulfill user's growing demands, developers have to continuously update their apps either to fix issues or to add new features. Users and market managers may have a hard time understanding what really changed in a new release though, and therefore may not make an informative guess of whether updating the app is recommendable, or whether it may pose new security and privacy threats for the user. We propose a ready-to-use framework to analyze the evolution of Android apps. Our framework extracts and visualizes various information - -such as how an app uses sensitive data, which third-party libraries it relies on, which URLs it connects to, etc. - - and combines it to create a comprehensive report on how the app evolved. Besides, we present the results of an empirical study on 235 applications with at least 50 releases using our framework. Our analysis reveals that Android apps tend to have more leaks of sensitive data over time, and that the majority of API calls relative to dangerous permissions are added to the code in releases posterior to the one where the corresponding permission was requested.",Android | app evolution | behavior change,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Calciati, Paolo;Kuznetsov, Konstantin;Bai, Xue;Gorla, Alessandra",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051665962,10.1145/3196398.3196416,An empirical evaluation of OSGi dependencies best practices in the eclipse IDE,"OSGi is a module system and service framework that aims to fill Java's lack of support for modular development. Using OSGi, developers divide software into multiple bundles that declare constrained dependencies towards other bundles. However, there are various ways of declaring and managing such dependencies, and it can be confusing for developers to choose one over another. Over the course of time, experts and practitioners have defined ""best practices"" related to dependency management in OSGi. The underlying assumptions are that these best practices (i) are indeed relevant and (ii) help to keep OSGi systems manageable and efficient. In this paper, we investigate these assumptions by first conducting a systematic review of the best practices related to dependency management issued by the OSGi Alliance and OSGi-endorsed organizations. Using a large corpus of OSGi bundles (1,124 core plug-ins of the Eclipse IDE), we then analyze the use and impact of 6 selected best practices. Our results show that the selected best practices are not widely followed in practice. Besides, we observe that following them strictly reduces classpath size of individual bundles by up to 23% and results in up to ±13% impact on performance at bundle resolution time. In summary, this paper contributes an initial empirical validation of industry-standard OSGi best practices. Our results should influence practitioners especially, by providing evidence of the impact of these best practices in real-world systems.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Ochoa, Lina;Degueule, Thomas;Vinju, Jurgen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051493131,10.1145/3196398.3196419,Characterising deprecated Android APIs,"Because of functionality evolution, or security and performance-related changes, some APIs eventually become unnecessary in a software system and thus need to be cleaned to ensure proper maintainability. Those APIs are typically marked first as deprecated APIs and, as recommended, follow through a deprecated-replaceremove cycle, giving an opportunity to client application developers to smoothly adapt their code in next updates. Such a mechanism is adopted in the Android framework development where thousands of reusable APIs are made available to Android app developers. In this work, we present a research-based prototype tool called CDA and apply it to different revisions (i.e., releases or tags) of the Android framework code for characterising deprecated APIs. Based on the data mined by CDA, we then perform an exploratory study on API deprecation in the Android ecosystem and the associated challenges for maintaining quality apps. In particular, we investigate the prevalence of deprecated APIs, their annotations and documentation, their removal and consequences, their replacement messages, as well as developer reactions to API deprecation. Experimental results reveal several findings that further provide promising insights for future research directions related to deprecated Android APIs. Notably, by mining the source code of the Android framework base, we have identified three bugs related to deprecated APIs. These bugs have been quickly assigned and positively appreciated by the framework maintainers, who claim that these issues will be updated in future releases.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Li, Li;Gao, Jun;Bissyandé, Tegawendé F.;Ma, Lei;Xia, Xin;Klein, Jacques",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051661014,10.1145/3196398.3196432,Prevalence of confusing code in software projects: Atoms of confusion in the wild,"Prior work has shown that extremely small code patterns, such as the conditional operator and implicit type conversion, can cause considerable misunderstanding in programmers. Until now, the real world impact of these patterns - known as 'atoms of confusion' - was only speculative. This work uses a corpus of 14 of the most popular and influential open source C and C++ projects to measure the prevalence and significance of these small confusing patterns. Our results show that the 15 known types of confusing micro patterns occur millions of times in programs like the Linux kernel and GCC, appearing on average once every 23 lines. We show there is a strong correlation between these confusing patterns and bug-fix commits as well as a tendency for confusing patterns to be commented. We also explore patterns at the project level showing the rate of security vulnerabilities is higher in projects with more atoms. Finally, we examine real code examples containing these atoms, including ones that were used to find and fix bugs in our corpus. In total this work demonstrates that beyond simple misunderstanding in the lab setting, atoms of confusion are both prevalent - occurring often in real projects, and meaningful - being removed by bug-fix commits at an elevated rate.",program understanding | programming languages,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Gopstein, Dan;Zhou, Hongwei Henry;Frankl, Phyllis;Cappos, Justin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051633893,10.1145/3196398.3196441,"""automatically assessing code understandability"" reanalyzed: Combined metrics matter","Previous research shows that developers spend most of their time understanding code. Despite the importance of code understandability for maintenance-related activities, an objective measure of it remains an elusive goal. Recently, Scalabrino et al. reported on an experiment with 46 Java developers designed to evaluate metrics for code understandability. The authors collected and analyzed data on more than a hundred features describing the code snippets, the developers' experience, and the developers' performance on a quiz designed to assess understanding. They concluded that none of the metrics considered can individually capture understandability. Expecting that understandability is better captured by a combination of multiple features, we present a reanalysis of the data from the Scalabrino et al. study, in which we use different statistical modeling techniques. Our models suggest that some computed features of code, such as those arising from syntactic structure and documentation, have a small but significant correlation with understandability. Further, we construct a binary classifier of understandability based on various interpretable code features, which has a small amount of discriminating power. Our encouraging results, based on a small data set, suggest that a useful metric of understandability could feasibly be created, but more data is needed.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Trockman, Asher;Cates, Keenen;Mozina, Mark;Nguyen, Tuan;Kästner, Christian;Vasilescu, Bogdan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051650034,10.1145/3196398.3196430,SOTorrent: Reconstructing and analyzing the evolution of stack overflow posts,"Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.",code snippets | open dataset | software evolution | stack overflow,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Baltes, Sebastian;Dumani, Lorik;Treude, Christoph;Diehl, Stephan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051650858,10.1145/3196398.3196429,Which contributions predict whether developers are accepted into github teams,"Open-source software (OSS) often evolves from volunteer contributions, so OSS development teams must cooperate with their communities to attract new developers. However, in view of the myriad ways that developers interact over platforms for OSS development, observers of these communities may have trouble discerning, and thus learning from, the successful patterns of developer-to-team interactions that lead to eventual team acceptance. In this work, we study project communities on GitHub to discover which forms of software contribution characterize developers who begin as development team outsiders and eventually join the team, in contrast to developers who remain team outsiders. From this, we identify and compare the forms of contribution, such as pull requests and several forms of discussion comments, that influence whether new developers join OSS teams, and we discuss the implications that these behavioral patterns have for the focus of designers and educators.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Middleton, Justin;Murphy-Hill, Emerson;Green, Demetrius;Meade, Adam;Mayer, Roger;White, David;McDonald, Steve",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051668966,10.1145/3196398.3196415,Analyzing requirements and traceability information to improve bug localization,"Locating bugs in industry-size software systems is time consuming and challenging. An automated approach for assisting the process of tracing from bug descriptions to relevant source code benefits developers. A large body of previous work aims to address this problem and demonstrates considerable achievements. Most existing approaches focus on the key challenge of improving techniques based on textual similarity to identify relevant files. However, there exists a lexical gap between the natural language used to formulate bug reports and the formal source code and its comments. To bridge this gap, state-of-the-art approaches contain a component for analyzing bug history information to increase retrieval performance. In this paper, we propose a novel approach TraceScore that also utilizes projects' requirements information and explicit dependency trace links to further close the gap in order to relate a new bug report to defective source code files. Our evaluation on more than 13,000 bug reports shows, that TraceScore significantly outperforms two state-of-the-art methods. Further, by integrating TraceScore into an existing bug localization algorithm, we found that TraceScore significantly improves retrieval performance by 49% in terms of mean average precision (MAP).",bug localization | machine learning | requirements traceability | software maintenance | traceability recovery | version history,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Rath, Michael;Lo, David;Mäder, Patrick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051676445,10.1145/3196398.3196425,Evaluating how developers use general-purpose web-search for code retrieval,"Search is an integral part of a software development process. Developers often use search engines to look for information during development, including reusable code snippets, API understanding, and reference examples. Developers tend to prefer general-purpose search engines like Google, which are often not optimized for code related documents and use search strategies and ranking techniques that are more optimized for generic, non-code related information. In this paper, we explore whether a general purpose search engine like Google is an optimal choice for code-related searches. In particular, we investigate whether the performance of searching with Google varies for code vs. non-code related searches. To analyze this, we collect search logs from 310 developers that contains nearly 150,000 search queries from Google and the associated result clicks. To differentiate between code-related searches and non-code-related searches, we build a model which identifies the code intent of queries. Leveraging this model, we build an automatic classifier that detects a code and non-code related query. We confirm the effectiveness of the classifier on manually annotated queries where the classifier achieves a precision of 87%, a recall of 86%, and an F1-score of 87%. We apply this classifier to automatically annotate all the queries in the dataset. Analyzing this dataset, we observe that code related searching often requires more effort (e.g., time, result clicks, and query modifications) than general non-code search, which indicates code search performance with a general search engine is less effective.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Rahman, Md Masudur;Barson, Jed;Paul, Sydney;Kayani, Joshua;Lois, Federico Andrés;Quezada, Sebastián Fernandez;Parnin, Christopher;Stolee, Kathryn T.;Ray, Baishakhi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051673259,10.1145/3196398.3196408,Learning to mine aligned code and natural language pairs from stack overflow,"For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Yin, Pengcheng;Deng, Bowen;Chen, Edgar;Vasilescu, Bogdan;Neubig, Graham",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051621669,10.1145/3196398.3196420,Exploring the use of automated API migrating techniques in practice: An experience report on Android,"In recent years, open source software libraries have allowed developers to build robust applications by consuming freely available application program interfaces (API). However, when these APIs evolve, consumers are left with the difficult task of migration. Studies on API migration often assume that software documentation lacks explicit information for migration guidance and is impractical for API consumers. Past research has shown that it is possible to present migration suggestions based on historical code-change information. On the other hand, research approaches with optimistic views of documentation have also observed positive results. Yet, the assumptions made by prior approaches have not been evaluated on large scale practical systems, leading to a need to affirm their validity. This paper reports our recent practical experience migrating the use of Android APIs in FDroid apps when leveraging approaches based on documentation and historical code changes. Our experiences suggest that migration through historical codechanges presents various challenges and that API documentation is undervalued. In particular, the majority of migrations from removed or deprecated Android APIs to newly added APIs can be suggested by a simple keyword search in the documentation. More importantly, during our practice, we experienced that the challenges of API migration lie beyond migration suggestions, in aspects such as coping with parameter type changes in new API. Future research may aim to design automated approaches to address the challenges that are documented in this experience report.",Android API | API migration | mining software repositories | software evolution,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Lamothe, Maxime;Shang, Weiyi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051635603,10.1145/3196398.3196423,Was self-admitted technical debt removal a real removal?: An in-depth perspective,"Technical Debt (TD) has been defined as ""code being not quite right yet"", and its presence is often self-admitted by developers through comments. The purpose of such comments is to keep track of TD and appropriately address it when possible. Building on a previous quantitative investigation by Maldonado et al. on the removal of self-admitted technical debt (SATD), in this paper we perform an in-depth quantitative and qualitative study of how SATD is addressed in five Java open source projects. On the one hand, we look at whether SATD is ""accidentally"" removed, and the extent to which the SATD removal is being documented. We found that that (i) between 20% and 50% of SATD comments are accidentally removed while entire classes or methods are dropped, (ii) 8% of the SATD removal is acknowledged in commit messages, and (iii) while most of the changes addressing SATD require complex source code changes, very often SATD is addressed by specific changes to method calls or conditionals. Our results can be used to better plan TD management or learn patterns for addressing certain kinds of TD and provide recommendations to developers.",,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Zampetti, Fiorella;Serebrenik, Alexander;Di Penta, Massimiliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3192366.3192420,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3192366.3192403,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3192366.3192383,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3192366.3192404,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058310579,10.1109/SCAM.2018.00011,A tool for optimizing Java 8 stream software via automated refactoring,"Streaming APIs are pervasive in mainstream Object-Oriented languages and platforms. For example, the Java 8 Stream API allows for functional-like, MapReduce-style operations in processing both finite, e.g., collections, and infinite data structures. However, using this API efficiently involves subtle considerations like determining when it is best for stream operations to run in parallel, when running operations in parallel can be less efficient, and when it is safe to run in parallel due to possible lambda expression side-effects. In this paper, we describe the engineering aspects of an open source automated refactoring tool called Optimize Streams that assists developers in writing optimal stream software in a semantics-preserving fashion. Based on a novel ordering and typestate analysis, the tool is implemented as a plug-in to the popular Eclipse IDE, using both the WALA and SAFE frameworks. The tool was evaluated on 11 Java projects consisting of ~642 thousand lines of code, where we found that 36.31% of candidate streams were refactorable, and an average speedup of 1.55 on a performance suite was observed. We also describe experiences gained from integrating three very different static analysis frameworks to provide developers with an easy-to-use interface for optimizing their stream code to its full potential.",Automated refactoring | Automatic parallelization | Eclipse | Java | Java 8 | Ordering | Refactoring | SAFE | Streams | Typestate analysis | WALA,"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",2018-11-09,Conference Paper,"Khatchadourian, Raffi;Tang, Yiming;Bagherzadeh, Mehdi;Ahmed, Syed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058316953,10.1109/SCAM.2018.00013,Analyzing the evolution of preprocessor-based variability: A tale of a thousand and one scripts,"Highly configurable software systems allow the efficient and reliable development of similar software variants based on a common code base. The C preprocessor CPP, which uses source code annotations that enable conditional compilation, is a simple yet powerful text-based tool for implementing such systems. However, since annotations interfere with the actual source code, the CPP has often been accused of being a source of errors and increased maintenance effort. In our research, we have been curious about whether high-level patterns of CPP misuse (i.e., code smells) can be identified, how they evolve, and whether they really hinder maintenance. To support this research, we started a simple tool which over the years evolved into a powerful toolchain. This evolution was possible because our toolchain is not monolithic, but is composed of many small tools connected by scripts and communicating via files. Moreover, we reused existing tools whenever possible and developed our own solutions only as a last resort. In this paper, we report our experiences of building this toolchain. In particular, we present design decisions we made and lessons learned, both positive and negative ones. We hope that this not only stimulates discussion and (in the best case) attracts more researchers in using our tools. Rather, we also want to encourage others to put emphasis on building tools instead of considering them 'yet another research prototype'.",C preprocessor annotations | Software evolution | Software maintenance | Source code analysis,"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",2018-11-09,Conference Paper,"Schulze, Sandro;Fenske, Wolfram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058271149,10.1109/SCAM.2018.00026,Built-in clone detection in meta languages,"Developers often practice re-use by copying and pasting code. Copied and pasted code is also known as clones. Clones may be found in all programming languages. Automated clone detection may help to detect clones in order to support software maintenance and language design. Syntax-based clone detectors find similar syntax subtrees and, hence, are guaranteed to yield only syntactic clones. They are also known to have high precision and good recall. Developing a syntax-based clone detector for each language from scratch may be an expensive task. In this paper, we explore the idea to integrate syntax-based clone detection into workbenches for language engineering. Such workbenches allow developers to create their own domain-specific language or to create parsers for existing languages. With the integration of clone detection into these workbenches, a clone detector comes as a free byproduct of the grammar specification. The effort is spent only once for the workbench and not multiple times for every language built with the workbench. We report our lessons learned in applying this idea for three language workbenches: The popular parser generator ANTLR and two language workbenches for domain-specific languages, namely, MPS, developed by JetBrains, and Xtext, which is based on the Eclipse Modeling Framework.",Antlr | Duplicated code | Language workbenchs | Meta languages | Mps | Software clones | Xtext,"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",2018-11-09,Conference Paper,"Koschke, Rainer;Schmidt, Urs Bjorn;Berger, Bernhard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058333184,10.1109/SCAM.2018.00029,"RECKA and RPromF: Two frama-C Plug-ins for optimizing registers usage in CUDA, OpenACC and OpenMP programs","Pointer aliasing still hinders compiler optimizations. The ISO C standard 99 has added the restrict keyword that allows programmer to specify non-aliasing as an aid to the compiler's optimizer. The task of annotating pointers with the restrict keyword is still left to the programmer and this task is, in general, tedious and prone to errors. Scalar replacement is an optimization widely used by compilers. In this paper, we present two new Frama-C plug-ins, RECKA for automatic annotation of CUDA kernels arguments with the restrict keyword, and RPromF for scalar replacement in OpenACC and OpenMP 4.0/4.5 codes for GPU. More specifically, RECKA works as follows: (i) an alias analysis is performed on CUDA kernels and their callers; (ii) if not found any alias then CUDA kernels are cloned, the clones are renamed and their arguments are annotated with the restrict qualifier; and (iii) instructions are added to kernels call sites to perform at runtime a less-than check analysis on kernel actuals parameters and determine if the clone must be called or the original one. RPromF includes five main steps: (i) OpenACC/OpenMP offloading regions are identified; (ii) functions containing these offloading codes and their callers are analyzed to check that there is no alias; (iii) if there is no alias then the offloading codes are cloned; (iv) clone's instructions are analyzed to retrieve data reuse information and perform scalar replacement; and instructions are added to be able to use the optimized clone whenever possible. We have evaluated the two plug-ins on PolyBench benchmark suite. The results show that both scalar replacement and the usage of restrict keyword are effective for improving the overall performance of OpenACC, OpenMP 4.0/4.5 and CUDA codes.",Alias analysis | CUDA | Frama C | OpenACC | OpenMP | Scalar replacement | Static analysis,"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",2018-11-09,Conference Paper,"Diarra, Rokiatou;Merigot, Alain;Vincke, Bastien",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3219617.3219657,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3219617.3219661,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058433440,10.1109/VISSOFT.2018.00009,RepoVis: Visual Overviews and Full-Text Search in Software Repositories,"Project managers and software developers often have difficulty maintaining an overview of the structure, evolution, and status of collaborative software projects. Some tools are available for typical source code management systems, which provide summary statistics or simple visual representations of merge-branch graphs. However, comprehensive visual overview and search facilities for such repositories are lacking. RepoVis is a new tool which provides comprehensive visual overviews and full-text search for projects maintained in Git repositories. The overview shows folders, files, and lines of code colour-coded according to last modification, developer, file type, or associated issues. Full-text searches can be performed for terms of interest within source code files, commit messages, or any associated metadata or usability findings, with matches displayed visually in the overview. The utility of the RepoVis approach is illustrated with three use cases of real-world software inspection. Insights are presented into the utility of full-text search and visual presentation of matches for program comprehension.",full-text search | git repositories | metrics | program comprehension | software visualization | usability | visual overview,"Proceedings - 6th IEEE Working Conference on Software Visualization, VISSOFT 2018",2018-11-09,Conference Paper,"Feiner, Johannes;Andrews, Keith",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058463111,10.1109/VISSOFT.2018.00015,Quality Models Inside Out: Interactive Visualization of Software Metrics by Means of Joint Probabilities,"Assessing software quality, in general, is hard; each metric has a different interpretation, scale, range of values, or measurement method. Combining these metrics automatically is especially difficult, because they measure different aspects of software quality, and creating a single global final quality score limits the evaluation of the specific quality aspects and trade-offs that exist when looking at different metrics. We present a way to visualize multiple aspects of software quality. In general, software quality can be decomposed hierarchically into characteristics, which can be assessed by various direct and indirect metrics. These characteristics are then combined and aggregated to assess the quality of the software system as a whole. We introduce an approach for quality assessment based on joint distributions of metrics values. Visualizations of these distributions allow users to explore and compare the quality metrics of software systems and their artifacts, and to detect patterns, correlations, and anomalies. Furthermore, it is possible to identify common properties and flaws, as our visualization approach provides rich interactions for visual queries to the quality models' multivariate data. We evaluate our approach in two use cases based on: 30 real-world technical documentation projects with 20,000 XML documents, and an open source project written in Java with 1000 classes. Our results show that the proposed approach allows an analyst to detect possible causes of bad or good quality.",data abstraction | hierarchical data exploration | joint probabilities | multivariate data visualization | t-SNE,"Proceedings - 6th IEEE Working Conference on Software Visualization, VISSOFT 2018",2018-11-09,Conference Paper,"Ulan, Maria;Hönel, Sebastian;Martins, Rafael M.;Ericsson, Morgan;Löwe, Welf;Wingkvist, Anna;Kerren, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051477464,10.1145/3196369.3196389,Challenges in scaling up a globally distributed legacy product: A case study of a matrix organization,"This paper presents our experiences with a 120-person matrixed software engineering product team, spread across three countries that successfully scaled their adoption of Scrum. The product is a legacy, mission-critical software system that conforms to stringent healthcare regulatory standards. We are practicing Obeya wall that brings solution to our large team communication challenges and OYA day that helps solving challenges in fostering innovation, and learning culture and collaboration. We also are describing our experience of defining focus areas of project manager and product manager. These roles are not defined in scrum guide, however, is relevant in our experience in scaled up distributed scrum environment. The authors bring our experiences as a Scrum Master, Product Owner and an architect who have been integral part of the journey and establishing these practices over several years. These practices have helped in scaling as well as stabilizing the team to an extent where each product version is meeting milestones on time and taking strong steps towards shorter release cycles of quarterly releases. This paper also summaries our lessons learned, and recommendations.",matrix organization | obeya wall | OYA day | product manager | product owner | project manager | roles | scrum master,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Gupta, Rajeev Kumar;Jain, Shivani;Singh, Bharat",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051117455,10.1007/978-3-319-96145-3_9,Exploiting synchrony and symmetry in relational verification,"Relational safety specifications describe multiple runs of the same program or relate the behaviors of multiple programs. Approaches to automatic relational verification often compose the programs and analyze the result for safety, but a naively composed program can lead to difficult verification problems. We propose to exploit relational specifications for simplifying the generated verification subtasks. First, we maximize opportunities for synchronizing code fragments. Second, we compute symmetries in the specifications to reveal and avoid redundant subtasks. We have implemented these enhancements in a prototype for verifying k-safety properties on Java programs. Our evaluation confirms that our approach leads to a consistent performance speedup on a range of benchmarks.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Pick, Lauren;Fedyukovich, Grigory;Gupta, Aarti",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051138565,10.1007/978-3-319-96145-3_16,Synthesizing reactive systems from hyperproperties,"We study the reactive synthesis problem for hyperproperties given as formulas of the temporal logic HyperLTL. Hyperproperties generalize trace properties, i.e., sets of traces, to sets of sets of traces. Typical examples are information-flow policies like noninterference, which stipulate that no sensitive data must leak into the public domain. Such properties cannot be expressed in standard linear or branching-time temporal logics like LTL, CTL, or CTL ∗. We show that, while the synthesis problem is undecidable for full HyperLTL, it remains decidable for the ∃ ∗, ∃ ∗∀ 1, and the linear∀∗ fragments. Beyond these fragments, the synthesis problem immediately becomes undecidable. For universal HyperLTL, we present a semi-decision procedure that constructs implementations and counterexamples up to a given bound. We report encouraging experimental results obtained with a prototype implementation on example specifications with hyperproperties like symmetric responses, secrecy, and information-flow.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Finkbeiner, Bernd;Hahn, Christopher;Lukert, Philip;Stenger, Marvin;Tentrup, Leander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051137041,10.1007/978-3-319-96145-3_17,Reactive control improvisation,"Reactive synthesis is a paradigm for automatically building correct-by-construction systems that interact with an unknown or adversarial environment. We study how to do reactive synthesis when part of the specification of the system is that its behavior should be random. Randomness can be useful, for example, in a network protocol fuzz tester whose output should be varied, or a planner for a surveillance robot whose route should be unpredictable. However, existing reactive synthesis techniques do not provide a way to ensure random behavior while maintaining functional correctness. Towards this end, we generalize the recently-proposed framework of control improvisation (CI) to add reactivity. The resulting framework of reactive control improvisation provides a natural way to integrate a randomness requirement with the usual functional specifications of reactive synthesis over a finite window. We theoretically characterize when such problems are realizable, and give a general method for solving them. For specifications given by reachability or safety games or by deterministic finite automata, our method yields a polynomial-time synthesis algorithm. For various other types of specifications including temporal logic formulas, we obtain a polynomial-space algorithm and prove matching PSPACE -hardness results. We show that all of these randomized variants of reactive synthesis are no harder in a complexity-theoretic sense than their non-randomized counterparts.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Fremont, Daniel J.;Seshia, Sanjit A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051111053,10.1007/978-3-319-96145-3_20,Synthesis of asynchronous reactive programs from temporal specifications,"Asynchronous interactions are ubiquitous in computing systems and complicate design and programming. Automatic construction of asynchronous programs from specifications (“synthesis”) could ease the difficulty, but known methods are complex, and intractable in practice. This work develops substantially simpler synthesis methods. A direct, exponentially more compact automaton construction is formulated for the reduction of asynchronous to synchronous synthesis. Experiments with a prototype implementation of the new method demonstrate feasibility. Furthermore, it is shown that for several useful classes of temporal properties, automaton-based methods can be avoided altogether and replaced with simpler Boolean constraint solving.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Bansal, Suguman;Namjoshi, Kedar S.;Sa’ar, Yaniv",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051127661,10.1007/978-3-319-96145-3_29,A counting semantics for monitoring LTL specifications over finite traces,"We consider the problem of monitoring a Linear Time Logic (LTL) specification that is defined on infinite paths, over finite traces. For example, we may need to draw a verdict on whether the system satisfies or violates the property “p holds infinitely often.” The problem is that there is always a continuation of a finite trace that satisfies the property and a different continuation that violates it. We propose a two-step approach to address this problem. First, we introduce a counting semantics that computes the number of steps to witness the satisfaction or violation of a formula for each position in the trace. Second, we use this information to make a prediction on inconclusive suffixes. In particular, we consider a good suffix to be one that is shorter than the longest witness for a satisfaction, and a bad suffix to be shorter than or equal to the longest witness for a violation. Based on this assumption, we provide a verdict assessing whether a continuation of the execution on the same system will presumably satisfy or violate the property.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Bartocci, Ezio;Bloem, Roderick;Nickovic, Dejan;Roeck, Franz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051124434,10.1007/978-3-319-96142-2_7,Permission inference for array programs,"Information about the memory locations accessed by a program is, for instance, required for program parallelisation and program verification. Existing inference techniques for this information provide only partial solutions for the important class of array-manipulating programs. In this paper, we present a static analysis that infers the memory footprint of an array program in terms of permission pre- and postconditions as used, for example, in separation logic. This formulation allows our analysis to handle concurrent programs and produces specifications that can be used by verification tools. Our analysis expresses the permissions required by a loop via maximum expressions over the individual loop iterations. These maximum expressions are then solved by a novel maximum elimination algorithm, in the spirit of quantifier elimination. Our approach is sound and is implemented; an evaluation on existing benchmarks for memory safety of array programs demonstrates accurate results, even for programs with complex access patterns and nested loops.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Dohrau, Jérôme;Summers, Alexander J.;Urban, Caterina;Münger, Severin;Müller, Peter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051109375,10.1007/978-3-319-96142-2_13,Symbolic algorithms for graphs and markov decision processes with fairness objectives,"Given a model and a specification, the fundamental model-checking problem asks for algorithmic verification of whether the model satisfies the specification. We consider graphs and Markov decision processes (MDPs), which are fundamental models for reactive systems. One of the very basic specifications that arise in verification of reactive systems is the strong fairness (aka Streett) objective. Given different types of requests and corresponding grants, the objective requires that for each type, if the request event happens infinitely often, then the corresponding grant event must also happen infinitely often. All ω -regular objectives can be expressed as Streett objectives and hence they are canonical in verification. To handle the state-space explosion, symbolic algorithms are required that operate on a succinct implicit representation of the system rather than explicitly accessing the system. While explicit algorithms for graphs and MDPs with Streett objectives have been widely studied, there has been no improvement of the basic symbolic algorithms. The worst-case numbers of symbolic steps required for the basic symbolic algorithms are as follows: quadratic for graphs and cubic for MDPs. In this work we present the first sub-quadratic symbolic algorithm for graphs with Streett objectives, and our algorithm is sub-quadratic even for MDPs. Based on our algorithmic insights we present an implementation of the new symbolic approach and show that it improves the existing approach on several academic benchmark examples.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Chatterjee, Krishnendu;Henzinger, Monika;Loitzenbauer, Veronika;Oraee, Simin;Toman, Viktor",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053709488,10.1145/3210459.3210460,Interrelating use cases and associated requirements by links: An eye tracking study on the impact of different linking variants on the reading behavior,"[Context:] The descriptions of interactions and system functions are two of the most important artifact types in requirements specifications. Their common notations are use cases and requirements which are related to each other. There are different variants to link a use case with its associated requirements due to a wide variety of use case templates. The main purpose of all linking variants is to highlight the interrelationships between use cases and requirements. Besides considering both artifacts for themselves, a reader needs to interrelate them to achieve a high understanding of the overall content. [Objective / Method:] Due to the effort to create and maintain links, we investigated the impact of different linking variants on the reading behavior in an eye tracking study with 15 subjects. [Results:] Our findings indicate that all investigated variants cause comparable visual effort and share the most frequent sequential reading pattern. In all cases, the use case was read first and then the requirements. Nevertheless, the different variants result in divergent reading behaviors. Especially, links embedded in the table of a use case description significantly increase the number of attention switches from the use case to the requirements. [Conclusion:] These attention switches represent the reading behavior of interrelating the use case and the associated requirements which only occurred in case of the most detailed linking variant.",Eye tracking | Interrelationship | Links | Reading behavior | Visual effort,ACM International Conference Proceeding Series,2018-06-28,Conference Paper,"Karras, Oliver;Risch, Alexandra;Schneider, Kurt",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053695411,10.1145/3210459.3210483,Why Johnny Can't store passwords securely?: A usability evaluation of bouncycastle password hashing,"Lack of usability of security Application Programming Interfaces (APIs) is one of the main reasons for mistakes that programmers make that result in security vulnerabilities in software applications they develop. Especially, APIs that provide cryptographic functionalities such as password hashing are sometimes too complex for programmers to learn and use. To improve the usability of these APIs to make them easy to learn and use, it is important to identify the usability issues exist on those APIs that make those harder to learn and use. In this work, we evaluated the usability of SCrypt password hashing functionality of Bouncycastle API to identify usability issues in it that persuade programmers to make mistakes while developing applications that would result in security vulnerabilities. We conducted a study with 10 programmers where each of them spent around 2 hours for the study and attempted to develop a secure password storage solution using Bouncycastle API. From data we collected, we identified 63 usability issues that exist in the SCrypt implementation of Bouncycastle API. Results of our study provided useful insights about how security/cryptographic APIs should be designed, developed and improved to provide a better experience for programmers who use them. Furthermore, we expect that this work will provide a guidance on how to conduct usability evaluations for security APIs to identify usability issues exist in them.",,ACM International Conference Proceeding Series,2018-06-28,Conference Paper,"Wijayarathna, Chamila;Arachchilage, Nalin A.G.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051477464,10.1145/3196369.3196389,Challenges in scaling up a globally distributed legacy product: A case study of a matrix organization,"This paper presents our experiences with a 120-person matrixed software engineering product team, spread across three countries that successfully scaled their adoption of Scrum. The product is a legacy, mission-critical software system that conforms to stringent healthcare regulatory standards. We are practicing Obeya wall that brings solution to our large team communication challenges and OYA day that helps solving challenges in fostering innovation, and learning culture and collaboration. We also are describing our experience of defining focus areas of project manager and product manager. These roles are not defined in scrum guide, however, is relevant in our experience in scaled up distributed scrum environment. The authors bring our experiences as a Scrum Master, Product Owner and an architect who have been integral part of the journey and establishing these practices over several years. These practices have helped in scaling as well as stabilizing the team to an extent where each product version is meeting milestones on time and taking strong steps towards shorter release cycles of quarterly releases. This paper also summaries our lessons learned, and recommendations.",matrix organization | obeya wall | OYA day | product manager | product owner | project manager | roles | scrum master,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Gupta, Rajeev Kumar;Jain, Shivani;Singh, Bharat",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053862604,10.1145/3208951,Prove it! Inferring formal proof scripts from CafeOBJ proof scores,"CafeOBJ is a language for writing formal specifications for a wide variety of software and hardware systems and for verifying their properties. CafeOBJ makes it possible to verify properties by using either proof scores, which consists of reducing goal-related terms in user-defined modules, or by using theorem proving. While the former is more flexible, it lacks the formal support to ensure that a property has been really proven. On the other hand, theorem proving might be too strict, since only a predefined set of commands can be applied to the current goal; hence, it hardens the verification of properties. In order to take advantage of the benefits of both techniques, we have extended CafeInMaude, a CafeOBJ interpreter implemented in Maude, with the CafeInMaude Proof Assistant (CiMPA) and the CafeInMaude Proof Generator (CiMPG). CiMPA is a proof assistant for proving inductive properties on CafeOBJ specifications that uses Maude metalevel features to allow programmers to create and manipulate CiMPA proofs. On the other hand, CiMPG provides a minimal set of annotations for identifying proof scores and generating CiMPA scripts for these proof scores. In this article, we present the CiMPA and CiMPG, detailing the behavior of the CiMPA and the algorithm underlying the CiMPG and illustrating the power of the approach by using the QLOCK protocol. Finally, we present some benchmarks that give us confidence in the matureness and usefulness of these tools.",CafeOBJ | Proof scores | Script inference | Theorem Proving,ACM Transactions on Software Engineering and Methodology,2018-07-01,Article,"Riesco, Adrián;Ogata, Kazuhiro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040724898,10.1109/TSE.2017.2658573,Empirical Evaluation of the Impact of Object-Oriented Code Refactoring on Quality Attributes: A Systematic Literature Review,"Software refactoring is a maintenance task that addresses code restructuring to improve its quality. Many studies have addressed the impact of different refactoring scenarios on software quality. This study presents a systematic literature review that aggregates, summarizes, and discusses the results of 76 relevant primary studies (PSs) concerning the impact of refactoring on several internal and external quality attributes. The included PSs were selected using inclusion and exclusion criteria applied to relevant articles published before the end of 2015. We analyzed the PSs based on a set of classification criteria, including software quality attributes and measures, refactoring scenarios, evaluation approaches, datasets, and impact results. We followed the vote-counting approach to determine the level of consistency among the PS reported results concerning the relationship between refactoring and software quality. The results indicated that different refactoring scenarios sometimes have opposite impacts on different quality attributes. Therefore, it is false that refactoring always improves all software quality aspects. The vote-counting study provided a clear view of the impacts of some individual refactoring scenarios on some internal quality attributes such as cohesion, coupling, complexity, inheritance, and size, but failed to identify their impacts on external and other internal quality attributes due to insufficient findings.",quality attribute | quality measure | refactoring scenario | systematic literature review,IEEE Transactions on Software Engineering,2018-01-01,Review,"Al Dallal, Jehad;Abdin, Anas",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85048692248,10.1109/TSE.2017.2694423,A PVS-Simulink Integrated Environment for Model-Based Analysis of Cyber-Physical Systems,"This paper presents a methodology, with supporting tool, for formal modeling and analysis of software components in cyber-physical systems. Using our approach, developers can integrate a simulation of logic-based specifications of software components and Simulink models of continuous processes. The integrated simulation is useful to validate the characteristics of discrete system components early in the development process. The same logic-based specifications can also be formally verified using the Prototype Verification System (PVS), to gain additional confidence that the software design complies with specific safety requirements. Modeling patterns are defined for generating the logic-based specifications from the more familiar automata-based formalism. The ultimate aim of this work is to facilitate the introduction of formal verification technologies in the software development process of cyber-physical systems, which typically requires the integrated use of different formalisms and tools. A case study from the medical domain is used to illustrate the approach. A PVS model of a pacemaker is interfaced with a Simulink model of the human heart. The overall cyber-physical system is co-simulated to validate design requirements through exploration of relevant test scenarios. Formal verification with the PVS theorem prover is demonstrated for the pacemaker model for specific safety aspects of the pacemaker design.",formal methods | modeling techniques | Real-time and embedded systems | specification,IEEE Transactions on Software Engineering,2018-06-01,Article,"Bernardeschi, Cinzia;Domenici, Andrea;Masci, Paolo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046480947,10.1109/TSE.2018.2832048,Mining semantic loop idioms,"To write code, developers stitch together patterns, like API protocols or data structure traversals. Discovering these patterns can identify inconsistencies in code or opportunities to replace these patterns with an API or a language construct. We present coiling, a technique for automatically mining code for semantic idioms: surprisingly probable, semantic patterns. We specialize coiling for loop idioms, semantic idioms of loops. First, we show that automatically identifiable patterns exist, in great numbers, with a large-scale empirical study of loops over 25MLOC. We find that most loops in this corpus are simple and predictable: 90 percent have fewer than 15LOC and 90 percent have no nesting and very simple control. Encouraged by this result, we then mine loop idioms over a second, buildable corpus. Over this corpus, we show that only 50 loop idioms cover 50 percent of the concrete loops. Our framework opens the door to data-driven tool and language design, discovering opportunities to introduce new API calls and language constructs. Loop idioms show that LINQ would benefit from an Enumerate operator. This can be confirmed by the exitence of a StackOverflow question with 542k views that requests precisely this feature.",code patterns | Data-driven tool design | idiom mining,IEEE Transactions on Software Engineering,2018-07-01,Article,"Allamanis, Miltiadis;Barr, Earl T.;Bird, Christian;Devanbu, Premkumar;Marron, Mark;Sutton, Charles",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045737457,10.1109/TSE.2018.2828848,On Accelerating Source Code Analysis at Massive Scale,"Encouraged by the success of data-driven software engineering (SE) techniques that have found numerous applications e.g., in defect prediction, specification inference, the demand for mining and analyzing source code repositories at scale has significantly increased. However, analyzing source code at scale remains expensive to the extent that data-driven solutions to certain SE problems are beyond our reach today. Extant techniques have focused on leveraging distributed computing to solve this problem, but with a concomitant increase in computational resource needs. This work proposes a technique that reduces the amount of computation performed by the ultra-large-scale source code mining task, especially those that make use of control and data flow analyses. Our key idea is to analyze the mining task to identify and remove the irrelevant portions of the source code, prior to running the mining task. We show a realization of our insight for mining and analyzing massive collections of control flow graphs of source codes. Our evaluation using 16 classical control-/data-flow analyses that are typical components of mining tasks and 7 Million CFGs shows that our technique can achieve on average a 40 percent reduction in the task computation time. Our case studies demonstrates the applicability of our technique to massive scale source code mining tasks.",data-driven software engineering | mining software repositories | Source code analysis | ultra-large-scale mining,IEEE Transactions on Software Engineering,2018-07-01,Article,"Upadhyaya, Ganesha;Rajan, Hridesh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051654899,10.1109/TSE.2017.2712621,A Formal Specification and Verification Framework for Timed Security Protocols,"Nowadays, protocols often use time to provide better security. For instance, critical credentials are often associated with expiry dates in system designs. However, using time correctly in protocol design is challenging, due to the lack of time related formal specification and verification techniques. Thus, we propose a comprehensive analysis framework to formally specify as well as automatically verify timed security protocols. A parameterized method is introduced in our framework to handle timing parameters whose values cannot be decided in the protocol design stage. In this work, we first propose timed applied π-calculus as a formal language for specifying timed security protocols. It supports modeling of continuous time as well as application of cryptographic functions. Then, we define its formal semantics based on timed logic rules, which facilitates efficient verification against various authentication and secrecy properties. Given a parameterized security protocol, our method either produces a constraint on the timing parameters which guarantees the security property satisfied by the protocol, or reports an attack that works for any parameter value. The correctness of our verification algorithm has been formally proved. We evaluate our framework with multiple timed and untimed security protocols and successfully find a previously unknown timing attack in Kerberos V.",parameterized verification | secrecy and authentication | timed applied π-calculus | Timed security protocol,IEEE Transactions on Software Engineering,2018-08-01,Article,"Li, Li;Sun, Jun;Liu, Yang;Sun, Meng;Dong, Jin Song",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/TSE.2017.2716950,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85028940218,10.1109/TSE.2017.2730870,Coordination Challenges in Large-Scale Software Development: A Case Study of Planning Misalignment in Hybrid Settings,"Achieving effective inter-Team coordination is one of the most pressing challenges in large-scale software development. Hybrid approaches of traditional and agile development promise combining the overview and predictability of long-Term planning on an inter-Team level with the flexibility and adaptability of agile development on a team level. It is currently unclear, however, why such hybrids often fail. Our case study within a large software development unit of 13 teams at a global enterprise software company explores how and why a combination of traditional planning on an inter-Team level and agile development on a team level can result in ineffective coordination. Based on a variety of data, including interviews with scrum masters, product owners, architects and senior management, and using Grounded Theory data analysis procedures, we identify a lack of dependency awareness across development teams as a key explanation of ineffective coordination. Our findings show how a lack of dependency awareness emerges from misaligned planning activities of specification, prioritization, estimation and allocation between agile team and traditional inter-Team levels and ultimately prevents effective coordination. Knowing about these issues, large-scale hybrid projects in similar contexts can try to better align their planning activities across levels to improve dependency awareness and in turn achieve more effective coordination.",agile | dependency awareness | hybrid | information systems development | inter-Team coordination | Large-scale software development | planning alignment,IEEE Transactions on Software Engineering,2018-10-01,Article,"Bick, Saskia;Spohrer, Kai;Hoda, Rashina;Scheerer, Alexander;Heinzl, Armin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85028931582,10.1109/TSE.2017.2734091,Measuring Program Comprehension: A Large-Scale Field Study with Professionals,"During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend ∼ 58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.",field study | inference model | Program comprehension,IEEE Transactions on Software Engineering,2018-10-01,Article,"Xia, Xin;Bao, Lingfeng;Lo, David;Xing, Zhenchang;Hassan, Ahmed E.;Li, Shanping",Include,
10.1016/j.jss.2022.111515,2-s2.0-85032230617,10.1109/TSE.2017.2764464,Metamorphic testing of RESTful Web APIs,"Web Application Programming Interfaces (APIs) allow systems to interact with each other over the network. Modern Web APIs often adhere to the REST architectural style, being referred to as RESTful Web APIs. RESTful Web APIs are decomposed into multiple resources (e.g., a video in the YouTube API) that clients can manipulate through HTTP interactions. Testing Web APIs is critical but challenging due to the difficulty to assess the correctness of API responses, i.e., the oracle problem. Metamorphic testing alleviates the oracle problem by exploiting relations (so-called metamorphic relations) among multiple executions of the program under test. In this paper, we present a metamorphic testing approach for the detection of faults in RESTful Web APIs. We first propose six abstract relations that capture the shape of many of the metamorphic relations found in RESTful Web APIs, we call these Metamorphic Relation Output Patterns (MROPs). Each MROP can then be instantiated into one or more concrete metamorphic relations. The approach was evaluated using both automatically seeded and real faults in six subject Web APIs. Among other results, we identified 60 metamorphic relations (instances of the proposed MROPs) in the Web APIs of Spotify and YouTube. Each metamorphic relation was implemented using both random and manual test data, running over 4.7K automated tests. As a result, 11 issues were detected (3 in Spotify and 8 in YouTube), 10 of them confirmed by the API developers or reproduced by other users, supporting the effectiveness of the approach.",Metamorphic testing | REST | RESTful Web services | Web API,IEEE Transactions on Software Engineering,2018-11-01,Article,"Segura, Sergio;Parejo, Jose A.;Troya, Javier;Ruiz-Cortes, Antonio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85052731880,10.1145/3233756.3233949,"Games, UX, and the gaps: Technical communication practices in an amateur game design community","Because professional game design processes and practices are often obfuscated, it is difficult for researchers to study how game design happens. In an effort to fill some of those gaps, this paper explores an amateur game design community with visible ongoing documentation practices. This research is meant to help establish cognates to professional game design practices in the service of building out the broader game design ecology. This discursive case study presents a way into a practice often closed off from technical communication scholars, UX specialists, and instructors attempting to train students in the daily work and technical communication practices of game design.",Amateur | Game design | Gaming | Iterative design | Participatory design | Player experience | Technical communication | User experience,SIGDOC 2018 - 36th ACM International Conference on the Design of Communication,2018-08-03,Conference Paper,"Karabinus, Alisha;Atherton, Rachel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85017104544,10.1145/3017678,Data-driven techniques in disaster information management,"Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users' information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations.",Application | Data management | Data mining | Disaster information management,ACM Computing Surveys,2018-01-31,Article,"Li, Tao;Xie, Ning;Zeng, Chunqiu;Zhou, Wubai;Zheng, Li;Jiang, Yexi;Yang, Yimin;Ha, Hsin Yu;Xue, Wei;Huang, Yue;Chen, Shu Ching;Navlakha, Jainendra;Iyengar, S. S.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85017178095,10.1145/3020266,A survey on data-flow testing,"Data-flow testing (DFT) is a family of testing strategies designed to verify the interactions between each program variable's definition and its uses. Such a test objective of interest is referred to as a def-use pair. DFT selects test data with respect to various test adequacy criteria (i.e., data-flow coverage criteria) to exercise each pair. The original conception of DFT was introduced by Herman in 1976. Since then, a number of studies have been conducted, both theoretically and empirically, to analyze DFT's complexity and effectiveness. In the past four decades, DFT has been continuously concerned, and various approaches from different aspects are proposed to pursue automatic and efficient data-flow testing. This survey presents a detailed overview of data-flow testing, including challenges and approaches in enforcing and automating it: (1) it introduces the data-flow analysis techniques that are used to identify def-use pairs; (2) it classifies and discusses techniques for data-flow-based test data generation, such as search-based testing, random testing, collateral-coverage-based testing, symbolic-execution-based testing, and model-checking-based testing; (3) it discusses techniques for tracking data-flow coverage; (4) it presents several DFT applications, including software fault localization, web security testing, and specification consistency checking; and (5) it summarizes recent advances and discusses future research directions toward more practical data-flow testing.","Algorithms | Coverage criteria | Coverage tracking | D.2.4 [Software/Program Verification]: Model checking | D.2.5 [Testing and Debugging]: Testing tools, Symbolic execution | Data-flow analysis | Data-flow testing | Experimentation | Reliability | Test data generation",ACM Computing Surveys,2018-01-31,Article,"Su, Ting;Wu, Ke;Miao, Weikai;Pu, Geguang;He, Jifeng;Chen, Yuting;Su, Zhendong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85017163319,10.1145/3038924,Presentation attack detection methods for face recognition systems: A comprehensive survey,"The vulnerability of face recognition systems to presentation attacks (also known as direct attacks or spoof attacks) has received a great deal of interest from the biometric community. The rapid evolution of face recognition systems into real-time applications has raised new concerns about their ability to resist presentation attacks, particularly in unattended application scenarios such as automated border control. The goal of a presentation attack is to subvert the face recognition system by presenting a facial biometric artifact. Popular face biometric artifacts include a printed photo, the electronic display of a facial photo, replaying video using an electronic display, and 3D face masks. These have demonstrated a high security risk for state-of-the-art face recognition systems. However, several presentation attack detection (PAD) algorithms (also known as countermeasures or antispoofing methods) have been proposed that can automatically detect and mitigate such targeted attacks. The goal of this survey is to present a systematic overview of the existing work on face presentation attack detection that has been carried out. This paper describes the various aspects of face presentation attacks, including different types of face artifacts, state-of-the-art PAD algorithms and an overview of the respective research labs working in this domain, vulnerability assessments and performance evaluation metrics, the outcomes of competitions, the availability of public databases for benchmarking new PAD algorithms in a reproducible manner, and finally a summary of the relevant international standardization in this field. Furthermore, we discuss the open challenges and future work that need to be addressed in this evolving field of biometrics.",Algorithms | Antispoofing | Attacks | Biometrics | C.2.2 [Pattern Recognition]: Applications | Countermeasure | Design | Face recognition | Performance | Security,ACM Computing Surveys,2018-01-31,Article,"Ramachandra, Raghavendra;Busch, Christoph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85017413241,10.1145/3037755,Systematic review of software behavioral model consistency checking,"In software development, models are often used to represent multiple views of the same system. Such models need to be properly related to each other in order to provide a consistent description of the developed system. Models may contain contradictory system specifications, for instance, when they evolve independently. Therefore, it is very crucial to ensure that models conform to each other. In this context, we focus on consistency checking of behavior models. Several techniques and approaches have been proposed in the existing literature to support behavioral model consistency checking. This article presents a Systematic Literature Review (SLR) that was carried out to obtain an overview of the various consistency concepts, problems, and solutions proposed regarding behavior models. In our study, the identification and selection of the primary studies was based on a well-planned search strategy. The search process identified a total of 1770 studies, out of which 96 have been thoroughly analyzed according to our predefined SLR protocol. The SLR aims to highlight the state-of-the-art of software behavior model consistency checking and identify potential gaps for future research. Based on research topics in selected studies, we have identified seven main categories: targeted software models, types of consistency checking, consistency checking techniques, inconsistency handling, type of study and evaluation, automation support, and practical impact. The findings of the systematic review also reveal suggestions for future research, such as improving the quality of study design and conducting evaluations, and application of research outcomes in industrial settings. For this purpose, appropriate strategy for inconsistency handling, better tool support for consistency checking and/or development tool integration should be considered in future studies.",Consistency checking | Consistency types | Software behavioral model | Systematic literature review,ACM Computing Surveys,2018-03-31,Review,"Ul Muram, Faiz;Tran, Huy;Zdun, Uwe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85023166595,10.1145/3060620,Arabic Online Handwriting Recognition (AOHR): A survey,"This article comprehensively surveys Arabic Online Handwriting Recognition (AOHR). We address the challenges posed by online handwriting recognition, including ligatures, dots and diacritic problems, online/offline touching of text, and geometric variations. Then we present a general model of an AOHR system that incorporates the different phases of an AOHR system. We summarize the main AOHR databases and identify their uses and limitations. Preprocessing techniques that are used in AOHR, viz. normalization, smoothing, de-hooking, baseline identification, and delayed stroke processing, are presented with illustrative examples. We discuss different techniques for Arabic online handwriting segmentation at the character and morpheme levels and identify their limitations. Feature extraction techniques that are used in AOHR are discussed and their challenges identified. We address the classification techniques of non-cursive (characters and digits) and cursive Arabic online handwriting and analyze their applications. We discuss different classification techniques, viz. structural approaches, Support Vector Machine (SVM), Fuzzy SVM, Neural Networks, Hidden Markov Model, Genetic algorithms, decision trees, and rule-based systems, and analyze their performance. Post-processing techniques are also discussed. Several tables that summarize the surveyed publications are provided for ease of reference and comparison. We summarize the current limitations and difficulties of AOHR and future directions of research.",Arabic online handwriting databases | Arabic online handwriting recognition | Classifiers | Feature extraction | Pre-processing | Segmentation,ACM Computing Surveys,2018-05-31,Article,"Al-Helali, Baligh M.;Mahmoud, Sabri A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027003491,10.1145/3066904,"A new classification framework to evaluate the entity profiling on the web: Past, present and future","Recently, we have witnessed entity profiling (EP) becoming increasingly one of the most important topics in information extraction, personalized applications, and web data analysis. EP aims to identify, extract, and represent a compact summary of valuable information about an entity based on the data related to it. To determine how EP systems have developed, during the last few years, this article reviews EP systems through a survey of the literature, from 2000 to 2015. To fulfill this aim, we introduce a comparison framework to compare and classify EP systems. Our comparison framework is composed of thirteen criteria that include: profiling source, the entity being modeled, the information that constitutes the profile, representation schema, profile construction technique, scale, scope/target domain, language, updating mechanism, enrichment technique, dynamicity, evaluation method, and application among others. Then, using the comparison framework, we discuss the recent development of the field and list some of the open problems and main trends that have emerged in EP to provide a proper guideline for researchers to develop or use robust profiling systems with suitable features according to their needs.",Entity profiling | Personalization | Semantic web,ACM Computing Surveys,2018-05-31,Article,"Barforoush, Ahmad Abdollahzadeh;Shirazi, Hossein;Emami, Hojjat",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85027360325,10.1145/3092697,Data-driven techniques in computing system management,"Modern forms of computing systems are becoming progressively more complex, with an increasing number of heterogeneous hardware and software components. As a result, it is quite challenging to manage these complex systems and meet the requirements in manageability, dependability, and performance that are demanded by enterprise customers. This survey presents a variety of data-driven techniques and applications with a focus on computing system management. In particular, the survey introduces intelligent methods for event generation that can transform diverse log data sources into structured events, reviews different types of event patterns and the corresponding event-mining techniques, and summarizes various event summarization methods and data-driven approaches for problem diagnosis in system management. We hope this survey will provide a good overview for data-driven techniques in computing system management.",Application | Computing system management | Data mining,ACM Computing Surveys,2018-05-31,Article,"Li, Tao;Zeng, Chunqiu;Jiang, Yexi;Zhou, Wubai;Tang, Liang;Liu, Zheng;Huang, Yue",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85028673940,10.1145/3107615,A survey on post-silicon functional validation for multicore architectures,"During a processor development cycle, post-silicon validation is performed on the first fabricated chip to detect and fix design errors. Design errors occur due to functional issues when a unit in a design does not meet its specification. The chances of occurrence of such errors are high when new features are added in a processor. Thus, in multicore architectures, with new features being added in core and uncore components, the task of verifying the functionality independently and in coordination with other units gains significance. Several new techniques are being proposed in the field of functional validation. In this article, we undertake a survey of these techniques to identify areas that need to be addressed for multicore designs. We start with an analysis of design errors in multicore architectures. We then survey different functional validation techniques based on hardware, software, and formal methods and propose a comprehensive taxonomy for each of these approaches. We also perform a critical analysis to identify gaps in existing research and propose new research directions for validation of multicore architectures.",Design errors | Functional validation | Multicore architectures,ACM Computing Surveys,2018-07-31,Article,"Jayaraman, Padma;Parthasarathi, Ranjani",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030660716,10.1145/3104031,Foundations of modern query languages for graph databases,"We survey foundational features underlying modern graph query languages. We first discuss two popular graph data models: edge-labelled graphs, where nodes are connected by directed, labelled edges, and property graphs, where nodes and edges can further have attributes. Next we discuss the two most fundamental graph querying functionalities: graph patterns and navigational expressions. We start with graph patterns, in which a graph-structured query is matched against the data. Thereafter, we discuss navigational expressions, in which patterns can be matched recursively against the graph to navigate paths of arbitrary length; we give an overview of what kinds of expressions have been proposed and how they can be combined with graph patterns. We also discuss several semantics under which queries using the previous features can be evaluated, what effects the selection of features and semantics has on complexity, and offer examples of such features in three modern languages that are used to query graphs: SPARQL, Cypher, and Gremlin. We conclude by discussing the importance of formalisation for graph query languages; a summary of what is known about SPARQL, Cypher, and Gremlin in terms of expressivity and complexity; and an outline of possible future directions for the area.",Aggregation | Graph databases | Graph patterns | Navigation | Property graphs | Query languages,ACM Computing Surveys,2018-09-30,Article,"Angles, Renzo;Arenas, Marcelo;Barceló, Pablo;Hogan, Aidan;Reutter, Juan;Vrgoč, Domagoj",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85030700921,10.1145/3124420,Automatic sarcasm detection: A survey,"Automatic sarcasm detection is the task of predicting sarcasm in text. This is a crucial step to sentiment analysis, considering prevalence and challenges of sarcasm in sentiment-bearing text. Beginning with an approach that used speech-based features, automatic sarcasm detection has witnessed great interest from the sentiment analysis community. This article is a compilation of past work in automatic sarcasm detection. We observe three milestones in the research so far: semi-supervised pattern extraction to identify implicit sentiment, use of hashtag-based supervision, and incorporation of context beyond target text. In this article, we describe datasets, approaches, trends, and issues in sarcasm detection. We also discuss representative performance values, describe shared tasks, and provide pointers to future work, as given in prior works. In terms of resources to understand the state-of-the-art, the survey presents several useful illustrations - most prominently, a table that summarizes past papers along different dimensions such as the types of features, annotation techniques, and datasets used.",Opinion | Sarcasm | Sarcasm detection | Sentiment | Sentiment analysis,ACM Computing Surveys,2018-09-30,Article,"Joshi, Aditya;Bhattacharyya, Pushpak;Carman, Mark J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040237134,10.1145/3128571,Graph processing on GPUs: A survey,"In the big data era, much real-world data can be naturally represented as graphs. Consequently, many application domains can be modeled as graph processing. Graph processing, especially the processing of the large-scale graphs with the number of vertices and edges in the order of billions or even hundreds of billions, has attracted much attention in both industry and academia. It still remains a great challenge to process such large-scale graphs. Researchers have been seeking for new possible solutions. Because of the massive degree of parallelism and the high memory access bandwidth in GPU, utilizing GPU to accelerate graph processing proves to be a promising solution. This article surveys the key issues of graph processing on GPUs, including data layout, memory access pattern, workload mapping, and specific GPU programming. In this article, we summarize the state-of-the-art research on GPU-based graph processing, analyze the existing challenges in detail, and explore the research opportunities for the future.",BSP model | GAS model | GPU | graph datasets | Graph processing | parallelism,ACM Computing Surveys,2018-11-30,Review,"Shi, Xuanhua;Zheng, Zhigao;Zhou, Yongluan;He, Ligang;Liu, Bo;Hua, Qiang Sheng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040218273,10.1145/3130876,Systematic literature review on usability of firewall configuration,"Firewalls are network security components that handle incoming and outgoing network traffic based on a set of rules. The process of correctly configuring a firewall is complicated and prone to error, and it worsens as the network complexity grows. A poorly configured firewall may result in major security threats; in the case of a network firewall, an organization's security could be endangered, and in the case of a personal firewall, an individual computer's security is threatened. A major reason for poorly configured firewalls, as pointed out in the literature, is usability issues. Our aim is to identify existing solutions that help professional and non-professional users to create and manage firewall configuration files, and to analyze the proposals in respect of usability. A systematic literature review with a focus on the usability of firewall configuration is presented in the article. Its main goal is to explore what has already been done in this field. In the primary selection procedure, 1,202 articles were retrieved and then screened. The secondary selection led us to 35 articles carefully chosen for further investigation, ofwhich 14 articleswere selected and summarized. As main contributions, we propose a taxonomy of existing solutions as well as a synthesis and in-depth discussion about the state of the art in firewall usability. Among the main findings, we perceived that there is a lack (or even an absence) of usability evaluation or user studies to validate the proposed models. Although all articles are related to the topic of usability, none of them clearly defines it, and only a few actually employ usability design principles and/or guidelines.",Firewall | systematic literature review | usability | visualization,ACM Computing Surveys,2018-11-30,Review,"Voronkov, Artem;Iwaya, Leonardo Horn;Martucci, Leonardo A.;Lindskog, Stefan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040230291,10.1145/3139222,"Geomagnetism for smartphone-based indoor localization: Challenges, advances, and comparisons","Geomagnetism has recently attracted considerable attention for indoor localization due to its pervasiveness and independence from extra infrastructure. Its location signature has been observed to be temporally stable and spatially discernible for localization purposes. This survey examines and analyzes the recent challenges and advances in geomagnetism-based indoor localization using smartphones. We first study smartphone-based geomagnetism measurements. We then review recent efforts in database construction and computation reduction, followed by state-of-the-art schemes in localizing the target. For each category, we identify practical deployment challenges and compare related studies. Finally, we summarize future directions and provide a guideline for new researchers in this field.",Geomagnetism | indoor localization | mobile computing | smartphone,ACM Computing Surveys,2018-11-30,Article,"He, Suining;Shin, Kang G.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85011660710,10.1007/s10664-016-9494-9,Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques,"In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.",Coverage | Debugging/fault localization | EXAM score | Genetic algorithm | Software product line | Test generation,Empirical Software Engineering,2018-02-01,Article,"Li, Xuelin;Wong, W. Eric;Gao, Ruizhi;Hu, Linghuan;Hosono, Shigeru",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85018360235,10.1007/s10664-017-9500-x,Search and similarity based selection of use case scenarios: An empirical study,"Use case modeling is a well-known requirements specification method and has been widely applied in practice. Use case scenarios of use case models are input elements for requirements inspection and analysis, requirements-based testing, and other downstream activities. It is, however, a practical challenge to inspect all use case scenarios that can be obtained from any non-trivial use case model, as such an inspection activity is often performed manually by domain experts. Therefore, it is needed to propose an automated solution for selecting a subset of use case scenarios with the ultimate aim of enabling cost-effective requirements (use case) inspection, analysis, and other relevant activities. Our solution is built on a natural language based, restricted use case modeling methodology (named as RUCM), in the sense that requirements specifications are specified as RUCM use case models. Use case scenarios can be automatically derived from RUCM use case models with the already established Zen-RUCM framework. In this paper, we propose a search-based and similarity-based approach called S3RCUM, through an empirical study, to select most diverse use case scenarios to enable cost-effective use case inspections. The empirical study was designed to evaluate the performance of three search algorithms together with eight similarity functions, through one real-world case study and six case studies from literature. Results show that (1+1) Evolutionary Algorithm together with Needleman-Wunsch similarity function significantly outperformed the other 31 combinations of the search algorithms and similarity functions. The combination managed to select 50% of all the generated RUCM use case scenarios for all the case studies to detect all the seeded defects.",Empirical study | Search algorithms | Similarity functions | Use case modeling | Use case scenario selection,Empirical Software Engineering,2018-02-01,Article,"Zhang, Huihui;Wang, Shuai;Yue, Tao;Ali, Shaukat;Liu, Chao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85019111869,10.1007/s10664-017-9522-4,Identifying self-admitted technical debt in open source projects using text mining,"Technical debt is a metaphor to describe the situation in which long-term code quality is traded for short-term goals in software projects. Recently, the concept of self-admitted technical debt (SATD) was proposed, which considers debt that is intentionally introduced, e.g., in the form of quick or temporary fixes. Prior work on SATD has shown that source code comments can be used to successfully detect SATD, however, most current state-of-the-art classification approaches of SATD rely on manual inspection of the source code comments. In this paper, we proposed an automated approach to detect SATD in source code comments using text mining. In our approach, we utilize feature selection to select useful features for classifier training, and we combine multiple classifiers from different source projects to build a composite classifier that identifies SATD comments in a target project. We investigate the performance of our approach on 8 open source projects that contain 212,413 comments. Our experimental results show that, on every target project, our approach outperforms the state-of-the-art and the baselines approaches in terms of F1-score. The F1-score achieved by our approach ranges between 0.518 - 0.841, with an average of 0.737, which improves over the state-of-the-art approach proposed by Potdar and Shihab by 499.19%. When compared with the text mining-based baseline approaches, our approach significantly improves the average F1-score by at least 58.49%. When compared with a natural language processing-based baseline, our approach also significantly improves its F1-score by 27.95%. Our proposed approach can be used by project personnel to effectively identify SATD with minimal manual effort.",Source code comments | Technical debt | Text mining,Empirical Software Engineering,2018-02-01,Article,"Huang, Qiao;Shihab, Emad;Xia, Xin;Lo, David;Li, Shanping",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85020632908,10.1007/s10664-017-9526-0,An exploratory qualitative and quantitative analysis of emotions in issue report comments of open source systems,"Software development—just like any other human collaboration—inevitably evokes emotions like joy or sadness, which are known to affect the group dynamics within a team. Today, little is known about those individual emotions and whether they can be discerned at all in the development artifacts produced during a project. This paper analyzes (a) whether issue reports—a common development artifact, rich in content—convey emotional information and (b) whether humans agree on the presence of these emotions. From the analysis of the issue comments of 117 projects of the Apache Software Foundation, we find that developers express emotions (in particular gratitude, joy and sadness). However, the more context is provided about an issue report, the more human raters start to doubt and nuance their interpretation. Based on these results, we demonstrate the feasibility of a machine learning classifier for identifying issue comments containing gratitude, joy and sadness. Such a classifier, using emotion-driving words and technical terms, obtains a good precision and recall for identifying the emotion love, while for joy and sadness a lower recall is obtained.",Emotion mining | Issue report | Parrott‘s framework | Text analysis,Empirical Software Engineering,2018-02-01,Article,"Murgia, Alessandro;Ortu, Marco;Tourani, Parastou;Adams, Bram;Demeyer, Serge",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85028588565,10.1007/s10664-017-9539-8,A multi-view context-aware approach to Android malware detection and malicious code localization,"Many existing Machine Learning (ML) based Android malware detection approaches use a variety of features such as security-sensitive APIs, system calls, control-flow structures and information flows in conjunction with ML classifiers to achieve accurate detection. Each of these feature sets provides a unique semantic perspective (or view) of apps’ behaviors with inherent strengths and limitations. Meaning, some views are more amenable to detect certain attacks but may not be suitable to characterize several other attacks. Most of the existing malware detection approaches use only one (or a selected few) of the aforementioned feature sets which prevents them from detecting a vast majority of attacks. Addressing this limitation, we propose MKLDroid, a unified framework that systematically integrates multiple views of apps for performing comprehensive malware detection and malicious code localization. The rationale is that, while a malware app can disguise itself in some views, disguising in every view while maintaining malicious intent will be much harder. MKLDroid uses a graph kernel to capture structural and contextual information from apps’ dependency graphs and identify malice code patterns in each view. Subsequently, it employs Multiple Kernel Learning (MKL) to find a weighted combination of the views which yields the best detection accuracy. Besides multi-view learning, MKLDroid’s unique and salient trait is its ability to locate fine-grained malice code portions in dependency graphs (e.g., methods/classes). Malicious code localization caters several important applications such as supporting human analysts studying malware behaviors, engineering malware signatures, and other counter-measures. Through our large-scale experiments on several datasets (incl. wild apps), we demonstrate that MKLDroid outperforms three state-of-the-art techniques consistently, in terms of accuracy while maintaining comparable efficiency. In our malicious code localization experiments on a dataset of repackaged malware, MKLDroid was able to identify all the malice classes with 94% average recall. Our work opens up two new avenues in malware research: (i) enables the research community to elegantly look at Android malware behaviors in multiple perspectives simultaneously, and (ii) performing precise and scalable malicious code localization.",Android malware detection | Graph kernels | Malicious code localization | Multiple kernel learning,Empirical Software Engineering,2018-06-01,Article,"Narayanan, Annamalai;Chandramohan, Mahinthan;Chen, Lihui;Liu, Yang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85029475777,10.1007/s10664-017-9547-8,Inference of development activities from interaction with uninstrumented applications,"Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.",Condition Random Field | Developers’ interaction data | Software development,Empirical Software Engineering,2018-06-01,Article,"Bao, Lingfeng;Xing, Zhenchang;Xia, Xin;Lo, David;Hassan, Ahmed E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85029604461,10.1007/s10664-017-9546-9,Sentiment Polarity Detection for Software Development,"The role of sentiment analysis is increasingly emerging to study software developers’ emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers’ communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.",Communication Channels | Sentiment Analysis | Social Software Engineering | Stack Overflow | Word Embedding,Empirical Software Engineering,2018-06-01,Article,"Calefato, Fabio;Lanubile, Filippo;Maiorano, Federico;Novielli, Nicole",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85031433548,10.1007/s10664-017-9551-z,Analyzing a decade of Linux system calls,"Over the past 25 years, thousands of developers have contributed more than 18 million lines of code (LOC) to the Linux kernel. As the Linux kernel forms the central part of various operating systems that are used by millions of users, the kernel must be continuously adapted to the changing demands and expectations of these users. The Linux kernel provides its services to an application through system calls. The combined set of all system calls forms the essential Application Programming Interface (API) through which an application interacts with the kernel. In this paper, we conduct an empirical study of 8,770 changes that were made to Linux system calls during the last decade (i.e., from April 2005 to December 2014). In particular, we study the size of the changes, and we manually identify the type of changes and bug fixes that were made. Our analysis provides an overview of the evolution of the Linux system calls over the last decade. We find that there was a considerable amount of technical debt in the kernel, that was addressed by adding a number of sibling calls (i.e., 26% of all system calls). In addition, we find that by far, the ptrace() and signal handling system calls are the most challenging to maintain. Our study can be used by developers who want to improve the design and ensure the successful evolution of their own kernel APIs.",API evolution | Empirical software engineering | Linux kernel | Software evolution | System calls,Empirical Software Engineering,2018-06-01,Article,"Bagherzadeh, Mojtaba;Kahani, Nafiseh;Bezemer, Cor Paul;Hassan, Ahmed E.;Dingel, Juergen;Cordy, James R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85034604579,10.1007/s10664-017-9569-2,An empirical study on the interplay between semantic coupling and co-change of software classes,"Software systems continuously evolve to accommodate new features and interoperability relationships between artifacts point to increasingly relevant software change impacts. During maintenance, developers must ensure that related entities are updated to be consistent with these changes. Studies in the static change impact analysis domain have identified that a combination of source code and lexical information outperforms using each one when adopted independently. However, the extraction of lexical information and the measure of how loosely or closely related two software artifacts are, considering the semantic information embedded in their comments and identifiers has been carried out using somewhat complex information retrieval (IR) techniques. The interplay between software semantic and change relationship strengths has also not been extensively studied. This work aims to fill both gaps by comparing the effectiveness of measuring semantic coupling of OO software classes using (i) simple identifier based techniques and (ii) the word corpora of the entire classes in a software system. Afterwards, we empirically investigate the interplay between semantic and change coupling. The empirical results show that: (1) identifier based methods have more computational efficiency but cannot always be used interchangeably with corpora-based methods of computing semantic coupling of classes and (2) there is no correlation between semantic and change coupling. Furthermore we found that (3) there is a directional relationship between the two, as over 70% of the semantic dependencies are also linked by change coupling but not vice versa.",Change impact analysis (CIA) | Clustering | Co-change | Co-evolution | Coupling | Hidden dependencies (HD) | Information retrieval (IR) | Object-oriented (OO) | Open-source software | Software components,Empirical Software Engineering,2018-06-01,Article,"Ajienka, Nemitari;Capiluppi, Andrea;Counsell, Steve",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85031416657,10.1007/s10664-017-9554-9,On the reaction to deprecation of clients of 4 + 1 popular Java APIs and the JDK,"Application Programming Interfaces (APIs) are a tremendous resource—that is, when they are stable. Several studies have shown that this is unfortunately not the case. Of those, a large-scale study of API changes in the Pharo Smalltalk ecosystem documented several findings about API deprecations and their impact on API clients. We extend this study, by analyzing clients of both popular third-party Java APIs and the JDK API. This results in a dataset consisting of more than 25,000 clients of five popular Java APIs on GitHub, and 60 clients of the JDK API from Maven Central. This work addresses several shortcomings of the previous study, namely: a study of several distinct API clients in a popular, statically-typed language, with more accurate version information. We compare and contrast our findings with the previous study and highlight new ones, particularly on the API client update practices and the startling similarities between reaction behavior in Smalltalk and Java. We make a comparison between reaction behavior for third-party APIs and JDK APIs, given that language APIs are a peculiar case in terms of wide-spread usage, documentation, and support from IDEs. Furthermore, we investigate the connection between reaction patterns of a client and the deprecation policy adopted by the API used.",API popularity | API usage | Application programming interface | Dataset,Empirical Software Engineering,2018-08-01,Article,"Sawant, Anand Ashok;Robbes, Romain;Bacchelli, Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85038089408,10.1007/s10664-017-9572-7,Cloned and non-cloned Java methods: a comparative study,"Reusing code via copy-and-paste, with or without modification is a common behavior observed in software engineering. Traditionally, cloning has been considered as a bad smell suggesting flaws in design decisions. Many studies exist targeting clone discovery, removal, and refactoring. However there are not many studies which empirically investigate and compare the quality of cloned code to that of the code which has not been cloned. To this end, we present a statistical study that shows whether qualitative differences exist between cloned methods and non-cloned methods in Java projects. The dataset consists of 3562 open source Java projects containing 412,705 cloned and 616,604 non-cloned methods. The study uses 27 software metrics as a proxy for quality, spanning across complexity, modularity, and documentation (code-comments) categories. When controlling for size, no statistically significant differences were found between cloned and non-cloned methods for most of the metrics, except for three of them. The main statistically significant difference found was that cloned methods are on an average 18% smaller than non-cloned methods. After doing a mixed method analysis, we provide some insight for why cloned methods are smaller.",Code clones | Open source software | Quality metrics,Empirical Software Engineering,2018-08-01,Article,"Saini, Vaibhav;Sajnani, Hitesh;Lopes, Cristina",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85045073158,10.1007/s10664-017-9591-4,Do software models based on the UML aid in source-code comprehensibility? Aggregating evidence from 12 controlled experiments,"In this paper, we present the results of long-term research conducted in order to study the contribution made by software models based on the Unified Modeling Language (UML) to the comprehensibility of Java source-code deprived of comments. We have conducted 12 controlled experiments in different experimental contexts and on different sites with participants with different levels of expertise (i.e., Bachelor’s, Master’s, and PhD students and software practitioners from Italy and Spain). A total of 333 observations were obtained from these experiments. The UML models in our experiments were those produced in the analysis and design phases. The models produced in the analysis phase were created with the objective of abstracting the environment in which the software will work (i.e., the problem domain), while those produced in the design phase were created with the goal of abstracting implementation aspects of the software (i.e., the solution/application domain). Source-code comprehensibility was assessed with regard to correctness of understanding, time taken to accomplish the comprehension tasks, and efficiency as regards accomplishing those tasks. In order to study the global effect of UML models on source-code comprehensibility, we aggregated results from the individual experiments using a meta-analysis. We made every effort to account for the heterogeneity of our experiments when aggregating the results obtained from them. The overall results suggest that the use of UML models affects the comprehensibility of source-code, when it is deprived of comments. Indeed, models produced in the analysis phase might reduce source-code comprehensibility, while increasing the time taken to complete comprehension tasks. That is, browsing source code and this kind of models together negatively impacts on the time taken to complete comprehension tasks without having a positive effect on the comprehensibility of source code. One plausible justification for this is that the UML models produced in the analysis phase focus on the problem domain. That is, models produced in the analysis phase say nothing about source code and there should be no expectation that they would, in any way, be beneficial to comprehensibility. On the other hand, UML models produced in the design phase improve source-code comprehensibility. One possible justification for this result is that models produced in the design phase are more focused on implementation details. Therefore, although the participants had more material to read and browse, this additional effort was paid back in the form of an improved comprehension of source code.",Aggregation | Controlled experiments | Heterogeneity | Unified modeling language,Empirical Software Engineering,2018-10-01,Article,"Scanniello, Giuseppe;Gravino, Carmine;Genero, Marcela;Cruz-Lemus, José A.;Tortora, Genoveffa;Risi, Michele;Dodero, Gabriella",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041817185,10.1007/s10664-017-9593-2,Program comprehension of domain-specific and general-purpose languages: replication of a family of experiments using integrated development environments,"Domain-specific languages (DSLs) allow developers to write code at a higher level of abstraction compared with general-purpose languages (GPLs). Developers often use DSLs to reduce the complexity of GPLs. Our previous study found that developers performed program comprehension tasks more accurately and efficiently with DSLs than with corresponding APIs in GPLs. This study replicates our previous study to validate and extend the results when developers use IDEs to perform program comprehension tasks. We performed a dependent replication of a family of experiments. We made two specific changes to the original study: (1) participants used IDEs to perform the program comprehension tasks, to address a threat to validity in the original experiment and (2) each participant performed program comprehension tasks on either DSLs or GPLs, not both as in the original experiment. The results of the replication are consistent with and expanded the results of the original study. Developers are significantly more effective and efficient in tool-based program comprehension when using a DSL than when using a corresponding API in a GPL. The results indicate that, where a DSL is available, developers will perform program comprehension better using the DSL than when using the corresponding API in a GPL.",Controlled experiment | Domain-specific languages | General-purpose languages | Program comprehension | Replication,Empirical Software Engineering,2018-10-01,Article,"Kosar, Tomaž;Gaberc, Sašo;Carver, Jeffrey C.;Mernik, Marjan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045030669,10.1007/s10664-018-9607-8,Getting the most from map data structures in Android,"A map is a data structure that is commonly used to store data as key–value pairs and retrieve data as keys, values, or key–value pairs. Although Java offers different map implementation classes, Android SDK offers other implementations supposed to be more efficient than HashMap: ArrayMap and SparseArray variants (SparseArray, LongSparseArray, SparseIntArray, SparseLongArray, and SparseBooleanArray). Yet, the performance of these implementations in terms of CPU time, memory usage, and energy consumption is lacking in the official Android documentation; although saving CPU, memory, and energy is a major concern of users wanting to increase battery life. Consequently, we study the use of map implementations by Android developers in two ways. First, we perform an observational study of 5713 Android apps in GitHub. Second, we conduct a survey to assess developers’ perspective on Java and Android map implementations. Then, we perform an experimental study comparing HashMap, ArrayMap, and SparseArray variants map implementations in terms of CPU time, memory usage, and energy consumption. We conclude with guidelines for choosing among the map implementations: HashMap is preferable over ArrayMap to improve energy efficiency of apps, and SparseArray variants should be used instead of HashMap and ArrayMap when keys are primitive types.",Android | CPU usage | Energy consumption | Map data structure | Map implementations | Memory usage,Empirical Software Engineering,2018-10-01,Article,"Saborido, Rubén;Morales, Rodrigo;Khomh, Foutse;Guéhéneuc, Yann Gaël;Antoniol, Giuliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85034242316,10.1007/s10664-017-9550-0,Do automated program repair techniques repair hard and important bugs?,"Existing evaluations of automated repair techniques focus on the fraction of the defects for which the technique can produce a patch, the time needed to produce patches, and how well patches generalize to the intended specification. However, these evaluations have not focused on the applicability of repair techniques and the characteristics of the defects that these techniques can repair. Questions such as “Can automated repair techniques repair defects that are hard for developers to repair?” and “Are automated repair techniques less likely to repair defects that involve loops?” have not, as of yet, been answered. To address such questions, we annotate two large benchmarks totaling 409 C and Java defects in real-world software, ranging from 22K to 2.8M lines of code, with measures of the defect’s importance, the developer-written patch’s complexity, and the quality of the test suite. We then analyze relationships between these measures and the ability to produce patches for the defects of seven automated repair techniques —AE, GenProg, Kali, Nopol, Prophet, SPR, and TrpAutoRepair. We find that automated repair techniques are less likely to produce patches for defects that required developers to write a lot of code or edit many files, or that have many tests relevant to the defect. Java techniques are more likely to produce patches for high-priority defects. Neither the time it took developers to fix a defect nor the test suite’s coverage correlate with the automated repair techniques’ ability to produce patches. Finally, automated repair techniques are less capable of fixing defects that require developers to add loops and new function calls, or to change method signatures. These findings identify strengths and shortcomings of the state-of-the-art of automated program repair along new dimensions. The presented methodology can drive research toward improving the applicability of automated repair techniques to hard and important bugs.",Automated program repair | Repairability,Empirical Software Engineering,2018-10-01,Article,"Motwani, Manish;Sankaranarayanan, Sandhya;Just, René;Brun, Yuriy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045033205,10.1007/s10664-017-9594-1,System requirements-OSS components: matching and mismatch resolution practices – an empirical study,"Developing systems by integrating Open Source Software (OSS) is increasingly gaining importance in the software industry. Although the literature claims that this approach highly impacts Requirements Engineering (RE) practices, there is a lack of empirical evidence to demonstrate this statement. To explore and understand problems and challenges of current system requirement–OSS component matching and mismatches resolution practices in software development projects that integrate one or more OSS components into their software products. Semi-structured in-depth interviews with 25 respondents that have performed RE activities in software development projects that integrate OSS components in 25 different software development companies in Spain, Norway, Sweden, and Denmark. The study uncovers 15 observations regarding system requirements-OSS components matching and mismatch resolution practices used in industrial projects that integrate OSS components. The assessed projects focused mainly on pre-release stages of software applications that integrate OSS components in an opportunistic way. The results also provide details of a set of previously unexplored scenarios when solving system requirement–OSS component mismatches; and clarify some challenges and related problems. For instance, although licensing issues and the potential changes in OSS components by their corresponding communities and/or changes in system requirements have been greatly discussed in the RE literature as problems for OSS component integration, they did not appear to be relevant in our assessed projects. Instead, practitioners highlighted the problem of getting suitable OSS component documentation/information.",Empirical study | Open Source software | OSS | Qualitative study | Requirements engineering | Survey,Empirical Software Engineering,2018-12-01,Article,"Ayala, Claudia;Nguyen-Duc, Anh;Franch, Xavier;Höst, Martin;Conradi, Reidar;Cruzes, Daniela;Babar, Muhammad Ali",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044386881,10.1007/s10664-018-9605-x,Using frame semantics for classifying and summarizing application store reviews,"Text mining techniques have been recently employed to classify and summarize user reviews on mobile application stores. However, due to the inherently diverse and unstructured nature of user-generated online textual data, text-based review mining techniques often produce excessively complicated models that are prone to overfitting. In this paper, we propose a novel approach, based on frame semantics, for app review mining. Semantic frames help to generalize from raw text (individual words) to more abstract scenarios (contexts). This lower-dimensional representation of text is expected to enhance the predictive capabilities of review mining techniques and reduce the chances of overfitting. Specifically, our analysis in this paper is two-fold. First, we investigate the performance of semantic frames in classifying informative user reviews into various categories of actionable software maintenance requests. Second, we propose and evaluate the performance of multiple summarization algorithms in generating concise and representative summaries of informative reviews. Three different datasets of app store reviews, sampled from a broad range of application domains, are used to conduct our experimental analysis. The results show that semantic frames can enable an efficient and accurate review classification process. However, in review summarization tasks, our results show that text-based summarization generates more comprehensive summaries than frame-based summarization. Finally, we introduces MARC 2.0, a review classification and summarization suite that implements the algorithms investigated in our analysis.",Application store | Classification | Frame semantics | FrameNet | Requirements elicitation | Summarization,Empirical Software Engineering,2018-12-01,Article,"Jha, Nishant;Mahmoud, Anas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047265527,10.1007/s10664-018-9623-8,Using human error information for error prevention,"Developing error-free software requirements is of critical importance to the success of a software project. Problems that occur during requirements collection and specification, if not fixed early, are costly to fix later. Therefore, it is important to develop techniques that help requirements engineers detect and prevent requirements problems. As a human-centric activity, requirements engineering can be influenced by psychological research about human errors, which are the failings of human cognition during the process of planning and executinge a task. We have employed human error research to describe the types of problems that occur during requirements engineering. The goals of this research are: (1) to evaluate whether understanding human errors contributes to the prevention of errors and concomitant faults during requirements engineering and (2) to identify error prevention techniques used in industrial practice. We conducted a controlled classroom experiment to evaluate the benefits that knowledge of errors has on error prevention. We then analyzed data from two industrial surveys to identify specific prevention and mitigation approaches employed in practice. The classroom study showed that the better a requirements engineer understands human errors, the fewer errors and concomitant faults that engineer makes when developing a new requirements document. Furthermore, different types of Human Errors have different impacts on fault prevention. The industry study results identified prevention and mitigation mechanisms for each error type. Human error information is useful for fault prevention during requirements engineering. There are practices that requirements engineers can employ to prevent or mitigate specific human errors.",Empirical study | Error prevention | Fault prevention | Human errors | Human factors | Software requirements,Empirical Software Engineering,2018-12-01,Article,"Hu, Wenhua;Carver, Jeffrey C.;Anu, Vaibhav;Walia, Gursimran S.;Bradshaw, Gary L.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85057041427,10.1007/s10664-018-9655-0,Four commentaries on the use of students and professionals in empirical software engineering experiments,,,Empirical Software Engineering,2018-12-01,Editorial,"Feldt, Robert;Zimmermann, Thomas;Bergersen, Gunnar R.;Falessi, Davide;Jedlitschka, Andreas;Juristo, Natalia;Münch, Jürgen;Oivo, Markku;Runeson, Per;Shepperd, Martin;Sjøberg, Dag I.K.;Turhan, Burak",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85031008685,10.1016/j.jss.2017.09.026,On the value of a prioritization scheme for resolving Self-admitted technical debt,"Programmers tend to leave incomplete, temporary workarounds and buggy codes that require rework in software development and such pitfall is referred to as Self-admitted Technical Debt (SATD). Previous studies have shown that SATD negatively affects software project and incurs high maintenance overheads. In this study, we introduce a prioritization scheme comprising mainly of identification, examination and rework effort estimation of prioritized tasks in order to make a final decision prior to software release. Using the proposed prioritization scheme, we perform an exploratory analysis on four open source projects to investigate how SATD can be minimized. Four prominent causes of SATD are identified, namely code smells (23.2%), complicated and complex tasks (22.0%), inadequate code testing (21.2%) and unexpected code performance (17.4%). Results show that, among all the types of SATD, design debts on average are highly prone to software bugs across the four projects analysed. Our findings show that a rework effort of approximately 10 to 25 commented LOC per SATD source file is needed to address the highly prioritized SATD (vital few) tasks. The proposed prioritization scheme is a novel technique that will aid in decision making prior to software release in an attempt to minimize high maintenance overheads.",Open source projects | Prioritization scheme | Self-admitted technical debt | Source code comment | Textual indicators,Journal of Systems and Software,2018-01-01,Article,"Mensah, Solomon;Keung, Jacky;Svajlenko, Jeffery;Bennin, Kwabena Ebo;Mi, Qing",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85031500265,10.1016/j.jss.2017.09.034,Lean Internal Startups for Software Product Innovation in Large Companies: Enablers and Inhibitors,"Context: Startups are disrupting traditional markets and replacing well-established actors with their innovative products.To compete in this age of disruption, large and established companies cannot rely on traditional ways of advancement, which focus on cost efficiency, lead time reduction and quality improvement. Corporate management is now looking for possibilities to innovate like startups. Along with it, the awareness and the use of the Lean startup approach have grown rapidly amongst the software startup community and large companies in recent years. Objective: The aim of this study is to investigate how Lean internal startup facilitates software product innovation in large companies. This study also identifies the enablers and inhibitors for Lean internal startups. Method: A multiple case study approach is followed in the investigation. Two software product innovation projects from two different large companies are examined, using a conceptual framework that is based on the method-in-action framework and extended with the previously developed Lean-Internal Corporate Venture model. Seven face-to-face in-depth interviews of the employees with different roles and responsibilities are conducted. The collected data is analysed through a careful coding process. Within-case analysis and cross-case comparison are applied to draw the findings from the two cases. Results: A generic process flow summarises the common key processes of Lean internal startups in the context of large companies. The findings suggest that an internal startup can be initiated top-down by management, or bottom-up by employees, which faces different challenges. A list of enablers and inhibitors of applying Lean startup in large companies are identified, including top management support and cross-functional team as key enablers. Both cases face different inhibitors due to the different process of inception, objective of the team and type of the product. Conclusions: The contribution of this study for research is threefold. First, this study is one of the first attempt to investigate the use of Lean startup approach in the context of large companies empirically. Second, the study shows the potential of the method-in-action framework to investigate the Lean startup approach in non-startup context. The third contribution is a general process of Lean internal startup and the evidence of the enablers and inhibitors of implementing it, which are both theory-informed and empirically grounded. Future studies could extend our study by addressing the limitations of the research approach undertaken in this study.",Internal startup | Large companies | Lean internal startup | Lean startup | Method-in-action | Software product innovation,Journal of Systems and Software,2018-01-01,Article,"Edison, Henry;Smørsgård, Nina M.;Wang, Xiaofeng;Abrahamsson, Pekka",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85048152167,10.1016/j.jss.2018.05.064,An exploratory case study on reusing architecture decisions in software-intensive system projects,"Reusing architecture decisions from previous projects promises to support architects when taking decisions. However, little is known about the state of art of decision-reuse and the benefits and challenges associated with reusing decisions. Therefore, we study how software architects reuse architecture decisions, the stakeholders and their concerns related to decision-reuse, and how architects perceive the ideal future state of decision-reuse. We conducted a qualitative explorative case study in the software-intensive systems industry. The study has shown that architects frequently reuse decisions but are confined to decisions they already know or have heard about. The results also suggest that architects reuse decisions in an ad-hoc manner. Moreover this study presents a conceptual model of decision-reuse and lists stakeholder concerns with regards to decision-reuse. The results of this study indicate that improving the documentation and discoverability of decisions holds a large potential to increase reuse of decisions and that decision documentation is not only important for system understanding or in the context of architecture reviews but also to support architects in upcoming projects.",,Journal of Systems and Software,2018-10-01,Article,"Manteuffel, Christian;Avgeriou, Paris;Hamberg, Roelof",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049335581,10.1016/j.jss.2018.06.073,Threat analysis of software systems: A systematic literature review,"Architectural threat analysis has become an important cornerstone for organizations concerned with developing secure software. Due to the large number of existing techniques it is becoming more challenging for practitioners to select an appropriate threat analysis technique. Therefore, we conducted a systematic literature review (SLR) of the existing techniques for threat analysis. In our study we compare 26 methodologies for what concerns their applicability, characteristics of the required input for analysis, characteristics of analysis procedure, characteristics of analysis outcomes and ease of adoption. We also provide insight into the obstacles for adopting the existing approaches and discuss the current state of their adoption in software engineering trends (e.g. Agile, DevOps, etc.). As a summary of our findings we have observed that: the analysis procedure is not precisely defined, there is a lack of quality assurance of analysis outcomes and tool support and validation are limited.",Risk assessment | Security-by-design | Software systems | Systematic literature review (SLR) | Threat analysis (modeling),Journal of Systems and Software,2018-10-01,Article,"Tuma, K.;Calikli, G.;Scandariato, R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049903342,10.1016/j.jss.2018.06.079,"Introducing TRAILS: A tool supporting traceability, integration and visualisation of engineering knowledge for product service systems development","Developing state of the art product service systems (PSS) requires the intense collaboration of different engineering domains, such as mechanical, software and service engineering. This can be a challenging task, since each engineering domain uses their own specification artefacts, software tools and data formats. However, to be able to seamlessly integrate the various components that constitute a PSS and also being able to provide comprehensive traceability throughout the entire solution life cycle it is essential to have a common representation of engineering data. To address this issue, we present TRAILS, a novel software tool that joins the heterogeneous artefacts, such as process models, requirements specifications or diagrams of the systems structure. For this purpose, our tool uses a semantic model integration ontology onto which various source formats can be mapped. Overall, our tool provides a wide range of features that supports engineers in ensuring traceability, avoiding system inconsistencies and putting collaborative engineering into practice. Subsequently, we show the practical implementation of our approach using the case study of a bike sharing system and discuss limitations as well as possibilities for future enhancement of TRAILS.",Model integration | Model-based systems engineering | Product service systems | Traceability,Journal of Systems and Software,2018-10-01,Article,"Wolfenstetter, Thomas;Basirati, Mohammad R.;Böhm, Markus;Krcmar, Helmut",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85050741829,10.1016/j.jss.2018.07.011,Adapting agile practices in university contexts,"Teaching agile practices has found its place in software engineering curricula in many universities across the globe. As a result, educators and students have embraced different ways to apply agile practices during their courses through lectures, games, projects, workshops and more for effective theoretical and practical learning. Practicing agile in university contexts comes with challenges for students and to counter these challenges, they perform some adaptations to standard agile practices making them effective and easier to use in university contexts. This study describes the constraints the students faced while applying agile practices in a university course taught at the University of Auckland, including difficulty in setting up common time for all team members to work together, limited availability of customer due to busy schedule and the modifications the students introduced to adapt agile practices to suit the university context, such as daily stand-ups with reduced frequency, combining sprint meetings, and rotating scrum master from team. In addition, it summarizes the effectiveness of these modifications based on reflection of the students. Recommendations for educators and students are also provided. Our findings and recommendations will help educators and students better coordinate and apply agile practices on industry-based projects in university contexts.",Adapting | Agile practices | Agile software development | Contextualization | Teaching | University,Journal of Systems and Software,2018-10-01,Article,"Masood, Zainab;Hoda, Rashina;Blincoe, Kelly",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85050809144,10.1016/j.jss.2018.06.075,Specifying uncertainty in use case models,"Context: Latent uncertainty in the context of software-intensive systems (e.g., Cyber-Physical Systems (CPSs)) demands explicit attention right from the start of development. Use case modeling—a commonly used method for specifying requirements in practice, should also be extended for explicitly specifying uncertainty. Objective: Since uncertainty is a common phenomenon in requirements engineering, it is best to address it explicitly by identifying, qualifying, and, where possible, quantifying uncertainty at the beginning stage. The ultimate aim, though not within the scope of this paper, was to use these use cases as the starting point to create test-ready models to support automated testing of CPSs under uncertainty. Method: We extend the Restricted Use Case Modeling (RUCM) methodology and its supporting tool to specify uncertainty as part of system requirements. Such uncertainties include those caused by insufficient domain expertise of stakeholders, disagreements among them, and known uncertainties about assumptions about the environment of the system. The extended RUCM, called U-RUCM, inherits the features of RUCM, such as automated analyses and generation of models, to mention but a few. Consequently, U-RUCM provides all the key benefits offered by RUCM (i.e., reducing ambiguities in requirements), but also, it allows specification of uncertainties with the possibilities of reasoning and refining existing ones and even uncovering unknown ones. Results: We evaluated U-RUCM with two industrial CPS case studies. After refining RUCM models (specifying initial requirements), by applying the U-RUCM methodology, we successfully identified and specified additional 306% and 512% (previously unknown) uncertainty requirements, as compared to the initial requirements specified in RUCM. This showed that, with U-RUCM, we were able to get a significantly better and more precise characterization of uncertainties in requirement engineering. Conclusion: Evaluation results show that U-RUCM is an effective methodology (with tool support) for dealing with uncertainty in requirements engineering. We present our experience, lessons learned, and future challenges, based on the two industrial case studies.",Belief | Uncertainty | Use case modeling,Journal of Systems and Software,2018-10-01,Article,"Zhang, Man;Yue, Tao;Ali, Shaukat;Selic, Bran;Okariz, Oscar;Norgre, Roland;Intxausti, Karmele",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051117215,10.1016/j.jss.2018.07.053,Early validation of system requirements and design through correctness-by-construction,"Early validation of requirements aims to reduce the need for the high-cost validation testing and corrective measures at late development stages. This work introduces a systematic process for the unambiguous specification of system requirements and the guided derivation of formal properties, which should be implied by the system ’s structure and behavior in conjunction with its external stimuli. This rigorous design takes place through the incremental construction of a model using the BIP (Behavior-Interaction-Priorities) component framework. It allows building complex designs by composing simpler reusable designs enforcing given properties. If some properties are neither enforced nor verified, the model is refined or certain requirements are revised. A validated model provides evidence of requirements’ consistency and design correctness. The process is semi-automated through a new tool and existing verification tools. Its effectiveness was evaluated on a set of requirements for the control software of the CubETH nanosatellite and an extract of software requirements for a Low Earth Orbit observation satellite. Our experience and obtained results helped in identifying open challenges for applying the method in industrial context. These challenges concern with the domain knowledge representation, the expressiveness of used specification languages, the library of reusable designs and scalability.",Correctness-by-construction | Model-based design | Requirements formalization | Rigorous system design,Journal of Systems and Software,2018-11-01,Article,"Stachtiari, Emmanouela;Mavridou, Anastasia;Katsaros, Panagiotis;Bliudze, Simon;Sifakis, Joseph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051778155,10.1016/j.jss.2018.08.030,SentiStrength-SE: Exploiting domain specificity for improved sentiment analysis in software engineering text,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed through building a domain dictionary and appropriate heuristics. These domain-specific techniques are then realized in SentiStrength-SE, a tool we have developed for improved sentiment analysis in text especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both qualitative and quantitative evaluations of our tool. We also separately evaluate the contributions of individual major components (i.e., domain dictionary and heuristics) of SentiStrength-SE. The empirical evaluations confirm that the domain specificity exploited in our SentiStrength-SE enables it to substantially outperform the existing domain-independent tools/toolkits (SentiStrength, NLTK, and Stanford NLP) in detecting sentiments in software engineering text.",Automation | Domain dictionary | Emotions | Empirical study | Sentiments | Software engineering,Journal of Systems and Software,2018-11-01,Article,"Islam, Md Rakibul;Zibran, Minhaz F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053763040,10.1016/j.jss.2018.09.001,Requirements traceability technologies and technology transfer decision support: A systematic review,"Requirements traceability (RT) is a core activity in Requirements Engineering. Various types of RT technologies have been extensively studied for decades. In this paper, we present a systematic literature review from 114 papers between 2006 and 2016 on RT techniques. We summarized 10 major challenges in current RT activities, and categorized existing RT techniques into 6 groups and 25 sub-groups. Moreover, we built mapping relations between these challenges and techniques, and identified 7 potential future research directions. Based on 83 empirical studies, the evaluations for technology transfer are conducted. The main conclusions are: (1) The “trustworthy” and “automated” challenges are the most widely investigated ones, while “scalable” “coordinated” “dynamic” and “lightweight” challenges receive much less attention; (2) “Trace link generation” especially information retrieval-based (IR-based) methods, are the most studied techniques; (3) IR-based methods have the most potential to be adopted by industry, as they have been validated from multiple viewpoints; (4) Seven promising future research directions are identified, which include developing scalable, dynamic and lightweight tracing techniques, introducing new approaches in other disciplines to meet the RT challenges, improving the express ability of trace links, promoting the industry adoption of RT technologies and developing new techniques to support developers’ coordination.",Quality assessment | Requirements traceability challenges | Requirements traceability technology | Systematic literature review | Technology transfer,Journal of Systems and Software,2018-12-01,Article,"Wang, Bangchao;Peng, Rong;Li, Yuanbang;Lai, Han;Wang, Zhuo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053813600,10.1016/j.jss.2018.09.039,A multi-target approach to estimate software vulnerability characteristics and severity scores,"Software vulnerabilities constitute a great risk for the IT community. The specification of the vulnerability characteristics is a crucial procedure, since the characteristics are used as input for a plethora of vulnerability scoring systems. Currently, the determination of the specific characteristics -that represent each vulnerability- is a process that is performed manually by the IT security experts. However, the vulnerability description can be very informative and useful to predict vulnerability characteristics. The primary goal of this research is the enhancement, the acceleration and the support of the manual procedure of the vulnerability characteristic assignment. To achieve this goal, a model, which combines texts analysis and multi-target classification techniques was developed. This model estimates the vulnerability characteristics and subsequently, calculates the vulnerability severity scores from the predicted characteristics. To perform the present research, a dataset that contains 99,091 records from a large -publicly available- vulnerability database was used. The results are encouraging, since they show accuracy in the prediction of the vulnerability characteristics and scores.",Information security | Multi-target classification | Software vulnerability | Text analysis,Journal of Systems and Software,2018-12-01,Article,"Spanos, Georgios;Angelis, Lefteris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85032923420,10.1016/j.jss.2017.10.032,Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions,"All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs.",Android | Exception handling | Maintainability | Robustness,Journal of Systems and Software,2018-02-01,Article,"Oliveira, Juliana;Borges, Deise;Silva, Thaisa;Cacho, Nelio;Castor, Fernando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85034035615,10.1016/j.jss.2017.08.011,A replicated experiment for evaluating the effectiveness of pairing practice in PSP education,"Background: Handling large-sized classes is one of the major challenges in Personal Software Process (PSP) education in a tertiary education environment. We applied a pairing approach in PSP education and managed to mitigate the size challenge without sacrificing education effectiveness, which has been verified in an experiment in 2010 (PSP2010). However, there are several issues (e.g., mutual interference among student pairs, confusing evaluation comments, untraceable corrections, etc.) existing in this experiment, which may create mist towards proper understanding of the education approach. Objective: In order to address the identified issues and better understand both pros and cons of the pairing approach, we replicated the experiment in 2014. Method: With new lab arrangement and evaluation mechanism devised, the replication (PSP2014) involved 120 students after their first academic year, who were separated into two groups with 40 pairs of students in one group and 40 solo students in the other. Results: Results of the replication include: 1) paired students conformed process discipline no worse (sometime better) than solo students; 2) paired students performed better than solo students in the final exam; 3) both groups spent comparable amount of time in preparing submissions; 4) both groups performed similar in size estimation and time estimation of the course assignments; 5) the quality of the programs developed by paired students is no less (sometime better) than solo students. Conclusion: The replication together with the original study confirms that, as an education approach, the pairing practice could reduce the amount of submissions required in a PSP training without sacrificing (sometime improving) the education effectiveness.",Personal software process | Replication | Software engineering education,Journal of Systems and Software,2018-02-01,Article,"Rong, Guoping;Zhang, He;Liu, Bohan;Shan, Qi;Shao, Dong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85037536057,10.1016/j.jss.2017.11.024,A Metrics Suite for code annotation assessment,"Code annotation is a language feature that enables the introduction of custom metadata on programming elements. In Java, this feature was introduced on version 5, and today it is widely used by main enterprise application frameworks and APIs. Although this language feature potentially simplifies metadata configuration, its abuse and misuse can reduce source code readability and complicate its maintenance. The goal of this paper is to propose software metrics regarding annotations in the source code and analyze their distribution in real-world projects. We have defined a suite of metrics to assess characteristics of the usage of source code annotations in a code base. Our study collected data from 24947 classes extracted from open source projects to analyze the distribution of the proposed metrics. We developed a tool to automatically extract the metrics and provide a full report on annotations usage. Based on the analysis of the distribution, we defined an appropriate approach for the calculation of thresholds to interpret the metric values. The results allow the assessment of annotated code characteristics. Using the thresholds values, we proposed a way to interpret the use of annotations, which can reveal potential problems in the source code.",Code annotation | Software metrics | Thresholds,Journal of Systems and Software,2018-03-01,Article,"Lima, Phyllipe;Guerra, Eduardo;Meirelles, Paulo;Kanashiro, Lucas;Silva, Hélio;Silveira, Fábio Fagundes",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85037997527,10.1016/j.jss.2017.11.067,Formal semantics of OMG's Interaction Flow Modeling Language (IFML) for mobile and rich-client application model driven development,"Model Driven Engineering relies on the availability of software models and of development tools supporting the transition from models to code. The generation of code from models requires the unambiguous interpretation of the semantics of the modeling languages used to specify the application. This paper presents the formalization of the semantics of the Interaction Flow Modeling Language (IFML), a recent OMG MDA standard conceived for the specification of the front-end part of interactive applications. IFML constructs are mapped to equivalent structures of Place Chart Nets (PCN), which allows their precise interpretation. The defined semantic mapping is implemented in an online Model-driven development environment that enables the creation and inspection of PCNs from IFML, the analysis of the behavior of IFML specifications via PCN simulation, and the generation of code for mobile and web-based architectures.",Mobile applications | Model-driven development | Rich-client applications | Translational semantics,Journal of Systems and Software,2018-03-01,Article,"Bernaschina, Carlo;Comai, Sara;Fraternali, Piero",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042222030,10.1016/j.jss.2017.12.007,On the use of replacement messages in API deprecation: An empirical study,"Libraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages.",API deprecation | Empirical software engineering | Mining software repositories | Software evolution,Journal of Systems and Software,2018-03-01,Article,"Brito, Gleison;Hora, Andre;Valente, Marco Tulio;Robbes, Romain",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85022065646,10.1016/j.jss.2017.06.004,Automated and reliable resource release in device drivers based on dynamic analysis,"In a modern operating system, device drivers acquire system resources to work. The acquired resources should be explicitly released by the drivers, because the operating system never reclaims them. Moreover, improper resource release can cause system crashes or hangs. Thus resource release is very important to driver reliability. However, according to our study on Linux driver mailing lists, many applied patches involve the modifications of resource release. Thus current resource management in drivers is not reliable enough. In this paper, we propose a novel approach named AutoRR, which can automatically and reliably release resources based on dynamic analysis. To identify resource handling operations, we use the dynamic specification-mining technique to mine resource acquiring and releasing functions. During execution, we maintain a resource-state list by intercepting the mined functions. If the driver fails to release acquired resources, AutoRR will report bugs and call corresponding releasing functions to safely release the resources. Dynamic analyses of resource dependency, allocation hierarchy, error handling form and releasing time are performed to avoid introducing new bugs when releasing resources. The evaluation on 12 Linux drivers shows that 40 detected bugs are all successfully and safely tolerated, and the overhead is only 7.84%.",Device drivers | Dynamic analysis | Reliability | Resource management,Journal of Systems and Software,2018-03-01,Article,"Bai, Jia Ju;Wang, Yu Ping;Hu, Shi Min",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85016586904,10.1016/j.jss.2017.03.012,Metric selection and anomaly detection for cloud operations using log and metric correlation analysis,"Cloud computing systems provide the facilities to make application services resilient against failures of individual computing resources. However, resiliency is typically limited by a cloud consumer's use and operation of cloud resources. In particular, system operations have been reported as one of the leading causes of system-wide outages. This applies specifically to DevOps operations, such as backup, redeployment, upgrade, customized scaling, and migration – which are executed at much higher frequencies now than a decade ago. We address this problem by proposing a novel approach to detect errors in the execution of these kinds of operations, in particular for rolling upgrade operations. Our regression-based approach leverages the correlation between operations’ activity logs and the effect of operation activities on cloud resources. First, we present a metric selection approach based on regression analysis. Second, the output of a regression model of selected metrics is used to derive assertion specifications, which can be used for runtime verification of running operations. We have conducted a set of experiments with different configurations of an upgrade operation on Amazon Web Services, with and without randomly injected faults to demonstrate the utility of our new approach.",Anomaly detection | Cloud application operations | Cloud monitoring | Error detection | Log analysis | Metric selection,Journal of Systems and Software,2018-03-01,Article,"Farshchi, Mostafa;Schneider, Jean Guy;Weber, Ingo;Grundy, John",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85023770296,10.1016/j.jss.2017.06.037,IoT–TEG: Test event generator system,"Internet of Things (IoT) has been paid increasingly attention by the government, academe and industry all over the world. One of the main drawbacks of the IoT systems is the amount of information they have to handle. This information arrives as events that need to be processed in real time in order to make correct decisions. Given that processing the data is crucial, testing the IoT systems that will manage that information is required. In order to test IoT systems, it is necessary to generate a huge number of events with specific structures and values to test the functionalities required by these systems. As this task is very hard and very prone to error if done by hand, this paper addresses the automated generation of appropriate events for testing. For this purpose, a general specification to define event types and its representation are proposed and an event generator is developed based on this definition. Thanks to the adaptability of the proposed specification, the event generator can generate events of an event type, or events which combine the relevant attributes of several event types. Results from experiments and real-world tests show that the developed system meets the demanded requirements.",Complex event processing | Event generator | Event type definition | Internet of Things | Testing,Journal of Systems and Software,2018-03-01,Article,"Gutiérrez-Madroñal, L.;Medina-Bulo, I.;Domínguez-Jiménez, J. J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85039428979,10.1016/j.jss.2017.12.013,Smells in software test code: A survey of knowledge in industry and academia,"As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal’ literature mapping (classification) on both the scientific literature and also practitioners’ grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects.",Automated testing | Multivocal literature mapping | Software testing | Survey | Systematic mapping | Test anti-patterns | Test automation | Test scripts | Test smells,Journal of Systems and Software,2018-04-01,Article,"Garousi, Vahid;Küçük, Barış",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042180333,10.1016/j.jss.2018.01.038,Cloud service evaluation method-based Multi-Criteria Decision-Making: A systematic literature review,"A substantial effort has been made to solve the cloud-service evaluation problem. Different Cloud Service Evaluation Methods (CSEMs) have been developed to address the problem. Cloud services are evaluated against multiple criteria, which leads to a Multi-Criteria Decision-Making (MCDM) problem. Yet, studies that assess, analyse, and summarize the unresolved problems and shortcomings of current CSEM-based MCDM are limited. In the existing review studies, only individual parts of CSEMs, rarely the full solution, are reviewed and examined. To investigate CSEMs comprehensively, we present a systematic literature review based on Evaluation Theory, a theory that generalizes six evaluation components, target, criteria, yardstick, data gathering techniques, synthesis techniques, and evaluation process. These six evaluation components and the CSEMs validation approach are the seven dimensions used to assess and analyse 77 papers published from 2006 to 2016. Sixteen research deficiencies were identified. The results confirm that the majority of the studies of the proposed CSEMs were either incomplete or lacked sufficient evidence. This research not only provides the relative strengths and weaknesses of the different CSEMs but also offers a basis for researchers and decision makers to develop improved CSEMs.",Cloud computing service | Cloud service evaluation method | Evaluation theory | Multi-Criteria Decision-Making (MCDM) | Systematic literature review,Journal of Systems and Software,2018-05-01,Article,"Alabool, Hamzeh;Kamil, Ahmad;Arshad, Noreen;Alarabiat, Deemah",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044093614,10.1016/j.jss.2018.02.065,The relation between developers’ communication and fix-Inducing changes: An empirical study,"Background Many open source and industrial projects involve several developers spread around the world and working in different timezones. Such developers usually communicate through mailing lists, issue tracking systems or chats. Lack of adequate communication can create misunderstanding and could possibly cause the introduction of bugs. Aim This paper aims at investigating the relation between the bug inducing and fixing phenomenon and the lack of written communication between committers in open source projects. Method We performed an empirical study that involved four open source projects, namely Apache httpd, GNU GCC, Mozilla Firefox, and Xorg Xserver. For each project change history data, issue tracker comments, mailing list messages, and chat logs were analyzed in order to answer four research questions about the relation between the social importance and communication level of committers and their proneness to induce bug fixes. Results and implications Results indicate that the majority of bugs are fixed by committers who did not induce them, a smaller but substantial percentage of bugs is fixed by committers that induced them, and very few bugs are fixed by committers that were not directly involved in previous changes on the same files of the fix. More importantly, committers inducing fixes tend to have a lower level of communication between each other than that of other committers. This last finding suggests that increasing the level of communication between fix-inducing committers could reduce the number of fixes induced in a software project.",Bug management | Developers’ communication | Empirical study | Social network analysis,Journal of Systems and Software,2018-06-01,Article,"Bernardi, Mario Luca;Canfora, Gerardo;Di Lucca, Giuseppe A.;Di Penta, Massimiliano;Distante, Damiano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056413411,10.1016/j.jss.2018.03.052,Towards cognitive support for unit testing: A qualitative study with practitioners,"Unit testing is an important component of software quality improvement. Several researchers proposed automated tools to improve this activity over the years. However, these research efforts have not been sufficient to help the practitioners to address some associated mental tasks. Motivated by this gap, we conducted a qualitative study of professionals with unit testing experience. The goal was to understand how to improve the cognitive support provided by the testing tools, by considering the practitioners’ perspective on their unit testing review practices. We obtained the responses from our volunteers through a questionnaire composed both of open-ended and closed questions. Our results revealed some primary tasks which require cognitive support, including monitoring of pending and executed unit testing tasks, and navigating across unit testing related artifacts. We summarize our results in a framework, and based on it, we develop a research agenda as an actionable instrument to the community. Our study's contributions comprise practical improvement suggestions for the current tools and describe further opportunities for research in software testing. Moreover, we comprehensively explain our qualitative methods.",Cognitive support | Qualitative study | Software Testing | Test Review Practice | Unit testing,Journal of Systems and Software,2018-07-01,Article,"Prado, Marllos Paiva;Vincenzi, Auri Marcelo Rizzo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045462134,10.1016/j.jss.2018.03.063,Understanding the impact of cloud patterns on performance and energy consumption,"Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application.",Cloud patterns | Energy consumption | Energy efficiency | Performance optimization,Journal of Systems and Software,2018-07-01,Article,"Khomh, Foutse;Abtahizadeh, S. Amirhossein",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045694306,10.1016/j.jss.2018.03.047,"Software search is not a science, even among scientists: A survey of how scientists and engineers find software","Improved software discovery is a prerequisite for greater software reuse: after all, if someone cannot find software for a particular task, they cannot reuse it. Understanding people's approaches and preferences when they look for software could help improve facilities for software discovery. We surveyed people working in several scientific and engineering fields to better understand their approaches and selection criteria. We found that even among highly-trained people, the rudimentary approaches of relying on general Web searches, the opinions of colleagues, and the literature were still the most commonly used. However, those who were involved in software development differed from nondevelopers in their use of social help sites, software project repositories, software catalogs, and organization-specific mailing lists or forums. For example, software developers in our sample were more likely to search in community sites such as Stack Overflow even when seeking ready-to-run software rather than source code, and likewise, asking colleagues was significantly more important when looking for ready-to-run software. Our survey also provides insight into the criteria that matter most to people when they are searching for ready-to-run software. Finally, our survey also identifies some factors that can prevent people from finding software.",Software catalogues | Software reuse | Software search | Survey,Journal of Systems and Software,2018-07-01,Article,"Hucka, M.;Graham, M. J.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85046150897,10.1016/j.jss.2018.04.046,Web service discovery based on goal-oriented query expansion,"With the broad adoption of service-oriented architecture, many software systems have been developed by composing loosely-coupled Web services. Service discovery, a critical step of building service-based systems (SBSs), aims to find a set of candidate services for each functional task to be performed by an SBS. The keyword-based search technology adopted by existing service registries is insufficient to retrieve semantically similar services for queries. Although many semantics-aware service discovery approaches have been proposed, they are hard to apply in practice due to the difficulties in ontology construction and semantic annotation. This paper aims to help service requesters (e.g., SBS designers) obtain relevant services accurately with a keyword query by exploiting domain knowledge about service functionalities (i.e., service goals) mined from textual descriptions of services. We firstly extract service goals from services’ textual descriptions using an NLP-based method and cluster service goals by measuring their semantic similarities. A query expansion approach is then proposed to help service requesters refine initial queries by recommending similar service goals. Finally, we develop a hybrid service discovery approach by integrating goal-based matching with two practical approaches: keyword-based and topic model-based. Experiments conducted on a real-world dataset show the effectiveness of our approach.",Query expansion | Service discovery | Service goal knowledge | Service-based system (SBS) | Web service,Journal of Systems and Software,2018-08-01,Article,"Zhang, Neng;Wang, Jian;Ma, Yutao;He, Keqing;Li, Zheng;Liu, Xiaoqing (Frank F.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046681152,10.1016/j.jss.2018.04.064,Quality of software requirements specification in agile projects: A cross-case analysis of six companies,"Agile Software Development (ASD) has several limitations concerning its requirements engineering activities. Improving the quality of Software Requirements Specifications (SRSs) in ASD may help to gain a competitive advantage in the software industry. Based on the findings of a Systematic Mapping study, six industrial case studies in different contexts were conducted to investigate and characterize the requirements specification activity in ASD. Data collected from documents, observations, and interviews with software engineers were triangulated, analyzed, and synthesized using Grounded Theory and Meta-Ethnography. The analysis and cross-synthesis of the six case studies resulted in a model describing the phenomenon. This model defines the simplicity and objectivity as essential quality factors of SRSs in ASD. The main factors that affect the SRSs quality in ASD projects are related to their customer-driven nature that leads to prolix SRSs, hindering its understanding from the developer perspective. The emerged model is supported by explanations and provides a deeper understanding of the requirements specification activity in ASD. This creates opportunities for further studies and improvements in SRSs for ASD in industry.",Agile methods | Agile Requirements Engineering | Empirical study | Requirements specification,Journal of Systems and Software,2018-08-01,Article,"Medeiros, Juliana;Vasconcelos, Alexandre;Silva, Carla;Goulão, Miguel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046647997,10.1016/j.jss.2018.05.002,Enable more frequent integration of software in industry projects,"Based on interviews with 20 developers from two case study companies that develop large-scale software-intensive embedded systems, this paper presents twelve factors that affect how often developers commit software to the mainline. The twelve factors are grouped into four themes: “Activity planning and execution”, “System thinking”, “Speed” and “Confidence through test activities”. Based on the interview results and a literature study we present the EMFIS model, which allows companies to explicate a representation of the organization's current situation regarding continuous integration impediments, and visualizes what the organization must focus on in order to enable more frequent integration of software. The model is used to perform an assessment of the twelve factors, where the ratings from participants representing the developers are summarized separately from ratings from participants representing the enablers (responsible for processes, development tools, test environments etc.). The EMFIS model has been validated in workshops and interviews, which in total included 46 individuals in five case study companies. The model was well received during the validation, and was appreciated for its simplicity and its ability to show differences in rating between developers and enablers.",Continuous delivery | Continuous integration | Embedded systems | Large-scale | Software integration,Journal of Systems and Software,2018-08-01,Article,"Mårtensson, Torvald;Ståhl, Daniel;Bosch, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047071722,10.1016/j.jss.2018.04.034,The exception handling riddle: An empirical study on the Android API,"We examine the use of the Java exception types in the Android platform's Application Programming Interface (API) reference documentation and their impact on the stability of Android applications. We develop a method that automatically assesses an API's quality regarding the exceptions listed in the API's documentation. We statically analyze ten versions of the Android platform's API (14–23) and 3539 Android applications to determine inconsistencies between exceptions that analysis can find in the source code and exceptions that are documented. We cross-check the analysis of the Android platform's API and applications with crash data from 901,274 application execution failures (crashes). We discover that almost 10% of the undocumented exceptions that static analysis can find in the Android platform's API source code manifest themselves in crashes. Additionally, we observe that 38% of the undocumented exceptions that developers use in their client applications to handle API methods also manifest themselves in crashes. These findings argue for documenting known might-thrown exceptions that lead to execution failures. However, a randomized controlled trial we run shows that relevant documentation improvements are ineffective and that making such exceptions checked is a more effective way for improving applications’ stability.",Application programming interfaces | Documentation | Exceptions,Journal of Systems and Software,2018-08-01,Article,"Kechagia, Maria;Fragkoulis, Marios;Louridas, Panos;Spinellis, Diomidis",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85047626396,10.1016/j.jss.2018.05.011,A likelihood-free Bayesian derivation method for service variants,"Application programming interfaces (API), allowing systems to be accessed by the services they expose, have proliferated on the Internet and gained strategic interest in the IT industry. However, integration opportunities for larger, enterprise systems are hampered by complex and overloaded operations of their interfaces, having hundreds of parameters and multiple levels of nesting, corresponding to multiple business entities. Static (code) analysis techniques have been proposed to analyse service interfaces of enterprise systems. They support the derivation of business entities and relationships from the parameters of interface operations, allowing the restructure of operations, based on individual entities. In this paper, we extend the repertoire of static interface analysis to derive service variants, whereby subsets of operation parameters correspond to multiple nested business entity subtypes of variants. Specifically, we apply a Monte Carlo sampling method, based on likelihood-free Bayesian sampling, to traverse large parameter spaces, based on higher probabilistic tree search, to efficiently find subsets of parameters related to prospective subtypes. The results demonstrate a method with significant success rates in massive search spaces, as applied to the FedEx Shipment interface whose operations have in excess of 1000 parameters.",Likelihood-free Bayesian methods | Service synthesis | Variant derivation,Journal of Systems and Software,2018-09-01,Article,"Rasmussen, Rune;Barros, Alistair;Wei, Fuguo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047095670,10.1016/j.infsof.2018.04.010,The use of artificial neural networks for extracting actions and actors from requirements document,"Context: The automatic extraction of actors and actions (i.e., use cases) of a system from natural language-based requirement descriptions, is considered a common problem in requirements analysis. Numerous techniques have been used to resolve this problem. Examples include rule-based (e.g., inference), keywords, query (e.g., bi-grams), library maintenance, semantic business vocabularies, and rules. The question remains: can combination of natural language processing (NLP) and artificial neural networks (ANNs) perform this job successfully and effectively? Objective: This paper proposes a new approach to automatically identify actors and actions in a natural language-based requirements’ description of a system. Included are descriptions of how NLP plays an important role in extracting actors and actions, and how ANNs can be used to provide definitive identification. Method: We used an NLP parser with a general architecture for text engineering, producing lexicons, syntaxes, and semantic analyses. An ANN was developed using five different use cases, producing different results due to their complexity and linguistic formation. Results: Binomial classification accuracy techniques were used to evaluate the effectiveness of this approach. Based on the five use cases, the results were 17–63% for precision, 5–6100% for recall, and 29–71% for F-measure. Conclusion: We successfully used a combination of NLP and ANN artificial intelligence techniques to reveal specific domain semantics found in a software requirements specification. An Intelligent Technique for Requirements Engineering (IT4RE) was developed to provide a semi-automated approach, classified as Intelligent Computer Aided Software Engineering (I-CASE).",ANN | GATE | I-CASE | MATLAB | NLP | Software requirements,Information and Software Technology,2018-09-01,Article,"Al-Hroob, Aysh;Imam, Ayad Tareq;Al-Heisa, Rawan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85048168263,10.1016/j.infsof.2018.05.010,"A tertiary study on technical debt: Types, management strategies, research trends, and base information for practitioners","Context: The concept of technical debt (TD) contextualizes problems faced during software evolution considering the tasks that are not carried out adequately during its development. Currently, it is common to associate any impediment related to the software product and its development process to the definition of TD. This can bring confusion and ambiguity in the use of the term. Besides, due to the increasing amount of work in the area, it is difficult to have a comprehensive view of the plethora of proposals on TD management. Objective: This paper intends to investigate the current state of research on TD by identifying what research topics have been considered, organizing research directions and practical knowledge that has already been defined, identifying the known types of TD, and organizing what activities, strategies and tools have been proposed to support the management of TD. Method: A tertiary study was performed based on a set of five research questions. In total, 13 secondary studies, dated from 2012 to March 2018, were evaluated. Results: The results of this tertiary study are beneficial for both practitioners and researchers. We evolved a taxonomy of TD types, identified a list of situations in which debt items can be found in software projects, and organized a map representing the state of the art of activities, strategies and tools to support TD management. Besides, we also summarized some research directions and practical knowledge, and identified the research topics that have been more considered in secondary studies. Conclusion: This tertiary study revisited the TD landscape. Its results can help to identify points that still require further investigation in TD research.",Management strategies | Technical debt | Technical debt types | Tertiary study,Information and Software Technology,2018-10-01,Review,"Rios, Nicolli;Mendonça Neto, Manoel Gomes de;Spínola, Rodrigo Oliveira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046661555,10.1016/j.infsof.2018.04.011,Towards a mutation analysis of IoT protocols,"Context: Mutation testing and analysis is concerned with the introduction of single faults (or errors) into a system's design, specification, implementation or interface and then testing or analysing the effects caused by those faults or errors on the system's properties and behaviour. Such faulty entities are called mutants. Objective: This short paper sketches the idea that mutations can be used to identify whether protocol implementations may deviate from the standards defining those protocols. Methods: We apply formal analysis techniques to analyse the harmful effects of mutations on IoT protocol specifications, most importantly, e.g. whether those mutations will violate the IoT protocol standard. Results: We discovered in our initial investigation one interesting case where a mutant protocol specification may lead to implementations that drop every message intended for publication to applications without breaking the standard of quality of message delivery in the protocol standard. Conclusion: We believe as a result, that there is some additional investigation needed in this direction that could be beneficial in implementing future more reliable protocols and also that the current standards need to be revised to take care of such scenarios.",,Information and Software Technology,2018-08-01,Article,"Aziz, Benjamin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85029563530,10.1016/j.infsof.2017.09.001,Factors influencing the understandability of process models: A systematic literature review,"Context Process models are key in facilitating communication in organizations and in designing process-aware information systems. Organizations are facing increasingly larger and more complex processes, which pose difficulties to the understandability of process models. The literature reports several factors that are considered to influence the understandability of process models. However, these studies typically focus on testing of a limited set of factors. A work that collects, abstracts and synthesizes an in-depth summary of the current literature will help in developing the research in this field. Objective We conducted a systematic literature review (SLR) focusing on the empirical studies in the existing literature in order to better understand the state of the research on process model understandability, and identify the gaps and opportunities for future research. Method We searched the studies between the years 1995 and 2015 in established electronic libraries. Out of 1066 publications retrieved initially, we selected 45 publications for thorough analysis. We identified, analyzed and categorized factors that are considered to influence the understandability of process models as studied in the literature using empirical methods. We also analyzed the indicators that are used to quantify process model understandability. Results Our analysis identifies several gaps in the field, as well as issues of inconsistent findings regarding the effect of some factors, unbalanced emphasis on certain indicators, and methodological concerns. Conclusions The existing research calls for comprehensive empirical studies to contribute to a better understanding of the factors of process model understandability. Our study is a comprehensive source for researchers working on the understandability of process models and related fields, and a useful guide for practitioners aiming to generate understandable process models.",Business process model | Comprehension | Process model understandability | Systematic literature review | Understandability,Information and Software Technology,2018-01-01,Review,"Dikici, Ahmet;Turetken, Oktay;Demirors, Onur",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85028800571,10.1016/j.infsof.2017.08.015,Two sides of the same coin – how agile software development teams approach uncertainty as threats and opportunities,"Context Uncertainty affects software development projects in many ways. Capabilities to manage uncertainty can determine the success or failure of entire projects and even companies. Against failure rates of software development projects, uncertainty is often viewed in a negative light. Its relevance as enabler of learning and innovation tends to be disregarded by scholars and practitioners. Objective This study extends research that empirically investigates approaches to dealing with different types of uncertainties. Acknowledging both the possibility for negative and positive consequences of uncertainty, we aim to expand existing knowledge and contribute to improving the practices to handle uncertainty in software development. Method A multiple-case study including field observations and in-depth interviews with 42 professional software developers was performed with a sample of 11 agile software development teams from different Swiss companies. Results We detail practices employed to mitigate threats while simultaneously allowing teams to remain open to opportunities. We establish a taxonomical classification of uncertainty in software development, which extends beyond often-addressed requirements uncertainty and additionally includes uncertainty related to resource availability and task specifications. Against the background of different uncertainty types, we discuss distinct practices and four overarching principles to approach uncertainty using strategies for threat avoidance versus opportunity realization. Specifically, we find that employed practices are guided by 1) uncertainty anticipation, 2) information accrual, 3) solution inspection, and 4) role-based coordination. Conclusion We argue that approaches to dealing efficiently and effectively with uncertainty can be improved when teams distinguish between different types of uncertainty and treat them explicitly as threats versus opportunities. Whereas potential to seize opportunities is under-used, agile methods could evolve into better frameworks supporting the systematic search for innovation. We discuss the implications of our findings for agile development teams in particular and product developing organizations in general.",Agile software development | Multi-case study | New product development | Opportunity | Risk | Threat | Uncertainty,Information and Software Technology,2018-01-01,Article,"Dönmez, Denniz;Grote, Gudela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044303597,10.1016/j.infsof.2018.03.005,Fixing class design inconsistencies using self regulating particle swarm optimization,"Context: The practice of using Unified Modeling Language models during software development and the chances of occurrence of model inconsistencies during software design are increasing. Hence detection of intra-model design inconsistencies is significant in the development of quality software. Objective: The existing approaches of detecting class attribute inconsistencies rely on human decision making. Manual detection of inconsistencies is exhaustive, time consuming and sometimes incomplete. Therefore, we propose an automated and novel approach to perform consistency check of class attributes using artificial intelligence. Method: Inconsistency in attribute definition and specification is detected and fixed with self regulating particle swarm optimization (SRPSO) algorithm that uses a fitness function to optimize the consistency of attributes in class diagram and activity diagrams. SRPSO is preferred since the best particle is not influenced by its or others experience and uses its direction as the best direction and the remaining particles use self and social knowledge to update their velocity and position. Result: The use of artificial intelligence technique for detection and fixing of inconsistencies during the software design phase ensures design completeness through generation of models with consistent attribute definitions and a significant improvement in software quality prediction, accurate code generation, meeting time deadlines, and software production and maintenance cost is achieved. Conclusion: Ensuring consistency and completeness of models is an inevitable aspect in software design and development. The proposed approach automates the process of inconsistency detection and correction in class attribute definition and specification using SRPSO algorithm during the design phase of software development.",Activity diagram | Artificial intelligence | Class diagram | Consistency | Self regulating particle swarm optimization,Information and Software Technology,2018-07-01,Article,"George, Renu;Samuel, Philip",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85039037417,10.1016/j.infsof.2017.11.007,A framework for the recovery and visualization of system availability scenarios from execution traces,"Context: Dynamic analysis is typically concerned with the analysis of system functional aspects at run time. However, less work has been devoted to the dynamic analysis of software quality attributes. The recovery of availability scenarios from system execution traces is particularly important for critical systems to verify that the running implementation supports and complies with availability requirements, especially if the source code is not available (e.g., in legacy systems) and after the system has undergone several ad-hoc maintenance tasks. Objective: Propose a dynamic analysis approach, along with tool support, to recover availability scenarios, from system execution traces running high availability features. Method: Native execution traces, collected from systems running high availability features, are pre-processed, filtered, merged, and segmented into execution phases. The segmented scenarios are then visualized, at a high level of abstraction, using the ITU-T standard Use Case Maps (UCM) language extended with availability annotations. Results: The applicability of our proposed approach has been demonstrated by implementing it as a prototype feature within the jUCMNav tool and by applying it to four real-world systems running high availability features. Furthermore, we have conducted an empirical study to prove that resulting UCM models improve the understandability of log files that contain high availability features. Conclusion: We have proposed a framework to filter, merge, segment, and visualize native log traces. The framework presents the following benefits: (1) it offers analysts the flexibility to specify what to include/exclude from an execution trace, (2) it provides a log segmentation method based on the type of information reported in the execution trace, (3) it uses the UCM language to visually describe availability scenarios at a high level of abstraction, and (4) it offers a scalable solution for the visualization problem through the use of the UCM stub-plug-in concept.",Availability | Dynamic analysis | Log filtering | Log segmentation | Non-functional requirements | Use case maps,Information and Software Technology,2018-04-01,Article,"Hassine, Jameleddine;Hamou-Lhadj, Abdelwahab;Alawneh, Luay",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047100794,10.1016/j.infsof.2018.05.002,Specification and automatic checking of architecture constraints on object oriented programs,"Context: Architecture constraints are specifications of conditions to which an architecture model must adhere in order to satisfy an architecture decision imposed by a given design principle. These constraints can be specified with predicate languages like OCL at design time and checked on design artifacts. Objective: Many works in the literature studied the importance of checking these constraints to guarantee quality on design models, and to prevent technical debt and maintenance difficulties. In this paper, we propose a process whose ultimate goal is to enable the checking of these constraints in the implementation stage. Method: The proposed process takes as input a textual specification of an architecture constraint specified at design stage. It translates this specification into meta-programs and then it uses them with aspect-oriented programming to check constraints at the implementation stage and at run-time on object-oriented programs. Results: We experimented an implementation of this process on a set of 12 architecture constraints. The results of this experimentation showed that our process is able to statically and dynamically detect architecture constraint violations on toy object-oriented applications, but also on real-world ones. Conclusion: The automatic checking of architecture constraints is important at source code level and at runtime. It avoids the disappearance of architecture decision knowledge in implementation artifacts, and facilitates later their maintenance.",AOP | Architecture constraint | Aspectj | Java reflect | Meta-program | Object constraint language,Information and Software Technology,2018-09-01,Article,"Kallel, Sahar;Tibermacine, Chouki;Kallel, Slim;Kacem, Ahmed Hadj;Dony, Christophe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85048475739,10.1016/j.infsof.2018.06.002,Dependability-enhanced unified modeling and simulation methodology for Critical Infrastructures,"Context: Critical infrastructures (CIs) are mission-critical, large-scale systems that provide essential products and services for everyday life. Modeling and simulation (M&S) technique is one of useful methods for understanding the complex and emergent behavior of CIs. However, the several characteristics of CIs, such as interdependence, adaptability, dependability, etc., can disturb modelers’ activities to develop the quality models for CIs. The quality of these models can affect the reliability of simulation results as well as the reusability of the models in further simulations. Objective: In this paper, we propose a M&S methodology that aims to improve the quality of CI models by means of focusing on dependability, which is one of important characteristics of CIs. Method: First, we propose a system to model and simulate CIs based on discrete event system specification, a belief-desire-intention model, and a role-oriented command hierarchy model. Next, we integrate several modeling methods, such as goal modeling, agent-based modeling, and object-oriented modeling, into a unified M&S methodology (UMAS) to seamlessly develop CI models from initial requirements to final modeling artifacts. Finally, we propose a dependability-enhanced UMAS (DUMAS) that can deal with the dependability of CI models. Results: In our case study of applying the DUMAS into the M&S of a smart grid, we show how the DUMAS reflects the characteristics of CIs on the overall modeling artifacts. Conclusion: The DUMAS provides a novel method that can improve the quality of CI models through elaborating on the activities of requirements engineering with regard to the dependability of CIs. Therefore, modelers can systematically develop quality models from requirements analysis to implementation in compliance with the DUMAS.",Critical Infrastructure | Dependability requirements | Modelling and simulation | Quality model,Information and Software Technology,2018-10-01,Article,"Kim, Hee Soo;Lee, Seok Won",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85029810360,10.1016/j.infsof.2017.09.007,An empirical study on the impact of refactoring activities on evolving client-used APIs,"Context Refactoring is recognized as an effective practice to maintain evolving software systems. For software libraries, we study how library developers refactor their Application Programming Interfaces (APIs), especially when it impacts client users by breaking an API of the library. Objective Our work aims to understand how clients that use a library API are affected by refactoring activities. We target popular libraries that potentially impact more library client users. Method We distinguish between library APIs based on their client-usage (referred to as client-used APIs) in order to understand the extent to which API breakages relate to refactorings. Our tool-based approach allows for a large-scale study across eight libraries (i.e., totaling 183 consecutive versions) with around 900 clients projects. Results We find that library maintainers are less likely to break client-used API classes. Quantitatively, we find that refactoring activities break less than 37% of all client-used APIs. In a more qualitative analysis, we show two documented cases of where non-refactoring API breaking changes are motivated other maintenance issues (i.e., bug fix and new features) and involve more complex refactoring operations. Conclusion Using our automated approach, we find that library developers are less likely to break APIs and tend to break client-used APIs when performing maintenance issues.",API Breakages | Refactoring | Software evolution | Software libraries,Information and Software Technology,2018-01-01,Article,"Kula, Raula Gaikovina;Ouni, Ali;German, Daniel M.;Inoue, Katsuro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85031715221,10.1016/j.infsof.2017.10.001,Feature-family-based reliability analysis of software product lines,"Context Verification techniques are being applied to ensure that software systems achieve desired quality levels and fulfill functional and non-functional requirements. However, applying these techniques to software product lines is challenging, given the exponential blowup of the number of products. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis, but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent exponential blowup of the configuration space. Objective The objectives of this paper are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines. Method We present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The feature-based step of our strategy divides the behavioral models into smaller units that can be analyzed more efficiently. The family-based step performs the reliability computation for all configurations at once by evaluating reliability expressions in terms of a suitable variational data structure. Results Our empirical results show that our feature-family-based strategy for reliability analysis outperforms, in terms of time and space, four state-of-the-art strategies (product-based, family-based, feature-product-based, and family-product-based) for the same property. It is the only one that could be scaled to a 220-fold increase in the size of the configuration space. Conclusion Our feature-family-based strategy leverages both feature- and family-based strategies by taming the size of the models to be analyzed and by avoiding the products enumeration inherent to some state-of-the-art analysis methods.",Parametric verification | Software product lines | Software reliability analysis,Information and Software Technology,2018-02-01,Article,"Lanna, André;Castro, Thiago;Alves, Vander;Rodrigues, Genaina;Schobbens, Pierre Yves;Apel, Sven",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046162342,10.1016/j.infsof.2018.04.007,Modeling Security and Privacy Requirements: a Use Case-Driven Approach,"Context: Modern internet-based services, ranging from food-delivery to home-caring, leverage the availability of multiple programmable devices to provide handy services tailored to end-user needs. These services are delivered through an ecosystem of device-specific software components and interfaces (e.g., mobile and wearable device applications). Since they often handle private information (e.g., location and health status), their security and privacy requirements are of crucial importance. Defining and analyzing those requirements is a significant challenge due to the multiple types of software components and devices integrated into software ecosystems. Each software component presents peculiarities that often depend on the context and the devices the component interact with, and that must be considered when dealing with security and privacy requirements. Objective: In this paper, we propose, apply, and assess a modeling method that supports the specification of security and privacy requirements in a structured and analyzable form. Our motivation is that, in many contexts, use cases are common practice for the elicitation of functional requirements and should also be adapted for describing security requirements. Method: We integrate an existing approach for modeling security and privacy requirements in terms of security threats, their mitigations, and their relations to use cases in a misuse case diagram. We introduce new security-related templates, i.e., a mitigation template and a misuse case template for specifying mitigation schemes and misuse case specifications in a structured and analyzable manner. Natural language processing can then be used to automatically report inconsistencies among artifacts and between the templates and specifications. Results: We successfully applied our approach to an industrial healthcare project and report lessons learned and results from structured interviews with engineers. Conclusion: Since our approach supports the precise specification and analysis of security threats, threat scenarios and their mitigations, it also supports decision making and the analysis of compliance to standards.",,Information and Software Technology,2018-08-01,Article,"Mai, Phu X.;Goknil, Arda;Shar, Lwin Khin;Pastore, Fabrizio;Briand, Lionel C.;Shaame, Shaban",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040131308,10.1016/j.infsof.2017.12.010,A systematic approach to API usability: Taxonomy-derived criteria and a case study,"Context: The currently existing literature about Application Program Interface (API) usability is heterogeneous in terms of goals, scope, and audience; and its connection to accepted definitions of usability is rarely made explicit. The use of metrics to measure API usability is focused only on measurable characteristics excluding those usability aspects that are related to the subjectivity of human opinions. Objective: Our objective is to build a comprehensive set of heuristics and guidelines for API usability that is a structured synthesis of the existing literature on API usability but which also covers other aspects that have been neglected so far. This set is explicitly connected with a usability model, something that allows us to check if we are addressing actual usability problems. Method: Our approach is to follow a systematic approach based on a comprehensive model of usability and context-of-use. From this comprehensive model we derived the set of heuristics and guidelines that are used to carry out a heuristic evaluation with usability experts and a subjective analysis with users. The influence of the context of use, something that is normally ignored, is explicitly analyzed. Results: Our heuristics and guidelines were integrated into a usability study of a sleep medicine API. In this study, we were able to identify several usability issues of the proposed API that are not explicitly addressed in the existing literature. The context of use helped us to identify those categories that were more relevant to consider in order to improve API usability. Conclusion: The literature on API usability is very technically-minded and tends to neglect the subjective component of usability. We contribute to a more global and comprehensive view of the usability of APIs that is not contradictory but complementary with metrics. Our criteria ease the always necessary usability evaluation with human evaluators and users.",APIs | Application program interfaces | Sleep medicine | Usability | Usability studies | Usability taxonomies,Information and Software Technology,2018-05-01,Article,"Mosqueira-Rey, Eduardo;Alonso-Ríos, David;Moret-Bonillo, Vicente;Fernández-Varela, Isaac;Álvarez-Estévez, Diego",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85034616789,10.1016/j.infsof.2017.10.008,Do the informal & formal software modeling notations satisfy practitioners for software architecture modeling?,"Context: Software architectures can be modeled using semantically informal (i.e., ambiguous) or formal (i.e., mathematically precise) software modeling notations. Objective: In this paper, 115 different practitioners from 28 different countries who work in companies that perform software development have been surveyed. The goal is to understand practitioners’ knowledge and experience about informal and formal modeling notations in specifying software architectures. Method: The survey consists of 35 different questions, divided into three parts, i.e., the participant profile, the informal modeling, and formal modeling. The results of the survey lead to many interesting findings: Results: (1) Informal software modeling notations (especially UML) are practitioners’ top choice in specifying software architectures (94%). (2) Despite many informal languages, some practitioners (40%) insist using informal ad-hoc techniques (i.e., simple boxes/lines and natural languages) to specify complex design decisions (e.g., structure, behaviour, and interaction). (3) Practitioners using informal notations are impressed by their low learning-curve (79%), visuality (62%), and general-purpose scope (66%). (4) Practitioners still criticise informal notations too, essentially for the lack of support for complex design decisions and their exhaustive analysis. (5) While formal modeling notations bridge this gap mentioned in step-4, many practitioners (70%) rarely use formal notations due essentially to the high-learning curve, the lack of knowledge among stakeholders, and the lack of popularity in industry. (6) Among the considered formal notations (i.e., process algebras, high-level formal specification languages, and architecture description languages (ADLs)), process algebras are never used and ADLs are the least used formal languages are ADLs (i.e., 12% frequently use ADLs). (7) Practitioners complain about ADLs’ weak tool support (38%) and their lack of tutorials/guidance/etc (33%). Conclusion: The survey results will let the community realise the advantages and disadvantages of informal and formal modeling notations for software architecture modeling from practitioners’ viewpoint.",A survey | ADLs | Formal specification languages | Informal/formal semantics | Software architectures | UML,Information and Software Technology,2018-03-01,Article,"Ozkaya, Mert",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046135039,10.1016/j.infsof.2018.04.003,Learning from the past: A process recommendation system for video game projects using postmortems experiences,"Context: The video game industry is a billion dollar industry that faces problems in the way games are developed. One method to address these problems is using developer aid tools, such as Recommendation Systems. These tools assist developers by generating recommendations to help them perform their tasks. Objective: This article describes a systematic approach to recommend development processes for video game projects, using postmortem knowledge extraction and a model of the context of the new project, in which “postmortems” are articles written by video game developers at the end of projects, summarizing the experience of their game development team. This approach aims to provide reflections about development processes used in the game industry as well as guidance to developers to choose the most adequate process according to the contexts they're in. Method: Our approach is divided in three separate phases: in the first phase, we manually extracted the processes from the postmortems analysis; in the second one, we created a video game context and algorithm rules for recommendation; and finally in the third phase, we evaluated the recommended processes by using quantitative and qualitative metrics, game developers feedback, and a case study by interviewing a video game development team. Contributions: This article brings three main contributions. The first describes a database of developers’ experiences extracted from postmortems in the form of development processes. The second defines the main attributes that a video game project contain, which it uses to define the contexts of the project. The third describes and evaluates a recommendation system for video game projects, which uses the contexts of the projects to identify similar projects and suggest a set of activities in the form of a process.",Recommendation system | Software development process | Video game development,Information and Software Technology,2018-08-01,Article,"Politowski, Cristiano;Fontoura, Lisandra M.;Petrillo, Fabio;Guéhéneuc, Yann Gaël",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85050459897,10.1016/j.infsof.2018.07.009,Exploring information from OSS repositories and platforms to support OSS selection decisions,"Context: Individuals and organizations are increasingly adopting Open Source Software (OSS) for the benefits it provides. Although the OSS evaluation process and the information it requires are nowadays well known, users still have problems finding the right information and are not supported by any decision support system. Objective: The aim of this study is to bridge the gap between OSS adoption models, especially with the aim of supporting users in evaluating the OSS they are planning to select. Method: To reach this aim, we studied the processes and the information considered by the major OSS assessment models. Then we carried out a case study to identify which information can be automatically retrieved from the main OSS platforms, namely GitHub, SonarCloud, and StackExchange. Finally, we characterized the maturity of the projects available on these three platforms. Results: Projects available on the three platforms are commonly old, stable, and mature ones. Moreover, thanks to the API provided, we were able to extract most of the information not commonly accessible from the main website. Conclusions: Our results confirm that it is possible to develop a decision support system based on these three platforms, and that is also possible to evaluate both the quality and the maturity of the projects available there.",,Information and Software Technology,2018-12-01,Article,"Sbai, Nesrine;Lenarduzzi, Valentina;Taibi, Davide;Sassi, Sihem Ben;Ghezala, Henda Hajjami Ben",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85031677311,10.1016/j.infsof.2017.10.004,A systematic mapping study about socio-technical congruence,"Context Lack of coordination may create significant problems between work teams, this problem is even most critical when team workers are geographically distributed as it results in cost increases and delays in the projects. There exists a technique called Socio-Technical Congruence (STC) that aims at helping to measure and control the coordination level existing in an organization at their different levels. Objective The objective of this paper is to carry out a systematic mapping of the field of socio-technical congruence. The aspects of particular interest for this article are: Socio-Technical Congruence definition, different ways to measure it, available tools to measure or that can help to measure it, the areas of application, its benefits and the case studies that analyze the effects Socio-Technical Congruence has on the organization as regards the products quality and the improvements in performance in the long term development, in an attempt to characterize the state of the art of this field identifying gaps and opportunities for further research. Therefore, companies could use this work as a starting point to apply STC measures in their work teams. Results This paper presents the results of a systematic mapping of Literature about Socio-Technical Congruence (STC) in order to investigate and classify the existing articles and conferences about the subject, as well as summarizing the most important aspects in regards to provide a general overview about the existing studies. Conclusions After analyzing the 40 papers found, we can conclude that there is no one standard measure of socio-technical congruence, although most take the proposal by Kwan et al. applying adaptations and improvements on it as regards the environment that it will be focused on. In general, most case studies talk about the benefits of STC control in organizations. However, only one paper focus on global software development where the problems of communication, coordination and control are an important risk. Moreover, there are only a few papers that explore the risks of excessively overloading users with coordination iterations when controlling STC. In fact, no case study to examine these risks and their effect on developers’ productivity has been found. The small number of studies found on STC, together with the research gaps we have pointed out, suggest that further investigation on socio-technical congruence is required.",Coordination | Social-technical dependencies | Socio-technical congruence | Systematic Mapping Study | Tools,Information and Software Technology,2018-02-01,Review,"Sierra, José María;Vizcaíno, Aurora;Genero, Marcela;Piattini, Mario",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044534167,10.1016/j.infsof.2018.01.015,What factors influence the reviewer assignment to pull requests?,"Context: When external contributors want to collaborate with an open-source project, they can send a pull request to the project core team. Subsequently, a core team member is assigned to review the pull request. Recently, some techniques for recommending reviewers to analyze pull requests were proposed. However, they replicate previous (not always desired) patterns and do not provide the rationale behind the recommendation. Objective: This paper aims at understanding the factors that influence on assigning reviewers to pull requests and evaluating the extent of this influence. Method: To reach this goal, we mined association rules from 22,523 pull requests belonging to 3 projects hosted on GitHub. In addition to showing such patterns, we also present a qualitative analysis that explains why the patterns arose. Results: In summary our results indicate that (i) some reviewers always analyze smaller pull requests, with few commits and few files; (ii) some reviewers frequently (up to 58% of the times) analyze pull requests filed by inexperienced requesters; (iii) some reviewers have more chances (up to 25 times) to analyze pull requests filed by requesters of their acquaintance; and (iv) some reviewers have more chances (up to 20 times) to analyze pull requests containing files that they have recently changed. Conclusion: In particular, the results allow for the following conclusions: (i) factors such as number of commits and files in the pull request may influence the reviewers assignment; (ii) factors regarding the requester profile may influence on reviewer allocation; (iii) the social relationship between requester and reviewer exert influence on pull request evaluation, that is, when the reviewer knows the requester, his or her chances of evaluating such contributions may increase; and (iv) factors such as ownership and locality of pull request artifacts are important predictors for the reviewer. Furthermore, we point out that, besides identifying influence factors related to pull request reviewer, the adopted approach allowed us to quantify the extent of that influence via support, confidence, and lift metrics.",Association rules | Pull request | Reviewer assignment,Information and Software Technology,2018-06-01,Article,"Soares, Daricélio M.;de Lima Júnior, Manoel L.;Plastino, Alexandre;Murta, Leonardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041687509,10.1016/j.infsof.2018.01.016,Feature interaction in software product line engineering: A systematic mapping study,"Context: Software product lines (SPL) engineering defines a set of systems that share common features and artifacts to achieve high productivity, quality, market agility, low time to market, and cost. An SPL product is derived from a configuration of features which need to be compounded together without violating their particular specifications. While it is easy to identify the behavior of a feature in isolation, specifying and resolving interactions among features may not be a straightforward task. The feature interaction problem has been a challenging subject for decades. Objective: This study aims at surveying existing research on feature interaction in SPL engineering in order to identify common practices and research trends. Method: A systematic mapping study was conducted with a set of seven research questions, in which the 35 studies found are mainly classified regarding the feature interaction solution presented: detection, resolution and general analysis. Results: 43% of the papers deal with feature interaction at early phases of a software lifecycle. The remaining is shared among the other categories: source code detection, resolution and analysis. For each category, it was also identified the main strategies used to deal with interactions. Conclusions: The findings can help to understand the needs in feature interaction for SPL engineering, and highlight aspects that still demand an additional investigation. For example, often strategies are partial and only address specific points of a feature interaction investigation.",Feature interaction | Software product lines | Systematic mapping,Information and Software Technology,2018-06-01,Article,"Soares, Larissa Rocha;Schobbens, Pierre Yves;do Carmo Machado, Ivan;de Almeida, Eduardo Santana",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85066778074,10.1109/APSEC.2018.00082,A Doc2Vec-Based Assessment of Comments and Its Application to Change-Prone Method Analysis,"Comments in a source program can be helpful artifacts for program comprehension. While many comments are useful documents embedded in source programs, there are also poorly-informative comments in the real world. In order to quantitatively assess the value of comments, this paper proposes applying the Doc2Vec model to comment evaluation. Doc2Vec is a useful model for vectorizing the content of a document. In this paper, a Java method is regarded as a document, and its content is expressed as a vector. Then, two vectors corresponding to different versions of a method are prepared - the original version and the comment-erased version - , and the vector similarity between these two versions are computed. If the erased comments provided richer information for the source code, the corresponding vector would have a larger change through the comment elimination. A method having poorly-informative comments may be low-quality and might require more code modifications. This paper analyzes the relationship between the value of comments in a method and the change-proneness, using the data collected from five popular open source software projects. The results show that a method having poorly-informative comments is likely to be change-prone, i.e., such a method could not survive unscathed after release.",assessment | change prone analysis | comment | Doc2Vec,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2018-07-02,Conference Paper,"Aman, Hirohisa;Amasaki, Sousuke;Yokogawa, Tomoyuki;Kawahara, Minoru",Include,
10.1016/j.jss.2022.111515,2-s2.0-85066801474,10.1109/APSEC.2018.00047,Analyzing Code Comments to Boost Program Comprehension,"We are trying to find source code comments that help programmers understand a nontrivial part of source code. One of such examples would be explaining to assign a zero as a way to 'clear' a buffer. Such comments are invaluable to programmers and identifying them correctly would be of great help. Toward this goal, we developed a method to discover explanatory code comments in a source code. We first propose 12 distinct categories of code comments. We then developed a decision-tree based classifier that can identify explanatory comments with 60% precision and 80% recall. We analyzed 2,000 GitHub projects that are written in two languages: Java and Python. This task is novel in that it focuses on a microscopic comment ('local comment') within a method or function, in contrast to the prior efforts that focused on API-or method-level comments. We also investigated how different category of comments is used in different projects. Our key finding is that there are two dominant types of comments: preconditional and postconditional. Our findings also suggest that many English code comments have a certain grammatical structure that are consistent across different projects.",decision tree | github | java | Natural language processing | Program Comprehension | python | Source code comments,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2018-07-02,Conference Paper,"Shinyama, Yusuke;Arahori, Yoshitaka;Gondow, Katsuhiko",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85060653210,10.1109/UBMK.2018.8566660,Creating Important Statement Type Comments in Autocomment: Automatic Comment Generation Framework,"This study explains important statement comments generated by Autocomment, which is an automatic comment generation framework described in [1]. To create the comments, the source code is not expected to be runnable, but to conform to the syntax rules of the Java programming language. Two types of comments are created: Summary comments and important statement comments. How the important statements are extracted in a given Java source, how they are classified, how comments are generated for each type of important statement have been explained in this work.",documentation generation | program comprehension | Source code summarization,UBMK 2018 - 3rd International Conference on Computer Science and Engineering,2018-12-06,Conference Paper,"Yildiz, Eren;Ekin, Emine",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85057206278,10.1109/SEAA.2018.00081,A systematic mapping study on API documentation generation approaches,"Background: Application Programming Interfaces (APIs) are key to software reuse. Software developers can link functionality and behaviour found in other software with their own software by taking an API into use. However, figuring out how an API works is usually demanding, and may require that the developers spend a notable amount of time familiarizing themselves with the API. Good API documentation is of key importance to simplify this task. Objective: To present a comprehensive, unbiased overview of the state-of-the-art on tools and approaches for API documentation generation. Method: A systematic mapping study on published tools and approaches that can be used for generating API documentation, or for assisting in the API documentation process. Results: 36 studies on API documentation generation tools and approaches analyzed and categorized in a variety of ways. Among other things, the paper presents an overview of what kind of tools have been developed, what kind of documentation they generate, and what sources the documentation approaches require. Conclusion: Out of the identified approaches, many contribute to API documentation in the areas of natural language documentation and code examples and templates. Many of the approaches contribute to ease API users' understanding and learning of the API, but also to the maintenance and generation of API documentation. Most of the approaches are automatic, simplifying the API documentation generation notably, under the assumption that relevant sources for the generation are available. Most of the API documentation approaches are evaluated either by exercise of the approach followed by analysis of the results, or by empirical evaluation methods.",API | API documentation | Systematic mapping study,"Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018",2018-10-18,Conference Paper,"Nybom, Kristian;Ashraf, Adnan;Porres, Ivan",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85055420907,10.1109/COMPSAC.2018.00029,Automatically Detecting the Scopes of Source Code Comments,"Comments which are an integral part of software development improve program comprehension and software maintainability. They convey useful information about the system functionalities and many text retrieval methods for software engineering tasks take comments as an important source for code semantic analysis. However, it is challenging to identify the relationship between the functional semantics of the code and its corresponding textual descriptions and apply it to automatic mining approaches in software engineering efficiently. In this paper, we use machine learning which utilizes features of code snippets and comments to detect the scopes of source code comments automatically in Java programs. Based on the dataset of comment-statement pairs from 4 popular open source projects, our method achieved a high accuracy of 81.15% in detecting the scopes of comments. Furthermore, the experimental results demonstrated the feasibility and effectiveness of our comment scope detection method.",Comment scope detection | Machine learning,Proceedings - International Computer Software and Applications Conference,2018-06-08,Conference Paper,"Chen, Huanchao;Liu, Zhiyong;Chen, Xiangping;Zhou, Fan;Luo, Xiaonan",Include,
10.1016/j.jss.2022.111515,2-s2.0-85055442415,10.1109/COMPSAC.2018.00028,Automatic Detection of Outdated Comments during Code Changes,"Comments are used as standard practice in software development to increase the readability of code and to express programmers' intentions in a more explicit manner. Nevertheless, keeping comments up-to-date is often neglected for programmers. In this paper, we proposed a machine learning based method for detecting the comments that should be changed during code changes. We utilized 64 features, taking the code before and after changes, comments and the relationship between the code and comments into account. Experimental results show that 74.6% of outdated comments can be detected using our method, and 77.2% of our detected outdated comments are real comments which require to be updated. In addition, the experimental results indicate that our model can help developers to discover outdated comments in historical versions of existing projects.",Code Changes | Outdated Comment Detection | Random Forest,Proceedings - International Computer Software and Applications Conference,2018-06-08,Conference Paper,"Liu, Zhiyong;Chen, Huanchao;Chen, Xiangping;Luo, Xiaonan;Zhou, Fan",Include,
10.1016/j.jss.2022.111515,2-s2.0-85048310407,10.1109/IC2E.2018.00041,A precise model for Google cloud platform,"Today, Google Cloud Platform (GCP) is one of the leaders among cloud APIs. Although it was established only five years ago, GCP has gained notable expansion due to its suite of public cloud services that it based on a huge, solid infrastructure. GCP allows developers to use these services by accessing GCP RESTful API that is described through HTML pages on its website. However, the documentation of GCP API is written in natural language (English prose) and therefore shows several drawbacks, such as Informal Heterogeneous Documentation, Imprecise Types, Implicit Attribute Metadata, Hidden Links, Redundancy and Lack of Visual Support. To avoid confusion and misunderstandings, the cloud developers obviously need a precise specification of the knowledge and activities in GCP. Therefore, this paper introduces GCP Model, an inferred formal model-driven specification of GCP which describes without ambiguity the resources offered by GCP. GCP Model is conform to the Open Cloud Computing Interface (OCCI) metamodel and is implemented based on the open source model-driven Eclipse-based OCCIware tool chain. Thanks to our GCP Model, we offer corrections to the drawbacks we identified.",Cloud computing | Google cloud platform | Model driven engineering | OCCI,"Proceedings - 2018 IEEE International Conference on Cloud Engineering, IC2E 2018",2018-05-16,Conference Paper,"Challita, Stephanie;Zalila, Faiez;Gourdin, Christophe;Merle, Philippe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85054846359,10.1145/3230977.3231009,Using program analysis to improve API learnability,"Learning from API documentation and tutorials is challenging for many programmers. Improving the learnability of APIs can reduce this barrier, especially for newprogrammers.We will use the tools of program analysis to extract key concepts and learning dependencies from API source code, API documentation, open source code, and other online sources of information on APIs. With this information we will generate learning maps for any user-provided code snippet, and will take users through each concept used in the code snippet. Users may also navigate through the most commonly used features of an API without providing a code snippet. We also hope to extend this work to help users find the features of an API they need and also help them integrate that into their code.",API learnability | Auto-generated documentation | Program analysis,ICER 2018 - Proceedings of the 2018 ACM Conference on International Computing Education Research,2018-08-08,Conference Paper,"Thayer, Kyle",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85056846084,10.1109/VLHCC.2018.8506523,API designers in the field: Design practices and challenges for creating usable APIs,"Application Programming Interfaces (APIs) are a rapidly growing industry and the usability of the APIs is crucial to programmer productivity. Although prior research has shown that APIs commonly suffer from significant usability problems, little attention has been given to studying how APIs are designed and created in the first place. We interviewed 24 professionals involved with API design from 7 major companies to identify their training and design processes. Interviewees had insights into many different aspects of designing for API usability and areas of significant struggle. For example, they learned to do API design on the job, and had little training for it in school. During the design phase they found it challenging to discern which potential use cases of the API users will value most. After an API is released, designers lack tools to gather aggregate feedback from this data even as developers openly discuss the API online.",API Usability | Developer Experience (DevX | DX) | Empirical Studies of Programmers | Web Services,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2018-10-23,Conference Paper,"Murphy, Lauren;Kery, Mary Beth;Alliyu, Oluwatosin;Macvean, Andrew;Myers, Brad A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85058328014,10.1109/DySDoc3.2018.00007,Automatically generating natural language documentation for methods,"A tool to automatically generate natural language documentation summaries for methods is presented. The approach uses prior work by the authors on stereotyping methods along with the source code analysis framework srcML. First, each method is automatically assigned a stereotype(s) based on static analysis and a set of heuristics. Then, the approach uses the stereotype information, static analysis, and predefined templates to generate a natural-language summary for each method. This summary is automatically added to the code base as a comment for each method. The predefined templates are designed to produce a generic summary for specific method stereotypes.",Documentation | Method | Stereotype | Summarization,"Proceedings - 3rd International Workshop on Dynamic Software Documentation, DySDoc3 2018",2018-11-09,Conference Paper,"Newman, Christian;Dragan, Natalia;Collard, Michael L.;Maletic, Jonathan;Decker, Michael;Guarnera, Drew;Abid, Nahla",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85048865182,10.1109/MALTESQUE.2018.8368458,Investigating type declaration mismatches in Python,"Past research provided evidence that developers making code changes sometimes omit to update the related documentation, thus creating inconsistencies that may contribute to faults and crashes. In dynamically typed languages, such as Python, an inconsistency in the documentation may lead to a mismatch in type declarations only visible at runtime. With our study, we investigate how often the documentation is inconsistent in a sample of 239 methods from five Python open-source software projects. Our results highlight that more than 20% of the comments are either partially defined or entirely missing and that almost 1% of the methods in the analyzed projects contain type inconsistencies. Based on these results, we create a tool, PyID, to early detect type mismatches in Python documentation and we evaluate its performance with our oracle.",,"2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings",2018-05-29,Conference Paper,"Pascarella, Luca;Ram, Achyudh;Nadeem, Azqa;Bisesser, Dinesh;Knyazev, Norman;Bacchelli, Alberto",Include,
10.1016/j.jss.2022.111515,2-s2.0-85048873436,10.1109/MALTESQUE.2018.8368457,ConfigFile++: Automatic comment enhancement for misconfiguration prevention,"Nowadays, misconfiguration has become one of the key factors leading to system problems. Most current research on the topic explores misconfiguration diagnosis, but is less concerned with educating users about how to configure correctly in order to prevent misconfiguration before it happens. In this paper, we manually study 22 open source software projects and summarize several observations on the comments of their configuration files, most of which lack sufficient information and are poorly formatted. Based on these observations and the general process of misconfiguration diagnosis, we design and implement a tool called ConfigFile++ that automatically enhances the comment in configuration files. By using name-based analysis and machine learning, ConfigFile++ extracts guiding information about the configuration option from the user manual and source code, and inserts it into the configuration files. The format of insert comment is also designed to make enhanced comments concise and clear. We use real-world examples of misconfigurations to evaluate our tool. The results show that ConfigFile++ can prevent 33 out of 50 misconfigurations.",comment enhancement | constraint extraction | misconfiguration prevention,"2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings",2018-05-29,Conference Paper,"Zhang, Yuanliang;Li, Shanshan;Xu, Xiangyang;Liao, Xiangke;Yang, Shazhou;Xiong, Yun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051872294,10.1145/3194793.3194796,Where does Google find API documentation?,"The documentation of popular APIs is spread across many formats, from vendor-curated reference documentation to Stack Overflow threads. For developers, it is often not obvious from where a particular piece of information can be retrieved. To understand this documentation landscape, we systematically conducted Google searches for the elements of ten popular APIs.We found that their documentation is widely dispersed among many sources, that GitHub and Stack Overflow play a prominent role among the search results, and that most sources are quick to document new API functionalities. These findings inform API vendors about where developers find documentation about their products, they inform developers about places to look for documentation, and they enable researchers to further study the software documentation landscape.",,Proceedings - International Conference on Software Engineering,2018-06-02,Conference Paper,"Treude, Christoph;Aniche, Maurício",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85051867089,10.1145/3194793.3194795,Discovering API usability problems at scale,"Software developers' productivity can be negatively impacted by using APIs incorrectly. In this paper, we describe an analysis technique we designed to find API usability problems by comparing successive file-level changes made by individual software developers. We applied our tool, StopMotion, to the file histories of real developers doing real tasks at Google. The results reveal several API usability challenges including simple typos, conceptual API misalignments, and conflation of similar APIs.",,Proceedings - International Conference on Software Engineering,2018-06-02,Conference Paper,"Murphy-Hill, Emerson;Sadowski, Caitlin;Head, Andrew;Daughtry, John;Macvean, Andrew;Jaspan, Ciera;Winter, Collin",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85046933574,10.1145/3173574.3174029,Doppio: Tracking UI flows and code changes for app development,"Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes.",Android | Demonstrations | IDEs | Mobile apps | Screencast videos | Screenflow diagram | Software documentation,Conference on Human Factors in Computing Systems - Proceedings,2018-04-20,Conference Paper,"Chi, Pei Yu;Hu, Sen Po;Li, Yang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.19153/CLEIEJ.21.1.5,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85031772226,10.1016/j.csi.2017.09.010,Spotting and Removing WSDL Anti-pattern Root Causes in Code-first Web Services Using NLP Techniques: A Thorough Validation of Impact on Service Discoverability,"To expose software as Web-accesible services, Web Service technologies demand developers to implement certain sofware artifacts, such as the service description using WSDL. Therefore, developers usually use automatic tools to perform this task, which take as input a code written in a programming language –e.g. Java– and generate the necessary artifacts for invoking it remotely. However, as a result of tool flaws and some bad coding practices, the description of the resulting Web Services might contain anti-patterns that difficult their discovery and use. In earlier work we proposed a tool-supported, code-first approach named Gapidt to develop Web Services in Java while early reducing the presence of anti-patterns in their descriptions through code refactorings. Bad coding practices, which potentially reduce the textual and structural information of generated WSDL documents, are automatically detected and informed to the developer by means of the GAnalyzer module so he/she can fix the service code. Moreover, developer provided information, such as service parameter names and operation comments, as well as re-utilization of data-type definitions, are exploited by the GMapper module upon generating WSDL documents. This paper focuses on a comprehensive experimental evaluation of the approach oriented at prospective users to assess expected discoverability gains and usage considerations taking into account various relevant service publishing technologies. In addition, we introduce a detailed comparison of Gapidt with a similar approach from the literature. The results show that Gapidt outperforms its competitor in terms of discoverability while improves Web Service description quality (better documentation and data-models). The Web Service discoverability levels of Gapidt outperforms that of third-party tools, either when using the GAnalyzer plus the GMapper, or only the GMapper.",Automatic detection | Code-first | Service discovery | Web services | WSDL anti-patterns,Computer Standards and Interfaces,2018-02-01,Article,"Hirsch, Matías;Rodriguez, Ana;Rodriguez, Juan Manuel;Mateos, Cristian;Zunino, Alejandro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.5220/0006839000910102,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ACCESS.2017.2777845,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85048688484,10.1109/MIC.2018.032501515,A Web API Ecosystem through Feature-Based Reuse,"The fast-growing web API landscape brings clients more options than ever before-in theory. In practice, they cannot easily switch between different providers offering similar functionality. We discuss a vision for developing web APIs based on reuse of interface parts called features. Through the introduction of five design principles, we investigate the impact of feature-based reuse on web APIs. Applying these principles enables a granular reuse of client and server code, documentation, and tools. Together, they can foster a measurable ecosystem with cross-API compatibility, opening the door to a more flexible generation of web clients.",reuse | Web APIs,IEEE Internet Computing,2018-05-01,Article,"Verborgh, Ruben;Dumontier, Michel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044008350,10.1016/j.ins.2018.03.014,Learning to answer programming questions with software documentation through social context embedding,"Official software documentation provides a comprehensive overview of software usages, but not on specific programming tasks or use cases. Often there is a mismatch between the documentation and a question on a specific programming task because of different wordings. We observe from Stack Overflow that the best answers to programmers’ questions often contain links to formal documentation. In this paper, we propose a novel deep-learning-to-answer framework, named QDLinker, for answering programming questions with software documentation. QDLinker learns from the large volume of discussions in community-based question answering site to bridge the semantic gap between programmers’ questions and software documentation. Specifically, QDLinker learns question-documentation semantic representation from these question answering discussions with a four-layer neural network, and incorporates semantic and content features into a learning-to-rank schema. Our approach does not require manual feature engineering or external resources to infer the degree of relevance between a question and documentation. Through extensive experiments, results show that QDLinker effectively answers programming questions with direct links to software documentation. QDLinker significantly outperforms the baselines based on traditional retrieval models and Web search services dedicated for software documentation retrieval. The user study shows that QDLinker effectively bridges the semantic gap between the intent of a programming question and the content of software documentation.",Community-based question answering | Neural network | Social context | Software documentation,Information Sciences,2018-06-01,Article,"Li, Jing;Sun, Aixin;Xing, Zhenchang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056731624,10.1145/3275219.3275233,An exploratory study on codes in heterogeneous software documents,"Different kinds of software documents are produced in the life cycle of a software project, such as Bug Reports, Mail Lists, etc. These documents have close relationship with source code, but it is difficult to recover their traceability relationship. In this paper, we conduct an exploratory study on codes in a software project’s heterogeneous documents, so that we can give some hints for traceability recovery from software documents to source code. We select a famous open source software project, Lucene, as sample, and collect its four kinds of software documents, including Bug Reports, Mail Lists, Stack Overflow Q&A Documents and Blogs. On this basis, we analyze these heterogeneous documents to answer the following questions: How much code is there in different kinds of documents? What APIs do these documents focus on? How many documents are relevant to the same APIs? Based on the study, we give 3 hints for recovering the traceability from software heterogeneous documents to source code.",API | Code element | Software document,ACM International Conference Proceeding Series,2018-09-16,Conference Paper,"Zou, Yanzhen;Cao, Yingkui;Xie, Bing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85052505816,10.1002/asi.24067,To Do or Not To Do: Distill crowdsourced negative caveats to augment api documentation,"Negative caveats of application programming interfaces (APIs) are about “how not to use an API,” which are often absent from the official API documentation. When these caveats are overlooked, programming errors may emerge from misusing APIs, leading to heavy discussions on Q&A websites like Stack Overflow. If the overlooked caveats could be mined from these discussions, they would be beneficial for programmers to avoid misuse of APIs. However, it is challenging because the discussions are informal, redundant, and diverse. For this, for example, we propose Disca, a novel approach for automatically Distilling desirable API negative caveats from unstructured Q&A discussions. Through sentence selection and prominent term clustering, Disca ensures that distilled caveats are context-independent, prominent, semantically diverse, and nonredundant. Quantitative evaluation in our experiments shows that the proposed Disca significantly outperforms four text-summarization techniques. We also show that the distilled API negative caveats could greatly augment API documentation through qualitative analysis.",,Journal of the Association for Information Science and Technology,2018-12-01,Article,"Li, Jing;Sun, Aixin;Xing, Zhenchang",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85050767981,10.1002/smr.1892,Impact of structural weighting on a latent Dirichlet allocation–based feature location technique,"Text retrieval–based feature location techniques (FLTs) use information from the terms present in documents in classes and methods. However, relevant terms originating from certain locations (eg, method names) often comprise only a small part of the entire method lexicon. Feature location techniques should benefit from techniques that make greater use of this information. The primary objective of this study was to investigate how weighting terms from different locations in source code can improve a latent Dirichlet allocation (LDA)-based FLT. We conducted an empirical study of 4 subject software systems and 372 features. For each subject system, we trained 1024 different LDA models with new weighting schemes applied to leading comments, method names, parameters, body comments, and local variables. We conducted both a quantitative and qualitative analysis to identify the effects of using the weighting schemes on the performance of the LDA-based FLT. We evaluated weighting schemes based on mean reciprocal rank and spread of effectiveness measures. In addition, we conducted a factorial analysis to identify which locations have a main impact on the results of the FLT. We then examined the effects of adding information from class comments, class names, and fields to the top 10 configurations for each system. This results in an additional 640 different LDA models for each system. From our results, we identified a significant effect in the performance of an LDA-based weighting configuration when applying our weighting schemes to the LDA-based FLT. Furthermore, we found that adding information from each method's containing class can improve the effectiveness of an LDA-based FLT. Finally, we identified a set of recommendations for identifying better weighting schemes for LDA.",feature location | program comprehension | static analysis | term weighting | text retrieval,Journal of Software: Evolution and Process,2018-01-01,Article,"Eddy, Brian P.;Kraft, Nicholas A.;Gray, Jeff",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85048492684,10.1002/smr.1958,A comprehensive model for code readability,"Unreadable code could compromise program comprehension, and it could cause the introduction of bugs. Code consists of mostly natural language text, both in identifiers and comments, and it is a particular form of text. Nevertheless, the models proposed to estimate code readability take into account only structural aspects and visual nuances of source code, such as line length and alignment of characters. In this paper, we extend our previous work in which we use textual features to improve code readability models. We introduce 2 new textual features, and we reassess the readability prediction power of readability models on more than 600 code snippets manually evaluated, in terms of readability, by 5K+ people. We also replicate a study by Buse and Weimer on the correlation between readability and FindBugs warnings, evaluating different models on 20 software systems, for a total of 3M lines of code. The results demonstrate that (1) textual features complement other features and (2) a model containing all the features achieves a significantly higher accuracy as compared with all the other state-of-the-art models. Also, readability estimation resulting from a more accurate model, ie, the combined model, is able to predict more accurately FindBugs warnings.",code readability | quality warning prediction | textual analysis,Journal of Software: Evolution and Process,2018-06-01,Conference Paper,"Scalabrino, Simone;Linares-Vásquez, Mario;Oliveto, Rocco;Poshyvanyk, Denys",Include,
10.1016/j.jss.2022.111515,,10.1186/s40411-018-0050-8,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85039973849,10.1016/j.scico.2017.11.005,A critical analysis of string APIs: The case of Pharo,"Most programming languages, besides C, provide a native abstraction for character strings, but string APIs vary widely in size, expressiveness, and subjective convenience across languages. In Pharo, while at first glance the API of the String class seems rich, it often feels cumbersome in practice; to improve its usability, we faced the challenge of assessing its design. However, we found hardly any guideline about design forces and how they structure the design space, and no comprehensive analysis of the expected string operations and their different variations. In this article, we first analyze the Pharo 4 String library, then contrast it with its Haskell, Java, Python, Ruby, and Rust counterparts. We harvest criteria to describe a string API, and reflect on features and design tensions. This analysis should help language designers in understanding the design space of strings, and will serve as a basis for a future redesign of the string library in Pharo.",API | Design | Library | Strings | Style,Science of Computer Programming,2018-09-01,Article,"Pollet, Damien;Ducasse, Stéphane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056858358,10.18293/SEKE2018-096,Mining intentions to improve bug report summarization,"In recent years, various automatic summarization techniques have been proposed to extract important information from bug reports. However, existing techniques mainly focus on common text features and ignore human intentions implied in bug reports. In fact, each bug report generally contains multiple intentions which are distributed in different sentences. Bug report readers are usually more interested in content that contains intentions of certain categories (e.g. fix solution, bug description). Based on the above observation, we introduce an intention taxonomy and implement the intention classification algorithm in this paper. Furthermore, we propose a new Intention-based Bug Report Summarization approach, namely IBRS, which leverages intention taxonomy to enhance bug report summarization. We evaluate our approach on Intention-BRC corpus and the experimental result shows that IBRS outperforms the state-of-The-Art approaches in terms of precision, recall, F-score, and pyramid precision.",Bug Report | Intention Mining | Intention Taxonomy | Text Summarization,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2018-01-01,Conference Paper,"Huai, Beibei;Li, Wenbo;Wu, Qiansheng;Wang, Meiling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056867026,10.18293/SEKE2018-191,Improving code summarization by combining deep learning and empirical knowledge,"Code summaries are human-readable text that describes the functionality of code blocks. Software developers use code summaries to understand the specification of API while code retrieve system relies on code summaries for effective code search. However, code summaries are often written by software developers. Writing good code summaries usually requires great effort. It could be helpful if developers use automatic code summarization system to generate code summaries. Recently, some works have applied deep learning methods to generate code summaries for code snippets. However, those deep learning methods treat code snippets as streams of text tokens while ignoring the inherent code structure information. In this paper, we propose a novel code summarization method named the CDEModel (Code summarization by Deep learning and Empirical knowledge) that combines inherent code structure information with deep learning models. The CDE-Model proposes several empirical strategies to transform code snippets to refined code representation and feeds them into an encoder-decoder neural network for text generation. We conduct large-scale experiments on 1500 popular Java projects on GitHub1 with 396,184 pairs of code snippets and summaries. Experimental results show that the quality of code summaries generated by our CDE-Model is better than other two methods. To the best of our knowledge, this paper is the first to combine code structure information with deep learning.",Code summarization | GitHub | Java language. | Recurrent neural network,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2018-01-01,Conference Paper,"Zeng, Lingbin;Zhang, Xunhui;Wang, Tao;Li, Xiao;Yu, Jie;Wang, Huaim",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85050372338,10.1145/3209978.3210170,API caveat explorer - Surfacing negative usages from practice: An API-oriented interactive exploratory search system for programmers,"Application programming interface (API) documentation well describes an API and how to use it. However, official documentation does not describe ""how not to use it"" or the different kinds of errors when an API is used wrongly. Programming caveats are negative usages of an API. When these caveats are overlooked, errors may emerge, leading to heavy discussions on Q&A websites like Stack Overflow. In this demonstration, we present API Caveat Explorer, a search system to explore API caveats that are mined from large-scale unstructured discussions on Stack Overflow. API Caveat Explorer takes API-oriented queries such as ""HashMap"" and retrieves API caveats by text summarization techniques. API caveats are represented by sentences, which are context-independent, prominent, semantically diverse and non-redundant. The system provides a web-based interface that allows users to interactively explore the full picture of all discovered caveats of an API, and the details of each. The potential users of API Caveat Explorer are programmers and educators for learning and teaching APIs.",Interactive exploratory search | Text summarization | User-generated content,"41st International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2018",2018-06-27,Conference Paper,"Li, Jing;Sun, Aixin;Xing, Zhenchang;Han, Lei",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85057261225,10.1007/978-3-030-02934-0_4,Classifying Python Code Comments Based on Supervised Learning,"Code comments can provide a great data source for understanding programmer’s needs and underlying implementation. Previous work has illustrated that code comments enhance the reliability and maintainability of the code, and engineers use them to interpret their code as well as help other developers understand the code intention better. In this paper, we studied comments from 7 python open source projects and contrived a taxonomy through an iterative process. To clarify comments characteristics, we deploy an effective and automated approach using supervised learning algorithms to classify code comments according to their different intentions. With our study, we find that there does exist a pattern across different python projects: Summary covers about 75% of comments. Finally, we conduct an evaluation on the behaviors of two different supervised learning classifiers and find that Decision Tree classifier is more effective on accuracy and runtime than Naive Bayes classifier in our research.",Code comments classification | Python | Supervised learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Zhang, Jingyi;Xu, Lei;Li, Yanhui",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85049087566,10.12821/ijispm060202,Perspectives on reusing codified project knowledge: A structured literature review,"Project documentation represents a valuable source of knowledge in project-based organizations. The practical reality is, however, that the knowledge codified in project documents is hardly re-used in future projects. A central problem in this context is the extensive amount of usually textual material. As a consequence, computer-assisted processes are indispensable in order to analytically manage the constantly growing and evolving databases of available project documents. The goal of this study is to summarize the current research focusing on the computer-assisted reuse of textually codified project knowledge and to define the corresponding state-of-the-art in this this specific field of information systems research. As a result of a literature review, this study structures the body of research contributions and outlines what kinds of computer-assisted techniques are incorporated, what practical application areas these solutions address, and in what business domains they are applied. In particular, this should point out research opportunities and thereby make a contribution to the further development of knowledge management in project environments.",Codified knowledge | Knowledge management | Project documentation | Project knowledge management | Text mining,International Journal of Information Systems and Project Management,2018-01-01,Review,"Coners, André;Matthies, Benjamin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.18260/1-2--30198,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85046018347,10.1177/0047281617721853,Application programming interface documentation: What do software developers want?,"The success of an application programming interface (API) crucially depends on how well its documentation meets the information needs of software developers. Previous research suggests that these information needs have not been sufficiently understood. This article presents the results of a series of semistructured interviews and a follow-up questionnaire conducted to explore the learning goals and learning strategies of software developers, the information resources they turn to and the quality criteria they apply to API documentation. Our results show that developers initially try to form a global understanding regarding the overall purpose and main features of an API, but then adopt either a concepts-oriented or a code-oriented learning strategy that API documentation both needs to address. Our results also show that general quality criteria such as completeness and clarity are relevant to API documentation as well. Developing and maintaining API documentation therefore need to involve the expertise of communication professionals.",Application programming interface documentation | Audience analysis | Information design | Technical documentation | Usability,Journal of Technical Writing and Communication,2018-07-01,Article,"Meng, Michael;Steinhardt, Stephanie;Schubert, Andreas",Include,
10.1016/j.jss.2022.111515,2-s2.0-85064509229,10.1002/pra2.2018.14505501171,Improving access to API documentation for developers with Docs-as-Code-as-a-service,"Web Application Programming Interfaces (APIs) provide the connective tissue that developers need to integrate Internet services and create “software-as-a-service” (SaaS) applications. To learn an API's usage, developers often turn to their web browsers to access documentation and try out APIs. However, traditional approaches to documentation present obstacles that can hamper API comprehension and developer productivity. To overcome these obstacles, practitioners are adopting a new approach called Docs-as-Code, in which documents are published with many of the same tools and practices used to deploy source code. In this paper, we present Dev Center, a system based in part on the Docs-as-Code framework and designed to provide ubiquitous access to a broad set of API documentation resources. Preliminary findings suggest that Dev Center could potentially improve API comprehension and developer productivity, while Docs-as-Code could benefit related research efforts.",API | Context Switching | Developer | Docs-as-Code | Documentation | SaaS | Traceability,Proceedings of the Association for Information Science and Technology,2018-01-01,Article,"Thomchick, Richard",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85058996198,10.1145/3281074.3281078,Does it make sense to have application-specific code conventions as a complementary approach to code annotations?,"Code annotations are extensively used by Java developers, especially in enterprise frameworks and APIs such as Spring and Java EE. Different code elements that frequently share some similarities, sometimes, repeat the annotations. The goal of this paper is to evaluate whether a real-world system could use code conventions as a means to avoid replicating code annotations. We report on a study on the software used for the EMBRACE Space Weather program at INPE to search for similarities.",Code annotation | Code convention | Metadata,"META 2018 - Proceedings of the 3rd ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection, co-located with SPLASH 2018",2018-11-05,Conference Paper,"Teixeira, Rodrigo;Guerra, Eduardo;Lima, Phyllipe;Meirelles, Paulo;Kon, Fabio",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85054952387,10.3966/160792642018091905030,A comment-driven approach to API usage patterns discovery and search,"Considerable effort has gone into the discovery of API usage patterns or examples. However, how to enable programmers to search for discovered API usage examples using natural language queries is still a significant research problem. This paper presents an approach, referred to as Codepus, to facilitate the discovery of API usage examples based on mining comments in open source code while permitting searches using natural language queries. The approach includes two key features: API usage patterns as well as multiple keywords and tf-idf values are discovered by mining open source comments and code snippets; and a matchmaking function is devised for searching for API usage examples using natural language queries by aggregating scores related to semantic similarity, correctness, and the number of APIs. In a practical application, the proposed approach discovered 43,721 API usage patterns with 641,591 API usage examples from 15,814 open source projects. Experiment results revealed the following: (1) Codepus reduced the browsing time required for locating API usage examples by 46.5%, compared to the time required when using a web search engine. (2) The precision of Codepus is 91% when using eleven real-world frequently asked questions, which is superior to those of Gists and Open Hub.",API usage pattern | Code example | Code search system,Journal of Internet Technology,2018-01-01,Article,"Lee, Shin Jie;Lin, Xavier;Su, Wu Chen;Chen, Hsi Min",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85051680256,10.1145/3197231.3198444,Classifying code comments in Java mobile applications,"Developers adopt code comments for different reasons such as document source codes or change program flows. Due to a variety of use scenarios, code comments may impact on readability and maintainability. In this study, we investigate how developers of 5 open-source mobile applications use code comments to document their projects. Additionally, we evaluate the performance of two machine learning models to automatically classify code comments. Initial results show marginal differences between desktop and mobile applications.",Android | code comments | mining software repositories,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Pascarella, Luca",Exclude,
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,,,,,,,,,Include,
10.1016/j.jss.2022.111515,2-s2.0-85046960318,10.1145/3173574.3174154,Visualizing API usage examples at scale,"Using existing APIs properly is a key challenge in programming, given that libraries and APIs are increasing in number and complexity. Programmers often search for online code examples in Q&A forums and read tutorials and blog posts to learn how to use a given API. However, there are often a massive number of related code examples and it is difficult for a user to understand the commonalities and variances among them, while being able to drill down to concrete details. We introduce an interactive visualization for exploring a large collection of code examples mined from open-source repositories at scale. This visualization summarizes hundreds of code examples in one synthetic code skeleton with statistical distributions for canonicalized statements and structures enclosing an API call. We implemented this interactive visualization for a set of Java APIs and found that, in a lab study, it helped users (1) answer significantly more API usage questions correctly and comprehensively and (2) explore how other programmers have used an unfamiliar API.",API | Code examples | Interactive visualization | Programming support,Conference on Human Factors in Computing Systems - Proceedings,2018-04-20,Conference Paper,"Glassman, Elena L.;Zhang, Tianyi;Hartmann, Björn;Kim, Miryung",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85056744512,10.1145/3209280.3209537,Exploiting patterns and templates for technical documentation,"There are several domains in which the documents are made of reusable pieces. Template languages have been widely studied by the document engineering community to deal with common structures and textual fragments. Though, templating mechanisms are often hidden in mainstream word-precessors and even unknown by common users. This paper presents a pattern-based language for templates, serialized in HTML and exploited in a user-friendly WYSIWYG editor for writing technical documentation. We discuss the deployment of the editor by an engineering company in the railway domain, as well as some generalized lessons learned about templates.",Collaborative editing | HTML | Technical documentation | Tem-plating,"Proceedings of the ACM Symposium on Document Engineering 2018, DocEng 2018",2018-08-28,Conference Paper,"Caponi, Alessandro;Di Iorio, Angelo;Vitali, Fabio;Alberti, Paolo;Scatá, Marcello",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044008350,10.1016/j.ins.2018.03.014,Learning to answer programming questions with software documentation through social context embedding,"Official software documentation provides a comprehensive overview of software usages, but not on specific programming tasks or use cases. Often there is a mismatch between the documentation and a question on a specific programming task because of different wordings. We observe from Stack Overflow that the best answers to programmers’ questions often contain links to formal documentation. In this paper, we propose a novel deep-learning-to-answer framework, named QDLinker, for answering programming questions with software documentation. QDLinker learns from the large volume of discussions in community-based question answering site to bridge the semantic gap between programmers’ questions and software documentation. Specifically, QDLinker learns question-documentation semantic representation from these question answering discussions with a four-layer neural network, and incorporates semantic and content features into a learning-to-rank schema. Our approach does not require manual feature engineering or external resources to infer the degree of relevance between a question and documentation. Through extensive experiments, results show that QDLinker effectively answers programming questions with direct links to software documentation. QDLinker significantly outperforms the baselines based on traditional retrieval models and Web search services dedicated for software documentation retrieval. The user study shows that QDLinker effectively bridges the semantic gap between the intent of a programming question and the content of software documentation.",Community-based question answering | Neural network | Social context | Software documentation,Information Sciences,2018-06-01,Article,"Li, Jing;Sun, Aixin;Xing, Zhenchang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053669168,10.1134/S0361768818050079,Detecting Near Duplicates in Software Documentation,"Abstract: Contemporary software documentation is as complicated as the software itself. During its lifecycle, the documentation accumulates a lot of “near duplicate” fragments, i.e. chunks of text that were copied from a single source and were later modified in different ways. Such near duplicates decrease documentation quality and thus hamper its further utilization. At the same time, they are hard to detect manually due to their fuzzy nature. In this paper we give a formal definition of near duplicates and present an algorithm for their detection in software documents. This algorithm is based on the exact software clone detection approach: the software clone detection tool Clone Miner was adapted to detect exact duplicates in documents. Then, our algorithm uses these exact duplicates to construct near ones. We evaluate the proposed algorithm using the documentation of 19 open source and commercial projects. Our evaluation is very comprehensive – it covers various documentation types: design and requirement specifications, programming guides and API documentation, user manuals. Overall, the evaluation shows that all kinds of software documentation contain a significant number of both exact and near duplicates. Next, we report on the performed manual analysis of the detected near duplicates for the Linux Kernel Documentation. We present both quantative and qualitative results of this analysis, demonstrate algorithm strengths and weaknesses, and discuss the benefits of duplicate management in software documents.",documentation reuse | near duplicates | software clone detection | software documentation,Programming and Computer Software,2018-09-01,Article,"Luciv, D. V.;Koznov, D. V.;Chernishev, G. A.;Terekhov, A. N.;Romanovsky, K. Yu;Grigoriev, D. A.",Include,
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078884371,10.1109/ASE.2019.00011,Assessing the generalizability of code2vec token embeddings,"Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.",Big Code | Code Embeddings | Distributed Representations,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Kang, Hong Jin;Bissyande, Tegawende F.;Lo, David",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85078887280,10.1109/ASE.2019.00024,"Discovering, explaining and summarizing controversial discussions in community Q&A sites","Developers often look for solutions to programming problems in community Q&A sites like Stack Overflow. Due to the crowdsourcing nature of these Q&A sites, many user-provided answers are wrong, less optimal or out-of-date. Relying on community-curated quality indicators (e.g., accepted answer, answer vote) cannot reliably identify these answer problems. Such problematic answers are often criticized by other users. However, these critiques are not readily discoverable when reading the posts. In this paper, we consider the answers being criticized and their critique posts as controversial discussions in community Q&A sites. To help developers notice such controversial discussions and make more informed choices of appropriate solutions, we design an automatic open information extraction approach for systematically discovering and summarizing the controversies in Stack Overflow and exploiting official API documentation to assist the understanding of the discovered controversies. We apply our approach to millions of Java/android-tagged Stack overflow questions and answers and discover a large scale of controversial discussions in Stack Overflow. Our manual evaluation confirms that the extracted controversy information is of high accuracy. A user study with 18 developers demonstrates the usefulness of our generated controversy summaries in helping developers avoid the controversial answers and choose more appropriate solutions to programming questions.",Controversial discussion | Open information extraction | Sentence embedding | Stack Overflow,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Ren, Xiaoxue;Xing, Zhenchang;Xia, Xin;Li, Guoqiang;Sun, Jianling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078929771,10.1109/ASE.2019.00026,Automatic generation of pull request descriptions,"Enabled by the pull-based development model, developers can easily contribute to a project through pull requests (PRs). When creating a PR, developers can add a free-form description to describe what changes are made in this PR and/or why. Such a description is helpful for reviewers and other developers to gain a quick understanding of the PR without touching the details and may reduce the possibility of the PR being ignored or rejected. However, developers sometimes neglect to write descriptions for PRs. For example, in our collected dataset with over 333K PRs, more than 34% of the PR descriptions are empty. To alleviate this problem, we propose an approach to automatically generate PR descriptions based on the commit messages and the added source code comments in the PRs. We regard this problem as a text summarization problem and solve it using a novel sequence-to-sequence model. To cope with out-of-vocabulary words in software artifacts and bridge the gap between the training loss function of the sequence-to-sequence model and the evaluation metric ROUGE, which has been shown to correspond to human evaluation, we integrate the pointer generator and directly optimize for ROUGE using reinforcement learning and a special loss function. We build a dataset with over 41K PRs and evaluate our approach on this dataset through ROUGE and a human evaluation. Our evaluation results show that our approach outperforms two baselines by significant margins.",Document Generation | Pull Request | Sequence to Sequence Learning,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Liu, Zhongxin;Xia, Xin;Treude, Christoph;Lo, David;Li, Shanping",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85078950175,10.1109/ASE.2019.00028,SEGATE: Unveiling semantic inconsistencies between code and specification of string inputs,"Automated testing techniques are often assessed on coverage based metrics. However, despite giving good coverage, the test cases may miss the gap between functional specification and the code implementation. This gap may be subtle in nature, arising due to the absence of logical checks, either in the implementation or in the specification, resulting in inconsistencies in the input definition. The inconsistencies may be prevalent especially for structured inputs, commonly specified using string-based data types. Our study on defects reported over popular libraries reveals that such gaps may not be limited to input validation checks. We propose a test generation technique for structured string inputs where we infer inconsistencies in input definition to expose semantic gaps in the method under test and the method specification. We assess this technique using our tool SEGATE, Semantic Gap Tester. SEGATE uses static analysis and automaton modeling to infer the gap and generate test cases. On our benchmark dataset, comprising of defects reported in 15 popular open-source libraries, written in Java, SEGATE was able to generate tests to expose 80% of the defects.",Automaton modeling | Data flow analysis | Regular expression | Static analysis | String input generation | Testing,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Sondhi, Devika;Purandare, Rahul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078903099,10.1109/ASE.2019.00032,Learning from examples to find fully qualified names of API elements in code snippets,"Developers often reuse code snippets from online forums, such as Stack Overflow, to learn API usages of software frameworks or libraries. These code snippets often contain ambiguous undeclared external references. Such external references make it difficult to learn and use those APIs correctly. In particular, reusing code snippets containing such ambiguous undeclared external references requires significant manual efforts and expertise to resolve them. Manually resolving fully qualified names (FQN) of API elements is a non-trivial task. In this paper, we propose a novel context-sensitive technique, called COSTER, to resolve FQNs of API elements in such code snippets. The proposed technique collects locally specific source code elements as well as globally related tokens as the context of FQNs, calculates likelihood scores, and builds an occurrence likelihood dictionary (OLD). Given an API element as a query, COSTER captures the context of the query API element, matches that with the FQNs of API elements stored in the OLD, and rank those matched FQNs leveraging three different scores: likelihood, context similarity, and name similarity scores. Evaluation with more than 600K code examples collected from GitHub and two different Stack Overflow datasets shows that our proposed technique improves precision by 4-6% and recall by 3-22% compared to state-of-the-art techniques. The proposed technique significantly reduces the training time compared to the StatType, a state-of-the-art technique, without sacrificing accuracy. Extensive analyses on results demonstrate the robustness of the proposed technique.",API usages | Context sensitive technique | Fully Qualified Name | Recommendation system,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Khaled Saifullah, C. M.;Asaduzzaman, Muhammad;Roy, Chanchal K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078914786,10.1109/ASE.2019.00036,OAUTHLINT: An empirical study on oauth bugs in android applications,"Mobile developers use OAuth APIs to implement Single-Sign-On services. However, the OAuth protocol was originally designed for the authorization for third-party websites not to authenticate users in third-party mobile apps. As a result, it is challenging for developers to correctly implement mobile OAuth securely. These vulnerabilities due to the misunderstanding of OAuth and inexperience of developers could lead to data leakage and account breach. In this paper, we perform an empirical study on the usage of OAuth APIs in Android applications and their security implications. In particular, we develop OAUTHLINT, that incorporates a query-driven static analysis to automatically check programs on the Google Play marketplace. OAUTHLINT takes as input an anti-protocol that encodes a vulnerable pattern extracted from the OAuth specifications and a program P. Our tool then generates a counter-example if the anti-protocol can match a trace of Ps possible executions. To evaluate the effectiveness of our approach, we perform a systematic study on 600+ popular apps which have more than 10 millions of downloads. The evaluation shows that 101 (32%) out of 316 applications that use OAuth APIs make at least one security mistake.",Android | Bug Finding | OAuth | Security | Static Analysis,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Rahat, Tamjid Al;Feng, Yu;Tian, Yuan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078878920,10.1109/ASE.2019.00041,IFeedback: Exploiting user feedback for real-time issue detection in large-scale online service systems,"Large-scale online systems are complex, fast-evolving, and hardly bug-free despite the testing efforts. Backend system monitoring cannot detect many types of issues, such as UI related bugs, bugs with small impact on backend system indicators, or errors from third-party co-operating systems, etc. However, users are good informers of such issues: They will provide their feedback for any types of issues. This experience paper discusses our design of iFeedback, a tool to perform real-time issue detection based on user feedback texts. Unlike traditional approaches that analyze user feedback with computation-intensive natural language processing algorithms, iFeedback is focusing on fast issue detection, which can serve as a system life-condition monitor. In particular, iFeedback extracts word combination-based indicators from feedback texts. This allows iFeedback to perform fast system anomaly detection with sophisticated machine learning algorithms. iFeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis. We present our representative experiences in successfully applying iFeedback in tens of large-scale production online service systems in ten months.",Bug and vulnerability detection,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Zheng, Wujie;Lu, Haochuan;Zhou, Yangfan;Liang, Jianming;Zheng, Haibing;Deng, Yuetang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078904622,10.1109/ASE.2019.00044,Re-factoring based program repair applied to programming assignments,"Automated program repair has been used to provide feedback for incorrect student programming assignments, since program repair captures the code modification needed to make a given buggy program pass a given test-suite. Existing student feedback generation techniques are limited because they either require manual effort in the form of providing an error model, or require a large number of correct student submissions to learn from, or suffer from lack of scalability and accuracy. In this work, we propose a fully automated approach for generating student program repairs in real-time. This is achieved by first re-factoring all available correct solutions to semantically equivalent solutions. Given an incorrect program, we match the program with the closest matching refactored program based on its control flow structure. Subsequently, we infer the input-output specifications of the incorrect program's basic blocks from the executions of the correct program's aligned basic blocks. Finally, these specifications are used to modify the blocks of the incorrect program via search-based synthesis. Our dataset consists of almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. Our experimental results suggest that our method is more effective and efficient than recently proposed feedback generation approaches. About 30% of the patches produced by our tool Refactory are smaller than those produced by the state-of-art tool Clara, and can be produced given fewer correct solutions (often a single correct solution) and in a shorter time. We opine that our method is applicable not only to programming assignments, and could be seen as a general-purpose program repair method that can achieve good results with just a single correct reference solution.",Program Repair | Programming Education | Software Refactoring,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Hu, Yang;Ahmed, Umair Z.;Mechtaev, Sergey;Leong, Ben;Roychoudhury, Abhik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078910100,10.1109/ASE.2019.00045,InFix: Automatically repairing novice program inputs,"This paper presents InFix, a technique for automatically fixing erroneous program inputs for novice programmers. Unlike comparable existing approaches for automatic debugging and maintenance tasks, InFix repairs input data rather than source code, does not require test cases, and does not require special annotations. Instead, we take advantage of patterns commonly used by novice programmers to automatically create helpful, high quality input repairs. InFix iteratively applies error-message based templates and random mutations based on insights about the debugging behavior of novices. This paper presents an implementation of InFix for Python. We evaluate on 29,995 unique scenarios with input-related errors collected from four years of data from Python Tutor, a free online programming tutoring environment. Our results generalize and scale; compared to previous work, we consider an order of magnitude more unique programs. Overall, InFix is able to repair 94.5% of deterministic input errors. We also present the results of a human study with 97 participants. Surprisingly, this simple approach produces high quality repairs; humans judged the output of InFix to be equally helpful and within 4% of the quality of human-generated repairs.",Human study | Input repair | Novice programs,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Endres, Madeline;Sakkas, Georgios;Cosman, Benjamin;Jhala, Ranjit;Weimer, Westley",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078924839,10.1109/ASE.2019.00057,Mutation analysis for Coq,"Mutation analysis, which introduces artificial defects into software systems, is the basis of mutation testing, a technique widely applied to evaluate and enhance the quality of test suites. However, despite the deep analogy between tests and formal proofs, mutation analysis has seldom been considered in the context of deductive verification. We propose mutation proving, a technique for analyzing verification projects that use proof assistants. We implemented our technique for the Coq proof assistant in a tool dubbed mCoq. mCoq applies a set of mutation operators to Coq definitions of functions and datatypes, inspired by operators previously proposed for functional programming languages. mCoq then checks proofs of lemmas affected by operator application. To make our technique feasible in practice, we implemented several optimizations in mCoq such as parallel proof checking. We applied mCoq to several medium and large scale Coq projects, and recorded whether proofs passed or failed when applying different mutation operators. We then qualitatively analyzed the mutants, finding many instances of incomplete specifications. For our evaluation, we made several improvements to serialization of Coq files and even discovered a notable bug in Coq itself, all acknowledged by developers. We believe mCoq can be useful both to proof engineers for improving the quality of their verification projects and to researchers for evaluating proof engineering techniques.",Coq | Mutation proving | Mutation testing | Prof assistants,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Celik, Ahmet;Palmskog, Karl;Parovic, Marinela;Arias, Emilio Jesus Gallego;Gligoric, Milos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078952975,10.1109/ASE.2019.00061,CodeKernel: A graph kernel based approach to the selection of API usage examples,"Developers often want to find out how to use a certain API (e.g., FileReader.read in JDK library). API usage examples are very helpful in this regard. Over the years, many automated methods have been proposed to generate code examples by clustering and summarizing relevant code snippets extracted from a code corpus. These approaches simplify source code as method invocation sequences or feature vectors. Such simplifications only model partial aspects of the code and tend to yield inaccurate examples. We propose CodeKernel, a graph kernel based approach to the selection of API usage examples. Instead of approximating source code as method invocation sequences or feature vectors, CodeKernel represents source code as object usage graphs. Then, it clusters graphs by embedding them into a continuous space using a graph kernel. Finally, it outputs code examples by selecting a representative graph from each cluster using designed ranking metrics. Our empirical evaluation shows that CodeKernel selects more accurate code examples than the related work (MUSE and eXoaDocs). A user study involving 25 developers in a multinational company also confirms the usefulness of CodeKernel in selecting API usage examples.",API usage example | Code search | Graph kernel | Software reuse,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Gu, Xiaodong;Zhang, Hongyu;Kim, Sunghun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078887604,10.1109/ASE.2019.00063,MARBLE: Mining for boilerplate code to identify API usability problems,"Designing usable APIs is critical to developers' productivity and software quality, but is quite difficult. One of the challenges is that anticipating API usability barriers and real-world usage is difficult, due to a lack of automated approaches to mine usability data at scale. In this paper, we focus on one particular grievance that developers repeatedly express in online discussions about APIs: 'boilerplate code.' We investigate what properties make code count as boilerplate, the reasons for boilerplate, and how programmers can reduce the need for it. We then present MARBLE, a novel approach to automatically mine boilerplate code candidates from API client code repositories. MARBLE adapts existing techniques, including an API usage mining algorithm, an AST comparison algorithm, and a graph partitioning algorithm. We evaluate MARBLE with 13 Java APIs, and show that our approach successfully identifies both already-known and new API-related boilerplate code instances.",API Usability | Boilerplate Code | Repository Mining,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Nam, Daye;Horvath, Amber;MacVean, Andrew;Myers, Brad;Vasilescu, Bogdan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078873379,10.1109/ASE.2019.00067,How Do API selections affect the runtime performance of data analytics tasks?,"As data volume and complexity grow at an unprecedented rate, the performance of data analytics programs is becoming a major concern for developers. We observed that developers sometimes use alternative data analytics APIs to improve program runtime performance while preserving functional equivalence. However, little is known on the characteristics and performance attributes of alternative data analytics APIs. In this paper, we propose a novel approach to extracting alternative implementations that invoke different data analytics APIs to solve the same tasks. A key appeal of our approach is that it exploits the comparative structures in Stack Overflow discussions to discover programming alternatives. We show that our approach is promising, as 86% of the extracted code pairs were validated as true alternative implementations. In over 20% of these pairs, the faster implementation was reported to achieve a 10x or more speedup over its slower alternative. We hope that our study offers a new perspective of API recommendation and motivates future research on optimizing data analytics programs.",API selection | Data analytics | Performance optimization | Stack Overflow,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Tao, Yida;Tang, Shan;Liu, Yepang;Xu, Zhiwu;Qin, Shengchao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85076167331,10.1109/ASE.2019.00080,An empirical study towards characterizing deep learning development and deployment across different frameworks and platforms,"Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.",Deep learning deployment | Deep learning frameworks | Deep learning platforms | Empirical study,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Guo, Qianyu;Chen, Sen;Xie, Xiaofei;Ma, Lei;Hu, Qiang;Liu, Hongtao;Liu, Yang;Zhao, Jianjun;Li, Xiaohong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078929637,10.1109/ASE.2019.00092,Improving the decision-making process of self-adaptive systems by accounting for tactic volatility,"When self-adaptive systems encounter changes withintheir surrounding environments, they enacttacticsto performnecessary adaptations. For example, a self-adaptive cloud-basedsystem may have a tactic that initiates additional computingresources when response time thresholds are surpassed, or theremay be a tactic to activate a specific security measure when anintrusion is detected. In real-world environments, these tacticsfrequently experiencetactic volatilitywhich is variable behaviorduring the execution of the tactic.Unfortunately, current self-adaptive approaches do not accountfor tactic volatility in their decision-making processes, and merelyassume that tactics do not experience volatility. This limitationcreates uncertainty in the decision-making process and mayadversely impact the system's ability to effectively and efficientlyadapt. Additionally, many processes do not properly account forvolatility that may effect the system's Service Level Agreement(SLA). This can limit the system's ability to act proactively, especially when utilizing tactics that contain latency.To address the challenge of sufficiently accounting for tacticvolatility, we propose aTactic Volatility Aware(TVA) solution.Using Multiple Regression Analysis (MRA), TVA enables self-adaptive systems to accurately estimate the cost and timerequired to execute tactics. TVA also utilizesAutoregressiveIntegrated Moving Average(ARIMA) for time series forecasting, allowing the system to proactively maintain specifications.",Artificial Intelligence | Machine Learning | Self-Adaptation,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Palmerino, Jeffrey;Yu, Qi;Desell, Travis;Krutz, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078939847,10.1109/ASE.2019.00099,CLCDSA: Cross language code clone detection using syntactical features and API documentation,"Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is two-fold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.",API documentation | Code Clone | Source Code Syntax | Word2Vector,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Nafi, Kawser Wazed;Kar, Tonny Shekha;Roy, Banani;Roy, Chanchal K.;Schneider, Kevin A.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85074534780,10.1109/ASE.2019.00111,Sip4J: Statically inferring access permission contracts for parallelising sequential Java programs,"This paper presents Sip4J, a fully automated, scalable and effective tool to automatically generate access permission contracts for a sequential Java program. The access permission contracts, which represent the dependency of code blocks, have been frequently used to enable concurrent execution of sequential programs. Those permission contracts, unfortunately, need to be manually created by programmers, which is known to be time-consuming, laborious and error-prone. To mitigate those manual efforts, Sip4J performs inter-procedural static analysis of Java source code to automatically extract the implicit dependencies in the program and subsequently leverages them to automatically generate access permission contracts, following the Design by Contract principle. The inferred specifications are then used to identify the concurrent (immutable) methods in the program. Experimental results further show that Sip4J is useful and effective towards generating access permission contracts for sequential Java programs. The implementation of Sip4J has been published as an open-sourced project at https://github.com/Sip4J/Sip4J and a demo video of Sip4J can be found at https://youtu.be/RjMTIxlhHTg.",Access permissions | Concurrency | Permission inference | Static analysis,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Sadiq, Ayesha;Li, Li;Li, Yuan Fang;Ahmed, Ijaz;Ling, Sea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078879627,10.1109/ASE.2019.00120,SiMPOSE-configurable N-way program merging strategies for superimposition-based analysis of variant-rich software,"Modern software often exists in many different, yet similar versions and/or variants, usually derived from a common code base (e.g., via clone-and-own). In the context of product-line engineering, family-based analysis has shown very promising potential for improving efficiency in applying quality-assurance techniques to variant-rich software, as compared to a variant-by-variant approach. Unfortunately, these strategies rely on a product-line representation superimposing all program variants in a syntactically well-formed, semantically sound and variant-preserving manner, which is manually hard to obtain in practice. We demonstrate the SiMPOSE methodology for automatically generating superimpositions of N given program versions and/or variants facilitating family-based analysis of variant-rich software. SiMPOSE is based on a novel N-way model-merging technique operating at the level of control-flow automata (CFA) representations of C programs. CFAs constitute a unified program abstraction utilized by many recent software-analysis tools. We illustrate different merging strategies supported by SiMPOSE, namely variant-by-variant, N-way merging, incremental 2-way merging, and partition-based N/2-way merging, and demonstrate how SiMPOSE can be used to systematically compare their impact on efficiency and effectiveness of family-based unit-test generation. The SiMPOSE tool, the demonstration of its usage as well as related artifacts and documentation can be found at http://pi.informatik.uni-siegen.de/projects/variance/simpose.",Family-Based Analyses | Model Merging | Program Merging | Software Testing,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Reuling, Dennis;Kelter, Udo;Ruland, Sebastian;Lochau, Malte",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078890470,10.1109/ASE.2019.00129,TsmartGP: A tool for finding memory defects with pointer analysis,"Precise pointer analysis is desired since it is a core technique to find memory defects. There are several dimensions of pointer analysis precision, flow sensitivity, context sensitivity, field sensitivity and path sensitivity. For static analysis tools utilizing pointer analysis, considering all dimensions is difficult because the trade-off between precision and efficiency should be balanced. This paper presents TsmartGP, a static analysis tool for finding memory defects in C programs with a precise and efficient pointer analysis. The pointer analysis algorithm is flow, context, field, and quasi path sensitive. Control flow automatons are the key structures for our analysis to be flow sensitive. Function summaries are applied to get context information and elements of aggregate structures are handled to improve precision. Path conditions are used to filter unreachable paths. For efficiency, a multi-entry mechanism is proposed. Utilizing the pointer analysis algorithm, we implement a checker in TsmartGP to find uninitialized pointer errors in 13 real-world applications. Cppcheck and Clang Static Analyzer are chosen for comparison. The experimental results show that TsmartGP can find more errors while its accuracy is also higher than Cppcheck and Clang Static Analyzer.",Multi-entry | Pointer analysis | Sensitivity | Uninitialized pointer,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Wang, Yuexing;Chen, Guang;Zhou, Min;Gu, Ming;Sun, Jiaguang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078941395,10.1109/ASE.2019.00130,Ares: Inferring error specifications through static analysis,"Misuse of APIs happens frequently due to misunderstanding of API semantics and lack of documentation. An important category of API-related defects is the error handling defects, which may result in security and reliability flaws. These defects can be detected with the help of static program analysis, provided that error specifications are known. The error specification of an API function indicates how the function can fail. Writing error specifications manually is time-consuming and tedious. Therefore, automatic inferring the error specification from API usage code is preferred. In this paper, we present Ares, a tool for automatic inferring error specifications for C code through static analysis. We employ multiple heuristics to identify error handling blocks and infer error specifications by analyzing the corresponding condition logic. Ares is evaluated on 19 real world projects, and the results reveal that Ares outperforms the state-of-the-art tool APEx by 37% in precision. Ares can also identify more error specifications than APEx. Moreover, the specifications inferred from Ares help find dozens of API-related bugs in well-known projects such as OpenSSL, among them 10 bugs are confirmed by developers. Video: https://youtu.be/nf1QnFAmu8Q. Repository: https://github.com/lc3412/Ares.",Error Handling | Error Specification | Static Analysis,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Li, Chi;Zhou, Min;Gu, Zuxing;Gu, Ming;Zhang, Hongyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078877159,10.1109/ASE.2019.00133,Manticore: A user-friendly symbolic execution framework for binaries and smart contracts,"An effective way to maximize code coverage in software tests is through dynamic symbolic execution-a technique that uses constraint solving to systematically explore a program's state space. We introduce an open-source dynamic symbolic execution framework called Manticore for analyzing binaries and Ethereum smart contracts. Manticore's flexible architecture allows it to support both traditional and exotic execution environments, and its API allows users to customize their analysis. Here, we discuss Manticore's architecture and demonstrate the capabilities we have used to find bugs and verify the correctness of code for our commercial clients.",Ethereum | Manticore | Mcore | Smart contract | Symbolic execution,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Mossberg, Mark;Manzano, Felipe;Hennenfent, Eric;Groce, Alex;Grieco, Gustavo;Feist, Josselin;Brunson, Trent;Dinaburg, Artem",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078914029,10.1109/ASE.2019.00134,"BuRRiTo: A framework to extract, specify, verify and analyze business rules","An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger & Acquisition (M&A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching.",Business Rules Extraction | Graphs | Match and Gap | Natural Language Processing | Rule Components | Rule Document | SBVR | Search and Query | Source Code | Text Mining,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Chittimalli, Pavan Kumar;Anand, Kritika;Pradhan, Shrishti;Mitra, Sayandeep;Prakash, Chandan;Shere, Rohit;Naik, Ravindra",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078900618,10.1109/ASE.2019.00145,Inference of properties from requirements and automation of their formal verification,"Over the past decades, various techniques for the application of formal program analysis of software for embedded systems have been proposed. However, the application of formal methods for software verification is still limited in practise. It is acknowledged that the task of formally stating requirements by specifying the formal properties is a major hindrance. The verification step itself has its shortcoming in its scalability and its limitation to predefined proof tactics in case of automated theorem proving (ATP). These constraints are reduced today by the interaction of the user with the theorem prover (TP) during the execution of the proof. However, this is difficult for non-experts. The objectives of the presented PhD project are the automated inference of declarative property specifications from example data specified by the engineer for a function under development and their automated verification on abstract model level and on code level. We propose the meta-model for Scenario Modeling Language (SML) that allows to specify example data. For the automated property generation we are motivated by Inductive Logic Programming (ILP) techniques for first-order logic in pure mathematics. We propose modifications to its algorithm that allow to process the information that is incorporated in the meta-model of SML. However, this technique is expected to produce too many uninteresting properties. To turn this weakness into strength, our approach proposes to tailor the algorithm towards selection of the right properties that facilitate the automation of the proof. Automated property generation and less user interaction with the prover will leverage formal verification as it will relieve the engineer in the specification as well as in proofing tasks.",Declarative requirement specification | Embedded systems | Formal properties | Formal verification | Specification mining,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Reich, Marina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078916585,10.1109/ASE.2019.00147,Improving patch quality by enhancing key components of automatic program repair,"The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes.",Automatic Program Repair | Patch Quality,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Soto, Mauricio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078947782,10.1109/ASE.2019.00153,API design implications of boilerplate client code,"Designing usable APIs is critical to developers' productivity and software quality but is quite difficult. In this paper, I focus on 'boilerplate' code, sections of code that have to be included in many places with little or no alteration, which many experts in API design have said can be an indicator of API usability problems. I investigate what properties make code count as boilerplate, and present a novel approach to automatically mine boilerplate code from a large set of client code. The technique combines an existing API usage mining algorithm, with novel filters using AST comparison and graph partitioning. With boilerplate candidates identified by the technique, I discuss how this technique could help API designers in reviewing their design decisions and identifying usability issues.",API Usability | Boilerplate Code | Repository Mining,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Nam, Daye",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078926148,10.1109/ASE.2019.00163,Towards comprehensible representation of controllers using machine learning,"From the point of view of a software engineer, having safe and optimal controllers for real life systems like cyber physical systems is a crucial requirement before deployment. Given the mathematical model of these systems along with their specifications, model checkers can be used to synthesize controllers for them. The given work proposes novel approaches for making controller analysis easier by using machine learning to represent the controllers synthesized by model checkers in a succinct manner, while also incorporating the domain knowledge of the system. It also proposes the implementation of a visualization tool which will be integrated into existing model checkers. A lucid controller representation along with a tool to visualize it will help the software engineer debug and monitor the system much more efficiently.",Controller Synthesis | Cyber Physical Systems | Domain Knowledge | Inductive Logic Programming | Machine Learning | Model Checking | Strategy Representation,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",2019-11-01,Conference Paper,"Balasubramaniam, Gargi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85063341634,10.1145/3297858.3304027,"Puddle: A Dynamic, Error-Correcting, Full-Stack Microfluidics Platform","Microfluidic devices promise to automate wetlab procedures by manipulating small chemical or biological samples. This technology comes in many varieties, all of which aim to save time, labor, and supplies by performing lab protocol steps typically done by a technician. However, existing microfluidic platforms remain some combination of inflexible, error-prone, prohibitively expensive, and difficult to program. We address these concerns with a full-stack digital microfluidic automation platform. Our main contribution is a runtime system that provides a high-level API for microfluidic manipulations. It manages fluidic resources dynamically, allowing programmers to freely mix regular computation with microfluidics, which results in more expressive programs than previous work. It also provides real-time error correction through a computer vision system, allowing robust execution on cheaper microfluidic hardware. We implement our stack on top of a low-cost droplet microfluidic device that we have developed. We evaluate our system with the fully-automated execution of polymerase chain reaction (PCR) and a DNA sequencing preparation protocol. These protocols demonstrate high-level programs that combine computational and fluidic operations such as input/output of reagents, heating of samples, and data analysis. We also evaluate the impact of automatic error correction on our system's reliability.",,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2019-04-04,Conference Paper,"Willsey, Max;Stephenson, Ashley P.;Takahashi, Chris;Vaid, Pranav;Nguyen, Bichlien H.;Piszczek, Michal;Betts, Christine;Newman, Sharon;Joshi, Sarang;Strauss, Karin;Ceze, Luis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064615355,10.1145/3297858.3304043,A Formal Analysis of the NVIDIA PTX Memory Consistency Model,"This paper presents the first formal analysis of the official memory consistency model for the NVIDIA PTX virtual ISA. Like other GPU memory models, the PTX memory model is weakly ordered but provides scoped synchronization primitives that enable GPU program threads to communicate through memory. However, unlike some competing GPU memory models, PTX does not require data race freedom, and this results in PTX using a fundamentally different (and more complicated) set of rules in its memory model. As such, PTX has a clear need for a rigorous and reliable memory model testing and analysis infrastructure. We break our formal analysis of the PTX memory model into multiple steps that collectively demonstrate its rigor and validity. First, we adapt the English language specification from the public PTX documentation into a formal axiomatic model. Second, we derive an up-to-date presentation of an OpenCL-like scoped C++ model and develop a mapping from the synchronization primitives of that scoped C++ model onto PTX. Third, we use the Alloy relational modeling tool to empirically test the correctness of the mapping. Finally, we compile the model and mapping into Coq and build a full machine-checked proof that the mapping is sound for programs of any size. Our analysis demonstrates that in spite of issues in previous generations, the new NVIDIA PTX memory model is suitable as a sound compilation target for GPU programming languages such as CUDA.",GPUs | Memory Consistency Models | Model Finding | SAT Solving | Theorem Proving,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2019-04-04,Conference Paper,"Lustig, Daniel;Sahasrabuddhe, Sameer;Giroux, Olivier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064635060,10.1145/3297858.3304065,DCNS: Automated Detection of Conservative Non-Sleep Defects in the Linux Kernel,"For waiting, the Linux kernel offers both sleep-able and nonsleep operations. However, only non-sleep operations can be used in atomic context. Detecting the possibility of execution in atomic context requires a complete inter-procedural flow analysis, often involving function pointers. Developers may thus conservatively use non-sleep operations even outside of atomic context, which may damage system performance, as such operations unproductively monopolize the CPU. Until now, no systematic approach has been proposed to detect such conservative non-sleep (CNS) defects. In this paper,we propose a practical static approach, named DCNS, to automatically detect conservative non-sleep defects in the Linux kernel. DCNS uses a summary-based analysis to effectively identify the code in atomic context and a novel file-connection-based alias analysis to correctly identify the set of functions referenced by a function pointer. We evaluate DCNS on Linux 4.16, and in total find 1629 defects. We manually check 943 defects whose call paths are not so difficult to follow, and find that 890 are real. We have randomly selected 300 of the real defects and sent them to kernel developers, and 251 have been confirmed.",atomic context | defect detection | function-pointer analysis | Linux kernel,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2019-04-04,Conference Paper,"Bai, Jia Ju;Lawall, Julia;Tan, Wende;Hu, Shi Min",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064631613,10.1145/3297858.3304056,Phoenix: A Substrate for Resilient Distributed Graph Analytics,"This paper presents Phoenix, a communication and synchronization substrate that implements a novel protocol for recovering from fail-stop faults when executing graph analytics applications on distributed-memory machines. The standard recovery technique in this space is checkpointing, which rolls back the state of the entire computation to a state that existed before the fault occurred. The insight behind Phoenix is that this is not necessary since it is sufficient to continue the computation from a state that will ultimately produce the correct result.We show that for graph analytics applications, the necessary state adjustment can be specified easily by the programmer using a thin API supported by Phoenix. Phoenix has no observable overhead during fault-free execution, and it is resilient to any number of faults while guaranteeing that the correct answer will be produced at the end of the computation. This is in contrast to other systems in this space which may either have overheads even during fault-free execution or produce only approximate answers when faults occur during execution. We incorporated Phoenix into D-Galois, the state-of-theart distributed graph analytics system, and evaluated it on two production clusters. Our evaluation shows that in the absence of faults, Phoenix is ∼ 24× faster than GraphX, which provides fault tolerance using the Spark system. Phoenix also outperforms the traditional checkpoint-restart technique implemented in D-Galois: In fault-free execution, Phoenix has no observable overhead, while the checkpointing technique has 31% overhead. Furthermore, Phoenix mostly outperforms checkpointing when faults occur, particularly in the common case when only a small number of hosts fail simultaneously.",big data | distributed-memory graph analytics | fault tolerance | self-stabilizing algorithms,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2019-04-04,Conference Paper,"Dathathri, Roshan;Gill, Gurbinder;Hoang, Loc;Pingali, Keshav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064658571,10.1145/3297858.3304068,Wasabi: A Framework for Dynamically Analyzing WebAssembly,"WebAssembly is the new low-level language for the web and has now been implemented in all major browsers since over a year. To ensure the security, performance, and correctness of future web applications, there is a strong need for dynamic analysis tools for WebAssembly. However, building such tools from scratch requires knowledge of low-level details of the language and its runtime environment. This paper presents Wasabi, the first general-purpose framework for dynamically analyzingWebAssembly.Wasabi provides an easy-to-use, high-level API that supports heavyweight dynamic analyses. It is based on binary instrumentation, which inserts calls to analysis functions written in JavaScript into a WebAssembly binary. Dynamically analyzing WebAssembly comes with several unique challenges, such as the problem of tracing type-polymorphic instructions with analysis functions that have a fixed type, which we address through on-demand monomorphization. Our evaluation on compute-intensive benchmarks and real-world applications shows that Wasabi (i) faithfully preserves the original program behavior, (ii) imposes an overhead that is reasonable for heavyweight dynamic analysis, and (iii) makes it straightforward to implement various dynamic analyses, including instruction counting, call graph extraction, memory access tracing, and taint analysis.",,International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS,2019-04-04,Conference Paper,"Lehmann, Daniel;Pradel, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074298992,10.1109/ESEM.2019.8870148,What should i document? A preliminary systematic mapping study into API documentation knowledge,"Background: Good API documentation facilitates the development process, improving productivity and quality. While the topic of API documentation quality has been of interest for the last two decades, there have been few studies to map the specific constructs needed to create a good document. In effect, we still need a structured taxonomy that captures such knowledge systematically.Aims: This study reports emerging results of a systematic mapping study. We capture key conclusions from previous studies that assess API documentation quality, and synthesise the results into a single framework.Method: By conducting a systematic review of 21 key works, we have developed a five dimensional taxonomy based on 34 categorised weighted recommendations.Results: All studies utilise field study techniques to arrive at their recommendations, with seven studies employing some form of interview and questionnaire, and four conducting documentation analysis. The taxonomy we synthesise reinforces that usage description details (code snippets, tutorials, and reference documents) are generally highly weighted as helpful in API documentation, in addition to design rationale and presentation.Conclusions: We propose extensions to this study aligned to developer utility for each of the taxonomy's categories.",API | DevX | documentation | systematic mapping study | taxonomy,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Cummaudo, Alex;Vasa, Rajesh;Grundy, John",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85074248217,10.1109/ESEM.2019.8870155,Enhancing Python Compiler Error Messages via Stack,"Background: Compilers tend to produce cryptic and uninformative error messages, leaving programmers confused and requiring them to spend precious time to resolve the underlying error. To find help, programmers often take to online question-and-answer forums such as Stack Overflow to start discussion threads about the errors they encountered.Aims: We conjecture that information from Stack Overflow threads which discuss compiler errors can be automatically collected and repackaged to provide programmers with enhanced compiler error messages, thus saving programmers' time and energy.Method: We present Pycee, a plugin integrated with the popular Sublime Text IDE to provide enhanced compiler error messages for the Python programming language. Pycee automatically queries Stack Overflow to provide customised and summarised information within the IDE. We evaluated two Pycee variants through a think-aloud user study during which 16 programmers completed Python programming tasks while using Pycee.Results: The majority of participants agreed that Pycee was helpful while completing the study tasks. When compared to a baseline relying on the official Python documentation to enhance compiler error messages, participants generally preferred Pycee in terms of helpfulness, citing concrete suggestions for fixes and example code as major benefits.Conclusions: Our results confirm that data from online sources such as Stack Overflow can be successfully used to automatically enhance compiler error messages. Our work opens up venues for future work to further enhance compiler error messages as well as to automatically reuse content from Stack Overflow for other aspects of programming.",Compiler errors | Stack Overflow | think-aloud,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Thiselton, Emillie;Treude, Christoph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074283181,10.1109/ESEM.2019.8870158,Comprehending Energy Behaviors of Java I/O APIs,"Background: APIs that implement I/O operations are the building blocks of many well-known, non-trivial software systems. These APIs are used for a great variety of programming tasks, from simple file management operations, to database communications and implementation of network protocols.Aims: Despite their ubiquity, there are few studies that focus on comprehending their energy behaviors in order to aid developers interested in building energy-conscious software systems. The goal of this work is two-fold. We first aim to characterize the landscape of the Java I/O programming APIs. After better comprehending their energy variations, our second goal is to refactor software systems that use energy inefficient I/O APIs to their efficient counterparts.Method: To achieve the first goal, we instrumented 22 Java micro-benchmarks that perform I/O operations. To achieve our second goal, we extensively experimented with three benchmarks already optimized for performance and five macro-benchmarks widely used in both software development practice and in software engineering optimization research.Results: Among the results, we found that the energy behavior of Java I/O APIs is diverse. In particular, popular I/O APIs are not always the most energy efficient ones. Moreover, we were able to create 22 refactored versions of the studied benchmarks, eight of which were more energy efficient than the original version. The (statistically significant) energy savings of the refactored versions varied from 0.87% up to 17.19%. More importantly, these energy savings stem from very simple refactorings (often touching less than five lines of code).Conclusions: Our work indicates that there is ample room for studies targeting energy optimization of Java I/O APIs.",,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Rocha, Gilson;Castor, Fernando;Pinto, Gustavo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074290940,10.1109/ESEM.2019.8870163,Detecting and Reporting Object-Relational Mapping Problems: An Industrial Report,"Background: Object-Relational Mapping (ORM) frameworks are regarded as key tools in the software engineer arsenal. However, developers often face ORM problems, and the solution to these problems are not always clear. To mitigate these problems, we created a framework that detects and reports a family of ORM problems. Aims: The aim of this work is to assess how practitioners perceive our framework, the problems, they face, and the eventual points for improvements. Method: We first report an observational study in which we curated 12 ORM-related problems, which are implemented in our framework. We then conducted a developer experience (DX) study with 13 developers (10 well-experienced and 3 students) to assess their experience with our framework to implement six ORM-related tasks. Results: All participants agreed that our framework helped them to finish the programming tasks. The participants perceived that our framework eases the ORM modeling, has precise error messages, and employs ORM best practices. As a shortcoming, however, one participant mentioned that some custom annotations are not very intuitive. Conclusions: Our findings indicate that developers are willing to use frameworks that catch ORM problems, which create opportunities for new research and tools.",,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Nazário, Marcos Felipe Carvalho;Guerra, Eduardo;Bonifácio, Rodrigo;Pinto, Gustavo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074292366,10.1109/ESEM.2019.8870169,An Empirical Study on Technical Debt in a Finnish SME,"Background. The need to release our products under tough time constraints has required us to take shortcuts during the implementation of our products and to postpone the correct implementation, thereby accumulating Technical Debt. Objective. In this work, we report the experience of a Finnish SME in managing Technical Debt (TD), investigating the most common types of TD they faced in the past, their causes, and their effects. Method. We set up a focus group in the case-company, involving different roles. Results. The results showed that the most significant TD in the company stems from disagreements with the supplier and lack of test automation. Specification and test TD are the most significant types of TD. Budget and time constraints were identified as the most important root causes of TD. Conclusion. TD occurs when time or budget is limited or the amount of work are not understood properly. However, not all postponed activities generated 'debt'. Sometimes the accumulation of TD helped meet deadlines without a major impact, while in other cases the cost for repaying the TD was much higher than the benefits. From this study, we learned that learning, careful estimations, and continuous improvement could be good strategies to mitigate TD These strategies include iterative validation with customers, efficient communication with stakeholders, meta-cognition in estimations, and value orientation in budgeting and scheduling.",Small and Medium-Sized Enterprise | Technical Debt,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Lenarduzzi, Valentina;Orava, Teemu;Saarimäki, Nyyti;Systä, Kari;Taibi, Davide",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074293829,10.1109/ESEM.2019.8870174,Towards Standardizing and Improving Classification of Bug-Fix Commits,"Background: Open source software repositories like GitHub are mined to gain useful empirical software engineering insights and answer critical research questions. However, the present state of the art mining approaches suffers from high error rate in the labeling of data that is used for such analysis. This is particularly true when labels are automatically generated from the commit message, and seriously undermines the results of these studies. Aim: Our goal is to label commit comments with high accuracy automatically. In this work, we focus on classifying a commit as a 'Bug-Fix commit' or not. Method: Traditionally, researchers have utilized keyword-based approaches to identify bug fix commits that leads to a significant increase in the error rate. We present an alternative methodology leveraging a deep neural network model called Bidirectional Encoder Representations from Transformers (BERT) that can understand the context of the commit message. We provide the rules for semantic interpretation of commit comments. We construct a hand-labeled dataset from real GitHub commits according to these rules and fine-tune BERT for classification. Results: Our initial evaluation shows that our approach significantly reduces the error rate, with up to 10% relative improvement in classification over keyword-based approaches. Future Direction: We plan on extending our dataset to cover more corner cases and reduce programming language specific biases. We also plan on refining the semantic rules. In this work, we have only considered a simple binary classification problem (Bug-Fix or not), which we plan to extend to other classes and extend the approach to consider multiclass problems. Conclusion: The rules, data, and the model proposed in this paper have the potential to be used by people analyzing open source repositories to improve the labeling of data used in their analysis.",Human Factors | Mining Software Repositories | Predictive Models | Software Maintenance,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Zafar, Sarim;Malik, Muhammad Zubair;Walia, Gursimran Singh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074289924,10.1109/ESEM.2019.8870176,PopCon: Mining Popular Software Configurations from Community,"Software system configuration problems are fairly prevalent and continue to impair the reliability of the underlying system software. Configurations also play an important role in establishing the quality of the software. With every configuration 'knob' we delegate a responsibility to the user and also, we might make the software vulnerable to a failure, poor performance and other system operational issues. Efforts to facilitate a healthy configuration can be summarized by the way of following steps: 1) Gain knowledge about what defines a configuration; 2) operationalize a mechanism to mine popular or recommended configuration defaults; and 3) leverage insights for improving software quality or faster troubleshooting and fixing in the case of a software failure. Using PopCon, a tool that we built, we target all three aspects in a closed-loop fashion, by focussing on storage system software from NetApp®, ONTAP® data management software. We learn popular configurations from the deployed community, evaluate active configurations, deliver actionable information through this tool. Our findings have been encouraging. We can report that about 99% of our ONTAP software user community gravitates towards popular configuration values. Though about 20% of the configuration parameters initially need a custom or user input, we have found that over a period of a few months, systems adopt these popular values. Also, there is a high correlation between the number of outstanding deviations from the popular values and the number of active support cases on these systems. Further, we have also learned that for about 40% of the systems with support cases, deviations disappear at about the time of case closures. Finally, PopCon capabilities presented here are simple to implement and operationalize in any software system.",big data | configurations | data analytics | deployed systems | probability | software engineering | software quality | statistical learning,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Talwadker, Rukma;Aggarwal, Deepti",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074254386,10.1109/ESEM.2019.8870184,The Impact of Developer Experience in Using Java Cryptography,"Background: Previous research has shown that crypto APIs are hard for developers to understand and difficult for them to use. They consequently rely on unvalidated boilerplate code from online resources where security vulnerabilities are common.Aims and method: We analyzed 2,324 open-source Java projects that rely on Java Cryptography Architecture (JCA) to understand how crypto APIs are used in practice, and what factors account for the performance of developers in using these APIs.Results: We found that, in general, the experience of developers in using JCA does not correlate with their performance. In particular, none of the factors such as the number or frequency of committed lines of code, the number of JCA APIs developers use, or the number of projects they are involved in correlate with developer performance in this domain.Conclusions: We call for qualitative studies to shed light on the reasons underlying the success of developers who are expert in using cryptography. Also, detailed investigation at API level is necessary to further clarify a developer obstacles in this domain.",empirical study | Java cryptography | security,International Symposium on Empirical Software Engineering and Measurement,2019-09-01,Conference Paper,"Hazhirpasand, Mohammadreza;Ghafari, Mohammad;Kruger, Stefan;Bodden, Eric;Nierstrasz, Oscar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071920488,10.1145/3338906.3338947,Assessing the quality of the steps to reproduce in bug reports,"A major problem with user-written bug reports, indicated by developers and documented by researchers, is the (lack of high) quality of the reported steps to reproduce the bugs. Low-quality steps to reproduce lead to excessive manual effort spent on bug triage and resolution. This paper proposes Euler, an approach that automatically identifies and assesses the quality of the steps to reproduce in a bug report, providing feedback to the reporters, which they can use to improve the bug report. The feedback provided by Euler was assessed by external evaluators and the results indicate that Euler correctly identified 98% of the existing steps to reproduce and 58% of the missing ones, while 73% of its quality annotations are correct.",Bug Report Quality | Dynamic Software Analysis | Textual Analysis,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Chaparro, Oscar;Bernal-Cárdenas, Carlos;Lu, Jing;Moran, Kevin;Marcus, Andrian;Di Penta, Massimiliano;Poshyvanyk, Denys;Ng, Vincent",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071946613,10.1145/3338906.3338963,A learning-based approach for automatic construction of domain glossary from source code and documentation,"A domain glossary that organizes domain-specific concepts and their aliases and relations is essential for knowledge acquisition and software development. Existing approaches use linguistic heuristics or term-frequency-based statistics to identify domain specific terms from software documentation, and thus the accuracy is often low. In this paper, we propose a learning-based approach for automatic construction of domain glossary from source code and software documentation. The approach uses a set of high-quality seed terms identified from code identifiers and natural language concept definitions to train a domain-specific prediction model to recognize glossary terms based on the lexical and semantic context of the sentences mentioning domain-specific concepts. It then merges the aliases of the same concepts to their canonical names, selects a set of explanation sentences for each concept, and identifies ""is a"", ""has a"", and ""related to"" relations between the concepts. We apply our approach to deep learning domain and Hadoop domain and harvest 5,382 and 2,069 concepts together with 16,962 and 6,815 relations respectively. Our evaluation validates the accuracy of the extracted domain glossary and its usefulness for the fusion and acquisition of knowledge from different documents of different projects.",Concept | Documentation | Domain glossary | Knowledge | Learning,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Wang, Chong;Peng, Xin;Liu, Mingwei;Xing, Zhenchang;Bai, Xuefang;Xie, Bing;Wang, Tuo",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85071925748,10.1145/3338906.3338943,On using machine learning to identify knowledge in API reference documentation,"Using API reference documentation like JavaDoc is an integral part of software development. Previous research introduced a grounded taxonomy that organizes API documentation knowledge in 12 types, including knowledge about the Functionality, Structure, and Quality of an API. We study how well modern text classification approaches can automatically identify documentation containing specific knowledge types. We compared conventional machine learning (k-NN and SVM) with deep learning approaches trained on manually-annotated Java and .NET API documentation (n = 5,574). When classifying the knowledge types individually (i.e., multiple binary classifiers) the best AUPRC was up to 87.",API documentation | Information needs | Machine learning,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Fucci, Davide;Mollaalizadehbahnemiri, Alireza;Maalej, Walid",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85071907616,10.1145/3338906.3338971,Generating query-specific class API summaries,"Source code summaries are concise representations, in form of text and/or code, of complex code elements and are meant to help developers gain a quick understanding that in turns help them perform specific tasks. Generation of summaries that are task-specific is still a challenge in the automatic code summarization field. We propose an approach for generating on-demand, extrinsic hybrid summaries for API classes, relevant to a programming task, formulated as a natural language query. The summaries include the most relevant sentences extracted from the API reference documentation and the most relevant methods. External evaluators assessed the summaries generated for classes retrieved from JDK and Android libraries for several programming tasks. The majority found that the summaries are complete, concise, and readable. A comparison with summaries produce by three baseline approaches revealed that the information present only in our summaries is more relevant than the one present only in the baselines summaries. Finally, an extrinsic evaluation study showed that the summaries help the users evaluating the correctness of API retrieval results, faster and more accurately.",API | Code retrieval | Code summarization | Knowledge Graph,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Liu, Mingwei;Peng, Xin;Marcus, Andrian;Xing, Zhenchang;Xie, Wenkai;Xing, Shuangshuang;Liu, Yang",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85071916026,10.1145/3338906.3338917,A large-scale empirical study of compiler errors in continuous integration,"Continuous Integration (CI) is a widely-used software development practice to reduce risks. CI builds often break, and a large amount of efforts are put into troubleshooting broken builds. Despite that compiler errors have been recognized as one of the most frequent types of build failures, little is known about the common types, fix efforts and fix patterns of compiler errors that occur in CI builds of open-source projects. To fill such a gap, we present a large-scale empirical study on 6,854,271 CI builds from 3,799 open-source Java projects hosted on GitHub. Using the build data, we measured the frequency of broken builds caused by compiler errors, investigated the ten most common compiler error types, and reported their fix time. We manually analyzed 325 broken builds to summarize fix patterns of the ten most common compiler error types. Our findings help to characterize and understand compiler errors during CI and provide practical implications to developers, tool builders and researchers.",Build Failures | Compiler Errors | Continuous Integration,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Zhang, Chen;Chen, Bihuan;Chen, Linlin;Peng, Xin;Zhao, Wenyun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071932399,10.1145/3338906.3338916,How bad can a bug get? An empirical analysis of software failures in the OpenStack cloud computing platform,"Cloud management systems provide abstractions and APIs for programmatically configuring cloud infrastructures. Unfortunately, residual software bugs in these systems can potentially lead to high-severity failures, such as prolonged outages and data losses. In this paper, we investigate the impact of failures in the context widespread OpenStack cloud management system, by performing fault injection and by analyzing the impact of the resulting failures in terms of fail-stop behavior, failure detection through logging, and failure propagation across components. The analysis points out that most of the failures are not timely detected and notified; moreover, many of these failures can silently propagate over time and through components of the cloud management system, which call for more thorough run-time checks and fault containment.",Bug analysis | Fault injection | OpenStack,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Cotroneo, Domenico;De Simone, Luigi;Liguori, Pietro;Natella, Roberto;Bidokhti, Nematollah",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071954606,10.1145/3338906.3338968,Effects of explicit feature traceability on program comprehension,"Developers spend a substantial amount of their time with program comprehension. To improve their comprehension and refresh their memory, developers need to communicate with other developers, read the documentation, and analyze the source code. Many studies show that developers focus primarily on the source code and that small improvements can have a strong impact. As such, it is crucial to bring the code itself into a more comprehensible form. A particular technique for this purpose are explicit feature traces to easily identify a programs functionalities. To improve our empirical understanding about the effects of feature traces, we report an online experiment with 49 professional software developers. We studied the impact of explicit feature traces, namely annotations and decomposition, on program comprehension and compared them to the same code without traces. Besides this experiment, we also asked our participants about their opinions in order to combine quantitative and qualitative data. Our results indicate that, as opposed to purely object-oriented code: (1) annotations can have positive effects on program comprehension; (2) decomposition can have a negative impact on bug localization; and (3) our participants perceive both techniques as beneficial. Moreover, none of the three code versions yields significant improvements on task completion time. Overall, our results indicate that lightweight traceability, such as using annotations, provides immediate benefits to developers during software development and maintenance without extensive training or tooling; and can improve current industrial practices that rely on heavyweight traceability tools (e.g., DOORS) and retroactive fulfillment of standards (e.g., ISO-26262, DO-178B).",Feature traceability | Program comprehension | Separation of concerns | Software maintenance,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Krüger, Jacob;Alkl, Gül;Berger, Thorsten;Leich, Thomas;Saake, Gunter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071955221,10.1145/3338906.3338965,A framework for writing trigger-action todo comments in executable format,"Natural language elements, e.g., todo comments, are frequently used to communicate among developers and to describe tasks that need to be performed (actions) when specific conditions hold on artifacts related to the code repository (triggers), e.g., from the Apache Struts project: remove expectedJDK15 and if() after switching to Java 1.6. As projects evolve, development processes change, and development teams reorganize, these comments, because of their informal nature, frequently become irrelevant or forgotten. We present the first framework, dubbed TrigIt, to specify trigger-action todo comments in executable format. Thus, actions are executed automatically when triggers evaluate to true. TrigIt specifications are written in the host language (e.g., Java) and are evaluated as part of the build process. The triggers are specified as query statements over abstract syntax trees, abstract representation of build configuration scripts, issue tracking systems, and system clock time. The actions are either notifications to developers or code transformation steps. We implemented TrigIt for the Java programming language and migrated 44 existing trigger-action comments from several popular open-source projects. Evaluation of TrigIt, via a user study, showed that users find TrigIt easy to learn and use. TrigIt has the potential to enforce more discipline in writing and maintaining comments in large code repositories.",Domain specific languages | Todo comments | Trigger-action,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Nie, Pengyu;Rai, Rishabh;Li, Junyi Jessy;Khurshid, Sarfraz;Mooney, Raymond J.;Gligoric, Milos",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85071947495,10.1145/3338906.3338979,Decomposing the rationale of code commits: The software developers perspective,"Communicating the rationale behind decisions is essential for the success of software engineering projects. In particular, understanding the rationale of code commits is an important and often difficult task. We posit that part of such difficulty lies in rationale often being treated as a single piece of information. In this paper, we set to discover the breakdown of components in which developers decompose the rationale of code commits in the context of software maintenance, and to understand their experience with it and with its individual components. For this goal, we apply a mixed-methods approach, interviewing 20 software developers to ask them how they decompose rationale, and surveying an additional 24 developers to understand their experiences needing, finding, and recording those components. We found that developers decompose the rationale of code commits into 15 components, each of which is differently needed, found, and recorded. These components are: goal, need, benefits, constraints, alternatives, selected alternative, dependencies, committer, time, location, modifications, explanation of modifications, validation, maturity stage, and side effects. Our findings provide multiple implications. Educators can now disseminate the multiple dimensions and importance of the rationale of code commits. For practitioners, our decomposition of rationale defines a ""common vocabulary"" to use when discussing rationale of code commits, which we expect to strengthen the quality of their rationale sharing and documentation process. For researchers, our findings enable techniques for automatically assessing, improving, and generating rationale of code commits to specifically target the components that developers need.",Software Changes Rationale | Software Evolution and Maintenance,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Safwan, Khadijah Al;Servant, Francisco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071904593,10.1145/3338906.3338909,Why arent regular expressions a lingua franca? An empirical study on the re-use and portability of regular expressions,"This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics? In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language.We experimentally evaluated the riskiness of this practice using a novel regex corpus 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences. We report that developers belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.",Developer perceptions | Empirical software engineering | Mining software repositories | Portability | Re-use | ReDoS | Regular expressions,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Davis, James C.;Michael, Louis G.;Coghlan, Christy A.;Servant, Francisco;Lee, Dongyoon",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071906749,10.1145/3338906.3338960,Effective error-specification inference via domain-knowledge expansion,"Error-handling code responds to the occurrence of runtime errors. Failure to correctly handle errors can lead to security vulnerabilities and data loss. This paper deals with error handling in software written in C that uses the return-code idiom: the presence and type of error is encoded in the return value of a function. This paper describes EESI, a static analysis that infers the set of values that a function can return on error. Such a function error-specification can then be used to identify bugs related to incorrect error handling. The key insight of EESI is to bootstrap the analysis with domain knowledge related to error handling provided by a developer. EESI uses a combination of intraprocedural, flow-sensitive analysis and interprocedural, context-insensitive analysis to ensure precision and scalability. We built a tool ECC to demonstrate how the function error-specifications inferred by EESI can be used to automatically find bugs related to incorrect error handling. ECC detected 246 bugs across 9 programs, of which 110 have been confirmed. ECC detected 220 previously unknown bugs, of which 99 are confirmed. Two patches have already been merged into OpenSSL.",Bug finding | Error handling | Static analysis,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Defreez, Daniel;Baldwin, Haaken Martinson;Rubio-González, Cindy;Thakur, Aditya V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071935205,10.1145/3338906.3338959,AggrePlay: Efficient record and replay of multi-threaded programs,"Deterministic replay presents challenges and often results in high memory and runtime overheads. Previous studies deterministically reproduce program outputs often only after several replay iterations or may produce a non-deterministic sequence of output to external sources. In this paper, we propose AggrePlay, a deterministic replay technique which is based on recording read-write interleavings leveraging thread-local determinism and summarized read values. During the record phase, AggrePlay records a read count vector clock for each thread on each memory location. Each thread checks the logged vector clock against the current read count in the replay phase before a write event. We present an experiment and analyze the results using the Splash2x benchmark suite as well as two real-world applications. The experimental results show that on average, AggrePlay experiences a better reduction in compressed log size, and 56% better runtime slowdown during the record phase, as well as a 41.58% higher probability in the replay phase than existing work.",Concurrency | Deterministic Replay | Multi-threading,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Pobee, Ernest;Chan, W. K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071947918,10.1145/3338906.3338952,Phoenix: Automated data-driven synthesis of repairs for static analysis violations,"Traditional automatic program repair (APR) tools rely on a test-suite as a repair specification. But test suites even when available are not of specification quality, limiting the performance and hence viability of test-suite based repair. On the other hand, static analysis-based bug finding tools are seeing increasing adoption in industry but still face challenges since the reported violations are viewed as not easily actionable. We propose a novel solution that solves both these challenges through a technique for automatically generating high-quality patches for static analysis violations by learning from examples. Our approach uses the static analyzer as an oracle and does not require a test suite. We realize our solution in a system, Phoenix, that implements a fully-automated pipeline that mines and cleans patches for static analysis violations from the wild, learns generalized executable repair strategies as programs in a novel Domain Specific Language (DSL), and then instantiates concrete repairs from them on new unseen violations. Using Phoenix we mine a corpus of 5,389 unique violations and patches from 517 Github projects. In a cross-validation study on this corpus Phoenix successfully produced 4,596 bug-fixes, with a recall of 85% and a precision of 54%. When applied to the latest revisions of a further5 Github projects, Phoenix produced 94 correct patches to previously unknown bugs, 19 of which have already been accepted and merged by the development teams. To the best of our knowledge this constitutes, by far the largest application of any automatic patch generation technology to large-scale real-world systems.",Program repair | Program synthesis | Programming-by-example | Static analysis,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Bavishi, Rohan;Yoshida, Hiroaki;Prasad, Mukul R.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071953670,10.1145/3338906.3338946,Java reflection API: Revealing the dark side of the mirror,"Developers of widely used Java Virtual Machines (JVMs) implement and test the Java Reflection API based on a Javadoc, which is specified using a natural language. However, there is limited knowledge on whether Java Reflection API developers are able to systematically reveal i) underdetermined specifications; and ii) non-conformances between their implementation and the Javadoc. Moreover, current automatic test suite generators cannot be used to detect them. To better understand the problem, we analyze test suites of two widely used JVMs, and we conduct a survey with 130 developers who use the Java Reflection API to see whether the Javadoc impacts on their understanding. We also propose a technique to detect underdetermined specifications and non-conformances between the Javadoc and the implementations of the Java Reflection API. It automatically creates test cases, and executes them using different JVMs. Then, we manually execute some steps to identify underdetermined specifications and to confirm whether a non-conformance candidate is indeed a bug. We evaluate our technique in 439 input programs. Our technique identifies underdetermined specification and non-conformance candidates in 32 Java Reflection API public methods of 7 classes. We report underdetermined specification candidates in 12 Java Reflection API methods. Java Reflection API specifiers accept 3 underdetermined specification candidates (25%). We also report 24 non-conformance candidates to Eclipse OpenJ9 JVM, and 7 to Oracle JVM. Eclipse OpenJ9 JVM developers accept and fix 21 candidates (87.5%), and Oracle JVM developers accept 5 and fix 4 non-conformance candidates.",Non-conformance | Reflection API | Underdetermined Specification,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Pontes, Felipe;Gheyi, Rohit;Souto, Sabrina;Garcia, Alessandro;Ribeiro, Márcio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071922367,10.1145/3338906.3338932,Finding and understanding bugs in software model checkers,"Software Model Checking (SMC) is a well-known automatic program verification technique and frequently adopted for checking safety-critical software. Thus, the reliability of SMC tools themselves (i.e., software model checkers) is critical. However, little work exists on validating software model checkers, an important problem that this paper tackles by introducing a practical, automated fuzzing technique. For its simplicity and generality, we focus on control-flow reachability (e.g., whether or how many times a branch is reached) and address two specific challenges for effective fuzzing: oracle and scalability. Given a deterministic program, we (1) leverage its concrete executions to synthesize valid branch reachability properties (thus solving the oracle problem) and (2) fuse such individual properties into a single safety property (thus improving the scalability of fuzzing and reducing manual inspection). We have realized our approach as the MCFuzz tool and applied it to extensively test three state-of-the-art C software model checkers, CPAchecker, CBMC, and SeaHorn. MCFuzz has found 62 unique bugs in all three model checkers - 58 have been confirmed, and 20 have been fixed. We have further analyzed and categorized these bugs (which are diverse), and summarized several lessons for building reliable and robust model checkers. Our testing effort has been well-appreciated by the model checker developers, and also led to improved tool usability and documentation.",Fuzz Testing | Software Model Checking | Software Testing,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Zhang, Chengyu;Su, Ting;Yan, Yichen;Zhang, Fuyuan;Pu, Geguang;Su, Zhendong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071931957,10.1145/3338906.3340445,The role of limitations and SLAs in the API industry,"As software architecture design is evolving to a microservice paradigm, RESTful APIs are being established as the preferred choice to build applications. In such a scenario, there is a shift towards a growing market of APIs where providers offer different service levels with tailored limitations typically based on the cost. In this context, while there are well established standards to describe the functional elements of APIs (such as the OpenAPI Specification), having a standard model for Service Level Agreements (SLAs) for APIs may boost an open ecosystem of tools that would represent an improvement for the industry by automating certain tasks during the development such as: SLA-aware scaffolding, SLA-aware testing, or SLA-aware requesters. Unfortunately, despite there have been several proposals to describe SLAs for software in general and web services in particular during the past decades, there is an actual lack of a widely used standard due to the complex landscape of concepts surrounding the notion of SLAs and the multiple perspectives that can be addressed. In this paper, we aim to analyze the landscape for SLAs for APIs in two different directions: i) Clarifying the SLA-driven API development lifecycle: its activities and participants; 2) Developing a catalog of relevant concepts and an ulterior prioritization based on different perspectives from both Industry and Academia. As a main result, we present a scored list of concepts that paves the way to establish a concrete road-map for a standard industry-aligned specification to describe SLAs in APIs.",API Gateways | OpenAPI Specification | RESTful APIs | SLA | SLA-driven APIs,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Gamez-Diaz, Antonio;Fernandez, Pablo;Ruiz-Cortés, Antonio;Molina, Pedro J.;Kolekar, Nikhil;Bhogill, Prithpal;Mohaan, Madhurranjan;Méndez, Francisco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071932713,10.1145/3338906.3340447,Evolving with patterns: A 31-month startup experience report,"Software startups develop innovative products under extreme conditions of uncertainty. At the same time they represent a fast-growing sector in the economy and scale up research and technological advancement. This paper describes findings after observing a startup during its first 31 months of life. The data was collected through observations, unstructured interviews as well as from technical and managerial documentation of the startup. The findings are based on a deductive analysis and summarized in 24 contextualized patterns that concern communication, interaction with customer, teamwork, and management. Furthermore, 13 lessons learned are presented with the aim of sharing experience with other startups. This industry report contributes to understanding the applicability and usefulness of startups' patterns, providing valuable knowledge for the startup software engineering community.",Industrial report | Patterns | Software engineering | Startups,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Morales-Trujillo, Miguel Ehécatl;García-Mireles, Gabriel Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071906201,10.1145/3338906.3341174,BIKER: A tool for Bi-information source based API method recommendation,"Application Programming Interfaces (APIs) in software libraries play an important role in modern software development. Although most libraries provide API documentation as a reference, developers may find it difficult to directly search for appropriate APIs in documentation using the natural language description of the programming tasks. We call such phenomenon as knowledge gap, which refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes. In this paper, we propose a Java API recommendation tool named BIKER (Bi-Information source based KnowledgE Recommendation) to bridge the knowledge gap. We implement BIKER as a search engine website. Given a query in natural language, instead of directly searching API documentation, BIKER first searches for similar API-related questions on Stack Overflow to extract candidate APIs. Then, BIKER ranks them by considering the querys similarity with both Stack Overflow posts and API documentation. Finally, to help developers better understand why each API is recommended and how to use them in practice, BIKER summarizes and presents supplementary information (e.g., API description, code examples in Stack Overflow posts) for each recommended API. Our quantitative evaluation and user study demonstrate that BIKER can help developers find appropriate APIs more efficiently and precisely.",API Documentation | API Recommendation | Stack Overflow,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Cai, Liang;Wang, Haoye;Huang, Qiao;Xia, Xin;Xing, Zhenchang;Lo, David",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85071902340,10.1145/3338906.3341177,DISCOVER: Detecting algorithmic complexity vulnerabilities,"Algorithmic Complexity Vulnerabilities (ACV) are a class of vulnerabilities that enable Denial of Service Attacks. ACVs stem from asymmetric consumption of resources due to complex loop termination logic, recursion, and/or resource intensive library APIs. Completely automated detection of ACVs is intractable and it calls for tools that assist human analysts. We present DISCOVER, a suite of tools that facilitates human-on-the-loop detection of ACVs. DISCOVER's workflow can be broken into three phases - (1) Automated characterization of loops, (2) Selection of suspicious loops, and (3) Interactive audit of selected loops. We demonstrate DISCOVER using a case study using a DARPA challenge app. DISCOVER supports analysis of Java source code and Java bytecode. We demonstrate it for Java bytecode.",Algorithmic Complexity Vulnerability | Cybersecurity | Software Analysis,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Awadhutkar, Payas;Santhanam, Ganesh Ram;Holland, Benjamin;Kothari, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071909129,10.1145/3338906.3341186,AnswerBot: An answer summary generation tool based on stack overflow,"Software Q&A sites (like Stack Overflow) play an essential role in developers day-to-day work for problem-solving. Although search engines (like Google) are widely used to obtain a list of relevant posts for technical problems, we observed that the redundant relevant posts and sheer amount of information barriers developers to digest and identify the useful answers. In this paper, we propose a tool AnswerBot which enables to automatically generate an answer summary for a technical problem. AnswerBot consists of three main stages, (1) relevant question retrieval, (2) useful answer paragraph selection, (3) diverse answer summary generation. We implement it in the form of a search engine website. To evaluate AnswerBot, we first build a repository includes a large number of Java questions and their corresponding answers from Stack Overflow. Then, we conduct a user study that evaluates the answer summary generated by AnswerBot and two baselines (based on Google and Stack Overflow search engine) for 100 queries. The results show that the answer summaries generated by AnswerBot are more relevant, useful, and diverse. Moreover, we also substantially improved the efficiency of AnswerBot (from 309 to 8 seconds per query).",Relevant Question Retrieval | Stack Overflow | Summary Generation,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Cai, Liang;Wang, Haoye;Xu, Bowen;Huang, Qiao;Xia, Xin;Lo, David;Xing, Zhenchang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071937916,10.1145/3338906.3341181,Eagle: A team practices audit framework for agile software development,"Agile/XP (Extreme Programming) software teams are expected to follow a number of specific practices in each iteration, such as estimating the effort (points) required to complete user stories, properly using branches and pull requests to coordinate merging multiple contributors code, having frequent standups to keep all team members in sync, and conducting retrospectives to identify areas of improvement for future iterations. We combine two observations in developing a methodology and tools to help teams monitor their performance on these practices. On the one hand, many Agile practices are increasingly supported by web-based tools whose data exhaust can provide insight into how closely the teams are following the practices. On the other hand, some of the practices can be expressed in terms similar to those developed for expressing service level objectives (SLO) in software as a service; as an example, a typical SLO for an interactive Web site might be over any 5-minute window, 99% of requests to the main page must be delivered within 200ms and, analogously, a potential Team Practice (TP) for an Agile/XP team might be over any 2-week iteration, 75% of stories should be 1-point stories. Following this similarity, we adapt a system originally developed for monitoring and visualizing service level agreement (SLA) compliance to monitor selected TPs for Agile/XP software teams. Specifically, the system consumes and analyzes the data exhaust from widely-used tools such as GitHub and Pivotal Tracker and provides team(s) and coach(es) a dashboard summarizing the teams adherence to various practices. As a qualitative initial investigation of its usefulness, we deployed it to twenty student teams in a four-sprint software engineering project course. We find an improvement of the adherence to team practice and a positive students self-evaluations of their team practices when using the tool, compared to previous experiences using an Agile/XP methodology. The demo video is located at <a>https://youtu.be/A4xwJMEQh9c</a> and a landing page with a live demo at <a>https://isa-group.github.io/2019-05-eagle-demo/</a>.",Agile | Team dashboard | Team practice | Team practice agreement,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Guerrero, Alejandro;Fresno, Rafael;Ju, An;Fox, Armando;Fernandez, Pablo;Muller, Carlos;Ruiz-Cortés, Antonio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071903250,10.1145/3338906.3342486,Recommending related functions from API usage-based function clone structures,"Developers need to be able to find reusable code for desired software features in a way that supports opportunistic programming for increased developer productivity. Our objective is to develop a recommendation system that provides a developer with function recommendations having functionality relevant to her development task. We employ a combination of information retrieval, static code analysis and data mining techniques to build the proposed recommendation system called FACER (Feature-driven API usage-based Code Examples Recommender). We performed an experimental evaluation on 122 projects from GitHub from selected categories to determine the accuracy of the retrieved code for related features. FACER recommended functions with a precision of 54% and 75% when evaluated using automated and manual methods respectively.",API usage | Code clones | Code recommendation | Software features,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Abid, Shamsa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071918433,10.1145/3338906.3342494,Understanding source code comments at large-scale,"Source code comments are important for any software, but the basic patterns of writing comments across domains and programming languages remain unclear. In this paper, we take a first step toward understanding differences in commenting practices by analyzing the comment density of 150 projects in 5 different programming languages. We have found that there are noticeable differences in comment density, which may be related to the programming language used in the project and the purpose of the project.",Comment Density | Empirical Study | Source Code Comments,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"He, Hao",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85071933245,10.1145/3338906.3342497,Analysing socio-technical congruence in the package dependency network of Cargo,"Software package distributions form large dependency networks maintained by large communities of contributors. My PhD research will consist of analysing the evolution of the socio-technical congruence of these package dependency networks, and studying its impact on the health of the ecosystem and its community. I have started a longitudinal empirical study of Cargo's dependency network and the social (commenting) and technical (development) activities in Cargo's package repositories on GitHub, and present some preliminary findings.",Package dependency network | Socio-Technical congruence | Software development | Software ecosystem | Software repository mining,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Golzadeh, Mehdi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICPC.2019.00013,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072340646,10.1109/ICPC.2019.00019,A large-scale empirical study on code-comment inconsistencies,"Code comments are a primary means to document source code. Keeping comments up-to-date during code change activities requires substantial time and attention. For this reason, researchers have proposed methods to detect code-comment inconsistencies (i.e., comments that are not kept in sync with the code they document) and studies have been conducted to investigate this phenomenon. However, these studies were performed at a small scale, relying on quantitative analysis, thus limiting the empirical knowledge about code-comment inconsistencies. We present the largest study at date investigating how code and comments co-evolve. The study has been performed by mining 1.3 Billion AST-level changes from the complete history of 1,500 systems. Moreover, we manually analyzed 500 commits to define a taxonomy of code-comment inconsistencies fixed by developers. Our analysis discloses the extent to which different types of code changes (e.g., change of selection statements) trigger updates to the related comments, identifying cases in which code-comment inconsistencies are more likely to be introduced. The defined taxonomy categorizes the types of inconsistencies fixed by developers. Our results can guide the development of tools aimed at detecting and fixing code-comment inconsistencies.",Code comments | Software evolution,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Wen, Fengcai;Nagy, Csaba;Bavota, Gabriele;Lanza, Michele",Include,
10.1016/j.jss.2022.111515,2-s2.0-85072315687,10.1109/ICPC.2019.00020,An empirical study on practicality of specification mining algorithms on a real-world application,"Dynamic model inference techniques have been the center of many research projects recently. There are now multiple open source implementations of state-of-the-art algorithms, which provide basic abstraction and merging capabilities. Most of these tools and algorithms have been developed with one particular application in mind, which is program comprehension. The output models can abstract away the details of the program and represent the software behaviour in a concise and easy to understand form. However, one application context that is less studied is using such inferred models for debugging, where the behaviour to abstract is a faulty behaviour (e.g., a set of execution traces including a failed test case). We tried to apply some of the existing model inference techniques in a real-world industrial context to support program comprehension for debugging. Our initial experiments have shown many limitations both in terms of implementation as well as the algorithms. The paper will discuss the root cause of the failures and proposes ideas for future improvement.",Debugging | Empirical Study | Fault Localization | Model Inference | Specification Mining,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Mashhadi, Mohammad Jafar;Hemmati, Hadi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/ICPC.2019.00036,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072341219,10.1109/ICPC.2019.00041,CCSpec: A correctness condition specification tool,"Concurrent libraries provide data structures whose operations appear to execute atomically when invoked individually. Although these libraries guarantee safety for the data structure operations, the composition of operations may be vulnerable to undefined behavior. The difficulty of reasoning about safety properties in a concurrent environment has led to the development of tools to verify that a concurrent data structure meets a correctness condition. The disadvantage of these tools is that they cannot verify that the composition of concurrent data structure operations respects the intended semantics of the algorithm. Formal logic has been proposed to enable the verification of correctness specifications for a concurrent algorithm. However, a large amount of manual labor is required to fully mechanize the correctness proofs of the concurrent algorithm and each concurrent data structure invoked in the algorithm. In this research, we propose Correctness Condition Specification (CCSpec), the first tool that automatically checks the correctness of a composition of concurrent multi-container operations performed in a non-atomic manner. In addition to checking the correctness of a composition of data structure operations in a concurrent algorithm, CCSpec also checks the correctness of each concurrent data structure utilized in the algorithm. A reference to a container is associated with each method called in a concurrent history to enable the evaluation of correctness for a composition of multiple containers. We develop a lightweight custom specification language that allows the user to define a correctness condition associated with the concurrent algorithm and a correctness condition associated with the concurrent data structures. We demonstrate the practical application of CCSpec by checking the correctness of a concurrent depth-first search utilizing a non-blocking stack, a concurrent breadth-first search utilizing a non-blocking queue, a concurrent shortest path algorithm utilizing a non-blocking priority queue, and a concurrent adjacency list utilizing non-blocking sets.",Concurrency | Correctness condition | Verification,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Peterson, Christina;Laborde, Pierre;Dechev, Damian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072348220,10.1109/ICPC.2019.00050,A replication study on code comprehension and expertise using lightweight biometric sensors,"Code comprehension has been recently investigated from physiological and cognitive perspectives using medical imaging devices. Floyd et al. (i.e., the original study) used fMRI to classify the type of comprehension tasks performed by developers and relate their results to their expertise. We replicate the original study using lightweight biometrics sensors. Our study participants-28 undergrads in computer science-performed comprehension tasks on source code and natural language prose. We developed machine learning models to automatically identify what kind of tasks developers are working on leveraging their brain-, heart-, and skin-related signals. The best improvement over the original study performance is achieved using solely the heart signal obtained through a single device (BAC 87%vs. 79.1%). Differently from the original study, we did not observe a correlation between the participants' expertise and the classifier performance (τ= 0.16, p= 0.31). Our findings show that lightweight biometric sensors can be used to accurately recognize comprehension opening interesting scenarios for research and practice.",Biometric sensors | Machine learning | Software development tasks,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Fucci, Davide;Girardi, Daniela;Novielli, Nicole;Quaranta, Luigi;Lanubile, Filippo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071952728,10.1109/ICPC.2019.00051,Measuring interprocess communications in distributed systems,"Due to the increasing demands for computational scalability and performance, more distributed software systems are being developed than single-process programs. As an important step in software quality assurance, software measurement provides essential means and evidences in quality assessment hence incentives and guidance for quality improvement. However, despite the rich literature on software measurement in general, existing measures are mostly defined for single-process programs only or limited to conventional metrics. In this paper, we propose a novel set of metrics for common distributed systems, with a focus on their interprocess communications (IPC), a vital aspect of their run-time behaviors. We demonstrated the practicality of characterizing IPC dynamics and complexity via the proposed IPC metrics, by computing the measures against nine real-world distributed systems and their varied executions. To demonstrate the practical usefulness of IPC measurements, we extensively investigated how the proposed metrics may help understand and analyze various quality factors of distributed systems, ranging from maintainability and stability to security and performance, on the same nine distributed systems and their executions. We found that higher IPC coupling tended to be generally detrimental to most of the quality aspects while interprocess sharing of common functionalities should be promoted due to its understandability and security benefits.",Coupling | Distributed system | Dynamic measurement | Interprocess communication | Quality factors,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Fu, Xiaoqin;Cai, Haipeng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85070651329,10.1109/ICPC.2019.00052,Meditor: Inference and application of API migration edits,"Developers build programs based on software libraries. When a library evolves, programmers need to migrate their client code from the library's old release(s) to new release(s). Due to the API backwards incompatibility issues, such code migration may require developers to replace API usage and apply extra edits (e.g., statement insertions or deletions) to ensure the syntactic or semantic correctness of migrated code. Existing tools extract API replacement rules without handling the additional edits necessary to fulfill a migration task. This paper presents our novel approach, Meditor, which extracts and applies the necessary edits together with API replacement changes. Meditor has two phases: inference and application of migration edits. For edit inference, Meditor mines open source repositories for migration-related (MR) commits, and conducts program dependency analysis on changed Java files to locate and cluster MR code changes. From these changes, Meditor further generalizes API migration edits by abstracting away unimportant details (e.g., concrete variable identifiers). For edit application, Meditor matches a given program with inferred edits to decide which edit is applicable, customizes each applicable edit, and produces a migrated version for developers to review. We applied Meditor to four popular libraries: Lucene, CraftBukkit, Android SDK, and Commons IO. By searching among 602,249 open source projects on GitHub, Meditor identified 1,368 unique migration edits. Among these edits, 885 edits were extracted from single updated statements, while the other 483 more complex edits were from multiple co-changed statements. We sampled 937 inferred edits for manual inspection and found all of them to be correct. Our evaluation shows that Meditor correctly applied code migrations in 218 out of 225 cases. This research will help developers automatically adapt client code to different library versions.",API migration edits | Automatic program transformation | Program dependency analysis,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Xu, Shengzhe;Dong, Ziqi;Meng, Na",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072326746,10.1109/ICPC.2019.00053,On the use of information retrieval to automate the detection of third-party Java library migration at the method level,"The migration process between different third-party libraries is hard, complex and error-prone. Typically, during a library migration, developers need to find methods in the new library that are most adequate in replacing the old methods of the retired library. This process is subjective and time-consuming as developers need to fully understand the documentation of both libraries' Application Programming Interfaces, and find the right matching between their methods, if it exists. In this context, several studies rely on mining existing library migrations to provide developers with by-example approaches for similar scenarios. In this paper, we introduce a novel mining approach that extracts existing instances of library method replacements that are manually performed by developers for a given library migration to automatically generate migration patterns in the method level. Thereafter, our approach combines the mined method-change patterns with method-related lexical similarity to accurately detect mappings between replacing/replaced methods. We conduct a large scale empirical study to evaluate our approach on a benchmark of 57,447 open-source Java projects leading to 9 popular library migrations. Our qualitative results indicate that our approach significantly increases the accuracy of mining method-level mappings by an average accuracy of 12%, as well as increasing the number of discovered method mappings, in comparison with existing state-of-the-art studies. Finally, we provide the community with an open source mining tool along with a dataset of all mined migrations at the method level.",API | API migration | Information Retrieval | Software Evolution | Third Party Library,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Alrubaye, Hussein;Mkaouer, Mohamed Wiem;Ouni, Ali",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072349412,10.1109/ICPC.2019.00054,Recommending comprehensive solutions for programming tasks by mining crowd knowledge,"Developers often search for relevant code examples on the web for their programming tasks. Unfortunately, they face two major problems. First, the search is impaired due to a lexical gap between their query (task description) and the information associated with the solution. Second, the retrieved solution may not be comprehensive, i.e., the code segment might miss a succinct explanation. These problems make the developers browse dozens of documents in order to synthesize an appropriate solution. To address these two problems, we propose CROKAGE (Crowd Knowledge Answer Generator), a tool that takes the description of a programming task (the query) and provides a comprehensive solution for the task. Our solutions contain not only relevant code examples but also their succinct explanations. Our proposed approach expands the task description with relevant API classes from Stack Overflow Q&A threads and then mitigates the lexical gap problems. Furthermore, we perform natural language processing on the top quality answers and then return such programming solutions containing code examples and code explanations unlike earlier studies. We evaluate our approach using 97 programming queries, of which 50% was used for training and 50% was used for testing, and show that it outperforms six baselines including the state-of-art by a statistically significant margin. Furthermore, our evaluation with 29 developers using 24 tasks (queries) confirms the superiority of CROKAGE over the state-of-art tool in terms of relevance of the suggested code examples, benefit of the code explanations and the overall solution quality (code + explanation).",Mining Crowd Knowledge | Stack Overflow | Word Embedding,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Silva, Rodrigo;Roy, Chanchal;Rahman, Mohammad;Schneider, Kevin;Paixao, Klerisson;Maia, Marcelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072319867,10.1109/ICPC.2019.00055,Using frugal user feedback with closeness analysis on code to improve IR-based traceability recovery,"Traceability recovery allows developers to extract and comprehend the trace links among software artifacts (e.g., requirements and code). These trace links can provide important support to software maintenance and evolution tasks. Information Retrieval (IR) is now widely accepted as the key technique of semi-automatic tools to recover candidate trace links based on textual similarities among artifacts. However, the vocabulary mismatch problem between different artifacts hinders the performance of these IR-based approaches. Thus, a growing body of enhancing strategies were proposed based on user feedback. They allow to adjust the textual similarities of candidate links after users accept or reject part of these links. Recently, several approaches successfully used this strategy to improve the performance of IR-based traceability recovery. However, these approaches require a large amount of user feedback, which is infeasible in practice. In this paper, we propose to improve IR-based traceability recovery by introducing only a small amount of user feedback into the closeness analysis on call and data dependencies in code. Specifically, our approach iteratively asks users to verify a chosen candidate link based on the quantified functional similarity for each code dependency (called closeness) and the generated IR values. The verified link is then used as the input to re-rank the unverified candidate links. An empirical evaluation based on five real-world systems shows that our approach can outperform four baseline approaches by using only a small amount of user feedback.",Closeness analysis | Code dependencies | Information retrieval | Traceability recovery | User feedback,IEEE International Conference on Program Comprehension,2019-05-01,Conference Paper,"Kuang, Hongyu;Gao, Hui;Hu, Hao;Ma, Xiaoxing;Lu, Jian;Mader, Patrick;Egyed, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071860361,10.1109/ICSE.2019.00021,On Learning Meaningful Code Changes Via Neural Machine Translation,"Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.",Empirical Study | Neural-Machine Translation,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Tufano, Michele;Pantiuchina, Jevgenija;Watson, Cody;Bavota, Gabriele;Poshyvanyk, Denys",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072268618,10.1109/ICSE.2019.00022,Natural Software Revisited,"Recent works have concluded that software code is more repetitive and predictable, i.e. more natural, than English texts. On re-examination, we find that much of the apparent 'naturalness' of source code is due to the presence of language specific syntax, especially separators, such as semi-colons and brackets. For example, separators account for 44% of all tokens in our Java corpus. When we follow the NLP practices of eliminating punctuation (e.g., separators) and stopwords (e.g., keywords), we find that code is still repetitive and predictable, but to a lesser degree than previously thought. We suggest that SyntaxTokens be filtered to reduce noise in code recommenders. Unlike the code written for a particular project, API code usage is similar across projects: a file is opened and closed in the same manner regardless of domain. When we restrict our n-grams to those contained in the Java API, we find that API usages are highly repetitive. Since API calls are common across programs, researchers have made reliable statistical models to recommend sophisticated API call sequences. Sequential n-gram models were developed for natural languages. Code is usually represented by an AST which contains control and data flow, making n-grams models a poor representation of code. Comparing n-grams to statistical graph representations of the same codebase, we find that graphs are more repetitive and contain higherlevel patterns than n-grams. We suggest that future work focus on statistical code graphs models that accurately capture complex coding patterns. Our replication package makes our scripts and data available to future researchers[1].",Basic Science | Entropy | Language Models | StackOverflow | Statistical Code Graphs,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Rahman, Musfiqur;Palani, Dharani;Rigby, Peter C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072292348,10.1109/ICSE.2019.00035,Automatically Generating Precise Oracles from Structured Natural Language Specifications,"Software specifications often use natural language to describe the desired behavior, but such specifications are difficult to verify automatically. We present Swami, an automated technique that extracts test oracles and generates executable tests from structured natural language specifications. Swami focuses on exceptional behavior and boundary conditions that often cause field failures but that developers often fail to manually write tests for. Evaluated on the official JavaScript specification (ECMA-262), 98.4% of the tests Swami generated were precise to the specification. Using Swami to augment developer-written test suites improved coverage and identified 1 previously unknown defect and 15 missing JavaScript features in Rhino, 1 previously unknown defect in Node.js, and 18 semantic ambiguities in the ECMA-262 specification.",natural language specification | oracle | test generation | test oracle,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Motwani, Manish;Brun, Yuriy",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85072274991,10.1109/ICSE.2019.00038,Towards Understanding and Reasoning about Android Interoperations,"Hybrid applications (apps) have become one of the most attractive options for mobile app developers thanks to its support for portability and device-specific features. Android hybrid apps, for example, support portability via JavaScript, device-specific features via Android Java, and seamless interactions between them. However, their interoperation semantics is often under-documented and unintuitive, which makes hybrid apps vulnerable to errors. While recent research has addressed such vulnerabilities, none of them are based on any formal grounds. In this paper, we present the first formal specification of Android interoperability to establish a firm ground for understanding and reasoning about the interoperations. We identify its semantics via extensive testing and thorough inspection of Android source code. We extend an existing multi-language semantics to formally express the key features of hybrid mechanisms, dynamic and indistinguishable interoperability. Based on the extensions, we incrementally define a formal interoperation semantics and disclose its numerous unintuitive and inconsistent behaviors. Moreover, on top of the formal semantics, we devise a lightweight type system that can detect bugs due to the unintuitive inter-language communication. We show that it detects more bugs more efficiently than HybriDroid, the state-of-the-art analyzer of Android hybrid apps, in real-world Android hybrid apps.",Android hybrid applications | interoperability | multi-language systems | operational semantics | type system,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Bae, Sora;Lee, Sungho;Ryu, Sukyoung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072273136,10.1109/ICSE.2019.00044,Active Inductive Logic Programming for Code Search,"Modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code. The key insight of our interactive code search technique ALICE is that user feedback can be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, ALICE automatically extracts a logic query from code features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). ALICE then infers a new logic query that separates positive examples from negative examples via active inductive logic programming. Our comprehensive simulation experiment shows that ALICE removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to user labeling mistakes. Our choice of leveraging both positive and negative examples and using nested program structure as an inductive bias is effective in refining search queries. Compared with an existing interactive code search technique, ALICE does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with much fewer search iterations. A case study with real developers shows that ALICE is easy to use and helps express complex code patterns.",Active Learning | Code Search | Inductive Logic Programming,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Sivaraman, Aishwarya;Zhang, Tianyi;Van Den Broeck, Guy;Kim, Miryung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072277911,10.1109/ICSE.2019.00045,NL2Type: Inferring JavaScript Function Types from Natural Language Information,"JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1% and a recall of 78.9% when considering only the top-most suggestion, and with a precision of 95.5% and a recall of 89.6% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.",comments | deep learning | identifiers | JavaScript | type inference,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Malik, Rabee Sohail;Patra, Jibesh;Pradel, Michael",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85072289678,10.1109/ICSE.2019.00050,How C++ Developers Use Immutability Declarations: An Empirical Study,"Best practices for developers, as encoded in recent programming language designs, recommend the use of immutability whenever practical. However, there is a lack of empirical evidence about the uptake of this advice. Our goal is to understand the usage of immutability by C++ developers in practice. This work investigates how C++ developers use immutability by analyzing their use of the C++ immutability qualifier, const, and by analyzing the code itself. We answer the following broad questions about const usage: 1) do developers actually write non-trivial (more than 3 methods) immutable classes and immutable methods? 2) do developers label their immutable classes and methods? We analyzed 7 medium-to-large open source projects and collected two sources of empirical data: 1) const annotations by developers, indicating an intent to write immutable code; and 2) the results of a simple static analysis which identified easily const-able methods - -those that clearly did not mutate state. We estimate that 5% of non-trivial classes (median) are immutable. We found the vast majority of classes do carry immutability labels on methods: surprisingly, developers const-annotate 46% of methods, and we estimate that at least 51% of methods could be const-annotated. Furthermore, developers missed immutability labels on at least 6% of unannotated methods. We provide an in-depth discussion on how developers use const and the results of our analyses.",empirical studies | immutability | language design | static analysis,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Eyolfson, Jonathan;Lam, Patrick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069452233,10.1109/ICSE.2019.00052,Developer Reading Behavior while Summarizing Java Methods: Size and Context Matters,"An eye-tracking study of 18 developers reading and summarizing Java methods is presented. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. Previous studies on this topic use only short methods presented in isolation usually as images. In contrast, this work presents the study in the Eclipse IDE allowing access to all the source code in the system. The developer can navigate via scrolling and switching files while writing the summary. New eye-tracking infrastructure allows for this improvement in the study environment. Data collected includes eye gazes on source code, written summaries, and time to complete each summary. Unlike prior work that concluded developers focus on the signature the most, these results indicate that they tend to focus on the method body more than the signature. Moreover, both experts and novices tend to revisit control flow terms rather than reading them for a long period. They also spend a significant amount of gaze time and have higher gaze visits when they read call terms. Experts tend to revisit the body of the method significantly more frequently than its signature as the size of the method increases. Moreover, experts tend to write their summaries from source code lines that they read the most.",empirical study | eye tracking | program comprehension | source code summarization,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Abid, Nahla J.;Sharif, Bonita;Dragan, Natalia;Alrasheed, Hend;Maletic, Jonathan I.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85072275562,10.1109/ICSE.2019.00058,Analysis and Detection of Information Types of Open Source Software Issue Discussions,"Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.",collaborative software engineering | issue discussion analysis | issue tracking system,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Arya, Deeksha;Wang, Wenting;Guo, Jin L.C.;Cheng, Jinghui",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85070646641,10.1109/ICSE.2019.00064,On Reliability of Patch Correctness Assessment,"Current state-of-the-art automatic software repair (ASR) techniques rely heavily on incomplete specifications, or test suites, to generate repairs. This, however, may cause ASR tools to generate repairs that are incorrect and hard to generalize. To assess patch correctness, researchers have been following two methods separately: (1) Automated annotation, wherein patches are automatically labeled by an independent test suite (ITS) - a patch passing the ITS is regarded as correct or generalizable, and incorrect otherwise, (2) Author annotation, wherein authors of ASR techniques manually annotate the correctness labels of patches generated by their and competing tools. While automated annotation cannot ascertain that a patch is actually correct, author annotation is prone to subjectivity. This concern has caused an on-going debate on the appropriate ways to assess the effectiveness of numerous ASR techniques proposed recently. In this work, we propose to assess reliability of author and automated annotations on patch correctness assessment. We do this by first constructing a gold set of correctness labels for 189 randomly selected patches generated by 8 state-of-the-art ASR techniques through a user study involving 35 professional developers as independent annotators. By measuring inter-rater agreement as a proxy for annotation quality - as commonly done in the literature - we demonstrate that our constructed gold set is on par with other high-quality gold sets. We then compare labels generated by author and automated annotations with this gold set to assess reliability of the patch assessment methodologies. We subsequently report several findings and highlight implications for future studies.",Automated program repair | empirical study | test case generationn,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Le, Dinh Xuan Bach;Bao, Lingfeng;Lo, David;Xia, Xin;Li, Shanping;Pasareanu, Corina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071935904,10.1109/ICSE.2019.00066,Pattern-Based Mining of Opinions in Q&A Websites,"Informal documentation contained in resources such as Q&A websites (e.g., Stack Overflow) is a precious resource for developers, who can find there examples on how to use certain APIs, as well as opinions about pros and cons of such APIs. Automatically identifying and classifying such opinions can alleviate developers' burden in performing manual searches, and can be used to recommend APIs that are good from some points of view (e.g., performance), or highlight those less ideal from other perspectives (e.g., compatibility). We propose POME (Pattern-based Opinion MinEr), an approach that leverages natural language parsing and pattern-matching to classify Stack Overflow sentences referring to APIs according to seven aspects (e.g., performance, usability), and to determine their polarity (positive vs negative). The patterns have been inferred by manually analyzing 4,346 sentences from Stack Overflow linked to a total of 30 APIs. We evaluated POME by (i) comparing the pattern-matching approach with machine learners leveraging the patterns themselves as well as n-grams extracted from Stack Overflow posts; (ii) assessing the ability of POME to detect the polarity of sentences, as compared to sentiment-analysis tools; (iii) comparing POME with the state-of-the-art Stack Overflow opinion mining approach, Opiner, through a study involving 24 human evaluators. Our study shows that POME exhibits a higher precision than a state-of-the-art technique (Opiner), in terms of both opinion aspect identification and polarity assessment.",aspect detection | opinion mining | sentiment analysis | Stack Overflow,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Lin, Bin;Zampetti, Fiorella;Bavota, Gabriele;Di Penta, Massimiliano;Lanza, Michele",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072288728,10.1109/ICSE.2019.00069,Investigating the Impact of Multiple Dependency Structures on Software Defects,"Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types.",Software Maintenance | Software Quality | Software Structure,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Cui, Di;Liu, Ting;Cai, Yuan;Zheng, Qinghua;Feng, Q.;Jin, Wuxia;Guo, Jiaqi;Qu, Yu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069167757,10.1109/ICSE.2019.00071,Statistical Algorithmic Profiling for Randomized Approximate Programs,"Many modern applications require low-latency processing of large data sets, often by using approximate algorithms that trade accuracy of the results for faster execution or reduced memory consumption. Although the algorithms provide probabilistic accuracy and performance guarantees, a software developer who implements these algorithms has little support from existing tools. Standard profilers do not consider accuracy of the computation and do not check whether the outputs of these programs satisfy their accuracy specifications. We present AXPROF, an algorithmic profiling framework for analyzing randomized approximate programs. The developer provides the accuracy specification as a formula in a mathematical notation, using probability or expected value predicates. AXPROF automatically generates statistical reasoning code. It first constructs the empirical models of accuracy, time, and memory consumption. It then selects and runs appropriate statistical tests that can, with high confidence, determine if the implementation satisfies the specification. We used AXPROF to profile 15 approximate applications from three domains - data analytics, numerical linear algebra, and approximate computing. AXPROF was effective in finding bugs and identifying various performance optimizations. In particular, we discovered five previously unknown bugs in the implementations of the algorithms and created fixes, guided by AXPROF.",profiler | randomized algorithms,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Joshi, Keyur;Fernando, Vimuth;Misailovic, Sasa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071934163,10.1109/ICSE.2019.00072,Safe Automated Refactoring for Intelligent Parallelization of Java 8 Streams,"Streaming APIs are becoming more pervasive in mainstream Object-Oriented programming languages. For example, the Stream API introduced in Java 8 allows for functional-like, MapReduce-style operations in processing both finite and infinite data structures. However, using this API efficiently involves subtle considerations like determining when it is best for stream operations to run in parallel, when running operations in parallel can be less efficient, and when it is safe to run in parallel due to possible lambda expression side-effects. In this paper, we present an automated refactoring approach that assists developers in writing efficient stream code in a semantics-preserving fashion. The approach, based on a novel data ordering and typestate analysis, consists of preconditions for automatically determining when it is safe and possibly advantageous to convert sequential streams to parallel and unorder or de-parallelize already parallel streams. The approach was implemented as a plug-in to the Eclipse IDE, uses the WALA and SAFE analysis frameworks, and was evaluated on 11 Java projects consisting of ?642K lines of code. We found that 57 of 157 candidate streams (36.31%) were refactorable, and an average speedup of 3.49 on performance tests was observed. The results indicate that the approach is useful in optimizing stream code to their full potential.",automatic parallelization | Java 8 | refactoring | static analysis | streams | typestate analysis,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Khatchadourian, Raffi;Tang, Yiming;Bagherzadeh, Mehdi;Ahmed, Syed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85066898148,10.1109/ICSE.2019.00083,RESTler: Stateful REST API Fuzzing,"This paper introduces RESTler, the first stateful REST API fuzzer. RESTler analyzes the API specification of a cloud service and generates sequences of requests that automatically test the service through its API. RESTler generates test sequences by (1) inferring producer-consumer dependencies among request types declared in the specification (e.g., inferring that 'a request B should be executed after request A' because B takes as an input a resource-id x produced by A) and by (2) analyzing dynamic feedback from responses observed during prior test executions in order to generate new tests (e.g., learning that 'a request C after a request sequence A;B is refused by the service' and therefore avoiding this combination in the future). We present experimental results showing that these two techniques are necessary to thoroughly exercise a service under test while pruning the large search space of possible request sequences. We used RESTler to test GitLab, an open-source Git service, as well as several Microsoft Azure and Office365 cloud services. RESTler found 28 bugs in GitLab and several bugs in each of the Azure and Office365 cloud services tested so far. These bugs have been confirmed and fixed by the service owners.",bug finding | cloud services | fuzzer | Fuzzing | REST API | testing,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Atlidakis, Vaggelis;Godefroid, Patrice;Polishchuk, Marina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072269055,10.1109/ICSE.2019.00085,Graph Embedding Based Familial Analysis of Android Malware using Unsupervised Learning,"The rapid growth of Android malware has posed severe security threats to smartphone users. On the basis of the familial trait of Android malware observed by previous work, the familial analysis is a promising way to help analysts better focus on the commonalities of malware samples within the same families, thus reducing the analytical workload and accelerating malware analysis. The majority of existing approaches rely on supervised learning and face three main challenges, i.e., low accuracy, low efficiency, and the lack of labeled dataset. To address these challenges, we first construct a fine-grained behavior model by abstracting the program semantics into a set of subgraphs. Then, we propose SRA, a novel feature that depicts the similarity relationships between the Structural Roles of sensitive API call nodes in subgraphs. An SRA is obtained based on graph embedding techniques and represented as a vector, thus we can effectively reduce the high complexity of graph matching. After that, instead of training a classifier with labeled samples, we construct malware link network based on SRAs and apply community detection algorithms on it to group the unlabeled samples into groups. We implement these ideas in a system called GefDroid that performs Graph embedding based familial analysis of AnDroid malware using unsupervised learning. Moreover, we conduct extensive experiments to evaluate GefDroid on three datasets with ground truth. The results show that GefDroid can achieve high agreements (0.707-0.883 in term of NMI) between the clustering results and the ground truth. Furthermore, GefDroid requires only linear run-time overhead and takes around 8.6s to analyze a sample on average, which is considerably faster than the previous work.",Android malware | familial analysis | graph embedding | unsupervised learning,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Fan, Ming;Luo, Xiapu;Liu, Jun;Wang, Meng;Nong, Chunyin;Zheng, Qinghua;Liu, Ting",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072289240,10.1109/ICSE.2019.00087,A Neural Model for Generating Natural Language Summaries of Program Subroutines,"Source code summarization - creating natural language descriptions of source code behavior - is a rapidly-growing research topic with applications to automatic documentation generation, program comprehension, and software maintenance. Traditional techniques relied on heuristics and templates built manually by human experts. Recently, data-driven approaches based on neural machine translation have largely overtaken template-based systems. But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names, the models fail to create good summaries. In this paper, we present a neural model that combines words from code with code structure from an AST. Unlike previous approaches, our model processes each data source as a separate input, which allows the model to learn code structure independent of the text in code. This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided. We evaluate our technique with a dataset we created from 2.1m Java methods. We find improvement over two baseline techniques from SE literature and one from NLP literature.",automatic documentation generation | code comment generation | source code summarization,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Leclair, Alexander;Jiang, Siyuan;McMillan, Collin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85072045046,10.1109/ICSE.2019.00092,Multifaceted Automated Analyses for Variability-Intensive Embedded Systems,"Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.",Embedded system design engineering | model checking | multi objective optimization | non functional property | variability modeling,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Lazreg, Sami;Cordy, Maxime;Collet, Philippe;Heymans, Patrick;Mosser, Sebastien",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85070633588,10.1109/ICSE.2019.00093,Exposing Library API Misuses Via Mutation Analysis,"Misuses of library APIs are pervasive and often lead to software crashes and vulnerability issues. Various static analysis tools have been proposed to detect library API misuses. They often involve mining frequent patterns from a large number of correct API usage examples, which can be hard to obtain in practice. They also suffer from low precision due to an over-simplified assumption that a deviation from frequent usage patterns indicates a misuse. We make two observations on the discovery of API misuse patterns. First, API misuses can be represented as mutants of the corresponding correct usages. Second, whether a mutant will introduce a misuse can be validated via executing it against a test suite and analyzing the execution information. Based on these observations, we propose MutApi, the first approach to discovering API misuse patterns via mutation analysis. To effectively mimic API misuses based on correct usages, we first design eight effective mutation operators inspired by the common characteristics of API misuses. MutApi generates mutants by applying these mutation operators on a set of client projects and collects mutant-killing tests as well as the associated stack traces. Misuse patterns are discovered from the killed mutants that are prioritized according to their likelihood of causing API misuses based on the collected information. We applied MutApi on 16 client projects with respect to 73 popular Java APIs. The results show that MutApi is able to discover substantial API misuse patterns with a high precision of 0.78. It also achieves a recall of 0.49 on the MuBench benchmark, which outperforms the state-of-the-art techniques.",Library API Misuse | Mutation Analysis,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Wen, Ming;Liu, Yepang;Wu, Rongxin;Xie, Xuan;Cheung, Shing Chi;Su, Zhendong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072284611,10.1109/ICSE.2019.00094,PIVOT: Learning API-Device Correlations to Facilitate Android Compatibility Issue Detection,"The heavily fragmented Android ecosystem has induced various compatibility issues in Android apps. The search space for such fragmentation-induced compatibility issues (FIC issues) is huge, comprising three dimensions: device models, Android OS versions, and Android APIs. FIC issues, especially those arising from device models, evolve quickly with the frequent release of new device models to the market. As a result, an automated technique is desired to maintain timely knowledge of such FIC issues, which are mostly undocumented. In this paper, we propose such a technique, PIVOT, that automatically learns API-device correlations of FIC issues from existing Android apps. PIVOT extracts and prioritizes API-device correlations from a given corpus of Android apps. We evaluated PIVOT with popular Android apps on Google Play. Evaluation results show that PIVOT can effectively prioritize valid API-device correlations for app corpora collected at different time. Leveraging the knowledge in the learned API-device correlations, we further conducted a case study and successfully uncovered ten previously-undetected FIC issues in open-source Android apps.",Android fragmentation | compatibility | learning | static analysis,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Wei, Lili;Liu, Yepang;Cheung, Shing Chi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072283090,10.1109/ICSE.2019.00095,SafeCheck: Safety Enhancement of Java Unsafe API,"Java is a safe programming language by providing bytecode verification and enforcing memory protection. For instance, programmers cannot directly access the memory but have to use object references. Yet, the Java runtime provides an Unsafe API as a backdoor for the developers to access the low- level system code. Whereas the Unsafe API is designed to be used by the Java core library, a growing community of third-party libraries use it to achieve high performance. The Unsafe API is powerful, but dangerous, which leads to data corruption, resource leaks and difficult-to-diagnose JVM crash if used improperly. In this work, we study the Unsafe crash patterns and propose a memory checker to enforce memory safety, thus avoiding the JVM crash caused by the misuse of the Unsafe API at the bytecode level. We evaluate our technique on real crash cases from the openJDK bug system and real-world applications from AJDK. Our tool reduces the efforts from several days to a few minutes for the developers to diagnose the Unsafe related crashes. We also evaluate the runtime overhead of our tool on projects using intensive Unsafe operations, and the result shows that our tool causes a negligible perturbation to the execution of the applications.",Bytecode | Dynamic Analysis | Java Unsafe API | Memoey Safety,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Huang, Shiyou;Guo, Jianmei;Li, Sanhong;Li, Xiang;Qi, Yumin;Chow, Kingsum;Huang, Jeff",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072278106,10.1109/ICSE.2019.00099,Socio-Technical Work-Rate Increase Associates with Changes in Work Patterns in Online Projects,"Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance. Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked.",collaboration | comments | commits | discussions | empirical analysis | focus switching | GitHub | issues | multitasking | online projects | open source software | pull requests | recommendations | regression modeling | repositories | sentiment | social activities | social coding | socio technical | software developers | software engineering | survey | team members | teams | technical activities | work patterns | work rate increase | work related stress,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Sarker, Farhana;Vasilescu, Bogdan;Blincoe, Kelly;Filkov, Vladimir",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072301050,10.1109/ICSE.2019.00101,When Code Completion Fails: A Case Study on Real-World Completions,"Code completion is commonly used by software developers and is integrated into all major IDE's. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientific understanding of developer needs and of the efficacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artificial completions to inform future research and tools in this area. We find that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data. Worse, on the few completions that consumed most of the developers' time, prediction accuracy was less than 20% - an effect that is invisible in synthetic benchmarks. Our findings have ramifications for future benchmarks, tool design and real-world efficacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artificial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249].",Benchmark | Code completion | Language models,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Hellendoorn, Vincent J.;Proksch, Sebastian;Gall, Harald C.;Bacchelli, Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064745329,10.1109/ICSE.2019.00109,FOCUS: A Recommender System for Mining API Function Calls and Usage Patterns,"Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate, accuracy, and execution time. Results indicate the suitability of context-aware collaborative-filtering recommender systems to provide API usage patterns.",api mining | api recommendation | api usage pattern | recommender system,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Nguyen, Phuong T.;Di Rocco, Juri;Di Ruscio, Davide;Ochoa, Lina;Degueule, Thomas;Di Penta, Massimiliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072282393,10.1109/ICSE.2019.00116,Global Optimization of Numerical Programs Via Prioritized Stochastic Algebraic Transformations,"Numerical code is often applied in the safety-critical, but resource-limited areas. Hence, it is crucial for it to be correct and efficient, both of which are difficult to ensure. On one hand, accumulated rounding errors in numerical programs can cause system failures. On the other hand, arbitrary/infinite-precision arithmetic, although accurate, is infeasible in practice and especially in resource-limited scenarios because it performs thousands of times slower than floating-point arithmetic. Thus, it has been a significant challenge to obtain high-precision, easy-to-maintain, and efficient numerical code. This paper introduces a novel global optimization framework to tackle this challenge. Using our framework, a developer simply writes the infinite-precision numerical program directly following the problem's mathematical requirement specification. The resulting code is correct and easy-to-maintain, but inefficient. Our framework then optimizes the program in a global fashion (i.e., considering the whole program, rather than individual expressions or statements as in prior work), the key technical difficulty this work solves. To this end, it analyzes the program's numerical value flows across different statements through a symbolic trace extraction algorithm, and generates optimized traces via stochastic algebraic transformations guided by effective rule selection. We first evaluate our technique on numerical benchmarks from the literature; results show that our global optimization achieves significantly higher worst-case accuracy than the state-of-the-art numerical optimization tool. Second, we show that our framework is also effective on benchmarks having complicated program structures, which are challenging for numerical optimization. Finally, we apply our framework on real-world code to successfully detect numerical bugs that have been confirmed by developers.",numerical analysis | program optimization | program transformation,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Wang, Xie;Wang, Huaijin;Su, Zhendong;Tang, Enyi;Chen, Xin;Shen, Weijun;Chen, Zhenyu;Wang, Linzhang;Zhang, Xianpei;Li, Xuandong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072298992,10.1109/ICSE.2019.00120,"Gigahorse: Thorough, Declarative Decompilation of Smart Contracts","The rise of smart contractsThe rise of smart contracts-autonomous applications running on blockchains-has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and controlflow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts-e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a 'batteries included' approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.autonomous applications running on blockchains - -has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a high-level 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - -e.g., Gigahorse can decompile over 99.98\% of deployed contracts, compared to 88\% for the recently-published Vandal decompiler and under 50\% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a ''batteries included'' approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.",Blockchain | Decompilation | Ethereum | Program Analysis | Security,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Grech, Neville;Brent, Lexi;Scholz, Bernhard;Smaragdakis, Yannis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072275739,10.1109/ICSE.2019.00122,Software Documentation Issues Unveiled,"(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.",Documentation | Empirical Study,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Aghajani, Emad;Nagy, Csaba;Vega-Marquez, Olga Lucero;Linares-Vasquez, Mario;Moreno, Laura;Bavota, Gabriele;Lanza, Michele",Include,
10.1016/j.jss.2022.111515,2-s2.0-85071919362,10.1109/ICSE.2019.00123,"9.6 Million Links in Source Code Comments: Purpose, Evolution, and Decay","Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10% of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.",code comment | knowledge sharing | link decay,Proceedings - International Conference on Software Engineering,2019-05-01,Conference Paper,"Hata, Hideaki;Treude, Christoph;Kula, Raula Gaikovina;Ishio, Takashi",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85077213555,10.1109/ICSME.2019.00011,Impact of Switching Bug Trackers: A Case Study on a Medium-Sized Open Source Project,"For most software projects, the bug tracker is an essential tool. In open source development, this tool plays an even more central role as it is generally open to all users, who are encouraged to test the software and report bugs. Previous studies have highlighted the act of reporting a bug as a first step leading a user to become an active contributor. The impact of the bug reporting environment on the bug tracking activity is difficult to assess because of the lack of comparison points. In this paper, we take advantage of the switch, from Bugzilla to GitHub, of the bug tracker of Coq, a medium-sized open source project, to evaluate and interpret the impact that such a change can have. We first report on the switch itself, including the migration of preexisting issues. Then we analyze data from before and after the switch using a regression discontinuity design, an econometric methodology imported from quantitative policy analysis. We complete this quantitative analysis with qualitative data from interviews with developers. We show that the switch induces an increase in bug reporting, particularly from principal developers themselves, and more generally an increased engagement with the bug tracking platform, with more comments by developers and also more external commentators.",bug report | bug tracker | Bugzilla | data mining | GitHub | interviews | issue | migration | open source | RDD | regression discontinuity design | switch,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Zimmermann, Theo;Casanueva Artis, Annali",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077219554,10.1109/ICSME.2019.00014,Can Everyone use my app? An Empirical Study on Accessibility in Android Apps,"Universal design principles aim to improve accessibility by ensuring product designs consider all users, including those with certain disabilities (e.g., visual impairments). In the case of mobile apps, accessibility is mostly provided by existing features in mobile devices, like TalkBack on Android that reads information to users. However, it is not clear to what extent developers actually implement universal design principles or utilize these technologies to support accessibility of their applications. By performing a mining-based pilot study, we observed developers seldom use Accessibility APIs and there is a limited usage of assistive descriptions. Then, we focused on understanding the perspective of developers through an investigation of posts from StackOverflow. We identified the aspects of accessibility that developers implemented as well as experienced difficulty (or lack of understanding). We performed a formal open-coding of 366 discussions threads with multi-author agreement to create a taxonomy regarding the aspects discussed by developers with respect to accessibility in Android. From the qualitative analysis, we distilled lessons to guide further research and actions in aiding developers with supporting users that require assistive features.",Empirical Studies | Mobile Accessibility | Universal Design,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Vendome, Christopher;Solano, Diana;Linan, Santiago;Linares-Vasquez, Mario",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077183714,10.1109/ICSME.2019.00016,An Empirical Study of UI Implementations in Android Applications,"Mobile app developers are able to design sophisticated user interfaces (UIs) that can improve a user's experience and contribute to an app's success. Developers invest in automated UI testing techniques, such as crawlers, to ensure that their app's UIs have a high level of quality. However, UI implementation mechanisms have changed significantly due to the availability of new APIs and mechanisms, such as fragments. In this paper, we study a large set of real-world apps to identify whether the mechanisms developers use to implement app UIs cause problems for those automated techniques. In addition, we examined the changes in these practices over time. Our results indicate that dynamic analyses face challenges in terms of completeness and changing development practices motivate the use of additional analyses, such as static analyses. We also discuss the implications of our results for current testing techniques and the design of new analyses.",dynamic analysis | Empirical study | mobile applications | static analysis | user interface,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Wan, Mian;Abolhassani, Negarsadat;Alotaibi, Ali;Halfond, William G.J.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077183129,10.1109/ICSME.2019.00022,An Approach to Recommendation of Verbosity Log Levels Based on Logging Intention,"Verbosity levels of logs are designed to discriminate highly diverse runtime events, which facilitates system failure identification through simple keyword search (e.g., fatal, error). Verbosity levels should be properly assigned to logging statements, as inappropriate verbosity levels would confuse users and cause a lot of redundant maintenance effort. However, to achieve such a goal is not an easy task due to the lack of practical specifications and guidelines towards verbosity log level usages. The existing research has built a classification model on log related quantitative metrics such as log density to improve logging level practice. Though such quantitative metrics can reveal logging characteristics, their contributions on logging level decision are limited, since valuable logging intention information buried in logging code context can not be captured. In this paper, we propose an automatic approach to help developers determine the appropriate verbosity log levels. More specially, our approach discriminates different verbosity log level usages based on code context features that contain underlying logging intention. To validate our approach, we implement a prototype tool, VerbosityLevelDirector, and perform a case study to measure its effectiveness on four well-known open source software projects. Evaluation results show that VerbosityLevelDirector achieves high performance on verbosity level discrimination and outperforms the baseline approaches on all those projects. Furthermore, through applying noise handling technique, our approach can detect previously unknown inappropriate verbosity level configurations in the code repository. We have reported 21 representative logging level errors with modification advice to issue tracking platforms of the examined software projects and received positive feedback from their developers. The above results confirm that our work can help developers make a better logging level decision in real-world engineering.",logging context feature | logging intention | Logging level decision | noise handling technique | static code analysis,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Anu, Han;Chen, Jie;Shi, Wenchang;Hou, Jianwei;Liang, Bin;Qin, Bo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077182774,10.1109/ICSME.2019.00029,Self-Admitted Technical Debt Removal and Refactoring Actions: Co-Occurrence or More?,"Technical Debt (TD) concerns the lack of an adequate solution in a software project, from its design to the source code. Its admittance through comments or commit messages is referred to as Self-Admitted Technical Debt (SATD). Previous research has studied SATD from different perspectives, including its distribution, impact on software quality, and removal. In this paper, we investigate the relationship between refactorings and SATD removal. By leveraging a dataset of SATD and their removals in four open-source projects and by using an automated refactoring detection tool, we study the co-occurrence of refactorings and SATD removals. Results of the study indicate that refactorings are more likely to co-occur with SATD removals than with other commits, however, in most cases, they belong to different quality improvement activities performed at the same time.",Empirical Study | Refactoring | Self-Admitted Technical Debt,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Iammarino, Martina;Zampetti, Fiorella;Aversano, Lerina;Di Penta, Massimiliano",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85077200772,10.1109/ICSME.2019.00039,Know-How in Programming Tasks: From Textual Tutorials to Task-Oriented Knowledge Graph,"Accomplishing a program task usually involves performing multiple activities in a logical order. Task-solving activities may have different relationships, such as subactivityof, precede-follow, and different attributes, such as location, condition, API, code. We refer to task-solving activities and their relationships and attributes as know-how knowledge. Programming task know-how knowledge is commonly documented in semi-structured textual tutorials. A formative study of the 20 top-viewed Android-tagged how-to questions on Stack Overflow suggests that developers are faced with three information barriers (incoherent modeling of task intent, tutorial information overload and unstructured task activity description) for effectively discovering and understanding task-solving knowledge in textual tutorials. Knowledge graph has been shown to be effective in representing relational knowledge and supporting knowledge search in a structured way. Unfortunately, existing knowledge graphs extract only know-what information (e.g., APIs, API caveats and API dependencies) from software documentation. In this paper, we devise open information extraction (OpenIE) techniques to extract candidates for task activities, activity attributes and activity relationships from programming task tutorials. The resulting knowledge graph, TaskKG, includes a hierarchical taxonomy of activities, three types of activities relationships and five types of activity attributes, and enables activity-centric knowledge search. As a proof-of-concept, we apply our approach to Android Developer Guide. A comprehensive evaluation of TaskKG shows high accuracy of our OpenIE techniques. A user study shows that TaskKG is promising in helping developers finding correct answers to programming how-to questions.",Knowledge Graph | Programming Tutorials | Task Search,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Sun, Jiamou;Xing, Zhenchang;Chu, Rui;Bai, Heilai;Wang, Jinshui;Peng, Xin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077219797,10.1109/ICSME.2019.00042,Estimating Software Task Effort in Crowds,"A key task during software maintenance is the refinement and elaboration of emerging software issues, such as feature implementations and bug resolution. It includes the annotation of software tasks with additional information, such as criticality, assignee and estimated cost of resolution. This paper reports on a first study to investigate the feasibility of using crowd workers supplied with limited information about an issue and project to provide comparably accurate estimates using planning poker. The paper describes our adaptation of planning poker to crowdsourcing and our initial trials. The results demonstrate the feasibility and potential efficiency of using crowds to deliver estimates. We also review the additional benefit that asking crowds for an estimate brings, in terms of further elaboration of the details of an issue. Finally, we outline our plans for a more extensive evaluation of planning poker in crowds.",crowdsourcing | human computation | Planning Poker | task effort estimation,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Alhamed, Mohammed;Storer, Tim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077218865,10.1109/ICSME.2019.00053,What Do Developers Discuss about Biometric APIs?,"With the emergence of biometric technology in various applications, such as access control (e.g. mobile lock/unlock), financial transaction (e.g. Alibaba smile-to-pay) and time attendance, the development of biometric system attracts increasingly interest to the developers. Despite a sound biometric system gains the security assurance and great usability, it is a rather challenging task to develop an effective biometric system. For instance, many public available biometric APIs do not provide sufficient instructions/precise documentations on the usage of biometric APIs. Many developers are struggling in implementing these APIs in various tasks. Moreover, quick update on biometric-based algorithms (e.g. feature extraction and matching) may propagate to APIs, which leads to potential confusion to the system developers. Hence, we conduct an empirical study to the problems that the developers currently encountered while implementing the biometric APIs as well as the issues that need to be addressed when developing biometric systems using these APIs. We manually analyzed a total of 500 biometric API-related posts from various online media such as Stack Overflow and Neurotechnology. We reveal that 1) most of the problems encountered are related to the lack of precise documentation on the biometric APIs; 2) the incompatibility of biometric APIs cross multiple implementation environments.",APIs misuse | Biometrics | Empirical software engineering,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Jin, Zhe;Chee, Kong Yik;Xia, Xin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077189251,10.1109/ICSME.2019.00074,OSAIFU: A Source Code Factorizer on Android Studio,"Programs written in the event-driven style that are typical of mobile and/or Web applications are not easy to read through. For the purpose of reducing the burden put on software developers, we propose a tool for supporting program understanding, named OSAIFU, implemented as a plugin for Android Studio. OSAIFU automatically factorizes source code at hand, i.e., extracts implemented features from source code by analyzing relations among program elements and shows the summary to the user as a list of clickable tag clouds. The user can check how each feature is implemented by clicking a tag cloud and looking at the corresponding part in the source code that is highlighted to make it easy to spot. The results of preliminary case studies show the effectiveness of OSAIFU. OSAIFU is available at http://imozuru.github.io/OSAIFU/.",Android Studio | event-driven programming | Program understanding | source code factorizing | tag cloud,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Hata, Masahiro;Nishimoto, Masashi;Nishiyama, Keiji;Kawabata, Hideyuki;Hironaka, Tetsuo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077195980,10.1109/ICSME.2019.00081,"Microservices Migration in Industry: Intentions, Strategies, and Challenges","To remain competitive in a fast changing environment, many companies started to migrate their legacy applications towards a Microservices architecture. Such extensive migration processes require careful planning and consideration of implications and challenges likewise. In this regard, hands-on experiences from industry practice are still rare. To fill this gap in scientific literature, we contribute a qualitative study on intentions, strategies, and challenges in the context of migrations to Microservices. We investigated the migration process of 14 systems across different domains and sizes by conducting 16 in-depth interviews with software professionals from 10 companies. Along with a summary of the most important findings, we present a separate discussion of each case. As primary migration drivers, maintainability and scalability were identified. Due to the high complexity of their legacy systems, most companies preferred a rewrite using current technologies over splitting up existing code bases. This was often caused by the absence of a suitable decomposition approach. As such, finding the right service cut was a major technical challenge, next to building the necessary expertise with new technologies. Organizational challenges were especially related to large, traditional companies that simultaneously established agile processes. Initiating a mindset change and ensuring smooth collaboration between teams were crucial for them. Future research on the evolution of software systems can in particular profit from the individual cases presented.",agile transformation | decomposition | empirical study | industry | interviews | Microservices | migration | refactoring,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Fritzsch, Jonas;Bogner, Justus;Wagner, Stefan;Zimmermann, Alfred",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077181601,10.1109/ICSME.2019.00091,A Qualitative Study on Framework Debugging,"Features of frameworks, such as inversion of control and the structure of framework applications, require developers to adjust their programming and debugging strategies as compared to sequential programs. However, the benefits and challenges of framework debugging are not fully understood, and gaining this knowledge could provide guidance in debugging strategies and framework tool design. To gain insight into the framework application debugging process, we performed two human studies investigating how developers fix applications that use a framework API incorrectly. These studies focused on the Android Fragment class and the ROS framework. We analyzed the results of the studies using a mixed-methods approach, using techniques from qualitative approaches. Our analysis found that participants benefited from the structure of frameworks and the pre-made solutions to common problems in the domain. Participants encountered challenges with understanding frame-work abstractions, and had particular difficulty with inversion of control and object protocol issues. When compared to prior work on debugging, these results show that framework applications have unique debugging challenges.",Debugging | Frameworks | Qualitative study,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Coker, Zack;Widder, David Gray;Le Goues, Claire;Bogart, Christopher;Sunshine, Joshua",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081101745,10.1109/ISSRE.2019.00016,How to Explain a Patch: An Empirical Study of Patch Explanations in Open Source Projects,"Bugs are inevitable in software development and maintenance processes. Recently a lot of research efforts have been devoted to automatic program repair, aiming to reduce the efforts of debugging. However, since it is difficult to ensure that the generated patches meet all quality requirements such as correctness, developers still need to review the patch. In addition, current techniques produce only patches without explanation, making it difficult for the developers to understand the patch. Therefore, we believe a more desirable approach should generate not only the patch but also an explanation of the patch. To generate a patch explanation, it is important to first understand how patches were explained. In this paper, we explored how developers explain their patches by manually analyzing 300 merged bug-fixing pull requests from six projects on GitHub. Our contribution is twofold. First, we build a patch explanation model, which summarizes the elements in a patch explanation, and corresponding expressive forms. Second, we conducted a quantitative analysis to understand the distributions of elements, and the correlation between elements and their expressive forms.",Bug-fixing | Explanation | Patch | Program repair,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Liang, Jingjing;Hou, Yaozong;Zhou, Shurui;Chen, Junjie;Xiong, Yingfei;Huang, Gang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081101570,10.1109/ISSRE.2019.00019,Spotting Problematic Code Lines using Nonintrusive Programmers' Biofeedback,"Recent studies have shown that programmers' cognitive load during typical code development activities can be assessed using wearable and low intrusive devices that capture peripheral physiological responses driven by the autonomic nervous system. In particular, measures such as heart rate variability (HRV) and pupillography can be acquired by nonintrusive devices and provide accurate indication of programmers' cognitive load and attention level in code related tasks, which are known elements of human error that potentially lead to software faults. This paper presents an experimental study designed to evaluate the possibility of using HRV and pupillography together with eye tracking to identify and annotate specific code lines (or even finer grain lexical tokens) of the program under development (or under inspection) with information on the cognitive load of the programmer while dealing with such lines of code. The experimental data is discussed in the paper to assess different alternatives for using code annotations representing programmers' cognitive load while producing or reading code. In particular, we propose the use of biofeedback code highlighting techniques to provide online programmer's warnings for potentially problematic code lines that may need a second look at (to remove possible bugs), and biofeedback-driven software testing to optimize testing effort, focusing the tests on code areas with higher bug probability.",Code annotation | Cognitive overload | Eye tracking | Heart rate variability | Human error | Mental effort | Pupillography | Software faults,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Couceiro, Ricardo;De Carvalho, Paulo;Branco, Miguel Castelo;Madeira, Henrique;Barbosa, Raul;Duraes, Joao;Duarte, Goncalo;Castelhano, Joao;Duarte, Catarina;Teixeira, Cesar;Laranjeiro, Nuno;Medeiros, Julio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85076146596,10.1109/ISSRE.2019.00020,An Empirical Study of Common Challenges in Developing Deep Learning Applications,"Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated - what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning.",Deep learning | Programming issues | Software reliability | Stack Overflow,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Zhang, Tianyi;Gao, Cuiyun;Ma, Lei;Lyu, Michael;Kim, Miryung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081102931,10.1109/ISSRE.2019.00043,FILO: FIx-LOcus Recommendation for Problems Caused by Android Framework Upgrade,"Dealing with the evolution of operating systems is challenging for developers of mobile apps, who have to deal with frequent upgrades that often include backward incompatible changes of the underlying API framework. As a consequence of framework upgrades, apps may show misbehaviours and unexpected crashes once executed within an evolved environment. Identifying the portion of the app that must be modified to correctly execute on a newly released operating system can be challenging. Although incompatibilities are visibile at the level of the interactions between the app and its execution environment, the actual methods to be changed are often located in classes that do not directly interact with any external element. To facilitate debugging activities for problems introduced by backward incompatible upgrades of the operating system, this paper presents FILO, a technique that can recommend the method that must be changed to implement the fix from the analysis of a single failing execution. FILO can also select key symptomatic anomalous events that can help the developer understanding the reason of the failure and facilitate the implementation of the fix. Our evaluation with multiple known compatibility problems introduced by Android upgrades shows that FILO can effectively and efficiently identify the faulty methods in the apps.",Android | API Upgrades | Debugging,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Mobilio, Marco;Riganelli, Oliviero;Micucci, Daniela;Mariani, Leonardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081104151,10.1109/ISSRE.2019.00044,HiRec: API Recommendation using Hierarchical Context,"Context-aware API recommendation techniques aim to generate a ranked list of candidate APIs on an editing position during development. The basic context used in traditional API recommendation mainly focuses on the APIs from third-party libraries, limit or even ignore the usage of project-specific code. The limited usage of project-specific code may result in the lack of context information, and degrade the effectiveness of API recommendation. To address this problem, we introduce a novel type of context, i.e., hierarchical context, which can leverage the hidden information of project-specific code by analyzing the call graph. In hierarchical context, a project-specific API is presented as a sequence of low-leveled APIs from third-party libraries. We propose an approach, i.e., HiRec, which builds on the basis of hierarchical context. HiRec is evaluated on 108 projects and the results show that HiRec can obtain much more accurate results than all the other selected approaches in terms of top-5 and top-10 accuracy due to the strong ability of context representation. And HiRec performs closely to the outstanding tools in terms of top-1 accuracy. The average time of recommending execution is less than 1 seconds in most cases, which is acceptable for interaction in an IDE. Unlike current approaches, the effectiveness of HiRec is not impacted much by editing positions. And we can obtain more accurate results from HiRec with larger sizes of training data and hierarchical context.",API recommendation | Hierarchical context | Inference model,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Xie, Rensong;Kong, Xianglong;Wang, Lulu;Zhou, Ying;Li, Bixin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081105122,10.1109/ISSRE.2019.00048,Benefits and Challenges of Model-Based Software Engineering: Lessons Learned Based on Qualitative and Quantitative Findings,"Even though Model-based Software Engineering (MBSwE) techniques and Autogenerated Code (AGC) have been increasingly used to produce complex software systems, there is only anecdotal knowledge about the state-of-the practice. Furthermore, there is a lack of empirical studies that explore the potential quality improvements due to the use of these techniques. This paper presents in-depth qualitative findings about development and Software Assurance (SWA) practices and detailed quantitative analysis of software bug reports of a NASA mission that used MBSwE and AGC. The mission's flight software is a combination of handwritten code and AGC developed by two different approaches: one based on state chart models (AGC-M) and another on specification dictionaries (AGC-D). The empirical analysis of fault proneness is based on 380 closed bug reports created by software developers. Our main findings include: (1) MBSwE and AGC provide some benefits, but also impose challenges. (2) SWA done only at a model level is not sufficient. AGC code should also be tested and the models and AGC should always be kept in-sync. AGC must not be changed manually. (3) Fixes made to address an individual bug report were spread both across multiple modules and across multiple files. On average, for each bug report 1.4 modules, that is, 3.4 files were fixed. (4) Most bug reports led to changes in more than one type of file. The majority of changes to auto-generated source code files were made in conjunction to changes in either file with state chart models or XML files derived from dictionaries. (5) For newly developed files, AGC-M and handwritten code were of similar quality, while AGC-D files were the least fault prone.",Analysis of faults and fixes | Autogenerated Code | Model based software engineering | Software quality assurance,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Goseva-Popstojanova, Katerina;Kyanko, Thomas;Nkwocha, Noble",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074877240,10.1109/MiSE.2019.00009,Detecting emergent behaviors and implied scenarios in scenario-based specifications: A machine learning approach,"Scenarios are commonly used for software requirements modeling. Scenarios describe how system components, users and the environment interact to complete the system functionality. However, several scenarios are needed to represent a complete system behavior and combining the scenarios may generate implied scenarios (IS) that are associated with some unexpected behavior. The unexpected behavior is commonly known as Emergent Behavior (EB), which is not evident in the requirements and design phase but may degrade the quality of service and/or cause irreparable damage during execution. Detecting and fixing EB/IS in the early phases can save on deployment cost while minimizing the run-time hazards. In this paper, we present a machine learning approach to model and identify the interactions between system components and verify which interactions are safe and which may lead to EB/IS. The experimental result shows that our approach can efficiently detect different types of EB/IS and applicable to large scale systems.",Distributed Software System | Emergent Behavior | Implied Scenarios | Machine Learning | Neural Network,"Proceedings - 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering, MiSE 2019",2019-05-01,Conference Paper,"Jahan, Munima;Abad, Zahra Shakeri Hossein;Far, Behrouz",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074892303,10.1109/MiSE.2019.00012,Evaluating the ability of developers to use metamodels in model-oriented development,"The applicability of models has evolved throughout the history of software engineering, from documentation, development and beyond. In this context, we study how to employ models for a common language shared by humans and computers. After studying a model-oriented development method for models at run-time systems, we have identified that this method would heavily rely on metamodels. Therefore, it is important to evaluate if developers are able to use metamodels in software development. In this paper we present a controlled experiment to evaluate the ability and efforts of professional and novice developers to effectively use metamodels. Participants of the experiment had access to newly created metamodeling definition tools, as well as standard Java code and UML diagrams in order to complete their tasks. Results indicate that the definition language was easy to be learned by experienced Java developers, who were able to comprehend metamodeling development artifacts without struggling with modeling concepts. We conclude developers would be able to adapt to new modeling concepts and tools as required by different systems that handle models at run-time.",Development tools | Experimental study | Metamodeling | Model comprehension | Model-oriented software,"Proceedings - 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering, MiSE 2019",2019-05-01,Conference Paper,"Gottardi, Thiago;Braga, Rosana Teresinha Vaccare",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074878358,10.1109/MiSE.2019.00018,On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: The case for signature extension,"Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.",API | Modularity | Reuse | Separation of concerns | Usage interface,"Proceedings - 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering, MiSE 2019",2019-05-01,Conference Paper,"Schottle, Matthias;Kienzle, Jorg",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072323400,10.1109/MSR.2019.00020,Exploring word embedding techniques to improve sentiment analysis of software engineering texts,"Sentiment analysis (SA) of text-based software artifacts is increasingly used to extract information for various tasks including providing code suggestions, improving development team productivity, giving recommendations of software packages and libraries, and recommending comments on defects in source code, code quality, possibilities for improvement of applications. Studies of state-of-the-art sentiment analysis tools applied to software-related texts have shown varying results based on the techniques and training approaches. In this paper, we investigate the impact of two potential opportunities to improve the training for sentiment analysis of SE artifacts in the context of the use of neural networks customized using the Stack Overflow data developed by Lin et al. We customize the process of sentiment analysis to the software domain, using software domain-specific word embeddings learned from Stack Overflow (SO) posts, and study the impact of software domain-specific word embeddings on the performance of the sentiment analysis tool, as compared to generic word embeddings learned from Google News. We find that the word embeddings learned from the Google News data performs mostly similar and in some cases better than the word embeddings learned from SO posts. We also study the impact of two machine learning techniques, oversampling and undersampling of data, on the training of a sentiment classifier for handling small SE datasets with a skewed distribution. We find that oversampling alone, as well as the combination of oversampling and undersampling together, helps in improving the performance of a sentiment classifier.",Sentiment Analysis | Software Engineering | Word Embeddings,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Biswas, Eeshita;Vijay-Shanker, K.;Pollock, Lori",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072344816,10.1109/MSR.2019.00021,Cleaning StackOverflow for machine translation,"Generating source code API sequences from an English query using Machine Translation (MT) has gained much interest in recent years. For any kind of MT, the model needs to be trained on a parallel corpus. In this paper we clean StackOverflow, one of the most popular online discussion forums for programmers, to generate a parallel English-Code corpus from Android posts. We contrast three data cleaning approaches: standard NLP, title only, and software task extraction. We evaluate the quality of the each corpus for MT. To provide indicators of how useful each corpus will be for machine translation, we provide researchers with measurements of the corpus size, percentage of unique tokens, and per-word maximum likelihood alignment entropy. We have used these corpus cleaning approaches to translate between English and Code [22, 23], to compare existing SMT approaches from word mapping to neural networks [24], and to re-examine the 'natural software' hypothesis [29]. After cleaning and aligning the data, we create a simple maximum likelihood MT model to show that English words in the corpus map to a small number of specific code elements. This model provides a basis for the success of using StackOverflow for search and other tasks in the software engineering literature and paves the way for MT. Our scripts and corpora are publicly available on GitHub [1] as well as at https://search.datacite.org/works/10.5281/zenodo.2558551.",Data cleaning | Machine Translation | StackOverflow,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Rahman, Musfiqur;Rigby, Peter;Palani, Dharani;Nguyen, Tien",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072340463,10.1109/MSR.2019.00022,Predicting good configurations for github and stack overflow topic models,"Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as a text-mining tool for the discovery of hidden semantic structures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by grouping texts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflow text corpora, (ii) an a-posteriori characterisation of text corpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories.",Algorithm portfolio | Corpus features | Topic modelling,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Treude, Christoph;Wagner, Markus",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072320947,10.1109/MSR.2019.00023,A dataset of parametric cryptographic misuses,"Cryptographic APIs (Crypto APIs) provide the foundations for the development of secure applications. Unfortunately, most applications do not use Crypto APIs securely and end up being insecure, e.g., by the usage of an outdated algorithm, a constant initialization vector, or an inappropriate hashing algorithm. Two different studies [1], [2] have recently shown that 88% to 95% of those applications using Crypto APIs are insecure due to misuses. To facilitate further research on these kinds of misuses, we created a collection of 201 misuses found in real-world applications along with a classification of those misuses. In the provided dataset, each misuse consists of the corresponding open-source project, the project's build information, a description of the misuse, and the misuse's location. Further, we integrated our dataset into MUBench [3], a benchmark for API misuse detection. Our dataset provides a foundation for research on Crypto API misuses. For example, it can be used to evaluate the precision and recall of detection tools, as a foundation for studies related to Crypto API misuses, or as a training set.",API-misuse | Benchmark | Cryptographic misuse | Dataset,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Wickert, Anna Katharina;Reif, Michael;Eichberg, Michael;Dodhy, Anam;Mezini, Mira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072338237,10.1109/MSR.2019.00030,The software heritage graph dataset: Public software development under one roof,"Software Heritage is the largest existing public archive of software source code and accompanying development history: it currently spans more than five billion unique source code files and one billion unique commits, coming from more than 80 million software projects. This paper introduces the Software Heritage graph dataset: a fully-deduplicated Merkle DAG representation of the Software Heritage archive. The dataset links together file content identifiers, source code directories, Version Control System (VCS) commits tracking evolution over time, up to the full states of VCS repositories as observed by Software Heritage during periodic crawls. The dataset's contents come from major development forges (including GitHub and GitLab), FOSS distributions (e.g., Debian), and language-specific package managers (e.g., PyPI). Crawling information is also included, providing timestamps about when and where all archived source code artifacts have been observed in the wild. The Software Heritage graph dataset is available in multiple formats, including downloadable CSV dumps and Apache Parquet files for local use, as well as a public instance on Amazon Athena interactive query service for ready-to-use powerful analytical processing. Source code file contents are cross-referenced at the graph leaves, and can be retrieved through individual requests using the Software Heritage archive API.",Dataset | Development history graph | Digital preservation | Free software | Mining software repositories | Open source software | Source code,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Pietri, Antoine;Spinellis, Diomidis;Zacchiroli, Stefano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072322327,10.1109/MSR.2019.00038,"SOTorrent: Studying the origin, evolution, and usage of stack overflow code snippets","Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of copyable code snippets. Like other software artifacts, code on SO evolves over time, for example when bugs are fixed or APIs are updated to the most recent version. To be able to analyze how code and the surrounding text on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text and code blocks. It connects code snippets from SO posts to other platforms by aggregating URLs from surrounding text blocks and comments, and by collecting references from GitHub files to SO posts. Our vision is that researchers will use SOTorrent to investigate and understand the evolution and maintenance of code on SO and its relation to other platforms such as GitHub.",Code snippets | Github | Open dataset | Software evolution | Stack overflow,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Baltes, Sebastian;Treude, Christoph;Diehl, Stephan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072325089,10.1109/MSR.2019.00044,Analyzing comment-induced updates on stack overflow,"Stack Overflow is home to a large number of technical questions and answers. These answers also include comments from the community and other users about the answer's validity. Such comments may point to flaws in the posted answer or may indicate deprecated code that is no longer valid due to API changes. In this paper, we explore how comments affect answer updates on Stack Overflow, using the SOTorrent dataset. Our results show that a large number of answers on Stack Overflow are not updated, even when they receive comments that warrant an update. Our results can be used to build recommender systems that automatically identify answers that require updating, or even automatically update answers as needed.",Answer updates | APIs | Comments | Stack overflow,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Soni, Abhishek;Nadi, Sarah",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072306012,10.1109/MSR.2019.00055,Data-driven solutions to detect API compatibility issues in android: An empirical study,"Android apps are inextricably linked to the official Android APIs. Such a strong form of dependency implies that changes introduced in new versions of the Android APIs can severely impact the apps' code, for example because of deprecated or removed APIs. In reaction to those changes, mobile app developers are expected to adapt their code and avoid compatibility issues. To support developers, approaches have been proposed to automatically identify API compatibility issues in Android apps. The state-of-the-art approach, named CiD, is a data-driven solution learning how to detect those issues by analyzing the changes in the history of Android APIs ('API side' learning). While it can successfully identify compatibility issues, it cannot recommend coding solutions. We devised an alternative data-driven approach, named ACRYL. ACRYL learns from changes implemented in other apps in response to API changes ('client side' learning). This allows not only to detect compatibility issues, but also to suggest a fix. When empirically comparing the two tools, we found that there is no clear winner, since the two approaches are highly complementary, in that they identify almost disjointed sets of API compatibility issues. Our results point to the future possibility of combining the two approaches, trying to learn detection/fixing rules on both the API and the client side.",Android | API Compatibility Issues | Empirical Study,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Scalabrino, Simone;Bavota, Gabriele;Linares-Vasquez, Mario;Lanza, Michele;Oliveto, Rocco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072349849,10.1109/MSR.2019.00057,Automatically generating documentation for lambda expressions in Java,"When lambda expressions were introduced to the Java programming language as part of the release of Java 8 in 2014, they were the language's first step into functional programming. Since lambda expressions are still relatively new, not all developers use or understand them. In this paper, we first present the results of an empirical study to determine how frequently developers of GitHub repositories make use of lambda expressions and how they are documented. We find that 11% of Java GitHub repositories use lambda expressions, and that only 6% of the lambda expressions are accompanied by source code comments. We then present a tool called LambdaDoc which can automatically detect lambda expressions in a Java repository and generate natural language documentation for them. Our evaluation of LambdaDoc with 23 professional developers shows that they perceive the generated documentation to be complete, concise, and expressive, while the majority of the documentation produced by our participants without tool support was inadequate. Our contribution builds an important step towards automatically generating documentation for functional programming constructs in an object-oriented language.",Documentation generation | Lambda expressions,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Alqaimi, Anwar;Thongtanunam, Patanamon;Treude, Christoph",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85072345840,10.1109/MSR.2019.00058,Extracting API tips from developer question and answer websites,"The success of question and answer (Q&A) websites attracts massive user-generated content for using and learning APIs, which easily leads to information overload: many questions for APIs have a large number of answers containing useful and irrelevant information, and cannot all be consumed by developers. In this work, we develop DeepTip, a novel deep learning-based approach using different Convolutional Neural Network architectures, to extract short practical and useful tips from developer answers. Our extensive empirical experiments prove that DeepTip can extract useful tips from a large corpus of answers to questions with high precision (i.e., avg. 0.854) and coverage (i.e., 0.94), and it outperforms two state-of-the-art baselines by up to 56.7% and 162%, respectively, in terms of Precision. Furthermore, qualitatively, a user study is conducted with real Stack Overflow users and its results confirm that tip extraction is useful and our approach generates high-quality tips.",API | CNN | Deep learning | Sentence classification | Tip extraction,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Wang, Shaohua;Phan, Nhathai;Wang, Yan;Zhao, Yong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072338324,10.1109/MSR.2019.00062,Splitting APIs: An exploratory study of software unbundling,"Software unbundling consists of dividing an existing software artifact into smaller ones. Unbundling can be useful for removing clutter from the original application or separating different features that may not share the same purpose, or simply for isolating an emergent functionality that merits to be an application on its own. This phenomenon is frequent with mobile apps and it is also propagating to APIs. This paper proposes a first empirical study on unbundling to understand its effects on popular APIs. We explore the possibilities of splitting libraries into 2 or more bundles based on the use that their client projects make of them. We mine over than 71,000 client projects of 10 open source APIs and automatically generate 2,090 sub-APIs to then study their properties. We find that it is possible to have sets of different ways of using a given API and to unbundle it accordingly; the bundles can vary their representativeness and uniqueness, which is analyzed thoroughly in this study.",Api usage | Exploratory study | Mining software repositories | Modularity | Software unbundling,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Severo De Matos, Anderson;Bosco Ferreira Filho, Joao;Souza Rocha, Lincoln",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072313648,10.1109/MSR.2019.00069,Characterizing the roles of contributors in open-source scientific software projects,"The development of scientific software is, more than ever, critical to the practice of science, and this is accompanied by a trend towards more open and collaborative efforts. Unfortunately, there has been little investigation into who is driving the evolution of such scientific software or how the collaboration happens. In this paper, we address this problem. We present an extensive analysis of seven open-source scientific software projects in order to develop an empirically-informed model of the development process. This analysis was complemented by a survey of 72 scientific software developers. In the majority of the projects, we found senior research staff (e.g. professors) to be responsible for half or more of commits (an average commit share of 72%) and heavily involved in architectural concerns (seniors were more likely to interact with files related to the build system, project meta-data, and developer documentation). Juniors (e.g. graduate students) also contribute substantially - in one studied project, juniors made almost 100% of its commits. Still, graduate students had the longest contribution periods among juniors (with 1.72 years of commit activity compared to 0.98 years for postdocs and 4 months for undergraduates). Moreover, we also found that third-party contributors are scarce, contributing for just one day for the project. The results from this study aim to help scientists to better understand their own projects, communities, and the contributors' behavior, while paving the road for future software engineering research.",Scientific software | Software repository mining,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Milewicz, Reed;Pinto, Gustavo;Rodeghero, Paige",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072331325,10.1109/MSR.2019.00073,Style-analyzer: Fixing code style inconsistencies with interpretable unsupervised algorithms,"Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes. This paper introduces style-analyzer, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. style-analyzer is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. style-analyzer can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of style-analyzer by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. style-analyzer includes a web application to visualize how the rules are triggered. We release style-analyzer as a reusable and extendable open source software package on GitHub for the benefit of the community.",Assisted code review | Code style | Decision tree forest | Interpretable machine learning,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Markovtsev, Vadim;Long, Waren;Mougard, Hugo;Slavnov, Konstantin;Bulychev, Egor",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072338473,10.1109/MSR.2019.00075,Exploratory study of slack Q&A chats as a mining source for software engineering tools,"Modern software development communities are increasingly social. Popular chat platforms such as Slack host public chat communities that focus on specific development topics such as Python or Ruby-on-Rails. Conversations in these public chats often follow a Q&A format, with someone seeking information and others providing answers in chat form. In this paper, we describe an exploratory study into the potential use-fulness and challenges of mining developer Q&A conversations for supporting software maintenance and evolution tools. We designed the study to investigate the availability of information that has been successfully mined from other developer communications, particularly Stack Overflow. We also analyze characteristics of chat conversations that might inhibit accurate automated analysis. Our results indicate the prevalence of useful information, including API mentions and code snippets with descriptions, and several hurdles that need to be overcome to automate mining that information.",Chat communities | Developer communications | Empirical study | Mining,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Chatterjee, Preetha;Damevski, Kostadin;Pollock, Lori;Augustine, Vinay;Kraft, Nicholas A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072308796,10.1109/MSR.2019.00079,SeSaMe: A data set of semantically similar Java methods,"In the past, techniques for detecting similarly behaving code fragments were often only evaluated with small, artificial oracles or with code originating from programming competitions. Such code fragments differ largely from production codes. To enable more realistic evaluations, this paper presents SeSaMe, a data set of method pairs that are classified according to their semantic similarity. We applied text similarity measures on JavaDoc comments mined from 11 open source repositories and manually classified a selection of 857 pairs.",Code clones | Data set | Semantic similarity,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Kamp, Marius;Kreutzer, Patrick;Philippsen, Michael",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85072322975,10.1109/MSR.2019.00080,Predicting co-changes between functionality specifications and source code in behavior driven development,"Behavior Driven Development (BDD) is an agile approach that uses. feature files to describe the functionalities of a software system using natural language constructs (English-like phrases). Because of the English-like structure of. feature files, BDD specifications become an evolving documentation that helps all (even non-technical) stakeholders to understand and contribute to a software project. After specifying a. feature files, developers can use a BDD tool (e.g., Cucumber) to automatically generate test cases and implement the code of the specified functionality. However, maintaining traceability between. feature files and source code requires human efforts. Therefore,. feature files can be out-of-date, reducing the advantages of using BDD. Furthermore, existing research do not attempt to improve the traceability between. feature files and source code files. In this paper, we study the co-changes between. feature files and source code files to improve the traceability between. feature files and source code files. Due to the English-like syntax of. feature files, we use natural language processing to identify co-changes, with an accuracy of 79%. We study the characteristics of BDD co-changes and build random forest models to predict when a. feature files should be modified before committing a code change. The random forest model obtains an AUC of 0.77. The model can assist developers in identifying when a. feature files should be modified in code commits. Once the traceability is up-to-date, BDD developers can write test code more efficiently and keep the software documentation up-to-date.",Behavior Driven Development | Co-Changes | Empirical Software Engineering | Traceability,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Yang, Aidan Z.H.;Alencar Da Costa, Daniel;Zou, Ying",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072321907,10.1109/MSR.2019.00089,ConPan: A tool to analyze packages in software containers,"Deploying software packages and services into containers is a popular software engineering practice that increases portability and reusability. Docker, the most popular containerization technology, helps DevOps practitioners in their daily activities. Despite being successfully and increasingly employed, containers may include buggy and vulnerable packages that put at risk the environments in which the containers have been deployed. Existing quality and security monitoring tools provide only limited support to analyze Docker containers, thus forcing practitioners to perform additional manual work or develop adhoc scripts when the analysis goes beyond security purposes. This limitation also affects researchers desiring to empirically study the evolution dynamics of Docker containers and their contained packages. To overcome this limitation, we present ConPan, an automated tool to inspect the characteristics of packages in Docker containers, such as their outdatedness and other possible flaws (e.g., bugs and security vulnerabilities). ConPan comes with a CLI and API, and the analysis results can be presented to the user in a variety of formats.",Bugs | Containers | Docker | Outdated-software | Vulnerabilities,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Zerouali, Ahmed;Cosentino, Valerio;Robles, Gregorio;Gonzalez-Barahona, Jesus M.;Mens, Tom",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067686177,10.1145/3314221.3314633,Scenic: A language for scenario specification and scene generation,"We propose a new probabilistic programming language for the design and analysis of perception systems, especially those based on machine learning. Specifically, we consider the problems of training a perception system to handle rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs and sampling these to generate specialized training and test sets. More generally, such languages can be used for cyber-physical systems and robotics to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment is a scene, a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.",Automatic test generation | Deep learning | Fuzz testing | Probabilistic programming | Scenario description language | Synthetic data,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Fremont, Daniel J.;Yue, Xiangyu;Dreossi, Tommaso;Sangiovanni-Vincentelli, Alberto L.;Ghosh, Shromona;Seshia, Sanjit A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067698437,10.1145/3314221.3314602,Resource-guided program synthesis,"This article presents resource-guided synthesis, a technique for synthesizing recursive programs that satisfy both a functional specification and a symbolic resource bound. The technique is type-directed and rests upon a novel type system that combines polymorphic refinement types with potential annotations of automatic amortized resource analysis. The type system enables efficient constraint-based type checking and can express precise refinement-based resource bounds. The proof of type soundness shows that synthesized programs are correct by construction. By tightly integrating program exploration and type checking, the synthesizer can leverage the user-provided resource bound to guide the search, eagerly rejecting incomplete programs that consume too many resources. An implementation in the resource-guided synthesizer ReSyn is used to evaluate the technique on a range of recursive data structure manipulations. The experiments show that ReSyn synthesizes programs that are asymptotically more efficient than those generated by a resource-agnostic synthesizer. Moreover, synthesis with ReSyn is faster than a naive combination of synthesis and resource analysis. ReSyn is also able to generate implementations that have a constant resource consumption for fixed input sizes, which can be used to mitigate side-channel attacks.",Automated Amortized Resource Analysis | Program Synthesis | Refinement Types,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Knoth, Tristan;Polikarpova, Nadia;Wang, Di;Hoffmann, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067684348,10.1145/3314221.3322485,Synthesis and machine learning for heterogeneous extraction,"We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data. We use machine learning models (“ML models”) such as conditional random fields to get an initial labeling of potential attribute values. However, such models are typically not interpretable, and the noise produced by such models is hard to manage or debug. We use (noisy) labels produced by such ML models as inputs to program synthesis, and generate interpretable programs that cover the input space. We also employ type specifications (called “field constraints”) to certify well-formedness of extracted values. Using synthesized programs and field constraints, we re-train the ML models with improved confidence on the labels. We then use these improved labels to re-synthesize a better set of programs. We iterate the process of re-synthesizing the programs and re-training the ML models, and find that such an iterative process improves the quality of the extraction process. This iterative approach, called HDEF, is novel, not only the in way it combines the ML models with program synthesis, but also in the way it adapts program synthesis to deal with noise and heterogeneity. More broadly, our approach points to ways by which machine learning and programming language techniques can be combined to get the best of both worlds - handling noise, transferring signals from one context to another using ML, producing interpretable programs using PL, and minimizing user intervention.",Data extraction | Heterogeneous data | Machine Learning | Program synthesis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Iyer, Arun;Jonnalagedda, Manohar;Parthasarathy, Suresh;Radhakrishna, Arjun;Rajamani, Sriram K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067658536,10.1145/3314221.3322484,Verifying message-passing programs with dependent behavioural types,"Concurrent and distributed programming is notoriously hard. Modern languages and toolkits ease this difficulty by offering message-passing abstractions, such as actors (e.g., Erlang, Akka, Orleans) or processes (e.g., Go): they allow for simpler reasoning w.r.t. shared-memory concurrency, but do not ensure that a program implements a given specification. To address this challenge, it would be desirable to specify and verify the intended behaviour of message-passing applications using types, and ensure that, if a program type-checks and compiles, then it will run and communicate as desired. We develop this idea in theory and practice. We formalise a concurrent functional language λπ, with a new blend of behavioural types (from π-calculus theory), and dependent function types (from the Dotty programming language, a.k.a. the future Scala 3). Our theory yields four main payoffs: (1) it verifies safety and liveness properties of programs via typeś level model checking; (2) unlike previous work, it accurately verifies channel-passing (covering a typical pattern of actor programs) and higher-order interaction (i.e., sending/receiving mobile code); (3) it is directly embedded in Dotty, as a toolkit called Effpi, offering a simplified actor-based API; (4) it enables an efficient runtime system for Effpi, for highly concurrent programs with millions of processes/actors.",Actors | Behavioural types | Dependent types | Dotty | Model checking | Processes | Scala | Temporal logic,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Scalas, Alceste;Yoshida, Nobuko;Benussi, Elias",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067646960,10.1145/3314221.3314603,Bidirectional type checking for relational properties,"Relational type systems have been designed for several applications including information flow, differential privacy, and cost analysis. In order to achieve the best results, these systems often use relational refinements and relational effects to maximally exploit the similarity in the structure of the two programs being compared. Relational type systems are appealing for relational properties because they deliver simpler and more precise verification than what could be derived from typing the two programs separately. However, relational type systems do not yet achieve the practical appeal of their non-relational counterpart, in part because of the lack of a general foundation for implementing them. In this paper, we take a step in this direction by developing bidirectional relational type checking for systems with relational refinements and effects. Our approach achieves the benefits of bidirectional type checking, in a relational setting. In particular, it significantly reduces the need for typing annotations through the combination of type checking and type inference. In order to highlight the foundational nature of our approach, we develop bidirectional versions of several relational type systems which incrementally combine many different components needed for expressive relational analysis.",Bidirectional type-checking | Refinement types | Relational type systems | Type-and-effect systems,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Çiçek, Ezgi;Qu, Weihao;Barthe, Gilles;Gaboardi, Marco;Garg, Deepak",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067678231,10.1145/3314221.3314638,An inductive synthesis framework for verifiable reinforcement learning,"Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.",Invariant Inference | Program Synthesis | Program Verification | Reinforcement Learning | Runtime Shielding,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Zhu, He;Magill, Stephen;Xiong, Zikang;Jagannathan, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067663026,10.1145/3314221.3314593,Programming support for autonomizing software,"Most traditional software systems are not built with the artificial intelligence support (AI) in mind. Among them, some may require human interventions to operate, e.g., the manual specification of the parameters in the data processing programs, or otherwise, would behave poorly. We propose a novel framework called Autonomizer to autonomize these systems by installing the AI into the traditional programs. Autonomizer is general so it can be applied to many real-world applications. We provide the primitives and the runtime support, where the primitives abstract common tasks of autonomization and the runtime support realizes them transparently. With the support of Autonomizer, the users can gain the AI support with little engineering efforts. Like many other AI applications, the challenge lies in the feature selection, which we address by proposing multiple automated strategies based on the program analysis. Our experiment results on nine real-world applications show that the autonomization only requires adding a few lines to the source code. Besides, for the data-processing programs, Autonomizer improves the output quality by 161% on average over the default settings. For the interactive programs such as game/driving, Autonomizer achieves higher success rate with lower training time than existing autonomized programs.",AI | Deep Learning | Dynamic Program Analysis | Software Autonomization,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Lee, Wen Chuan;Liu, Peng;Liu, Yingqi;Ma, Shiqing;Zhang, Xiangyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067653390,10.1145/3314221.3314640,Unsupervised learning of API aliasing specifications,"Real world applications make heavy use of powerful libraries and frameworks, posing a significant challenge for static analysis as the library implementation may be very complex or unavailable. Thus, obtaining specifications that summarize the behaviors of the library is important as it enables static analyzers to precisely track the effects of APIs on the client program, without requiring the actual API implementation. In this work, we propose a novel method for discovering aliasing specifications of APIs by learning from a large dataset of programs. Unlike prior work, our method does not require manual annotation, access to the library's source code or ability to run its APIs. Instead, it learns specifications in a fully unsupervised manner, by statically observing usages of APIs in the dataset. The core idea is to learn a probabilistic model of interactions between API methods and aliasing objects, enabling identification of additional likely aliasing relations, and to then infer aliasing specifications of APIs that explain these relations. The learned specifications are then used to augment an API-aware points-to analysis. We implemented our approach in a tool called USpec and used it to automatically learn aliasing specifications from millions of source code files. USpec learned over 2000 specifications of various Java and Python APIs, in the process improving the results of the points-to analysis and its clients.",Big code | Pointer analysis | Specification | Unsupervised machine learning,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Eberhardt, Jan;Raychev, Veselin;Steffen, Samuel;Vechev, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067662708,10.1145/3314221.3314648,Scalable taint specification inference with big code,"We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library APIs (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on GitHub), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7, 000 API roles from over 210, 000 candidate APIs with very little supervision (less than 300 annotations) and with high estimated precision (67%). Further, using the learned specifications, our taint analyzer flagged more than 20, 000 violations in open source projects, 97% of which were undetectable without the inferred specifications.",Big Code | Specification Inference | Taint Analysis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Chibotaru, Victor;Raychev, Veselin;Bichsel, Benjamin;Vechev, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067674886,10.1145/3314221.3314643,Size-change termination as a contract: Dynamically and statically enforcing termination for higher-order programs,"Termination is an important but undecidable program property, which has led to a large body of work on static methods for conservatively predicting or enforcing termination. One such method is the size-change termination approach of Lee, Jones, and Ben-Amram, which operates in two phases: (1) abstract programs into “size-change graphs,” and (2) check these graphs for the size-change property: the existence of paths that lead to infinite decreasing sequences. We transpose these two phases with an operational semantics that accounts for the run-time enforcement of the size-change property, postponing (or entirely avoiding) program abstraction. This choice has two key consequences: (1) size-change termination can be checked at run-time and (2) termination can be rephrased as a safety property analyzed using existing methods for systematic abstraction. We formulate run-time size-change checks as contracts in the style of Findler and Felleisen. The result compliments existing contracts that enforce partial correctness specifications to obtain contracts for total correctness. Our approach combines the robustness of the size-change principle for termination with the precise information available at run-time. It has tunable overhead and can check for nontermination without the conservativeness necessary in static checking. To obtain a sound and computable termination analysis, we apply existing abstract interpretation techniques directly to the operational semantics, avoiding the need for custom abstractions for termination. The resulting analyzer is competitive with with existing, purpose-built analyzers.",Size-change principle | Termination | Verification,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2019-06-08,Conference Paper,"Nguyn, Phúc C.;Tobin-Hochstadt, Sam;Gilray, Thomas;Van Horn, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077819462,10.1109/SCAM.2019.00025,Characterizing leveraged stack overflow posts,"Stack Overflow is the most popular question and answer website on computer programming with more than 2.5M users, 16M questions, and a new answer posted, on average, every five seconds. This wide availability of data led researchers to develop techniques to mine Stack Overflow posts. The aim is to find and recommend posts with information useful to developers. However, and not surprisingly, not every Stack Overflow post is useful from a developer's perspective. We empirically investigate what the characteristics of 'useful' Stack Overflow posts are. The underlying assumption of our study is that posts that were used (referenced in the source code) in the past by developers are likely to be useful. We refer to these posts as leveraged posts. We study the characteristics of leveraged posts as opposed to the non-leveraged ones, focusing on community aspects (e.g., the reputation of the user who authored the post), the quality of the included code snippets (e.g., complexity), and the quality of the post's textual content (e.g., readability). Then, we use these features to build a prediction model to automatically identify posts that are likely to be leveraged by developers. Results of the study indicate that post meta-data (e.g., the number of comments received by the answer) is particularly useful to predict whether it has been leveraged or not, whereas code readability appears to be less useful. A classifier can classify leveraged posts with a precision of 65% and recall of 49% and non-leveraged ones with a precision of 95% and recall of 97%. This opens the road towards an automatic identification of 'high-quality content' in Stack Overflow.",Q&A forums | Recommender systems | Reuse,"Proceedings - 19th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2019",2019-09-01,Conference Paper,"Geremia, Salvatore;Bavota, Gabriele;Oliveto, Rocco;Lanza, Michele;Di Penta, Massimiliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077817405,10.1109/SCAM.2019.00026,Interactive refactoring documentation bot,"The documentation of code changes is significantly important but developers ignore it, most of the time, due to the pressure of the deadlines. While developers may document the most important features modification or bugs fixing, recent empirical studies show that the documentation of quality improvements and/or refactoring is often omitted or not accurately described. However, the automated or semi-automated documentation of refactorings has not been yet explored despite the extensive work on the remaining steps of refactoring including the detection, prioritization and recommendation. In this paper, we propose a semi-automated refactoring documentation bot that helps developers to interactively check and validate the documentation of the refactorings and/or quality improvements at the file level for each opened pull-request before being reviewed or merged to the master. The bot starts by checking the pullrequest if there are significant quality changes and refactorings at the file level and whether they are documented by the developer. Then, it checks the validity of the developers description of the refactorings, if any. Based on that analysis, the documentation bot will recommend a message to document the refactorings, their locations and the quality improvement for that pull-request when missing information is found. Then, the developer can modify his pull request description by interacting with the bot to accept/modify/reject part of the proposed documentation. Since refactoring do not happen in isolation most of the time, the bot is documenting the impact of a sequence of refactorings, in a pull-request, on quality and not each refactoring in isolation. We conducted a human survey with 14 active developers to manually evaluate the relevance and the correctness of our tool on different pull requests of 5 open source projects and one industrial system. The results show that the participants found that our bot facilitates the documentation of their quality-related changes and refactorings.",Documentation | Intelligent bot | Refactoring,"Proceedings - 19th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2019",2019-09-01,Conference Paper,"Rebai, Soumaya;Ben Sghaier, Oussama;Alizadeh, Vahid;Kessentini, Marouane;Chater, Meriem",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077823835,10.1109/SCAM.2019.00028,WAL: A tool for diagnosing accessibility issues and evolving legacy web systems at runtime,"The vast majority of pages available on the Web are developed under the tacit assumption that the final user will not have any kind of disability. Thus, accessibility aspects are not contemplated in their development process. As a consequence, many users encounter interaction barriers and cannot access much of the content in the Web. But the improvement of Websites can be challenging due to their high complexity and lack of documentation, the impossibility of accessing the source code, the use of out-of-dated technologies, and the fact that development teams are often unaware of accessibility rules. To mitigate this problem, this paper proposes a tool, called Website Accessibility Layer (WAL), which aims at both inserting accessibility features into existing Web pages at runtime and diagnosing accessibility issues through a dynamic source code analysis. The current version of the tool contributes to the accessibility of users with visual impairment, dyslexia, and reading difficulties, with respect to 13 Web page resources. The proposal has been validated by being applied to 40 widely accessed Websites with satisfactory results, proving the feasibility of the solution. Furthermore, real users also evaluated positively the benefits brought by the tool to their daily work activities.",Dynamic source code analysis | Legacy Web systems | Web accessibility,"Proceedings - 19th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2019",2019-09-01,Conference Paper,"Teotonio, Wanessa;Gonzalez, Pablo;Maia, Paulo;Muniz, Pedro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077824091,10.1109/SCAM.2019.00036,LUDroid: A large scale analysis of android - Web hybridization,"Many Android applications embed webpages via WebView components and execute JavaScript code within Android. Hybrid applications leverage dedicated APIs to load a resource and render it in WebView. Furthermore, Android objects can be shared with the JavaScript world. However, bridging the interfaces of the Android and JavaScript world might also incur severe security threats: Potentially untrusted webpages and their JavaScript might interfere with the Android environment and its access to native features. No general analysis is currently available to assess the implications of such hybrid apps bridging the two worlds. To understand the semantics and effects of hybrid apps, we perform a large-scale study on the usage of the hybridization APIs in the wild. We analyze and categorize the parameters to hybridization APIs for 7,500 randomly selected applications from the Google Playstore. Our results advance the general understanding of hybrid applications, as well as implications for potential program analyses, and the current security situation: We discover 6,375 flows of sensitive data from Android to JavaScript, out of which 82% could flow to potentially untrustworthy code. Our analysis identified 365 web pages embedding vulnerabilities and we exemplarily exploit them. Additionally, we discover 653 applications in which potentially untrusted Javascript code may interfere with (trusted) Android objects.",Android hybrid apps | Information flow control | Static analysis,"Proceedings - 19th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2019",2019-09-01,Conference Paper,"Tiwari, Abhishek;Prakash, Jyoti;Grob, Sascha;Hammer, Christian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3309697.3331499,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85075855720,10.1109/VISSOFT.2019.00013,A tertiary systematic literature review on software visualization,"Software visualization (SV) allows us to visualize different aspects and artifacts related to software, thus helping engineers understanding its underlying design and functionalities in a more efficient and faster way. In this paper, we conducted a tertiary systematic literature review to identify, classify, and evaluate the current state of the art on software visualization from 48 software visualization secondary studies, following three perspectives: publication trends, software visualization topics and techniques, and issues related to research field. Hence, we summarized the main findings among popular sub-fields of SV, identifying potential research directions and fifteen shared recommendations for developers, instructors and researchers. Our main findings are the lack of rigorous evaluation or theories support to assess SV tools effectiveness, the disconnection between tool design and their scope, and the dispersal of the research community.",Information Visualization | Literature Review | Recommendations | SLR | Software Engineering | Software Visualization | Tertiary | Visualization techniques,"Proceedings - 7th IEEE Working Conference on Software Visualization, VISSOFT 2019",2019-09-01,Conference Paper,"Bedu, Laure;Tinh, Olivier;Petrillo, Fabio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072111608,10.1109/ICGSE.2019.00019,Investigating the Adoption and Application of Large-Scale Scrum at a German Automobile Manufacturer,"Over the last two decades, agile methods have been adopted by an increasing number of organizations to improve their software development processes. In contrast to traditional methods, agile methods place more emphasis on flexible processes than on detailed upfront plans and heavy documentation. Since agile methods have proved to be successful at the team level, large organizations are now aiming to scale agile methods to the enterprise level by adopting and applying so-called scaling agile frameworks such as Large-Scale Scrum (LeSS) or Scaled Agile Framework (SAFe). Although there is a growing body of literature on large-scale agile development, literature documenting actual experiences related to scaling agile frameworks is still scarce. This paper aims to fill this gap by providing a case study on the adoption and application of LeSS in four different products of a German automobile manufacturer. Based on seven interviews, we present how the organization adopted and applied LeSS, and discuss related challenges and success factors. The comparison of the products indicates that transparency, training courses and workshops, and change management are crucial for a successful adoption.",case study | large-scale agile development | large-scale scrum | scaling agile frameworks,"Proceedings - 2019 ACM/IEEE 14th International Conference on Global Software Engineering, ICGSE 2019",2019-05-01,Conference Paper,"Uludag, Omer;Kleehaus, Martin;Dreymann, Niklas;Kabelin, Christian;Matthes, Florian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072109392,10.1109/ICGSE.2019.00020,Challenges in Adopting Continuous Delivery and DevOps in a Globally Distributed Product Team: A Case Study of a Healthcare Organization,"This paper presents our experiences in a project of a software engineering team spread across three countries that successfully established continuous delivery, DevOps and short release cycles with agile scrum. We had the challenge to find a way from established regulatory heavy-weight processes, long release strategies, legacy tools and technologies and people mindset towards adopting continuous delivery and DevOps. We are describing our experiences in the journey towards timeboxed release strategies compared to legacy fixed scope-based releases; value stream-based execution compared to traditional milestone-based execution; operation, test, and infrastructure as a code compared to executing these activities manually. This paper also describes experiences in transforming traditional scrum team into a DevOps team, technological landscape into lightweight tools. The authors bring their experiences as a Project Manager, Quality Manager, and an Architect, who has been an integral part of this journey. These practices have helped in stabilizing processes and methods to an extent where we have released several products versions within a year. The other business units are adopting our practices for continuous delivery and DevOps. This paper also summaries our lessons learned, and recommendations.",Continuous Delivery | DevOps | Operation as code | Test as code,"Proceedings - 2019 ACM/IEEE 14th International Conference on Global Software Engineering, ICGSE 2019",2019-05-01,Conference Paper,"Gupta, Rajeev Kumar;Venkatachalapathy, Mekanathan;Jeberla, Ferose Khan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069868616,10.1007/978-3-030-25540-4_15,Efficient Synthesis with Probabilistic Constraints,"We consider the problem of synthesizing a program given a probabilistic specification of its desired behavior. Specifically, we study the recent paradigm of distribution-guided inductive synthesis (digits), which iteratively calls a synthesizer on finite sample sets from a given distribution. We make theoretical and algorithmic contributions: (i)Â We prove the surprising result that digits only requires a polynomial number of synthesizer calls in the size of the sample set, despite its ostensibly exponential behavior. (ii)Â We present a property-directed version of digits that further reduces the number of synthesizer calls, drastically improving synthesis performance on a range of benchmarks.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Drews, Samuel;Albarghouthi, Aws;D’Antoni, Loris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069809837,10.1007/978-3-030-25540-4_17,Overfitting in Synthesis: Theory and Practice,"In syntax-guided synthesis (SyGuS), a synthesizer’s goal is to automatically generate a program belonging to a grammar of possible implementations that meets a logical specification. We investigate a common limitation across state-of-the-art SyGuS tools that perform counterexample-guided inductive synthesis (CEGIS). We empirically observe that as the expressiveness of the provided grammar increases, the performance of these tools degrades significantly. We claim that this degradation is not only due to a larger search space, but also due to overfitting. We formally define this phenomenon and prove no-free-lunch theorems for SyGuS, which reveal a fundamental tradeoff between synthesizer performance and grammar expressiveness. A standard approach to mitigate overfitting in machine learning is to run multiple learners with varying expressiveness in parallel. We demonstrate that this insight can immediately benefit existing SyGuS tools. We also propose a novel single-threaded technique called hybrid enumeration that interleaves different grammars and outperforms the winner of the 2018 SyGuS competition (Inv track), solving more problems and achieving a times mean speedup.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Padhi, Saswat;Millstein, Todd;Nori, Aditya;Sharma, Rahul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069854284,10.1007/978-3-030-25540-4_21,Extending nuXmv with Timed Transition Systems and Timed Temporal Properties,"nuXmv is a well-known symbolic model checker, which implements various state-of-the-art algorithms for the analysis of finite- and infinite-state transition systems and temporal logics. In this paper, we present a new version that supports timed systems and logics over continuous super-dense semantics. The system specification was extended with clocks to constrain the timed evolution. The support for temporal properties has been expanded to include 0, formulas with parametric intervals. The analysis is performed via a reduction to verification problems in the discrete-time case. The internal representation of traces has been extended to go beyond the lasso-shaped form, to take into account the possible divergence of clocks. We evaluated the new features by comparing nuXmv with other verification tools for timed automata and , considering different benchmarks from the literature. The results show that nuXmv is competitive with and in many cases performs better than state-of-the-art tools, especially on validity problems for.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Cimatti, Alessandro;Griggio, Alberto;Magnago, Enrico;Roveri, Marco;Tonetta, Stefano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069803289,10.1007/978-3-030-25543-5_1,Satisfiability checking for mission-time LTL,"Mission-time LTL (MLTL) is a bounded variant of MTL over naturals designed to generically specify requirements for mission-based system operation common to aircraft, spacecraft, vehicles, and robots. Despite the utility of MLTL as a specification logic, major gaps remain in analyzing MLTL, e.g., for specification debugging or model checking, centering on the absence of any complete MLTL satisfiability checker. We prove that the MLTL satisfiability checking problem is NEXPTIME-complete and that satisfiability checking, the variant of MLTL where all intervals start at 0, is PSPACE-complete. We introduce translations for MLTL-to-LTL,, MLTL-to-SMV, and MLTf-to-SMT, creating four options for MLTL satisfiability checking. Our extensive experimental evaluation shows that the MLTL-to-SMT transition with the Z3 SMT solver offers the most scalable performance.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Li, Jianwen;Vardi, Moshe Y.;Rozier, Kristin Y.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064750564,10.1145/3319008.3319016,Automated classification of class role-stereotypes via machine learning,"Role stereotypes indicate generic roles that classes play in the design of software systems (e.g. controller, information holder, or interfacer). Knowledge about the role-stereotypes can help in various tasks in software development and maintenance, such as program understanding, program summarization, and quality assurance. This paper presents an automated machine learning-based approach for classifying the role-stereotype of classes in Java. We analyse the performance of this approach against a manually labelled ground truth for a sizable open source project (of 770+ Java classes) for the Android platform. Moreover, we compare our approach to an existing rule-based classification approach. The contributions of this paper include an analysis of which machine learning algorithms and which features provide the best classification performance. This analysis shows that the Random Forest algorithm yields the best classification performance. We find however, that the performance of the ML-classifier varies a lot for classifying different role-stereotypes. In particular its performs degrades for rare role-types. Our ML-classifier improves over the existing rule-based classification method in that the ML-approach classifies all classes, while rule-based approaches leave a significant number of classes unclassified.",Program understanding | Reverse engineering | Software design,ACM International Conference Proceeding Series,2019-04-15,Conference Paper,"Nurwidyantoro, Arif;Ho-Quang, Truong;Chaudron, Michel R.V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3319008.3319025,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064749745,10.1145/3319008.3319350,An empirical usability analysis of the google authentication API,"Millions of web users today use their Google accounts to sign into millions of relying party websites. This is enabled through the Google authentication API, which allows third party application developers to embed Google sign-in into their application. However, regardless to the strength of the Google authentication mechanism, the majority of these applications have been identified to be infected with broken authentication, which made them vulnerable to cyber attacks. A major reason for this is mistakes that developers make while embedding Google sign-in into their application. High complexity and lack of usability of the Google authentication API makes it difficult for programmers to use it correctly and lead them to make mistakes while using the API. In this study, we evaluated the usability of the Google authentication API by conducting a user study with 10 programmers, where they attempted to embed Google sign-in and sign-out into a web application via Google authentication API. We employed think-aloud approach to evaluate the experience of the programmers and they also provided their feedback by answering the cognitive dimension framework based questionnaire. Results of the experiment revealed 12 usability issues that exist in the Google authentication API. We discussed how these usability issues would affect the security of the application that are developed using the Google authentication API and how the API should be improved to provide a better experience to application developers.",API usability | Security APIs | Usability evaluation,ACM International Conference Proceeding Series,2019-04-15,Conference Paper,"Wijayarathna, Chamila;Arachchilage, Nalin A.G.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072111608,10.1109/ICGSE.2019.00019,Investigating the Adoption and Application of Large-Scale Scrum at a German Automobile Manufacturer,"Over the last two decades, agile methods have been adopted by an increasing number of organizations to improve their software development processes. In contrast to traditional methods, agile methods place more emphasis on flexible processes than on detailed upfront plans and heavy documentation. Since agile methods have proved to be successful at the team level, large organizations are now aiming to scale agile methods to the enterprise level by adopting and applying so-called scaling agile frameworks such as Large-Scale Scrum (LeSS) or Scaled Agile Framework (SAFe). Although there is a growing body of literature on large-scale agile development, literature documenting actual experiences related to scaling agile frameworks is still scarce. This paper aims to fill this gap by providing a case study on the adoption and application of LeSS in four different products of a German automobile manufacturer. Based on seven interviews, we present how the organization adopted and applied LeSS, and discuss related challenges and success factors. The comparison of the products indicates that transparency, training courses and workshops, and change management are crucial for a successful adoption.",case study | large-scale agile development | large-scale scrum | scaling agile frameworks,"Proceedings - 2019 ACM/IEEE 14th International Conference on Global Software Engineering, ICGSE 2019",2019-05-01,Conference Paper,"Uludag, Omer;Kleehaus, Martin;Dreymann, Niklas;Kabelin, Christian;Matthes, Florian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072109392,10.1109/ICGSE.2019.00020,Challenges in Adopting Continuous Delivery and DevOps in a Globally Distributed Product Team: A Case Study of a Healthcare Organization,"This paper presents our experiences in a project of a software engineering team spread across three countries that successfully established continuous delivery, DevOps and short release cycles with agile scrum. We had the challenge to find a way from established regulatory heavy-weight processes, long release strategies, legacy tools and technologies and people mindset towards adopting continuous delivery and DevOps. We are describing our experiences in the journey towards timeboxed release strategies compared to legacy fixed scope-based releases; value stream-based execution compared to traditional milestone-based execution; operation, test, and infrastructure as a code compared to executing these activities manually. This paper also describes experiences in transforming traditional scrum team into a DevOps team, technological landscape into lightweight tools. The authors bring their experiences as a Project Manager, Quality Manager, and an Architect, who has been an integral part of this journey. These practices have helped in stabilizing processes and methods to an extent where we have released several products versions within a year. The other business units are adopting our practices for continuous delivery and DevOps. This paper also summaries our lessons learned, and recommendations.",Continuous Delivery | DevOps | Operation as code | Test as code,"Proceedings - 2019 ACM/IEEE 14th International Conference on Global Software Engineering, ICGSE 2019",2019-05-01,Conference Paper,"Gupta, Rajeev Kumar;Venkatachalapathy, Mekanathan;Jeberla, Ferose Khan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85060893585,10.1145/3293455,RESTful API automated test case generation with Evomaster,"RESTful APIs are widespread in industry, especially in enterprise applications developed with a microservice architecture. A RESTful web service will provide data via an API over the network using HTTP, possibly interacting with databases and other web services. Testing a RESTful API poses challenges, because inputs/outputs are sequences of HTTP requests/responses to a remote server. Many approaches in the literature do black-box testing, because often the tested API is a remote service whose code is not available. In this article, we consider testing from the point of view of the developers, who have full access to the code that they are writing. Therefore, we propose a fully automated white-box testing approach, where test cases are automatically generated using an evolutionary algorithm. Tests are rewarded based on code coverage and fault-finding metrics. However, REST is not a protocol but rather a set of guidelines on how to design resources accessed over HTTP endpoints. For example, there are guidelines on how related resources should be structured with hierarchical URIs and how the different HTTP verbs should be used to represent well-defined actions on those resources. Test-case generation for RESTful APIs that only rely on white-box information of the source code might not be able to identify how to create prerequisite resources needed before being able to test some of the REST endpoints. Smart sampling techniques that exploit the knowledge of best practices in RESTful API design are needed to generate tests with predefined structures to speed up the search. We implemented our technique in a tool called EvoMaster, which is open source. Experiments on five open-source, yet non-trivial, RESTful services show that our novel technique automatically found 80 real bugs in those applications. However, obtained code coverage is lower than the one achieved by the manually written test suites already existing in those services. Research directions on how to further improve such an approach are therefore discussed, such as the handling of SQL databases.",REST | Software engineering | Testing | Web service,ACM Transactions on Software Engineering and Methodology,2019-01-01,Article,"Arcuri, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85062349093,10.1145/3295739,Understanding and analyzing Java reflection,"Java reflection has been widely used in a variety of applications and frameworks. It allows a software system to inspect and change the behaviour of its classes, interfaces, methods, and fields at runtime, enabling the software to adapt to dynamically changing runtime environments. However, this dynamic language feature imposes significant challenges to static analysis, because the behaviour of reflection-rich software is logically complex and statically hard to predict. As a result, existing static analysis tools either ignore reflection or handle it partially, resulting in missed, important behaviours, i.e., unsound results. Therefore, improving or even achieving soundness in static reflection analysis-an analysis that infers statically the behaviour of reflective code-will provide significant benefits to many analysis clients, such as bug detectors, security analyzers, and program verifiers. In this article, we provide a comprehensive understanding of Java reflection through examining its underlying concept, API, and real-world usage, and, building on this, we introduce a new static approach to resolving Java reflection effectively in practice. We have implemented our reflection analysis in an open-source tool, called Solar, and evaluated its effectiveness extensively with large Java programs and libraries. Our experimental results demonstrate that Solar is able to (1) resolve reflection more soundly than the state-of-the-art reflection analyses; (2) automatically and accurately identify the parts of the program where reflection is resolved unsoundly or imprecisely; and (3) guide users to iteratively refine the analysis results by using lightweight annotations until their specific requirements are satisfied.",Java reflection | Points-to analysis | Reflection analysis | Static analysis,ACM Transactions on Software Engineering and Methodology,2019-02-01,Article,"Li, Yue;Tan, Tian;Xue, Jingling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85062345543,10.1145/3306607,Status quo in requirements engineering: A theory and a global family of surveys,"Requirements Engineering (RE) has established itself as a software engineering discipline over the past decades. While researchers have been investigating the RE discipline with a plethora of empirical studies, attempts to systematically derive an empirical theory in context of the RE discipline have just recently been started. However, such a theory is needed if we are to define and motivate guidance in performing high quality RE research and practice. We aim at providing an empirical and externally valid foundation for a theory of RE practice, which helps software engineers establish effective and efficient RE processes in a problem-driven manner. We designed a survey instrument and an engineer-focused theory that was first piloted in Germany and, after making substantial modifications, has now been replicated in 10 countries worldwide. We have a theory in the form of a set of propositions inferred from our experiences and available studies, as well as the results from our pilot study in Germany. We evaluate the propositions with bootstrapped confidence intervals and derive potential explanations for the propositions. In this article, we report on the design of the family of surveys, its underlying theory, and the full results obtained from the replication studies conducted in 10 countries with participants from 228 organisations. Our results represent a substantial step forward towards developing an empirical theory of RE practice. The results reveal, for example, that there are no strong differences between organisations in different countries and regions, that interviews, facilitated meetings and prototyping are the most used elicitation techniques, that requirements are often documented textually, that traces between requirements and code or design documents are common, that requirements specifications themselves are rarely changed and that requirements engineering (process) improvement endeavours are mostly internally driven. Our study establishes a theory that can be used as starting point for many further studies for more detailed investigations. Practitioners can use the results as theory-supported guidance on selecting suitable RE methods and techniques.",Replication | Requirements engineering | Survey research | Theory,ACM Transactions on Software Engineering and Methodology,2019-02-01,Article,"Wagner, Stefan;Fernández, Daniel Méndez;Felderer, Michael;Vetrò, Antonio;Kalinowski, Marcos;Wieringa, Roel;Pfahl, Dietmar;Conte, Tayana;Christiansson, Marie Therese;Greer, Desmond;Lassenius, Casper;Männistö, Tomi;Nayebi, Maleknaz;Oivo, Markku;Penzenstadler, Birgit;Prikladnicki, Rafael;Ruhe, Guenther;Schekelmann, André;Sen, Sagar;Spínola, Rodrigo;Tuzcu, Ahmed;De La Vara, Jose Luis;Winkler, Dietmar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85075677304,10.1145/3344158,Recommending new features from mobile app descriptions,"The rapidly evolving mobile applications (apps) have brought great demand for developers to identify new features by inspecting the descriptions of similar apps and acquire missing features for their apps. Unfortunately, due to the huge number of apps, this manual process is time-consuming and unscalable. To help developers identify new features, we propose a new approach named SAFER. In this study, we first develop a tool to automatically extract features from app descriptions. Then, given an app, we leverage the topic model to identify its similar apps based on the extracted features and API names of apps. Finally, we design a feature recommendation algorithm to aggregate and recommend the features of identified similar apps to the specified app. Evaluated over a collection of 533 annotated features from 100 apps, SAFER achieves a Hit@15 score of up to 78.68% and outperforms the baseline approach KNN+ by 17.23% on average. In addition, we also compare SAFER against a typical technique of recommending features from user reviews, i.e., CLAP. Experimental results reveal that SAFER is superior to CLAP by 23.54% in terms of Hit@15.",Domain analysis | Feature recommender system | Mobile applications | Topic model,ACM Transactions on Software Engineering and Methodology,2019-10-01,Article,"Jiang, He;Zhang, Jingxuan;Li, Xiaochen;Ren, Zhilei;Lo, David;Wu, Xindong;Luo, Zhongxuan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85075020632,10.1145/3324916,Neural network-based detection of self-Admitted technical debt: From performance to explainability,"Technical debt is a metaphor to reflect the tradeoff software engineers make between short-Term benefits and long-Term stability. Self-Admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is intentionally introduced during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model's prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-The-Art traditional text-mining-based methods for SATD classification.",Convolutional neural network | cross project prediction | Model adaptability | model explainability | Model generalizability | Self-Admitted technical debt,ACM Transactions on Software Engineering and Methodology,2019-08-01,Article,"Ren, Xiaoxue;Xing, Zhenchang;Xia, Xin;Lo, David;Wang, Xinyu;Grundy, John",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85032443846,10.1109/TSE.2017.2765640,"A Rigorous Framework for Specification, Analysis and Enforcement of Access Control Policies","Access control systems are widely used means for the protection of computing systems. They are defined in terms of access control policies regulating the access to system resources. In this paper, we introduce a formally-defined, fully-implemented framework for specification, analysis and enforcement of attribute-based access control policies. The framework rests on FACPL, a language with a compact, yet expressive, syntax for specification of real-world access control policies and with a rigorously defined denotational semantics. The framework enables the automated verification of properties regarding both the authorisations enforced by single policies and the relationships among multiple policies. Effectiveness and performance of the analysis rely on a semantic-preserving representation of FACPL policies in terms of SMT formulae and on the use of efficient SMT solvers. Our analysis approach explicitly addresses some crucial aspects of policy evaluation, such as missing attributes, erroneous values and obligations, which are instead overlooked in other proposals. The framework is supported by Java-based tools, among which an Eclipse-based IDE offering a tailored development and analysis environment for FACPL policies and a Java library for policy enforcement. We illustrate the framework and its formal ingredients by means of an e-Health case study, while its effectiveness is assessed by means of performance stress tests and experiments on a well-established benchmark.",Attribute-based access control | policy analysis | policy languages | SMT,IEEE Transactions on Software Engineering,2019-01-01,Article,"Margheri, Andrea;Masi, Massimiliano;Pugliese, Rosario;Tiezzi, Francesco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85035777556,10.1109/TSE.2017.2776152,"Developer Testing in the IDE: Patterns, Beliefs, and Behavior","Software testing is one of the key activities to achieve software quality in practice. Despite its importance, however, we have a remarkable lack of knowledge on how developers test in real-world projects. In this paper, we report on a large-scale field study with 2,443 software engineers whose development activities we closely monitored over 2.5 years in four integrated development environments (IDEs). Our findings, which largely generalized across the studied IDEs and programming languages Java and C#, question several commonly shared assumptions and beliefs about developer testing: half of the developers in our study do not test; developers rarely run their tests in the IDE; most programming sessions end without any test execution; only once they start testing, do they do it extensively; a quarter of test cases is responsible for three quarters of all test failures; 12 percent of tests show flaky behavior; Test-Driven Development (TDD) is not widely practiced; and software developers only spend a quarter of their time engineering tests, whereas they think they test half of their time. We summarize these practices of loosely guiding one's development efforts with the help of testing in an initial summary on Test-Guided Development (TGD), a behavior we argue to be closer to the development reality of most developers than TDD.",Developer testing | field study | JUnit | KaVE FeedBag++ | test-driven development (TDD) | testing effort | TestRoots WatchDog | unit tests,IEEE Transactions on Software Engineering,2019-03-01,Article,"Beller, Moritz;Gousios, Georgios;Panichella, Annibale;Proksch, Sebastian;Amann, Sven;Zaidman, Andy",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85038846845,10.1109/TSE.2017.2782280,An Empirical Study on API Usages,"API libraries provide thousands of APIs, and are essential in daily programming tasks. To understand their usages, it has long been a hot research topic to mine specifications that formally define legal usages for APIs. Furthermore, researchers are working on many other research topics on APIs. Although the research on APIs is intensively studied, many fundamental questions on APIs are still open. For example, the answers to open questions, such as which format can naturally define API usages and in which case, are still largely unknown. We notice that many such open questions are not concerned with concrete usages of specific APIs, but usages that describe how to use different types of APIs. To explore these questions, in this paper, we conduct an empirical study on API usages, with an emphasis on how different types of APIs are used. Our empirical results lead to nine findings on API usages. For example, we find that single-type usages are mostly strict orders, but multi-type usages are more complicated since they include both strict orders and partial orders. Based on these findings, for the research on APIs, we provide our suggestions on the four key aspects such as the challenges, the importance of different API elements, usage patterns, and pitfalls in designing evaluations. Furthermore, we interpret our findings, and present our insights on data sources, extraction techniques, mining techniques, and formats of specifications for the research of mining specifications.",API usage | empirical study | mining specification,IEEE Transactions on Software Engineering,2019-04-01,Article,"Zhong, Hao;Mei, Hong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040086250,10.1109/TSE.2017.2786286,Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas,"Data is one of an organization's most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding strength of a test suite. As with program mutation, however, relational database schema mutation results in many ineffective mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems-HyperSQL, PostgreSQL, and SQLite-the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24 percent of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100 percent scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with heavyweight DBMSs like PostgreSQL.",relational databases | software quality | software testing | software tools,IEEE Transactions on Software Engineering,2019-05-01,Article,"McMinn, Phil;Wright, Chris J.;McCurdy, Colton J.;Kapfhammer, Gregory M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040092215,10.1109/TSE.2017.2788018,Automatic Loop Summarization via Path Dependency Analysis,"Analyzing loops is very important for various software engineering tasks such as bug detection, test case generation and program optimization. However, loops are very challenging structures for program analysis, especially when (nested) loops contain multiple paths that have complex interleaving relationships. In this paper, we propose the path dependency automaton (PDA) to capture the dependencies among the multiple paths in a loop. Based on the PDA, we first propose a loop classification to understand the complexity of loop summarization. Then, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects (i.e., disjunctive loop summary) on the variables of interest. An algorithm is proposed to traverse the PDA to summarize the effect for all possible executions in the loop. We have evaluated Proteus using loops from five open-source projects and two well-known benchmarks and applying the disjunctive loop summary to three applications: loop bound analysis, program verification and test case generation. The evaluation results have demonstrated that Proteus can compute a more precise bound than the existing loop bound analysis techniques; Proteus can significantly outperform the state-of-the-art tools for loop program verification; and Proteus can help generate test cases for deep loops within one second, while symbolic execution tools KLEE and Pex either need much more time or fail.",Disjunctive loop summary | path dependency automaton | path interleaving,IEEE Transactions on Software Engineering,2019-06-01,Article,"Xie, Xiaofei;Chen, Bihuan;Zou, Liang;Liu, Yang;Le, Wei;Li, Xiaohong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040976130,10.1109/TSE.2018.2796554,Toward Methodological Guidelines for Process Theories and Taxonomies in Software Engineering,"Software engineering is increasingly concerned with theory because the foundational knowledge comprising theories provides a crucial counterpoint to the practical knowledge expressed through methods and techniques. Fortunately, much guidance is available for generating and evaluating theories for explaining why things happen (variance theories). Unfortunately, little guidance is available concerning theories for explaining how things happen (process theories), or theories for analyzing and understanding situations (taxonomies). This paper therefore attempts to clarify the nature and functions of process theories and taxonomies in software engineering research, and to synthesize methodological guidelines for their generation and evaluation. It further advances the key insight that most process theories are taxonomies with additional propositions, which helps inform their evaluation. The proposed methodological guidance has many benefits: it provides a concise summary of existing guidance from reference disciplines, it adapts techniques from reference disciplines to the software engineering context, and it promotes approaches that better facilitate scientific consensus.",action research | case study | experiment | framework | grounded theory | guidelines | model | process theory | questionnaire | Research methodology | taxonomy | theory for analysis | theory for understanding,IEEE Transactions on Software Engineering,2019-07-01,Article,"Ralph, Paul",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85041863929,10.1109/TSE.2018.2804939,Verification Templates for the Analysis of User Interface Software Design,"The paper describes templates for model-based analysis of usability and safety aspects of user interface software design. The templates crystallize general usability principles commonly addressed in user-centred safety requirements, such as the ability to undo user actions, the visibility of operational modes, and the predictability of user interface behavior. These requirements have standard forms across different application domains, and can be instantiated as properties of specific devices. The modeling and analysis process is carried out using the Prototype Verification System (PVS), and is further facilitated by structuring the specification of the device using a format that is designed to be generic across interactive systems. A concrete case study based on a commercial infusion pump is used to illustrate the approach. A detailed presentation of the automated verification process using PVS shows how failed proof attempts provide precise information about problematic user interface software features.",formal specifications | formal verification | Human-computer interaction | model-based development | prototype verification system (PVS),IEEE Transactions on Software Engineering,2019-08-01,Article,"Harrison, Michael D.;Masci, Paolo;Campos, Jose C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85044354578,10.1109/TSE.2018.2819180,UniDoSA: The Unified Specification and Detection of Service Antipatterns,"Service-based Systems (SBSs) are developed on top of diverse Service-Oriented Architecture (SOA) technologies or architectural styles. Like any other complex systems, SBSs face both functional and non-functional changes at the design or implementation-level. Such changes may degrade the design quality and quality of service (QoS) of the services in SBSs by introducing poor solutions-service antipatterns. The presence of service antipatterns in SBSs may hinder the future maintenance and evolution of SBSs. Assessing the quality of design and QoS of SBSs through the detection of service antipatterns may ease their maintenance and evolution. However, the current literature lacks a unified approach for modelling and evaluating the design of SBSs in term of design quality and QoS. To address this lack, this paper presents a meta-model unifying the three main service technologies: REST, SCA, and SOAP. Using the meta-model, it describes a unified approach, UniDoSA (Unified Specification and Detection of Service Antipatterns), supported by a framework, SOFA (Service Oriented Framework for Antipatterns), for modelling and evaluating the design quality and QoS of SBSs. We apply and validate UniDoSA on: (1) 18 RESTful APIs, (2) two SCA systems with more than 150 services, and (3) more than 120 SOAP Web services. With a high precision and recall, the detection results provide evidence of the presence of service antipatterns in SBSs, which calls for future studies of their impact on QoS.",Antipatterns | design | detection | quality of service | REST | SCA | service-based systems | SOAP | software maintenance and evolution | specification | web services,IEEE Transactions on Software Engineering,2019-10-01,Article,"Palma, Francis;Moha, Naouel;Gueheneuc, Yann Gael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85045682050,10.1109/TSE.2018.2827384,A Systematic Evaluation of Static API-Misuse Detectors,"Application Programming Interfaces (APIs) often have usage constraints, such as restrictions on call order or call conditions. API misuses, i.e., violations of these constraints, may lead to software crashes, bugs, and vulnerabilities. Though researchers developed many API-misuse detectors over the last two decades, recent studies show that API misuses are still prevalent. Therefore, we need to understand the capabilities and limitations of existing detectors in order to advance the state of the art. In this paper, we present the first-ever qualitative and quantitative evaluation that compares static API-misuse detectors along the same dimensions, and with original author validation. To accomplish this, we develop MuC, a classification of API misuses, and MuBenchPipe, an automated benchmark for detector comparison, on top of our misuse dataset, MuBench. Our results show that the capabilities of existing detectors vary greatly and that existing detectors, though capable of detecting misuses, suffer from extremely low precision and recall. A systematic root-cause analysis reveals that, most importantly, detectors need to go beyond the naive assumption that a deviation from the most-frequent usage corresponds to a misuse and need to obtain additional usage examples to train their models. We present possible directions towards more-powerful API-misuse detectors.",API-misuse detection | benchmark | misuse classification | MUBench | survey,IEEE Transactions on Software Engineering,2019-12-01,Article,"Amann, Sven;Nguyen, Hoan Anh;Nadi, Sarah;Nguyen, Tien N.;Mezini, Mira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046362904,10.1109/TSE.2018.2831232,Automating Change-Level Self-Admitted Technical Debt Determination,"Technical debt (TD) is a metaphor to describe the situation where developers introduce suboptimal solutions during software development to achieve short-term goals that may affect the long-term software quality. Prior studies proposed different techniques to identify TD, such as identifying TD through code smells or by analyzing source code comments. Technical debt identified using comments is known as Self-Admitted Technical Debt (SATD) and refers to TD that is introduced intentionally. Compared with TD identified by code metrics or code smells, SATD is more reliable since it is admitted by developers using comments. Thus far, all of the state-of-the-art approaches identify SATD at the file-level. In essence, they identify whether a file has SATD or not. However, all of the SATD is introduced through software changes. Previous studies that identify SATD at the file-level in isolation cannot describe the TD context related to multiple files. Therefore, it is beneficial to identify the SATD once a change is being made. We refer to this type of TD identification as 'Change-level SATD Determination', which determines whether or not a change introduces SATD. Identifying SATD at the change-level can help to manage and control TD by understanding the TD context through tracing the introducing changes. To build a change-level SATD Determination model, we first identify TD from source code comments in source code files of all versions. Second, we label the changes that first introduce the SATD comments as TD-introducing changes. Third, we build the determination model by extracting 25 features from software changes that are divided into three dimensions, namely diffusion, history and message, respectively. To evaluate the effectiveness of our proposed model, we perform an empirical study on 7 open source projects containing a total of 100,011 software changes. The experimental results show that our model achieves a promising and better performance than four baselines in terms of AUC and cost-effectiveness (i.e., percentage of TD-introducing changes identified when inspecting 20 percent of changed LOC). On average across the 7 experimental projects, our model achieves AUC of 0.82, cost-effectiveness of 0.80, which is a significant improvement over the comparison baselines used. In addition, we found that 'Diffusion' is the most discriminative dimension among the three dimensions of features for determining TD-introducing changes.",change-level determination | self-admitted technical debt | software change | Technical debt,IEEE Transactions on Software Engineering,2019-12-01,Article,"Yan, Meng;Xia, Xin;Shihab, Emad;Lo, David;Yin, Jianwei;Yang, Xiaohu",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85074762646,10.1145/3328020.3353941,"Professional practice, amateur profile: Mapping amateur game design communities","Amateur game design communities are fruitful spaces for research in professional and technical communication (PTC) [9, 10, 16]; these communities often mirror many professional practices such as iterative design and the creation of extensive documentation. Before crossing into these spaces, however, researchers and practitioners must understand the makeup of these communities, which may (anecdotally) serve as potential spaces for marginalized people in ways the games industry does not. In this study, we used the International Game Developers’ Association’s Developer Satisfaction Survey as a base for our own surveys of three amateur game design communities. In mapping these communities, we found that each mirrored the games industry in several categories, but that each community also displayed unique diferences that necessitate a variety of approaches to conducting research in or on such sites.",Amateur | Demographics | Game design | Gaming | Participatory design | Technical communication,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,2019-10-04,Conference Paper,"Atherton, Rachel;Karabinus, Alisha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074760221,10.1145/3328020.3353932,How to debate a border: Supporting infrastructure publics through communication system design,"While crucial infrastructures in the United States and similar countries are increasingly in conditions of crisis and decay, people living in these countries often lack a shared understanding of these systems from which to imagine the kind of radical change that theorists such as Lauren Berlant have argued is needed. As recent political controversies make clear, this lack of shared understanding extends to national infrastructures such as borders and points of entry. expert knowledge about present infrastructures is communicated alongside more future-oriented commentary.",Infrastructure | Online communication design | Participatory design | Speculative design,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,2019-10-04,Conference Paper,"Carter, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074756225,10.1145/3328020.3353947,Communicating design-related intellectual influence: Towards visual references,"Prototype-driven design research often involves collecting and analyzing designed artifacts in annotated portfolios and design workbooks. These collections constitute important sources of intellectual influence for researchers, yet communicating this influence presents unique challenges, such as the difficulty of translating the aesthetic, material, or interactive qualities of a designed artifact into written text. Building on discourses of visual thinking and visual imagery in science communication and HCI research, this paper introduces, and elaborates, a novel research communication design concept called “visual references,” which combine bibliographic information with photographic images, textual annotations, and diagrammatic annotations in order to communicate design-related intellectual influence.",Citing and referencing | Science communication | Visual communication | Visual references | Visual thinking,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,2019-10-04,Conference Paper,"Beck, Jordan;Sosa-Tzec, Omar;Carroll, John M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074765477,10.1145/3328020.3353935,Disaster documentation: The impact of Oregon’s evolving damage assessment methodology for emergency declarations,"This experience report focuses on the impact of Oregon’s evolving methodology for documenting and publishing data and information about damage from natural disasters and other emergencies. In tracing public damage assessment genre sets through organizational levels and user groups, the report (a) outlines the current processes by which data and information are generated and transferred and (b) connects the potential future damage assessment methodology to a larger paradigm shift in the state’s broader data-sharing approach.",Damage assessment | FEMA | Information design | Localization | Technical communication | Usability | User experience,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,2019-10-04,Conference Paper,"Covey, Henry",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85074759751,10.1145/3328020.3353936,Designing metrics to evaluate the help center of Baidu cloud,"Help centers are mainly designed to assist users with their product uses. The question as to how we measure the quality of a help center remains unanswered. As the frst step of a joint research initiated by Peking University and Baidu Cloud that aims to develop a set of computable metrics to evaluate the quality of help centers, this experience report shares the results of data analysis on correlation between user behavioral data and technical documentation quality. The documents and data we use are a suite of cloud computing services provided by Baidu Cloud. The report begins with an introduction of the research goal; following reviews on the related work, it then lays out the design of the experiments with user data collected from Baidu Cloud. In our experiments, we categorize all documents into three groups and try to identify which metrics would afect documentation quality most. The result shows that the key index that contributes most to the model is PV/UV. At last, the report concludes with our current experimental eforts and future work in our plan.",Help center evaluation | Quality evaluation | Technical information | Web metrics,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,2019-10-04,Conference Paper,"Gao, Zhijun;Gao, Yuxin;Xu, Jingjing",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85074780919,10.1145/3328020.3353953,Using content analysis to explore users’ perceptions and experiences using a novel citizen first responder app,"Our experience report describes a pilot project that investigates users’ self-reported perceptions and experiences using PulsePoint—a novel citizen first responder app. Our study includes analyzing select reviewer comments posted on the iOS and Android platforms and using grounded theory distillation practices to develop an evaluative heuristic/framework. We argue that these comments provide a rich source of untapped data for understanding how users experience mHealth apps from users’ perspectives. We propose that the preliminary findings from this study can be used to inform the development and design of mHealth apps in the future from a patient/user-centered perspective, especially those that rely on citizen first responders.",Content Analysis | Grounded Theory | MHealth Apps | User Experience,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,2019-10-04,Conference Paper,"Welhausen, Candice A.;Bivens, Kristin M.",Exclude,Not from technical track
10.1016/j.jss.2022.111515,,10.1145/3143560,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85040770771,10.1145/3156818,A study on garbage collection algorithms for big data environments,"The need to process and store massive amounts of data-Big Data-is a reality. In areas such as scientific experiments, social networks management, credit card fraud detection, targeted advertisement, and financial analysis, massive amounts of information are generated and processed daily to extract valuable, summarized information. Due to its fast development cycle (i.e., less expensive to develop), mainly because of automatic memory management, and rich community resources,managed object-oriented programming languages (e.g., Java) are the first choice to develop Big Data platforms (e.g., Cassandra, Spark) on which such Big Data applications are executed. However, automaticmemorymanagement comes at a cost. This cost is introduced by the garbage collector, which is responsible for collecting objects that are no longer being used. Although current (classic) garbage collection algorithms may be applicable to small-scale applications, these algorithms are not appropriate for large-scale Big Data environments, as they do not scale in terms of throughput and pause times. In this work, current Big Data platforms and their memory profiles are studied to understand why classic algorithms (which are still the most commonly used) are not appropriate, and also to analyze recently proposed and relevant memory management algorithms, targeted to Big Data environments. The scalability of recent memory management algorithms is characterized in terms of throughput (improves the throughput of the application) and pause time (reduces the latency of the application) when compared to classic algorithms. The study is concluded by presenting a taxonomy of the described works and some open problems, with regard to Big Data memory management, that could be addressed in future works.",Big data | Big data environment | Garbage collection | Java | Memory managed runtime | Processing platforms | Scalability | Storage platform,ACM Computing Surveys,2019-01-31,Article,"Bruno, Rodrigo;Ferreira, Paulo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042630008,10.1145/3167476,Evaluating computational creativity: An interdisciplinary tutorial,"This article is a tutorial for researchers who are designing software to perform a creative task and want to evaluate their system using interdisciplinary theories of creativity. Researchers who study human creativity have a great deal to offer computational creativity. We summarize perspectives from psychology, philosophy, cognitive science, and computer science as to how creativity can be measured both in humans and in computers. We survey how these perspectives have been used in computational creativity research and make recommendations for how they should be used.",Computational aesthetics | Computational creativity | Digital art,ACM Computing Surveys,2019-03-31,Review,"Lamb, Carolyn;Brown, Daniel G.;Clarke, Charles L.A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046541645,10.1145/3141772,A survey of modelling trends in temporal GIS,"The main achievements of spatio-temporal modelling in the field of Geographic Information Science that spans the past three decades are surveyed. This article offers an overviewof: (i) the origins and history of Temporal Geographic Information Systems (T-GIS); (ii) relevant spatio-temporal data models proposed; (iii) the evolution of spatio-temporal modelling trends; and (iv) an analysis of the future trends and developments in T-GIS. It also presents some current theories and concepts that have emerged from the research performed, as well as a summary of the current progress and the upcoming challenges and potential research directions for T-GIS. One relevant result of this survey is the proposed taxonomy of spatio-temporal modelling trends, which classifies 186 modelling proposals surveyed from more than 1,450 articles.",Literature review | Spatio-temporal databases | Spatio-temporal models | Survey | Temporal GIS | Temporal models | Time geography,ACM Computing Surveys,2019-03-31,Article,"Siabato, Willington;Claramunt, Christophe;Ilarri, Sergio;Manso-Callejo, Miguel Angel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042614195,10.1145/3161603,Detection and resolution of rumours in social media: A survey,"Despite the increasing use of social media platforms for information and news gathering, its unmoderated nature often leads to the emergence and spread of rumours, i.e., items of information that are unverified at the time of posting. At the same time, the openness of social media platforms provides opportunities to study how users share and discuss rumours, and to explore how to automatically assess their veracity, using natural language processing and data mining techniques. In this article, we introduce and discuss two types of rumours that circulate on social media: long-standing rumours that circulate for long periods of time, and newly emerging rumours spawned during fast-paced events such as breaking news, where reports are released piecemeal and often with an unverified status in their early stages. We provide an overview of research into social media rumours with the ultimate goal of developing a rumour classification system that consists of four components: rumour detection, rumour tracking, rumour stance classification, and rumour veracity classification. We delve into the approaches presented in the scientific literature for the development of each of these four components. We summarise the efforts and achievements so far toward the development of rumour classification systems and conclude with suggestions for avenues for future research in social media mining for the detection and resolution of rumours.",Disinformation | Misinformation | Rumour classification | Rumour detection | Rumour resolution | Social media | Veracity,ACM Computing Surveys,2019-07-31,Review,"Zubiaga, Arkaitz;Aker, Ahmet;Bontcheva, Kalina;Liakata, Maria;Procter, Rob",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85042603476,10.1145/3170432,Recent advancements in event processing,"Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.",Complex event processing | Data stream processing | Event processing,ACM Computing Surveys,2019-03-31,Review,"Dayarathna, Miyuru;Perera, Srinath",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051440817,10.1145/3177849,Content-based music information retrieval (CB-MIR) and its applications toward the music industry: A review,"A huge increase in the number of digital music tracks has created the necessity to develop an automated tool to extract the useful information from these tracks. As this information has to be extracted from the contents of the music, it is known as content-based music information retrieval (CB-MIR). In the past two decades, several research outcomes have been observed in the area of CB-MIR. There is a need to consolidate and critically analyze these research findings to evolve future research directions. In this survey article, various tasks of CB-MIR and their applications are critically reviewed. In particular, the article focuses on eight MIR-related tasks such as vocal/non-vocal segmentation, artist identification, genre classification, raga identification, query-by-humming, emotion recognition, instrument recognition, and music clip annotation. The fundamental concepts of Indian classical music are detailed to attract future research on this topic. The article elaborates on the signal-processing techniques to extract useful features for performing specific tasks mentioned above and discusses their strengths as well as weaknesses. This article also points to some general research issues in CB-MIR and probable approaches toward their solutions so as to improve the efficiency of the existing CB-MIR systems. 2018 Copyright is held by the owner/author(s).",Artist identification | Indian classical music | Instrument identification | Music annotation | Music genre | Music mood estimation | Music recommendation system | Music related features | Open problems in music information retrieval | Query-by-humming/singing | Segmentation of vocal and non-vocal regions | Survey of music information retrieval,ACM Computing Surveys,2019-11-30,Review,"Srinivasa Murthy, Y. V.;Koolagudi, Shashidhar G.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051436184,10.1145/3179993,Co-simulation: A survey,"Modeling and simulation techniques are today extensively used both in industry and science. Parts of larger systems are, however, typically modeled and simulated by different techniques, tools, and algorithms. In addition, experts from different disciplines use various modeling and simulation techniques. Both these facts make it difficult to study coupled heterogeneous systems. Co-simulation is an emerging enabling technique, where global simulation of a coupled system can be achieved by composing the simulations of its parts. Due to its potential and interdisciplinary nature, cosimulation is being studied in different disciplines but with limited sharing of findings. In this survey, we study and survey the state-of-the-art techniques for co-simulation, with the goal of enhancing future research and highlighting the main challenges. To study this broad topic, we start by focusing on discrete-event-based co-simulation, followed by continuous-time-based co-simulation. Finally, we explore the interactions between these two paradigms, in hybrid co-simulation. To survey the current techniques, tools, and research challenges, we systematically classify recently published research literature on co-simulation, and summarize it into a taxonomy. As a result, we identify the need for finding generic approaches for modular, stable, and accurate coupling of simulation units, as well as expressing the adaptations required to ensure that the coupling is correct. C 2018 ACM.",Co-simulation | Compositionality | Simulation,ACM Computing Surveys,2019-05-31,Review,"Gomes, Cláudio;Thule, Casper;Broman, David;Larsen, Peter Gorm;Vangheluwe, Hans",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051432495,10.1145/3186727,Graph summarization methods and applications: A survey,"While advances in computing resources have made processing enormous amounts of data possible, human ability to identify patterns in such data has not scaled accordingly. Efficient computational methods for condensing and simplifying data are thus becoming vital for extracting actionable insights. In particular, while data summarization techniques have been studied extensively, only recently has summarizing interconnected data, or graphs, become popular. This survey is a structured, comprehensive overview of the state-of-the-art methods for summarizing graph data.We first broach the motivation behind and the challenges of graph summarization. We then categorize summarization approaches by the type of graphs taken as input and further organize each category by coremethodology. Finally, we discuss applications of summarization on real-world graphs and conclude by describing some open problems in the field.",Graph mining | Graph summarization,ACM Computing Surveys,2019-05-31,Review,"Liu, Yike;Safavi, Tara;Dighe, Abhilash;Koutra, Danai",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051748827,10.1145/3190616,Sequence-aware recommender systems,"Recommender systems are one of the most successful applications of data mining and machine-learning technology in practice. Academic research in the field is historically often based on the matrix completion problem formulation, where for each user-item-pair only one interaction (e.g., a rating) is considered. In many application domains, however, multiple user-item interactions of different types can be recorded over time. And, a number of recent works have shown that this information can be used to build richer individual user models and to discover additional behavioral patterns that can be leveraged in the recommendation process. In this work, we review existing works that consider information from such sequentially ordered user-item interaction logs in the recommendation process. Based on this review, we propose a categorization of the corresponding recommendation tasks and goals, summarize existing algorithmic solutions, discuss methodological approaches when benchmarking what we call sequence-aware recommender systems, and outline open challenges in the area.",Algorithms | Dataset | Evaluation | Recommendation | Sequence | Session | Trend,ACM Computing Surveys,2019-07-31,Review,"Quadrana, Massimo;Cremonesi, Paolo;Jannach, Dietmar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053269894,10.1145/3204940,Evaluation in contextual information retrieval: Foundations and recent advances within the challenges of context dynamicity and data privacy,"Context such as the user's search history, demographics, devices, and surroundings, has become prevalent in various domains of information seeking and retrieval such as mobile search, task-based search, and social search. While evaluation is central and has a long history in information retrieval, it faces the big challenge of designing an appropriate methodology that embeds the context into evaluation settings. In this article, we present a unified summary of a wide range of main and recent progress in contextual information retrieval evaluation that leverages diverse context dimensions and uses different principles, methodologies, and levels of measurements. More specifically, this survey article aims to fill two main gaps in the literature: First, it provides a critical summary and comparison of existing contextual information retrieval evaluation methodologies and metrics according to a simple stratification model; second, it points out the impact of context dynamicity and data privacy on the evaluation design. Finally, we recommend promising research directions for future investigations.",Context | Evaluation | Information retrieval | Relevance | Tasks | Users,ACM Computing Surveys,2019-07-31,Article,"Tamine, Lynda;Daoud, Mariam",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85052205246,10.1145/3197978,A survey on compiler autotuning using machine learning,"Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.",Autotuning | Compilers | Machine learning | Optimizations | Phase ordering,ACM Computing Surveys,2019-09-30,Review,"Ashouri, Amir H.;Killian, William;Cavazos, John;Palermo, Gianluca;Silvano, Cristina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85046832140,10.1007/s10664-018-9619-4,Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system,"Among the many different kinds of program repair techniques, one widely studied family of techniques is called test suite based repair. However, test suites are in essence input-output specifications and are thus typically inadequate for completely specifying the expected behavior of the program under repair. Consequently, the patches generated by test suite based repair techniques can just overfit to the used test suite, and fail to generalize to other tests. We deeply analyze the overfitting problem in program repair and give a classification of this problem. This classification will help the community to better understand and design techniques to defeat the overfitting problem. We further propose and evaluate an approach called UnsatGuided, which aims to alleviate the overfitting problem for synthesis-based repair techniques with automatic test case generation. The approach uses additional automatically generated tests to strengthen the repair constraint used by synthesis-based repair techniques. We analyze the effectiveness of UnsatGuided: 1) analytically with respect to alleviating two different kinds of overfitting issues; 2) empirically based on an experiment over the 224 bugs of the Defects4J repository. The main result is that automatic test generation is effective in alleviating one kind of overfitting, issue–regression introduction, but due to oracle problem, has minimal positive impact on alleviating the other kind of overfitting issue–incomplete fixing.",Automatic test case generation | Patch overfitting | Program repair | Synthesis-based repair,Empirical Software Engineering,2019-02-15,Article,"Yu, Zhongxing;Martinez, Matias;Danglot, Benjamin;Durieux, Thomas;Monperrus, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047198507,10.1007/s10664-018-9620-y,Software engineering in start-up companies: An analysis of 88 experience reports,"Context: Start-up companies have become an important supplier of innovation and software-intensive products. The flexibility and reactiveness of start-ups enables fast development and launch of innovative products. However, a majority of software start-up companies fail before achieving any success. Among other factors, poor software engineering could be a significant contributor to the challenges experienced by start-ups. However, the state-of-practice of software engineering in start-ups, as well as the utilization of state-of-the-art is largely an unexplored area. Objective: In this study we investigate how software engineering is applied in start-up context with a focus to identify key knowledge areas and opportunities for further research. Method: We perform a multi-vocal exploratory study of 88 start-up experience reports. We develop a custom taxonomy to categorize the reported software engineering practices and their interrelation with business aspects, and apply qualitative data analysis to explore influences and dependencies between the knowledge areas. Results: We identify the most frequently reported software engineering (requirements engineering, software design and quality) and business aspect (vision and strategy development) knowledge areas, and illustrate their relationships. We also present a summary of how relevant software engineering knowledge areas are implemented in start-ups and identify potentially useful practices for adoption in start-ups. Conclusions: The results enable a more focused research on engineering practices in start-ups. We conclude that most engineering challenges in start-ups stem from inadequacies in requirements engineering. Many promising practices to address specific engineering challenges exists, however more research on adaptation of established practices, and validation of new start-up specific practices is needed.",Experience reports | Software engineering practices | Software start-up,Empirical Software Engineering,2019-02-15,Article,"Klotins, Eriks;Unterkalmsteiner, Michael;Gorschek, Tony",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047389289,10.1007/s10664-018-9625-6,Understanding the behaviour of hackers while performing attack tasks in a professional setting and in a public challenge,"When critical assets or functionalities are included in a piece of software accessible to the end users, code protections are used to hinder or delay the extraction or manipulation of such critical assets. The process and strategy followed by hackers to understand and tamper with protected software might differ from program understanding for benign purposes. Knowledge of the actual hacker behaviours while performing real attack tasks can inform better ways to protect the software and can provide more realistic assumptions to the developers, evaluators, and users of software protections. Within Aspire, a software protection research project funded by the EU under framework programme FP7, we have conducted three industrial case studies with the involvement of professional penetration testers and a public challenge consisting of eight attack tasks with open participation. We have applied a systematic qualitative analysis methodology to the hackers’ reports relative to the industrial case studies and the public challenge. The qualitative analysis resulted in 459 and 265 annotations added respectively to the industrial and to the public challenge reports. Based on these annotations we built a taxonomy consisting of 169 concepts. They address the hacker activities related to (i) understanding code; (ii) defining the attack strategy; (iii) selecting and customizing the tools; and (iv) defeating the protections. While there are many commonalities between professional hackers and practitioners, we could spot many fundamental differences. For instance, while industrial professional hackers aim at elaborating automated and reproducible deterministic attacks, practitioners prefer to minimize the effort and try many different manual tasks. This analysis allowed us to distill a number of new research directions and potential improvements for protection techniques. In particular, considering the critical role of analysis tools, protection techniques should explicitly attack them, by exploiting analysis problems and complexity aspects that available automated techniques are bad at addressing.",Analysis tools | Attack model | Attack process | Attack taxonomy | Code understanding | Defeat protections | Empirical study | Hacker model | Reverse engineering | Software hacking | Software protection,Empirical Software Engineering,2019-02-15,Article,"Ceccato, Mariano;Tonella, Paolo;Basile, Cataldo;Falcarin, Paolo;Torchiano, Marco;Coppens, Bart;De Sutter, Bjorn",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047981983,10.1007/s10664-018-9629-2,What can Android mobile app developers do about the energy consumption of machine learning?,"Machine learning is a popular method of learning functions from data to represent and to classify sensor inputs, multimedia, emails, and calendar events. Smartphone applications have been integrating more and more intelligence in the form of machine learning. Machine learning functionality now appears on most smartphones as voice recognition, spell checking, word disambiguation, face recognition, translation, spatial reasoning, and even natural language summarization. Excited app developers who want to use machine learning on mobile devices face one serious constraint that they did not face on desktop computers or cloud virtual machines: the end-user’s mobile device has limited battery life, thus computationally intensive tasks can harm end users’ phone availability by draining batteries of their stored energy. Currently, there are few guidelines for developers who want to employ machine learning on mobile devices yet are concerned about software energy consumption of their applications. In this paper, we combine empirical measurements of different machine learning algorithm implementations with complexity theory to provide concrete and theoretically grounded recommendations to developers who want to employ machine learning on smartphones. We conclude that some implementations of algorithms, such as J48, MLP, and SMO, do generally perform better than others in terms of energy consumption and accuracy, and that energy consumption is well-correlated to algorithmic complexity. However, to achieve optimal results a developer must consider their specific application as many factors — dataset size, number of data attributes, whether the model will require updating, etc. — affect which machine learning algorithm and implementation will provide the best results.",Android | App development | Machine learning | Mobile | Software energy consumption,Empirical Software Engineering,2019-04-15,Article,"McIntosh, Andrea;Hassan, Safwat;Hindle, Abram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049571873,10.1007/s10664-018-9634-5,How do developers utilize source code from stack overflow?,"Technical question and answer Q&A platforms, such as Stack Overflow, provide a platform for users to ask and answer questions about a wide variety of programming topics. These platforms accumulate a large amount of knowledge, including hundreds of thousands lines of source code. Developers can benefit from the source code that is attached to the questions and answers on Q&A platforms by copying or learning from (parts of) it. By understanding how developers utilize source code from Q&A platforms, we can provide insights for researchers which can be used to improve next-generation Q&A platforms to help developers reuse source code fast and easily. In this paper, we first conduct an exploratory study on 289 files from 182 open-source projects, which contain source code that has an explicit reference to a Stack Overflow post. Our goal is to understand how developers utilize code from Q&A platforms and to reveal barriers that may make code reuse more difficult. In 31.5% of the studied files, developers needed to modify source code from Stack Overflow to make it work in their own projects. The degree of required modification varied from simply renaming variables to rewriting the whole algorithm. Developers sometimes chose to implement an algorithm from scratch based on the descriptions from Stack Overflow answers, even if there was an implementation readily available in the post. In 35.5% of the studied files, developers used Stack Overflow posts as an information source for later reference. To further understand the barriers of reusing code and to obtain suggestions for improving the code reuse process on Q&A platforms, we conducted a survey with 453 open-source developers who are also on Stack Overflow. We found that the top 3 barriers that make it difficult for developers to reuse code from Stack Overflow are: (1) too much code modification required to fit in their projects, (2) incomprehensive code, and (3) low code quality. We summarized and analyzed all survey responses and we identified that developers suggest improvements for future Q&A platforms along the following dimensions: code quality, information enhancement & management, data organization, license, and the human factor. For instance, developers suggest to improve the code quality by adding an integrated validator that can test source code online, and an outdated code detection mechanism. Our findings can be used as a roadmap for researchers and developers to improve code reuse.",,Empirical Software Engineering,2019-04-15,Article,"Wu, Yuhao;Wang, Shaowei;Bezemer, Cor Paul;Inoue, Katsuro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85051721021,10.1007/s10664-018-9636-3,An empirical study on the issue reports with questions raised during the issue resolving process,"An issue report describes a bug or a feature request for a software system. When resolving an issue report, developers may discuss with other developers and/or the reporter to clarify and resolve the reported issue. During this process, questions can be raised by developers in issue reports. Having unnecessary questions raised may impair the efficiency to resolve the reported issues, since developers may have to wait a considerable amount of time before receiving the answers to their questions. In this paper, we perform an empirical study on the questions raised in the issue resolution process to understand the further delay caused by these questions. Our goal is to gain insights on the factors that may trigger questions in issue reports. We build prediction models to capture such issue reports when they are submitted. Our results indicate that it is feasible to give developers an early warning as to whether questions will be raised in an issue report at the issue report filling time. We examine the raised questions in 154,493 issue reports of three large-scale systems (i.e., Linux, Firefox and Eclipse). First, we explore the topics of the raised questions. Then, we investigate four characteristics of issue reports with raised questions: (i) resolving time, (ii) number of developers, (iii) comments, and (iv) reassignments. Finally, we build a prediction model to predict if questions are likely to be raised by a developer in an issue report. We apply the random forest, logistic regression and Naïve Bayes models to predict the possibility of raising questions in issue reports. Our prediction models obtain an Area Under Curve (AUC) value of 0.78, 0.65, and 0.70 in the Linux, Firefox, and Eclipse systems, respectively. The most important variables according to our prediction models are the number of Carbon Copies (CC), the issue severity and priority, and the reputation of the issue reporter.",Bug fixing | Empirical study | Issue reports | Questions,Empirical Software Engineering,2019-04-15,Article,"Huang, Yonghui;da Costa, Daniel Alencar;Zhang, Feng;Zou, Ying",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85054196945,10.1007/s10664-018-9658-x,Comparing the influence of using feature-oriented programming and conditional compilation on comprehending feature-oriented software,"Several variability representations have been proposed over the years. Software maintenance in the presence of variability is known to be hard. One of the reasons is that maintenance tasks require a large amount of cognitive effort for program comprehension. In fact, the different ways of representing variability in source code might influence the comprehension process in different ways. Despite the differences, there is little evidence about how these variability representations – such as conditional-compilation directives or feature-oriented programming – influence program comprehension. Existing research has focused primarily on either understanding how code using modern paradigms evolves compared to the traditional way of realizing variability, namely conditional compilation, or on the aspects influencing the comprehension of conditional compilation only. We used two different programs implemented in Java and each of these variability representations. As Java does not support conditional compilation natively, we relied on the mimicking (i.e., preprocessing annotations in comments) that has been used in the literature. Our results show no significant statistical differences regarding the evaluated measures (correctness, understanding, or response time) in the tasks. Our heterogeneous sample allowed us to produce evidence about the influence of using CC and FOP variability representations on the aspects involved in the comprehension of feature-oriented software, while addressing bug-finding tasks.",Conditional compilation | Feature-oriented software development | FeatureHouse | Graduate students | Quasi-experiments | Replication,Empirical Software Engineering,2019-06-15,Article,"Rodrigues Santos, Alcemir;do Carmo Machado, Ivan;Santana de Almeida, Eduardo;Siegmund, Janet;Apel, Sven",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85055320828,10.1007/s10664-018-9660-3,Categorizing the Content of GitHub README Files,"README files play an essential role in shaping a developer’s first impression of a software repository and in documenting the software project that the repository hosts. Yet, we lack a systematic understanding of the content of a typical README file as well as tools that can process these files automatically. To close this gap, we conduct a qualitative study involving the manual annotation of 4,226 README file sections from 393 randomly sampled GitHub repositories and we design and evaluate a classifier and a set of features that can categorize these sections automatically. We find that information discussing the ‘What’ and ‘How’ of a repository is very common, while many README files lack information regarding the purpose and status of a repository. Our multi-label classifier which can predict eight different categories achieves an F1 score of 0.746. To evaluate the usefulness of the classification, we used the automatically determined classes to label sections in GitHub README files using badges and showed files with and without these badges to twenty software professionals. The majority of participants perceived the automated labeling of sections based on our classifier to ease information discovery. This work enables the owners of software repositories to improve the quality of their documentation and it has the potential to make it easier for the software development community to discover relevant information in GitHub README files.",Classification | Documentation | GitHub README files,Empirical Software Engineering,2019-06-15,Article,"Prana, Gede Artha Azriadi;Treude, Christoph;Thung, Ferdian;Atapattu, Thushari;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065125921,10.1007/s10664-019-09694-w,Classifying code comments in Java software systems,"Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how 14 diverse Java open and closed source software projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments; subsequently, we investigate how often each category occur by manually classifying more than 40,000 lines of code comments from the aforementioned projects. In addition, we investigate how to automatically classify code comments at line level into our taxonomy using machine learning; initial results are promising and suggest that an accurate classification is within reach, even when training the machine learner on projects different than the target one. Data and Materials [https://doi.org/10.5281/zenodo.2628361].",Code comment classification | Code comments usage | Dataset,Empirical Software Engineering,2019-06-15,Article,"Pascarella, Luca;Bruntink, Magiel;Bacchelli, Alberto",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85050366201,10.1007/s10664-018-9640-7,GreenScaler: training software energy models with automatic test generation,"Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications (apps) are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data we can make more accurate energy models. This data, however, was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. In this paper, we show that automatic random test generation with resource-utilization heuristics can be used successfully to build accurate software energy consumption models. Code coverage, although well-known as a heuristic for generating and selecting tests in traditional software testing, performs poorly at selecting energy hungry tests. We propose an accurate software energy model, GreenScaler, that is built on random tests with CPU-utilization as the test selection heuristic. GreenScaler not only accurately estimates energy consumption for randomly generated tests, but also for meaningful developer written tests. Also, the produced models are very accurate in detecting energy regressions between versions of the same app. This is directly helpful for the app developers who want to know if a change in the source code, for example, is harmful for the total energy consumption. We also show that developers can use GreenScaler to select the most energy efficient API when multiple APIs are available for solving the same problem. Researchers can also use our test generation methodology to further study how to build more accurate software energy models.",Automatic software testing | Energy modeling | Energy optimization | Machine learning | Mining software repositories | Software energy consumption | Software energy efficiency | Test generation,Empirical Software Engineering,2019-08-15,Article,"Chowdhury, Shaiful;Borle, Stephanie;Romansky, Stephen;Hindle, Abram",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069680730,10.1007/s10664-018-9668-8,A controlled experiment on time pressure and confirmation bias in functional software testing,"Context: Confirmation bias is a person’s tendency to look for evidence that strengthens his/her prior beliefs rather than refutes them. Manifestation of confirmation bias in software testing may have adverse effects on software quality. Psychology research suggests that time pressure could trigger confirmation bias. Objective: In the software industry, this phenomenon may deteriorate software quality. In this study, we investigate whether testers manifest confirmation bias and how it is affected by time pressure in functional software testing. Method: We performed a controlled experiment with 42 graduate students to assess manifestation of confirmation bias in terms of the conformity of their designed test cases to the provided requirements specification. We employed a one factor with two treatments between-subjects experimental design. Results: We observed, overall, participants designed significantly more confirmatory test cases as compared to disconfirmatory ones, which is in line with previous research. However, we did not observe time pressure as an antecedent to an increased rate of confirmatory testing behaviour. Conclusion: People tend to design confirmatory test cases regardless of time pressure. For practice, we find it necessary that testers develop self-awareness of confirmation bias and counter its potential adverse effects with a disconfirmatory attitude. We recommend further replications to investigate the effect of time pressure as a potential contributor to the manifestation of confirmation bias.",Cognitive biases | Confirmation bias | Experiment | Human factors | Software testing | Test quality | Time pressure,Empirical Software Engineering,2019-08-15,Article,"Salman, Iflaah;Turhan, Burak;Vegas, Sira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85059560862,10.1007/s10664-018-9677-7,An empirical study of DLL injection bugs in the Firefox ecosystem,"DLL injection is a technique used for executing code within the address space of another process by forcing the load of a dynamic-link library. In a software ecosystem, the interactions between the host and third-party software increase the maintenance challenges of the system and may lead to bugs. In this work, we empirically investigate bugs that were caused by third-party DLL injections into the Mozilla Firefox browser. Among the 103 studied DLL injection bugs, we found that 93 bugs (90.3%) led to crashes and 57 bugs (55.3%) were caused by antivirus software. Through a survey with third-party software vendors, we observed that some vendors did not perform any QA with pre-release versions nor intend to use a public API (WebExtensions) but insist on using DLL injection. To reduce DLL injection bugs, host software vendors may strengthen the collaboration with third-party vendors, e.g., build a publicly accessible validation test framework. Host software vendors may also use a whitelist approach to only allow vetted DLLs to inject.",DLL injection | Mining software repositories | Software ecosystem,Empirical Software Engineering,2019-08-15,Article,"An, Le;Castelluccio, Marco;Khomh, Foutse",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85060482732,10.1007/s10664-018-9671-0,Automatic query reformulation for code search using crowdsourced knowledge,"Traditional code search engines (e.g., Krugle) often do not perform well with natural language queries. They mostly apply keyword matching between query and source code. Hence, they need carefully designed queries containing references to relevant APIs for the code search. Unfortunately, preparing an effective search query is not only challenging but also time-consuming for the developers according to existing studies. In this article, we propose a novel query reformulation technique–RACK–that suggests a list of relevant API classes for a natural language query intended for code search. Our technique offers such suggestions by exploiting keyword-API associations from the questions and answers of Stack Overflow (i.e., crowdsourced knowledge). We first motivate our idea using an exploratory study with 19 standard Java API packages and 344K Java related posts from Stack Overflow. Experiments using 175 code search queries randomly chosen from three Java tutorial sites show that our technique recommends correct API classes within the Top-10 results for 83% of the queries, with 46% mean average precision and 54% recall, which are 66%, 79% and 87% higher respectively than that of the state-of-the-art. Reformulations using our suggested API classes improve 64% of the natural language queries and their overall accuracy improves by 19%. Comparisons with three state-of-the-art techniques demonstrate that RACK outperforms them in the query reformulation by a statistically significant margin. Investigation using three web/code search engines shows that our technique can significantly improve their results in the context of code search.",Code search | Crowdsourced knowledge | Keyword-API association | Query reformulation | Stack Overflow,Empirical Software Engineering,2019-08-15,Article,"Rahman, Mohammad M.;Roy, Chanchal K.;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85061506841,10.1007/s10664-018-9670-1,On the search for industry-relevant regression testing research,"Regression testing is a means to assure that a change in the software, or its execution environment, does not introduce new defects. It involves the expensive undertaking of rerunning test cases. Several techniques have been proposed to reduce the number of test cases to execute in regression testing, however, there is no research on how to assess industrial relevance and applicability of such techniques. We conducted a systematic literature review with the following two goals: firstly, to enable researchers to design and present regression testing research with a focus on industrial relevance and applicability and secondly, to facilitate the industrial adoption of such research by addressing the attributes of concern from the practitioners’ perspective. Using a reference-based search approach, we identified 1068 papers on regression testing. We then reduced the scope to only include papers with explicit discussions about relevance and applicability (i.e. mainly studies involving industrial stakeholders). Uniquely in this literature review, practitioners were consulted at several steps to increase the likelihood of achieving our aim of identifying factors important for relevance and applicability. We have summarised the results of these consultations and an analysis of the literature in three taxonomies, which capture aspects of industrial-relevance regarding the regression testing techniques. Based on these taxonomies, we mapped 38 papers reporting the evaluation of 26 regression testing techniques in industrial settings.",Industrial relevance | Recommendations | Regression testing | Systematic literature review | Taxonomy,Empirical Software Engineering,2019-08-15,Article,"Ali, Nauman bin;Engström, Emelie;Taromirad, Masoumeh;Mousavi, Mohammad Reza;Minhas, Nasir Mehmood;Helgesson, Daniel;Kunze, Sebastian;Varshosaz, Mahsa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065306542,10.1007/s10664-019-09717-6,AspectOCL: using aspects to ease maintenance of evolving constraint specification,"Constraints play an important role in Model-Driven Software Engineering. Industrial systems commonly exhibit cross-cutting behaviors in design artifacts. Aspect-orientation is a well-established approach to deal with cross-cutting behaviors and has been successfully used for programming and design languages. In model-driven software engineering, the presence of cross-cutting constraints makes it difficult to maintain constraints defined on the models of large-scale industrial systems. In this work, we improve our previous work on AspectOCL, which is an extension of OCL that allows modeling of cross-cutting constraints. We provide the abstract and concrete syntax of the language. We add support for new constructs such as composite aspects and invariant specification on a package. We also provide tool support for writing cross-cutting constraints using AspectOCL. To evaluate AspectOCL, we apply it on benchmark case studies from the OCL repository. The results show that by separating the cross-cutting constraints, the number of constructs in the constraint specifications can be reduced to a large amount. AspectOCL reduces the maintenance effort by up to 55% in one case study. To explore the impact on maintenance time and accuracy, we also perform a controlled experiment with 90 student subjects. The results show that AspectOCL has a small magnitude of improvement in terms of maintenance time when compared to OCL, whereas modifications to OCL specification are more accurate. The post-experiment survey indicates that the majority of subjects favored AspectOCL, but faced challenges in applying aspect-orientation to constraint specification due to a lack of prior exposure.",Aspect-orientation | Cross-cutting constraints | Empirical evaluation | Model-Driven Software Engineering (MDSE) | Object Constraint Language (OCL),Empirical Software Engineering,2019-08-15,Article,"Khan, Muhammad Uzair;Sartaj, Hassan;Iqbal, Muhammad Zohaib;Usman, Muhammad;Arshad, Numra",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85061227403,10.1007/s10664-019-09687-9,Studying the characteristics of logging practices in mobile apps: a case study on F-Droid,"Logging is a common practice in software engineering. Prior research has investigated the characteristics of logging practices in system software (e.g., web servers or databases) as well as desktop applications. However, despite the popularity of mobile apps, little is known about their logging practices. In this paper, we sought to study logging practices in mobile apps. In particular, we conduct a case study on 1,444 open source Android apps in the F-Droid repository. Through a quantitative study, we find that although mobile app logging is less pervasive than server and desktop applications, logging is leveraged in almost all studied apps. However, we find that there exist considerable differences between the logging practices of mobile apps and the logging practices in server and desktop applications observed by prior studies. In order to further understand such differences, we conduct a firehouse email interview and a qualitative annotation on the rationale of using logs in mobile app development. By comparing the logging level of each logging statement with developers’ rationale of using the logs, we find that all too often (35.4%), the chosen logging level and the rationale are inconsistent. Such inconsistency may prevent the useful runtime information to be recorded or may generate unnecessary logs that may cause performance overhead. Finally, to understand the magnitude of such performance overhead, we conduct a performance evaluation between generating all the logs and not generating any logs in eight mobile apps. In general, we observe a statistically significant performance overhead based on various performance metrics (response time, CPU and battery consumption). In addition, we find that if the performance overhead of logging is significantly observed in an app, disabling the unnecessary logs indeed provides a statistically significant performance improvement. Our results show the need for a systematic guidance and automated tool support to assist in mobile logging practices.",Logging performance | Logging practices | Mining software repositories | Software logs,Empirical Software Engineering,2019-12-01,Article,"Zeng, Yi;Chen, Jinfu;Shang, Weiyi;Chen, Tse Hsun (Peter)",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85066155686,10.1007/s10664-019-09721-w,ESSMArT way to manage customer requests,"Quality and market acceptance of software products is strongly influenced by responsiveness to customer requests. Once a customer request is received, a decision must be made whether to escalate it to the development team. Once escalated, the ticket must be formulated as a development task and assigned to a developer. To make the process more efficient and reduce the time between receiving and escalating the customer request, we aim to automate the complete customer request management process. We propose a holistic method called ESSMArT. The method performs text summarization, predicts ticket escalation, creates the ticket’s title and content, and ultimately assigns the ticket to an available developer. We began evaluating the method through an internal assessment of 4114 customer tickets from Brightsquid’s secure health care communication platform - Secure-Mail. Next, we conducted an external evaluation of the usefulness of the approach and concluded that: i) supervised learning based on context specific data performs best for extractive summarization; ii) Random Forest trained on a combination of conversation and extractive summarization works best for predicting escalation of tickets, with the highest precision (of 0.9) and recall (of 0.55). Through external evaluation, we furthermore found that ESSMArT provides suggestions that are 71% aligned with human ones. Applying the prototype implementation to 315 customer requests resulted in an average time reduction of 9.2 min per request. ESSMArT helps to make ticket management faster and with reduced effort for human experts. We conclude that ESSMArT not only expedites ticket management, but furthermore reduces human effort. ESSMArT can help Brightsquid to (i) minimize the impact of staff turnover and (ii) shorten the cycle from an issue being reported to a developer being assigned to fix it.",Automation | Case study evaluation | customer request management | Mining software repositories | Text summarization | Ticket assignment | Ticket escalation,Empirical Software Engineering,2019-12-01,Article,"Nayebi, Maleknaz;Dicke, Liam;Ittyipe, Ron;Carlson, Chris;Ruhe, Guenther",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85066627393,10.1007/s10664-019-09713-w,"To react, or not to react: Patterns of reaction to API deprecation","Application Programming Interfaces (API) provide reusable functionality to aid developers in the development process. The features provided by these APIs might change over time as the API evolves. To allow API consumers to peacefully transition from older obsolete features to new features, API producers make use of the deprecation mechanism that allows them to indicate to the consumer that a feature should no longer be used. The Java language designers noticed that no one was taking these deprecation warnings seriously and continued using outdated features. Due to this, they decided to change the implementation of this feature in Java 9. We question as to what extent this issue exists and whether the Java language designers have a case. We start by identifying the various ways in which an API consumer can react to deprecation. Following this we benchmark the frequency of the reaction patterns by creating a dataset consisting of data mined from 50 API consumers totalling 297,254 GitHub based projects and 1,322,612,567 type-checked method invocations. We see that predominantly consumers do not react to deprecation and we try to explain this behavior by surveying API consumers and by analyzing if the API’s deprecation policy has an impact on the consumers’ decision to react.",API usage | Application programming interface | Deprecation | Java,Empirical Software Engineering,2019-12-01,Article,"Sawant, Anand Ashok;Robbes, Romain;Bacchelli, Alberto",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85066992526,10.1007/s10664-019-09720-x,How does code style inconsistency affect pull request integration? An exploratory study on 117 GitHub projects,"GitHub is a popular code platform that provides infrastructures to facilitate collaborative development. A Pull Request (PR) is one of the key ideas to support collaboration. Developers are encouraged to submit PRs to ask for the integration of their contributions. In practice, not all submitted PRs can be integrated into the codebase by project maintainers. Existing studies have investigated factors affecting PR integration. Nevertheless, the code style of PRs, which is largely considered by project maintainers, has not been deeply studied yet. In this paper, we performed an exploratory analysis on the effect of code style on PR integration in GitHub. We modeled the code style via the inconsistency between a submitted PR and the existing code in its target codebase. Such modeling makes our study not limited by a specific definition of code style. We conducted our experiments on 50,092 closed PRs in 117 Java projects. Our findings show that: (1) There indeed exists code style inconsistency between PRs and the codebase. (2) Several code style criteria on how to use spaces or indents, make comments, and write code lines with a suitable length, tend to show more inconsistency among PRs. (3) A PR that is consistent with the current code style tends to be merged into the codebase more easily. (4) A PR that violates the current code style is likely to take more time to get closed. Our study shows evidence to developers about how to deliver better contributions to facilitate efficient collaboration.",Code style inconsistency | Exploratory study | Pull request,Empirical Software Engineering,2019-12-01,Article,"Zou, Weiqin;Xuan, Jifeng;Xie, Xiaoyuan;Chen, Zhenyu;Xu, Baowen",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85067484623,10.1016/j.jss.2019.06.001,Summarizing vulnerabilities’ descriptions to support experts during vulnerability assessment activities,"Vulnerabilities affecting software and systems have to be promptly fixed, to prevent violations to integrity, availability and confidentiality policies of targeted organizations. Once a vulnerability is discovered, it is published on the Common Vulnerabilities and Exposures (CVE) database, freely available on the web. However, vulnerabilities are described using natural language, which makes them hard to be automatically interpreted by machines. As a consequence, vulnerability assessment activities tend to be time-consuming and imprecise, as the assessors must manually read the majority of the vulnerabilities concerning the perimeter to be protected, to make a decision on which vulnerabilities have the highest priority for patching. In this paper we present CVErizer, an approach able to automatically generate summaries of daily posted vulnerabilities and categorize them according to a taxonomy modeled for industry. We empirically assess the classification capabilities of the approach on a set of 3369 pre-labeled CVE records and perform an end-to-end evaluation of CVErizer summaries involving 15 cybersecurity master students and 4 professional security experts. Our study demonstrates the high performance of the proposed approach in correctly extracting and classifying information from CVE descriptions. Summaries are also considered highly useful for helping analysts during the vulnerability assessment processes.",Natural language processing | Software maintenance | Software security | Summarization,Journal of Systems and Software,2019-10-01,Article,"Russo, Ernesto Rosario;Di Sorbo, Andrea;Visaggio, Corrado A.;Canfora, Gerardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85068222830,10.1016/j.jss.2019.06.073,Do concern mining tools really help requirements analysts? An empirical study of the vetting process,"Software requirements are often described in natural language because they are useful to communicate and validate. Due to their focus on particular facets of a system, this kind of specifications tends to keep relevant concerns (also known as early aspects) from the analysts’ view. These concerns are known as crosscutting concerns because they appear scattered among documents. Concern mining tools can help analysts to uncover concerns latent in the text and bring them to their attention. Nonetheless, analysts are responsible for vetting tool-generated solutions, because the detection of concerns is currently far from perfect. In this article, we empirically investigate the role of analysts in the concern vetting process, which has been little studied in the literature. In particular, we report on the behavior and performance of 55 subjects in three case-studies working with solutions produced by two different tools, assessed in terms of binary classification measures. We discovered that analysts can improve “bad” solutions to a great extent, but performed significantly better with “good” solutions. We also noticed that the vetting time is not a decisive factor to their final accuracy. Finally, we observed that subjects working with solutions substantially different from those of existing tools (better recall) can also achieve a good performance.",Crosscutting concern | Empirical study | Human behavior | Requirements engineering | Tool support | Use case specifications,Journal of Systems and Software,2019-10-01,Article,"Rago, Alejandro;Diaz-Pace, J. Andres;Marcos, Claudia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85068484825,10.1016/j.jss.2019.07.001,M<sup>3</sup> - A hybrid measurement-modeling approach for CPU-bound applications on cross-platform architectures,"Predicting performance of CPU intensive applications for a target platform is of significant importance to IT industries. However, the target hardware platform for which we want to predict the performance is often different from the testbed on which the application performance measurements are done, and may not be unavailable for deployment for various practical reasons. This paper presents M3, a Measure-Measure-Model method, which uses a pipeline of three steps to address this problem. The methodology starts with measuring CPU service demands of the application on the testbed. Then, it builds clones that mimic the application code in terms of the type of operations, the number and size of network calls with external servers and API calls made at different layers of the technology stack. The clones are simple and easy to deploy, yet demonstrate the same speedup factor between the source and the target as the original application. The clones are then deployed on the testbed and on the target to estimate the application CPU service demand under light load generation. In the final step, this estimated CPU service demand is fed into specific performance modeling tools that are capable of predicting application performance on the target under arbitrary higher workload. Predictions made using this approach are validated against direct measurements made on five applications in Java and PHP, and on a number of combinations of testbed and target platforms (Intel and AMD servers) and the prediction error is always less than 20%.",Benchmark | Cross-platform | Modeling | Multi-tier | Performance | Prediction,Journal of Systems and Software,2019-10-01,Article,"Duttagupta, Subhasri;Apte, Varsha;Gawali, Devidas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069005716,10.1016/j.jss.2019.06.100,Reproducing performance bug reports in server applications: The researchers’ experiences,"Performance is one of the key aspects of non-functional qualities as performance bugs can cause significant performance degradation and lead to poor user experiences. While bug reports are intended to help developers to understand and fix bugs, they are also extensively used by researchers for finding benchmarks to evaluate their testing and debugging approaches. Although researchers spend a considerable amount of time and effort in finding usable performance bugs from bug repositories, they often get only a few. Reproducing performance bugs is difficult even for performance bugs that are confirmed by developers with domain knowledge. The amount of information disclosed in a bug report may not always be sufficient to reproduce the performance bug for researchers, and thus hinders the usability of bug repository as the resource for finding benchmarks. In this paper, we study the characteristics of confirmed performance bugs by reproducing them using only informations available from the bug report to examine the challenges of bug reproduction from the perspective of researchers. We spent more than 800 h over the course of six months to study and to try to reproduce 93 confirmed performance bugs, which are randomly sampled from two large-scale open-source server applications. We (1) studied the characteristics of the reproduced performance bug reports; (2) summarized the causes of failed-to-reproduce performance bug reports from the perspective of researchers by reproducing bugs that have been solved in bug reports; (3) shared our experience on suggesting workarounds to improve the bug reproduction success rate; (4) delivered a virtual machine image that contains a set of 17 ready-to-execute performance bug benchmarks. The findings of our study provide guidance and a set of suggestions to help researchers to understand, evaluate, and successfully replicate performance bugs.",Bug characteristics study | Experience report | Performance bug reproduction,Journal of Systems and Software,2019-10-01,Article,"Han, Xue;Carroll, Daniel;Yu, Tingting",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069201861,10.1016/j.jss.2019.07.033,Modeling stack overflow tags and topics as a hierarchy of concepts,"Developers rely on online Q&A forums to look up technical solutions, to pose questions on implementation problems, and to enhance their community profile by contributing answers. Many popular developer communication platforms, such as the Stack Overflow Q&A forum, require threads of discussion to be tagged by their contributors for easier lookup in both asking and answering questions. In this paper, we propose to leverage Stack Overflow's tags to create a hierarchical organization of concepts discussed on this platform. The resulting concept hierarchy couples tags with a model of their relevancy to prospective questions and answers. For this purpose, we configure and apply a supervised multi-label hierarchical topic model to Stack Overflow questions and demonstrate the quality of the model in several ways: by identifying tag synonyms, by tagging previously unseen Stack Overflow posts, and by exploring how the hierarchy could aid exploratory searches of the corpus. The results suggest that when traversing the inferred hierarchical concept model of Stack Overflow the questions become more specific as one explores down the hierarchy and more diverse as one jumps to different branches. The results also indicate that the model is an improvement over the baseline for the detection of tag synonyms and that the model could enhance existing ensemble methods for suggesting tags for new questions. The paper indicates that the concept hierarchy as a modeling imperative can create a useful representation of the Stack Overflow corpus. This hierarchy can be in turn integrated into development tools which rely on information retrieval and natural language processing, and thereby help developers more efficiently navigate crowd-sourced online documentation.",Concept hierarchy | Entropy-based search evaluation | Hierarchical topic model | Stack overflow | Tag prediction | Tag synonym identification,Journal of Systems and Software,2019-10-01,Article,"Chen, Hui;Coogle, John;Damevski, Kostadin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069831870,10.1016/j.jss.2019.07.087,Augmenting Java method comments generation with context information based on neural networks,"Code comments are crucial to program comprehension. In this paper, we propose a novel approach ContextCC to automatically generate concise comments for Java methods based on neural networks, leveraging techniques of program analysis and natural language processing. Firstly, ContextCC employs program analysis techniques, especially abstract syntax tree parsing, to extract context information including methods and their dependency. Secondly, it filters code and comments out of the context information to build up a high-quality data set based on a set of pre-defined templates and rules. Finally, ContextCC trains a code comment generation model based on recurrent neural networks. Experiments are conducted on Java projects crawled from GitHub. We show empirically that the performance of ContextCC is superior to state-of-the-art baseline methods.",Comment generation | Natural language processing | Neural networks,Journal of Systems and Software,2019-10-01,Article,"Zhou, Yu;Yan, Xin;Yang, Wenhua;Chen, Taolue;Huang, Zhiqiu",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85070519994,10.1016/j.jss.2019.07.100,A systematic literature review of techniques and metrics to reduce the cost of mutation testing,"Historically, researchers have proposed and applied many techniques to reduce the cost of mutation testing. It has become difficult to find all techniques and to understand the cost-benefit tradeoffs among them, which is critical to transitioning this technology to practice. This paper extends a prior workshop paper to summarize and analyze the current knowledge about reducing the cost of mutation testing through a systematic literature review. We selected 175 peer-reviewed studies, from which 153 present either original or updated contributions. Our analysis resulted in six main goals for cost reduction and 21 techniques. In the last decade, a growing number of studies explored techniques such as selective mutation, evolutionary algorithms, control-flow analysis, and higher-order mutation. Furthermore, we characterized 18 metrics, with particular interest in the number of mutants to be executed, test cases required, equivalent mutants generated and detected, and mutant execution speedup. We found that cost reduction for mutation is increasingly becoming interdisciplinary, often combining multiple techniques. Additionally, measurements vary even for studies that use the same techniques. Researchers can use our results to find more detailed information about particular techniques, and to design comparable and reproducible experiments.",Cost reduction | Mutation analysis | Mutation testing | Systematic review,Journal of Systems and Software,2019-11-01,Article,"Pizzoleto, Alessandro Viola;Ferrari, Fabiano Cutigi;Offutt, Jeff;Fernandes, Leo;Ribeiro, Márcio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85070923402,10.1016/j.jss.2019.110397,An automated change impact analysis approach for User Requirements Notation models,"Requirements and their models often evolve to reflect changing needs, technologies, and regulations. The decision to implement proposed changes to requirements models relies on means to capture and analyze the potential impact of such changes. The User Requirements Notation (URN) is a standardized requirements modeling language that incorporates two complementary views based on Use Case Maps (UCM), for expressing scenarios and processes bound to architectural components, and on the Goal-oriented Requirement Language (GRL), for capturing the goals of actors and their relationships. This paper presents a new Change Impact Analysis (CIA) approach for URN models. Following a proposed change, this approach helps identify potentially impacted URN constructs within the selected GRL/UCM view, as well as throughout other view elements connected with URN links. This URN-oriented CIA approach is implemented as an extension of the jUCMNav modeling environment, and its applicability is demonstrated using an illustrative URN specification and three real and publicly available specifications. Furthermore, an empirical study involving 10 participants is used to assess the accuracy of this approach in identifying impacted URN elements upon specification changes. Results indicate excellent accuracy and a significant reduction in user-perceived difficulty when estimating the impact of changes in URN specifications.",Change impact analysis | Goal-Oriented Requirement Language (GRL) | jUCMNav | Requirements | Use Case Map (UCM) | User Requirements Notation (URN),Journal of Systems and Software,2019-11-01,Article,"Alkaf, Hasan;Hassine, Jameleddine;Binalialhag, Taha;Amyot, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85070497510,10.1016/j.jss.2019.07.008,A dataflow-driven approach to identifying microservices from monolithic applications,"Microservices architecture emphasizes employing multiple small-scale and independently deployable microservices, rather than encapsulating all function capabilities into one monolith. Correspondingly, microservice-oriented decomposition, which has been identified to be an extremely challenging task, plays a crucial and prerequisite role in developing microservice-based systems. To address the challenges in such a task, we propose a dataflow-driven semi-automatic decomposition approach. In particular, a four-step decomposition procedure is defined: (1) conduct the business requirement analysis to generate use case and business logic specification; (2) construct the fine-grained Data Flow Diagrams (DFD) and the process-datastore version of DFD (DFDPS) representing the business logics; (3) extract the dependencies between processes and datastores into decomposable sentence sets; and (4) identify candidate microservices by clustering processes and their closely related datastores into individual modules from the decomposable sentence sets. To validate this microservice-oriented decomposition approach, we performed a case study on Cargo Tracking System that is a typical case decomposed by other microservices identification methods (Service Cutter and API Analysis), and made comparisons in terms of specific coupling and cohesion metrics. The results show that the proposed dataflow-driven decomposition approach can recommend microservice candidates with sound coupling and cohesion through a rigorous and easy-to-operate implementation with semi-automatic support.",Business logic(s) | Data flow | Decomposition | Microservices | Monolith | Software engineering,Journal of Systems and Software,2019-11-01,Article,"Li, Shanshan;Zhang, He;Jia, Z.;Li, Z.;Zhang, C.;Li, J.;Gao, Q.;Ge, Jidong;Shan, Zhihao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072228048,10.1016/j.jss.2019.110419,Automated code-based test selection for software product line regression testing,"Regression testing for software product lines (SPLs) is challenging and can be expensive because it must ensure that all the products of a product family are correct whenever changes are made. SPL regression testing can be made efficient through a test case selection method that selects only the test cases relevant to the changes. Some approaches for SPL test case selection have been proposed but either they were not efficient by requiring intervention from human experts or they cannot be used if requirements specifications, architecture and/or traceabilities for test cases are not available or partially eroded. To address these limitations, we propose an automated method of source code-based regression test selection for SPLs. Our method reduces the repetition of the selection procedure and minimizes the in-depth analysis effort for source code and test cases based on the commonality and variability of a product family. Evaluation results of our method using six product lines show that our method reduces the overall time to perform regression testing by 14.8% ∼ 49.1% on average compared to an approach of repetitively applying Ekstazi, which is the state-of-the-art regression test selection method for a single product, to each product of a product family.",Product lines testing | Regression test selection | Software evolution | Software maintenance,Journal of Systems and Software,2019-12-01,Article,"Jung, Pilsu;Kang, Sungwon;Lee, Jihyun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072521286,10.1016/j.jss.2019.110422,Collaborative configuration approaches in software product lines engineering: A systematic mapping study,"In the context of software product line engineering, collaborative configuration is a decision-making process where multiple stakeholders contribute in building a single product specification. Several approaches addressing collaboration during configuration have already been proposed, but we still have little hard evidence about their effectiveness and little understanding about how collaborative configuration process should be carried out. This paper presents a classification framework to help understand existing collaborative configuration approaches. To elaborate it, a systematic mapping study was conducted guided by three research questions and 41 primary studies was selected out of 238 identified ones. The proposed framework is composed of four dimensions capturing main aspects related to configuration approaches: purpose, collaboration, process and tool. Each dimension is itself multi-faceted and a set of attributes is associated to each facet. Using this framework, we position and classify existing approaches, structure the representation of each approach characteristics, highlight their strengths and weaknesses, compare them to each other, and identify open issues. This study gives a solid foundation for classifying existing and future approaches for product lines collaborative configuration. Researchers and practitioners can use our framework for identifying existing research/technical gaps to attack, better scoping their own contributions, or understanding existing ones.",Collaborative configuration | Framework | Product lines | Systematic mapping study,Journal of Systems and Software,2019-12-01,Article,"Edded, Sabrine;Sassi, Sihem Ben;Mazo, Raúl;Salinesi, Camille;Ghezala, Henda Ben",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072605849,10.1016/j.jss.2019.110425,Characterization of implied scenarios as families of common behavior,"Concurrent systems face a threat to their reliability in emergent behaviors, which are not included in the specification but can happen during runtime. When concurrent systems are modeled in a scenario-based manner, it is possible to detect emergent behaviors as implied scenarios (ISs) which, analogously, are unexpected scenarios that can happen due to the concurrent nature of the system. Until now, the process of dealing with ISs can demand significant time and effort from the user, as they are detected and dealt with in a one by one basis. In this paper, a new methodology is proposed to deal with various ISs at a time, by finding Common Behaviors (CBs) among them. Additionally, we propose a novel way to group CBs into families utilizing a clustering technique using the Smith-Waterman algorithm as a similarity measure. Thus allowing the removal of multiple ISs with a single fix, decreasing the time and effort required to achieve higher system reliability. A total of 1798 ISs were collected across seven case studies, from which 14 families of CBs were defined. Consequently, only 14 constraints were needed to resolve all collected ISs, applying our approach. These results support the validity and effectiveness of our methodology.",Concurrent systems | Dependability | Hierarchical clustering | Implied scenarios | Smith-Waterman algorithm,Journal of Systems and Software,2019-12-01,Article,"de Melo, Caio Batista;Cançado, André Luiz Fernandes;Rodrigues, Genaína Nunes",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072534077,10.1016/j.jss.2019.08.001,Industry requirements for FLOSS governance tools to facilitate the use of open source software in commercial products,"Virtually all software products incorporate free/libre and open source software (FLOSS) components. However, ungoverned use of FLOSS components can result in legal and financial risks, and risks to a firm's intellectual property. To avoid these risks, companies must govern their FLOSS use through open source governance processes and by following industry best practices. A particular challenge is license compliance. To manage the complexity of governance and compliance, companies should use tools and well-defined processes. This paper investigates and presents industry requirements for FLOSS governance tools, followed by an evaluation of the suggested requirements. We chose eleven companies with an advanced understanding of open source governance and interviewed their FLOSS governance experts to derive a theory of industry requirements for tooling. We extended our previous work adding the requirement category on the architecture model for software products. We then analyzed the features of leading governance tools and used this analysis to evaluate two categories of our theory: FLOSS license scanning and FLOSS components in product bills of materials. The result is a list of FLOSS governance requirements. For practical relevance, we cast our theory as a requirements specification for FLOSS governance tools.",Company requirements for FLOSS tools | FLOSS | FLOSS governance tools | FOSS | Open source governance | Open source software,Journal of Systems and Software,2019-12-01,Article,"Harutyunyan, Nikolay;Bauer, Andreas;Riehle, Dirk",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072530259,10.1016/j.jss.2019.110423,Measuring the reusability of software components using static analysis metrics and reuse rate information,"Nowadays, the continuously evolving open-source community and the increasing demands of end users are forming a new software development paradigm; developers rely more on reusing components from online sources to minimize the time and cost of software development. An important challenge in this context is to evaluate the degree to which a software component is suitable for reuse, i.e. its reusability. Contemporary approaches assess reusability using static analysis metrics by relying on the help of experts, who usually set metric thresholds or provide ground truth values so that estimation models are built. However, even when expert help is available, it may still be subjective or case-specific. In this work, we refrain from expert-based solutions and employ the actual reuse rate of source code components as ground truth for building a reusability estimation model. We initially build a benchmark dataset, harnessing the power of online repositories to determine the number of reuse occurrences for each component in the dataset. Subsequently, we build a model based on static analysis metrics to assess reusability from five different properties: complexity, cohesion, coupling, inheritance, documentation and size. The evaluation of our methodology indicates that our system can effectively assess reusability as perceived by developers.",Code reuse | Developer-perceived reusability | Reusability estimation | Static analysis metrics,Journal of Systems and Software,2019-12-01,Article,"Papamichail, Michail D.;Diamantopoulos, Themistoklis;Symeonidis, Andreas L.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072785004,10.1016/j.jss.2019.110431,"Survey-based investigation, feature extraction and classification of Greek municipalities maturity for open source adoption and migration prospects","While FOSS solutions have attracted a significant amount of attention, alongside with necessary and growing IT related expenditure for the digitalization of public administration, authorities face the question of whether migration to such solutions is feasible and/or worthwhile. The purpose of the presented work is to analyze existing domain research and extract key features, grouped in three major areas (namely Readiness, Ease of Use and Gain from migration), that should be investigated prior to a migration attempt. Following and building upon an extensive survey on Greek municipalities, the latter are categorized via the k-means non-supervised machine learning method to 3 levels of maturity regarding candidate participation in relevant projects. A combined scoring approach, based on similar features of the target FOSS solution as well as the aforementioned municipality categorization, is presented in order to detect a priori good candidate combinations (municipality and software) for minimizing a migration project risk. The method is validated through the aforementioned survey on municipalities with confirmed FOSS usage, indicating that selection in the proposed organized manner can aid in harvesting FOSS benefits. Furthermore, it is compared against a popular maturity model method (BPMMM) in order to comment on the applicability and classification process.",Classification | FOSS | Maturity models | Migration | Risk analysis | Survey,Journal of Systems and Software,2019-12-01,Article,"Koloniaris, Stavros;Kousiouris, George;Anagnostopoulos, Dimosthenis;Nikolaidou, Mara;Tserpes, Konstantinos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056241321,10.1016/j.jss.2018.11.007,A distributed data management system to support large-scale data analysis,"Distributed data management is a key technology to enable efficient massive data processing and analysis in cluster-computing environments. Specifically, in environments where the data volumes are beyond the system capabilities, big data files are required to be summarized by representative samples with the same statistical properties as the whole dataset. This paper proposes a big data management system (BDMS) based on distributed random sample data blocks. It presents a high-level architecture design of the BDMS which extends the current distributed file systems. This system offers certain functionalities for block-level management such as statistically-aware data partitioning, data blocks organization, and data blocks selection. This paper also presents a round-random partitioning scheme to represent a big dataset as a set of non-overlapping data blocks; each block is a random sample of the whole dataset. Based on the presented scheme, two algorithms are introduced as an implementation strategy to convert the HDFS blocks of a big file into a set of random sample data blocks which is also stored in HDFS. The experimental results show that the execution time of partitioning operation is acceptable in the real applications because this operation is only performed once on each input data file.",Big data | Data management | Distributed and parallel processing | Random sample partition | Randomness,Journal of Systems and Software,2019-02-01,Article,"Emara, Tamer Z.;Huang, Joshua Zhexue",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056773833,10.1016/j.jss.2018.11.016,Introduction to the special issue on affect awareness in software engineering,,,Journal of Systems and Software,2019-02-01,Editorial,"Novielli, Nicole;Begel, Andrew;Maalej, Walid",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85062227387,10.1016/j.jss.2019.02.055,Synthesizing tradeoff spaces with quantitative guarantees for families of software systems,"Designing software in a way that guarantees run-time behavior while achieving an acceptable balance among multiple quality attributes is an open problem. Providing guarantees about the satisfaction of the same requirements under uncertain environments is even more challenging. Tools and techniques to inform engineers about poorly-understood design spaces in the presence of uncertainty are needed, so that engineers can explore the design space, especially when tradeoffs are crucial. To tackle this problem, we describe an approach that combines synthesis of spaces of system design alternatives from formal specifications of architectural styles with probabilistic formal verification. The main contribution of this paper is a formal framework for specification-driven synthesis and analysis of design spaces that provides formal guarantees about the correctness of system behaviors and satisfies quantitative properties (e.g., defined over system qualities) subject to uncertainty, which is treated as a first-class entity. We illustrate our approach in two case studies: a service-based adaptive system and a mobile robotics architecture. Our results show how the framework can provide useful insights into how average case probabilistic guarantees can differ from worst case guarantees, emphasizing the relevance of combining quantitative formal verification methods with structural synthesis, in contrast with techniques based on simulation and dynamic analysis that can only provide estimates about average case probabilistic properties.",Architectural style | Architecture synthesis | Probabilistic model checking | Quantitative guarantees | Tradeoff analysis | Uncertainty,Journal of Systems and Software,2019-06-01,Article,"Cámara, Javier;Garlan, David;Schmerl, Bradley",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85062242630,10.1016/j.jss.2019.02.056,A survey of self-admitted technical debt,"Technical Debt is a metaphor used to express sub-optimal source code implementations that are introduced for short-term benefits that often need to be paid back later, at an increased cost. In recent years, various empirical studies have focused on investigating source code comments that indicate Technical Debt often referred to as Self-Admitted Technical Debt (SATD). Since the introduction of SATD as a concept, an increasing number of studies have examined various aspects pertaining to SATD. Therefore, in this paper we survey research work on SATD, analyzing the characteristics of current approaches and techniques for SATD detection, comprehension, and repayment. To motivate the submission of novel and improved work, we compile tools, resources, and data sets made available to replicate or extend current SATD research. To set the stage for future work, we identify open challenges in the study of SATD, areas that are missing investigation, and discuss potential future research avenues.",Literature survey | Self admitted technical debt | Software maintenance | Source code comments,Journal of Systems and Software,2019-06-01,Review,"Sierra, Giancarlo;Shihab, Emad;Kamei, Yasutaka",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85062387758,10.1016/j.jss.2019.01.044,Feature-oriented contract composition,"A software product line comprises a set of products that share a common code base, but vary in specific characteristics called features. Ideally, features of a product line are developed in isolation and composed subsequently. Product lines are increasingly used for safety–critical software, for which quality assurance becomes indispensable. While the verification of product lines gained considerable interest in research over the last decade, the subject of how to specify product lines is only covered rudimentarily. A challenge to overcome is composition; similar to inheritance in object-oriented programming, features of a product line may refine other features along with their specifications. To investigate how refinement and composition of specifications can be established, we derive a notion of feature-oriented contracts comprising preconditions, postconditions, and framing conditions of a method. We discuss six mechanisms to perform contract composition between original and refining contracts. Moreover, we identify and discuss desired properties for contract composition and evaluate which properties are established by which mechanism. Our three main insights are that (a) contract refinement is seldom but crucial, (b) the Liskov principle does not apply to features, and (c) it is sufficient to accommodate techniques from object-orientation in the contract-composition mechanisms for handling frame refinements.",Deductive verification | Design by contract | Feature-oriented programming | Formal methods | Software product lines,Journal of Systems and Software,2019-06-01,Article,"Thüm, Thomas;Knüppel, Alexander;Krüger, Stefan;Bolle, Stefanie;Schaefer, Ina",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85063458121,10.1016/j.jss.2019.03.010,Automatically detecting the scopes of source code comments,"Comments convey useful information about the system functionalities and many methods for software engineering tasks take comments as an important source for many software engineering tasks such as code semantic analysis, code reuse and so on. However, unlike structural doc comments, it is challenging to identify the relationship between the functional semantics of the code and its corresponding textual descriptions nested inside the code and apply it to automatic analyzing and mining approaches in software engineering tasks efficiently. In this paper, we propose a general method for the detection of source code comment scopes. Based on machine learning, our method utilized features of code snippets and comments to detect the scopes of source code comments automatically in Java programs. On the dataset of comment-statement pairs from 4 popular open source projects, our method achieved a high accuracy of 81.45% in detecting the scopes of comments. Furthermore, the results demonstrated the feasibility and effectiveness of our comment scope detection method on new projects. Moreover, our method was applied to two specific software engineering tasks in our studies: analyzing software repositories for outdated comment detection and mining software repositories for comment generation. As a general approach, our method provided a solution to comment-code mapping. It improved the performance of baseline methods in both tasks, which demonstrated that our method is conducive to automatic analyzing and mining approaches on software repositories.",Comment scope detection | Machine learning | Software repositories,Journal of Systems and Software,2019-07-01,Article,"Chen, Huanchao;Huang, Yuan;Liu, Zhiyong;Chen, Xiangping;Zhou, Fan;Luo, Xiaonan",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85063969297,10.1016/j.jss.2019.04.004,Communication channels in safety analysis: An industrial exploratory case study,"Context: Safety analysis is a predominant activity in developing safety-critical systems. It is a highly cooperative task among multiple functional departments due to increasingly sophisticated safety-critical systems and close-knit development processes. Communication occurs pervasively. Motivation: Effective communication channels among multiple functional departments influence safety analysis quality as well as a safe product delivery. However, the use of communication channels during safety analysis is sometimes arbitrary and poses challenges. Objective: In this article, we aim to investigate the existing communication channels, their usage frequencies, their purposes and challenges during safety analysis in industry. Method: We conducted a multiple case study by surveying 39 experts and interviewing 21 experts in safety-critical companies including software developers, quality engineers and functional safety managers. Direct observations and documentation review were also conducted. Results: Popular communication channels during safety analysis include formal meetings, project coordination tools, documentation and telephone. Email, personal discussion, training, internal communication software and boards are also in use. Training involving safety analysis happens 1-4 times per year, while other aforementioned communication channels happen ranging from 1-4 times per day to 1-4 times per month. We summarise 28 purposes of using these aforementioned communication channels. Communication happens mostly for the purpose of clarifying safety requirements, fixing temporary problems, conflicts and obstacles and sharing safety knowledge. The top 10 challenges are: (1) sensitiveness and confidentiality of safety analysis information; (2) fragmented safety analysis information; (3) inconsistent safety analysis information; (4) asynchronous channels; (5) a lack of tool support; (6) misunderstanding between developers and safety analysts; (7) language, geographic and culture limitations; (8) unwillingness to communicate (groupthink); (9) storage, authority, regulation and monitoring of safety analysis information; (10) a lack of documentation concerning safety analysis to support communication. Conclusion: During safety analysis, to use communication channels effectively and avoid challenges, a clear purpose of communication during safety analysis should be established at the beginning. We have limitations primarily on the research context namely the scope of domains, participants and countries. To derive countermeasures of fixing the top 10 challenges are potential next steps.",Case study | Challenges | Communication | Purposes | Safety analysis | Safety-critical systems,Journal of Systems and Software,2019-07-01,Article,"Wang, Yang;Graziotin, Daniel;Kriso, Stefan;Wagner, Stefan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064228488,10.1016/j.jss.2019.04.027,A systematic literature review on crowdsourcing in software engineering,"Background: Crowdsourcing outsources a task to large groups of people by open call format, and it recently plays significant role for software practitioners. Aim: The purpose of this study is to conduct a comprehensive overview on crowdsourcing in software engineering (CSE), concerning business models, tools, platforms, software development processes, and software economics. Method: We conducted a systematic literature review on CSE. We identified 158 relevant studies and 6 secondary studies. We further reviewed 67 primary studies that passed our quality assessment criteria. We defined 10 research questions and synthesized different approaches used in primary studies regarding each question. Results: Majority of studies report the application of crowdsourcing for coding and testing tasks. Crowdsourcing follows a unique methodology in which project planning, task specification and deployment have more emphasis. There is not enough literature on effort estimation approaches in CSE and associated cost factors. Complexity of the task and its expected duration play significant role in estimation. Conclusions: Future studies should focus more on economic models, experience reports, specific software development methodologies, and strategic pricing mechanism for CSE.",Crowdsourcing | Crowdsourcing in software engineering | Empirical software engineering | Systematic literature review,Journal of Systems and Software,2019-07-01,Article,"Sarı, Aslı;Tosun, Ayşe;Alptekin, Gülfem Işıklar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065200156,10.1016/j.jss.2019.04.060,Using acceptance tests to predict files changed by programming tasks,"In a collaborative development context, conflicting code changes might compromise software quality and developers productivity. To reduce conflicts, one could avoid the parallel execution of potentially conflicting tasks. Although hopeful, this strategy is challenging because it relies on the prediction of the required file changes to complete a task. As predicting such file changes is hard, we investigate its feasibility for BDD (Behaviour-Driven Development) projects, which write automated acceptance tests before implementing features. We develop a tool that, for a given task, statically analyzes Cucumber tests and infers test-based interfaces (files that could be executed by the tests), approximating files that would be changed by the task. To assess the accuracy of this approximation, we measure precision and recall of test-based interfaces of 513 tasks from 18 Rails projects on GitHub. We also compare such interfaces with randomly defined interfaces, interfaces obtained by textual similarity of test specifications with past tasks, and interfaces computed by executing tests. Our results give evidence that, in the specific context of BDD, Cucumber tests might help to predict files changed by tasks. We find that the better the test coverage, the better the predictive power. A hybrid approach for computing test-based interfaces is promising.",Behaviour-driven development | Collaborative development | File change prediction | Task scheduling,Journal of Systems and Software,2019-08-01,Article,"Rocha, Thaís;Borba, Paulo;Santos, João Pedro",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065495606,10.1016/j.jss.2019.05.013,Model based system assurance using the structured assurance case metamodel,"Assurance cases are used to demonstrate confidence in system properties of interest (e.g. safety and/or security). A number of system assurance approaches are adopted by industries in the safety-critical domain. However, the task of constructing assurance cases remains a manual, lenghty and informal process. The Structured Assurance Case Metamodel (SACM)is a standard specified by the Object Management Group (OMG). SACM provides a richer set of features than existing system assurance languages/approaches. SACM provides a foundation for model-based system assurance, which bears great application potentials in growing technology domains such as Open Adaptive Systems. However, the intended usage of SACM has not been sufficiently explained. In addition, there has not been support to interoperate between existing assurance case (models)and SACM models. In this article, we explain the intended usage of SACM based on our involvement in the OMG specification process of SACM. In addition, to promote a model-based approach, we provide SACM compliant metamodels for existing system assurance approaches (the Goal Structuring Notation and Claims-Arguments-Evidence), and the transformations from these models to SACM. We also briefly discuss the tool support for model-based system assurance which helps practitioners make the transition from existing system assurance approaches to model-based system assurance using SACM.",Claims-Arguments-Evidence | Goal structuring notation | Model based system assurance | Model driven engineering | Structured assurance case metamodel,Journal of Systems and Software,2019-08-01,Article,"Wei, Ran;Kelly, Tim P.;Dai, Xiaotian;Zhao, Shuai;Hawkins, Richard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065781168,10.1016/j.jss.2019.05.024,Software project scheduling problem in the context of search-based software engineering: A systematic review,"This work provides a systematic literature review of the software project scheduling problem, in the context of search-based software engineering, and summarizes the main models, techniques, search algorithms and evaluation criteria applied to solve this problem. We also discuss trends and research opportunities. Our keyword search found 438 papers, published in the last 20 years. After considering the inclusion and exclusion criteria and performing the snowballing procedure, we have analyzed 37 primary studies. The results show the predominance of the use of evolutionary algorithms. The static model, in which the scheduling is performed once during the project, is considered in the majority of the papers. Synthetic instances are commonly used to validate the heuristic and hypervolume and execution time are the mostly applied evaluating criteria.",Search-based software engineering | Software project scheduling problem | Systematic review,Journal of Systems and Software,2019-09-01,Article,"Rezende, Allan Vinicius;Silva, Leila;Britto, André;Amaral, Rodrigo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065795226,10.1016/j.jss.2019.05.026,Sentiment based approval prediction for enhancement reports,"The maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user's requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%.",Classification | Enhancement reports | Machine learning algorithms,Journal of Systems and Software,2019-09-01,Article,"Umer, Qasim;Liu, Hui;Sultan, Yasir",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065016737,10.1016/j.infsof.2019.04.008,Towards a reduction in architectural knowledge vaporization during agile global software development,"Context: The adoption of agile methods is a trend in global software development (GSD), but may result in many challenges. One important challenge is architectural knowledge (AK) management, since agile developers prefer sharing knowledge through face-to-face interactions, while in GSD the preferred manner is documents. Agile knowledge-sharing practices tend to predominate in GSD companies that practice agile development (AGSD), leading to a lack of documents, such as architectural designs, data models, deployment specifications, etc., resulting in the loss of AK over time, i.e., it vaporizes. Objective: In a previous study, we found that there is important AK in the log files of unstructured textual electronic media (UTEM), such as instant messengers, emails, forums, etc., which are the preferred means employed in AGSD to contact remote teammates. The objective of this paper is to present and evaluate a proposal with which to recover AK from UTEM logs. We developed and evaluated a prototype that implements our proposal in order to determine its feasibility. Method: The evaluation was performed by conducting a study with agile/global developers and students, who used the prototype and different UTEM to execute tasks that emulate common situations concerning AGSD teams’ lack of documentation during development phases. Results: Our prototype was considered a useful, usable and unobtrusive tool when retrieving AK from UTEM logs. The participants also preferred our prototype when searching for AK and found AK faster with the prototype than with UTEM when the origin of the AK required was unknown. Conclusion: The participants’ performance and perceptions when using our prototype provided evidence that our proposal could reduce AK vaporization in AGSD environments. These results encourage us to evaluate our proposal in a long-term test as future work.",Agile global software development | Architectural knowledge | Documentation debt | Knowledge vaporization,Information and Software Technology,2019-08-01,Article,"Borrego, Gilberto;Morán, Alberto L.;Palacio, Ramón R.;Vizcaíno, Aurora;García, Félix O.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056226726,10.1016/j.infsof.2018.10.012,On semantic detection of cloud API (anti)patterns,"Context: Open standards are urgently needed for enabling software interoperability in Cloud Computing. Open Cloud Computing Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their understandability and reusability. Objective: We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles. Method: First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)patterns and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility. Results: We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an acceptable level of maturity regarding REST principles. Conclusion: Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.",Analysis | Anti-pattern | Cloud computing | Detection | OCCI | Ontology | Pattern | REST | Specification,Information and Software Technology,2019-03-01,Article,"Brabra, Hayet;Mtibaa, Achraf;Petrillo, Fabio;Merle, Philippe;Sliman, Layth;Moha, Naouel;Gaaloul, Walid;Guéhéneuc, Yann Gaël;Benatallah, Boualem;Gargouri, Faïez",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85059460418,10.1016/j.infsof.2018.12.008,Live programming in practice: A controlled experiment on state machines for robotic behaviors,"Context: Live programming environments are gaining momentum across multiple programming languages. A tenet of live programming is a development feedback cycle, resulting in faster development practices. Although practitioners of live programming consider it a positive inclusion in their workflow, no in-depth investigations have yet been conducted on its benefits in a realistic scenario, nor using complex API. Objective: This paper carefully studies the advantage of using live programming in defining nested state machines for robot behaviors. We analyzed two important aspects of developing robotic behaviors using these machines: program comprehension and program writing. We analyzed both development practices in terms of speed and accuracy. Method: We conducted two controlled experiments, one for program comprehension and another for program writing. We measured the speed and accuracy of randomized assigned participants on completing programming tasks, against a baseline. Results: In a robotic behavior context, we found that a live programming system for nested state machine programs does not significantly outperform a non-live language in program comprehension nor in program writing in terms of speed and accuracy. However, the feedback of test subjects indicates their preference for the live programming system. Conclusions: The results of this work seem to contradict the studies of live programming in other areas, even while participants still favor using live programming techniques. We learned that the complex API chosen in this work has a strong negative influence on the results. To the best of our knowledge, this is the first in-depth live programming experiment in a complex domain.",Controlled experiment | Live programming | Live robot programming | Nested state machines | Robot behaviors,Information and Software Technology,2019-04-01,Article,"Campusano, Miguel;Fabry, Johan;Bergel, Alexandre",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85063080612,10.1016/j.infsof.2019.03.004,Reusability in goal modeling: A systematic literature review,"Context: Goal modeling is an important instrument for the elicitation, specification, analysis, and validation of early requirements. Goal models capture hierarchical representations of stakeholder objectives, requirements, possible solutions, and their relationships to help requirements engineers understand stakeholder goals and explore solutions based on their impact on these goals. To reuse a goal model and benefit from the strengths of goal modeling, we argue that it is necessary (i) to make sure that analysis and validation of goal models is possible through reuse hierarchies, (ii) to provide the means to delay decision making to a later point in the reuse hierarchy, (iii) to take constraints imposed by other modeling notations into account during analysis, (iv) to allow context dependent information to be modeled so that the goal model can be used in various reuse contexts, and (v) to provide an interface for reuse. Objective: In this two-part systematic literature review, we (i) evaluate how well existing goal modeling approaches support reusability with our five desired characteristics of contextual and reusable goal models, (ii) categorize these approaches based on language constructs for context modeling and connection to other modeling formalisms, and then (iii) draw our conclusions on future research themes. Method: Following guidelines by Kitchenham, the review is conducted on seven major academic search engines. Research questions, inclusion criteria, and categorization criteria are specified, and threats to validity are discussed. A final list of 146 publications and 34 comparisons/assessments of goal modeling approaches is discussed in more detail. Results: Five major research themes are derived to realize reusable goal models with context dependent information. Conclusion: The results indicate that existing goal modeling approaches do not fully address the required capabilities for reusability in different contexts and that further research is needed to fill this gap in the landscape of goal modeling approaches.",Context | Goal model | Model-driven requirements engineering | Requirements reuse | Reuse | Systematic literature review,Information and Software Technology,2019-06-01,Review,"Duran, Mustafa Berk;Mussbacher, Gunter",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053936006,10.1016/j.infsof.2018.08.015,Metrics for analyzing variability and its implementation in software product lines: A systematic literature review,"Context: Software Product Line (SPL) development requires at least concepts for variability implementation and variability modeling for deriving products from a product line. These variability implementation concepts are not required for the development of single systems and, thus, are not considered in traditional software engineering. Metrics are well established in traditional software engineering, but existing metrics are typically not applicable to SPLs as they do not address variability management. Over time, various specialized product line metrics have been described in literature, but no systematic description of these metrics and their characteristics is currently available. Objective: This paper describes and analyzes variability-aware metrics, designed for the needs of software product lines. More precisely we restrict the scope of our study explicitly to metrics designed for variability models, code artifacts, and metrics taking both kinds of artifacts into account. Further, we categorize the purpose for which these metrics were developed. We also analyze to what extent these metrics were evaluated to provide a basis for researchers for selecting adequate metrics. Method: We conducted a systematic literature review to identify variability-aware implementation metrics. We discovered 42 relevant papers reporting metrics intended to measure aspects of variability models or code artifacts. Results: We identified 57 variability model metrics, 34 annotation-based code metrics, 46 code metrics specific to composition-based implementation techniques, and 10 metrics integrating information from variability model and code artifacts. For only 31 metrics, an evaluation was performed assessing their suitability to draw any qualitative conclusions. Conclusions: We observed several problematic issues regarding the definition and the use of the metrics. Researchers and practitioners benefit from the catalog of variability-aware metrics, which is the first of its kind. Also, the research community benefits from the identified observations in order to avoid those problems when defining new metrics.",Implementation | Metrics | Software product lines | SPL | Systematic literature review,Information and Software Technology,2019-02-01,Review,"El-Sharkawy, Sascha;Yamagishi-Eichler, Nozomi;Schmid, Klaus",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065799464,10.1016/j.infsof.2019.05.003,Adopting configuration management principles for managing experiment materials in families of experiments,"Context: Replication is a key component of experimentation for verifying previous results and findings. Experiment replication requires products like documentation describing the baseline experiment and a version of the experimental material. When replicating an experiment, changes may have to be made to some of the products, leading to new or modified versions of materials. After the replication has been conducted, part of or all the materials should be added to the family history or to the baseline experiment documentation. As the number of replications increases, more versions of the materials are generated. This can lead to product management chaos in replications sharing the same protocol. Objective: The aim of this paper is to adopt configuration management principles to manage experimental materials. We apply and validate these principles in a code inspection technique comparison experiment and a personality quasi-experiment. Method: The study was conducted within a research group with lengthy experience in experiment replication. This research group has had trouble with the management of the materials used to run some of the experiments replicated by other colleagues. This is a suitable context for applying action research. We used action research to adopt the configuration management principles and build a materials management framework. Result: We generated the instances of an experiment and a quasi-experiment, identifying the status and traceability of the materials. Additionally, we documented the workload required for instantiation in person-hours. We also checked the ease of use and understanding of the framework for instantiating the personality quasi-experiment configuration plan executed by researchers who did not develop the framework, as well as its usefulness for managing the experimental materials. Conclusion: The experimental materials management framework is useful for establishing the status and traceability of the experimental materials. Additionally, it improves the storage, search, location and retrieval of the experimental material versions.",Experiment replication | Experimental material | Experimental software configuration management | Experimental software engineering,Information and Software Technology,2019-09-01,Article,"Espinosa, Edison;Acuña, Silvia Teresita;Vegas, Sira;Juristo, Natalia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85053877709,10.1016/j.infsof.2018.09.006,Guidelines for including grey literature and conducting multivocal literature reviews in software engineering,"Context: A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in software engineering (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results. Objective: There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE. Method: To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example. Results: The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations. Conclusion: Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.",Evidence-based software engineering | Grey literature | Guidelines | Literature study | Multivocal literature review | Systematic literature review | Systematic mapping study,Information and Software Technology,2019-02-01,Article,"Garousi, Vahid;Felderer, Michael;Mäntylä, Mika V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85056477605,10.1016/j.infsof.2018.11.002,Information quality requirements engineering with STS-IQ,"Context: Information Quality (IQ) is particularly important for organizations: they depend on information for managing their daily tasks and relying on low-quality information may negatively influence their overall performance. Despite this, the literature shows that most software development approaches do not consider IQ requirements during the system design, which leaves the system open to different kinds of vulnerabilities. Objective: The main objective of this research is proposing a framework for modeling and analyzing IQ requirements for Socio-Technical Systems (STS). Method: We propose STS-IQ, a goal-oriented framework for modeling and analyzing IQ requirements in their social and organizational context since the early phases of the system design. The framework extends and refines our previous work, and it consists of: (i) a modeling language that provides concepts and constructs for modeling IQ requirements; (ii) a set of analysis techniques that support the verification of the correctness and consistency of the IQ requirements model; (iii) a mechanism for deriving the final IQ specifications in terms of IQ policies; (iv) a methodology to assist software engineers during the system design; and (v) a CASE tool, namely STS-IQ Tool. Result: We demonstrated the applicability, usefulness, and scalability of the modeling and reasoning techniques within a stock market case study, and we also evaluated the usability and utility of the framework with end-users. Conclusion: We conclude that the STS-IQ framework supports the modeling and analysis of IQ requirements, and also the derivation of precise IQ specifications in terms of IQ policies. Therefore, we believe it has potential in practice.",Conceptual modelling | Framework | Goal-oriented requirements engineering | GORE | Information quality requirements,Information and Software Technology,2019-03-01,Article,"Gharib, Mohamad;Giorgini, Paolo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85068963810,10.1016/j.infsof.2019.07.004,Multi-reviewing pull-requests: An exploratory study on GitHub OSS projects,"Context:GitHub has enabled developers to easily contribute their review comments on multiple pull-requests and switch their review focus between different pull-requests, i.e., multi-reviewing. Reviewing multiple pull-requests simultaneously may enhance work efficiency. However, multi-reviewing also relies on developers’ rationally allocating their focus, which may bring a different influence to the resolution of pull-requests. Objective: In this paper, we present an ongoing study of the impact of multi-reviewing on pull-request resolution in GitHub open source projects. Method: We collected and analyzed 1,836,280 pull-requests from 760 GitHub projects to explore how multi-reviewing affects the resolution of a pull-request. Results: We find that multi-reviewing is a common behavior in GitHub. However, more multi-reviewing behaviors tend to bring longer pull-request resolution latency. Conclusion: Multi-reviewing is a complex behavior of developers, and has an important impact on the efficiency of pull-request resolution. Our study motivates the need for more research on multi-reviewing.",GitHub | Multi-reviewing | Pull-requests,Information and Software Technology,2019-11-01,Article,"Hu, Dongyang;Zhang, Yang;Chang, Junsheng;Yin, Gang;Yu, Yue;Wang, Tao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85067831031,10.1016/j.infsof.2019.06.004,Model driven transformation development (MDTD): An approach for developing model to model transformation,"Context: In the Model Driven Development (MDD) approach, model transformations are responsible for the semi-automation of software development process converting models between different abstraction levels. The development of model transformations involves a complexity inherent to the transformation domain, in addition to the complexity of software development in general. Therefore, the construction of model transformations requires software engineering feature such as processes and languages to facilitate its development and maintenance. Objective: This paper presents a framework to develop unidirectional relational model transformation using the MDD approach itself, which integrates: (i) a software development process suitable for the model transformation domain (ii) a Domain specific language for transformation modeling (iii) a transformation chain, to (semi) automate the proposed process, and (iv) a development environment to support it. Methods: The proposal systematizes the development of model transformation, following the MDD principles. An iterative and incremental process guides transformation development from requirement specification to transformation codification. The proposal has been evaluated through a case study and a controlled experiment. Results: The framework enables model transformation specification at a high abstraction level and (semi) automatically transforms it into models at a low abstraction level until the transformation code. The results of the case study showed that people with different levels of knowledge of MDD, or without experience in transformation languages, were able to develop transformations through the framework and generated executable code. Conclusions: The framework integrates the essential elements involved in the development of model transformation and enables the abstraction of technological details. The results of the case study and controlled experiment showed the feasibility of the proposal and its use in dealing with the complexity involved in model transformation development.",Development process | Framework | Model to model transformation | Modeling language | Uml profile,Information and Software Technology,2019-10-01,Article,"Magalhaes, Ana Patrícia Fontes;Andrade, Aline Maria Santos;Maciel, Rita Suzana Pitangueira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065756126,10.1016/j.infsof.2019.05.002,Interactive semi-automated specification mining for debugging: An experience report,"Context: Specification mining techniques are typically used to extract the specification of a software in the absence of (up-to-date) specification documents. This is useful for program comprehension, testing, and anomaly detection. However, specification mining can also potentially be used for debugging, where a faulty behavior is abstracted to give developers a context about the bug and help them locating it. Objective: In this project, we investigate this idea in an industrial setting. We propose a very basic semi-automated specification mining approach for debugging and apply that on real reported issues from an AutoPilot software system from our industry partner, MicroPilot Inc. The objective is to assess the feasibility and usefulness of the approach in a real-world setting. Method: The approach is developed as a prototype tool, working on C code, which accept a set of relevant state fields and functions, per issue, and generates an extended finite state machine that represents the faulty behavior, abstracted with respect to the relevant context (the selected fields and functions). Results: We qualitatively evaluate the approach by a set of interviews (including observational studies) with the company's developers on their real-world reported bugs. The results show that (a) our approach is feasible, (b) it can be automated to some extent, and (c) brings advantages over only using their code-level debugging tools. We also compared this approach with traditional fully automated state-merging algorithms and reported several issues when applying those techniques on a real-world debugging context. Conclusion: The main conclusion of this study is that the idea of an “interactive” specification mining rather than a fully automated mining tool is NOT impractical and indeed is useful for the debugging use case.",Case study | Debugging | Interview | Semi-automated | Specification mining,Information and Software Technology,2019-09-01,Article,"Mashhadi, Mohammad Jafar;Siddiqui, Taha R.;Hemmati, Hadi;Loewen, Howard",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065622242,10.1016/j.infsof.2019.05.004,Multi-armed bandits in the wild: Pitfalls and strategies in online experiments,"Context: Delivering faster value to customers with online experimentation is an emerging practice in industry. Multi-Armed Bandit (MAB) based experiments have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, the incorrect use of MAB-based experiments can lead to incorrect conclusions that can potentially hurt the company's business. Objective: The objective of this study is to understand the pitfalls and restrictions of using MABs in online experiments, as well as the strategies that are used to overcome them. Method: This research uses a multiple case study method with eleven experts across five software companies and simulations to triangulate the data of some of the identified limitations. Results: This study analyzes some limitations faced by companies using MAB and discusses strategies used to overcome them. The results are summarized into practitioners’ guidelines with criteria to select an appropriated experimental design. Conclusion: MAB algorithms have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, potential mistakes can occur and hinder the potential benefits of such approach. Together with the provided guidelines, we aim for this paper to be used as reference material for practitioners during the design of an online experiment.",A/B tests | Multi-armed bandit | Multi-armed bandit pitfalls | Online experiments,Information and Software Technology,2019-09-01,Article,"Issa Mattos, David;Bosch, Jan;Olsson, Helena Holmström",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85068128936,10.1016/j.infsof.2019.06.011,Log mining to re-construct system behavior: An exploratory study on a large telescope system,"Context: A large amount of information about system behavior is stored in logs that record system changes. Such information can be exploited to discover anomalies of a system and the operations that cause them. Given their large size, manual inspection of logs is hard and infeasible in a desired timeframe (e.g., real-time), especially for critical systems. Objective: This study proposes a semi-automated method for reconstructing sequences of tasks of a system, revealing system anomalies, and associating tasks and anomalies to code components. Method: The proposed approach uses unsupervised machine learning (Latent Dirichlet Allocation) to discover latent topics in messages of log events and introduces a novel technique based on pattern recognition to derive the semantic of such topics (topic labelling). The approach has been applied to the big data generated by the ALMA telescope system consisting of more than 2000 log events collected in about five hours of telescope operation. Results: With the application of our approach to such data, we were able to model the behavior of the telescope over 16 different observations. We found five different behavior models and three different types of errors. We use the models to interpret each error and discuss its cause. Conclusions: With this work, we have also been able to discuss some of the known challenges in log mining. The experience we gather has been then summarized in lessons learned.",Latent Dirichlet Allocation | Log mining | System behavior | Text processing,Information and Software Technology,2019-10-01,Article,"Pettinato, Michele;Gil, Juan Pablo;Galeas, Patricio;Russo, Barbara",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85064848170,10.1016/j.infsof.2019.04.011,Enhancing context specifications for dependable adaptive systems: A data mining approach,"Context: Adaptive systems are expected to cater for various operational contexts by having multiple strategies in achieving their objectives and the logic for matching strategies to an actual context. The prediction of relevant contexts at design time is paramount for dependability. With the current trend on using data mining to support the requirements engineering process, this task of understanding context for adaptive system at design time can benefit from such techniques as well.Objective: The objective is to provide a method to refine the specification of contextual variables and their relation to strategies for dependability. This refinement shall detect dependencies between such variables, priorities in monitoring them, and decide on their relevance in choosing the right strategy in a decision tree.Method: Our requirements-driven approach adopts the contextual goal modelling structure in addition to the operationalization values of sensed information to map contexts to the system's behaviour. We propose a design time analysis process using a subset of data mining algorithms to extract a list of relevant contexts and their related variables, tasks, and/or goals.Results: We experimentally evaluated our proposal on a Body Sensor Network system (BSN), simulating 12 resources that could lead to a variability space of 4096 possible context conditions. Our approach was able to elicit subtle contexts that would significantly affect the service provided to assisted patients and relations between contexts, assisting the decision on their need, and priority in monitoring.Conclusion: The use of some data mining techniques can mitigate the lack of precise definition of contexts and their relation to system strategies for dependability. Our method is practical and supportive to traditional requirements specification methods, which typically require intense human intervention.",Context uncertainty | Data mining | Dependability | Design time | Goal modelling | Self-adaptive system,Information and Software Technology,2019-08-01,Article,"Rodrigues, Arthur;Rodrigues, Genaína Nunes;Knauss, Alessia;Ali, Raian;Andrade, Hugo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85061638243,10.1016/j.infsof.2019.01.010,State of the art in hybrid strategies for context reasoning: A systematic literature review,"Context: Several strategies have been used to implement context reasoning, and a strategy that can be applied satisfactorily in different smart systems applications has not yet been found. Because of this, hybrid proposals for context reasoning are gaining prominence. These proposals allow the combination of two or more strategies. Objective: This work aims to identify the state of the art in the context awareness field, considering papers that use hybrid strategies for context reasoning. Method: A Systematic Literature Review was explored, contributing to the identification of relevant works in the field, as well as the specification of criteria for its selection. In this review, we analyzed papers published between 2004 and 2018. Results: During the process, we identified 3241 papers. After applying filtering and conditioning processes, ten papers about hybrid strategies for context reasoning were selected. We described, discussed, and compared the selected papers. Conclusion: The Systematic Literature Review showed that some researchers explore hybrid proposals, but these proposals do not offer flexibility regarding the reasoning strategies used. Thus, we noted that research efforts related to the topic are still necessary, mainly focusing on the development of dynamic approaches that allow the applications to choose how they want to use the different resources available.",Context awareness | Hybrid context reasoning | Internet of Things | Reasoning strategy,Information and Software Technology,2019-07-01,Review,"Machado, Roger S.;Almeida, Ricardo B.;Pernas, Ana Marilza;Yamin, Adenauer C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85071098676,10.1016/j.infsof.2019.106176,Finding key classes in object-oriented software systems by techniques based on static analysis,"Context: Software maintenance is burdened by program comprehension activities which consume a big part of project resources. Program comprehension is difficult because the code to be analyzed is very large and the documentation may not be well structured to help navigating through the code. Objective: Tools should support the early stages of program comprehension. Our goal is to build tools that analyze the code and filter this large amount of information such that only the most important information is presented to the software maintenance team. In the case of object-oriented systems, finding the important information means finding the most important classes, also called the key classes of the system. Method: In this work, we formulate and explore several hypotheses regarding which are the class attributes that characterize important classes. By class attributes, we understand here different metrics that quantify properties of the class such as its connections and relationships with other classes. All the necessary input data for computing class attributes are extracted from code by static analysis. We experimentally investigate which attributes are best suited to rank classes according to their importance, doing an extensive empirical study on fifteen software systems. Result: Attributes from the categories of direct connections and network centrality are the best for finding key classes. We identified three class attributes which are best as class ranking criteria: PR-U2-W and CONN-TOTAL-W when the target set of key classes is small and CONN-TOTAL when the target set has a large and variable size. We show that the method of ranking classes based on these attributes outperforms known related work approaches of finding key classes. Conclusions: Our method allows us to build easy-to-use fully automatic tools which find almost instantly the key classes of a software system starting from its code.",Key classes | Program comprehension | Static analysis,Information and Software Technology,2019-12-01,Article,"Şora, Ioana;Chirila, Ciprian Bogdan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85060004787,10.1016/j.infsof.2019.01.004,Deriving architectural models from requirements specifications: A systematic mapping study,"Context: Software architecture design creates and documents the high-level structure of a software system. Such structure, expressed in architectural models, comprises software elements, relations among them, and properties of these elements and relations. Existing software architecture methods offer ways to derive architectural models from requirements specifications. These models must balance different forces that should be analyzed during this derivation process, such as those imposed by different application domains and quality attributes. Such balance is difficult to achieve, requiring skilled and experienced architects. Object: The purpose of this paper is to provide a comprehensive overview of the existing methods to derive architectural models from requirements specifications and offer a research roadmap to challenge the community to address the identified limitations and open issues that require further investigation. Method: To achieve this goal, we performed a systematic mapping study following the good practices from the Evidence-Based Software Engineering field. Results: This study resulted in 39 primary studies selected for analysis and data extraction, from the 2575 initially retrieved. Conclusion: The major findings indicate that current architectural derivation methods rely heavily on the architects’ tacit knowledge (experience and intuition), do not offer sufficient support for inexperienced architects, and lack explicit evaluation mechanisms. These and other findings are synthesized in a research roadmap which results would benefit researchers and practitioners.",Literature review | Mapping study | Software architecture,Information and Software Technology,2019-05-01,Article,"Souza, Eric;Moreira, Ana;Goulão, Miguel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85063324662,10.1016/j.infsof.2019.03.009,Bootstrapping cookbooks for APIs from crowd knowledge on Stack Overflow,"Context: Well established libraries typically have API documentation. However, they frequently lack examples and explanations, possibly making difficult their effective reuse. Stack Overflow is a question-and-answer website oriented to issues related to software development. Despite the increasing adoption of Stack Overflow, the information related to a particular topic (e.g., an API) is spread across the website. Thus, Stack Overflow still lacks organization of the crowd knowledge available on it. Objective: Our target goal is to address the problem of the poor quality documentation for APIs by providing an alternative artifact to document them based on the crowd knowledge available on Stack Overflow, called crowd cookbook. A cookbook is a recipe-oriented book, and we refer to our cookbook as crowd cookbook since it contains content generated by a crowd. The cookbooks are meant to be used through an exploration process, i.e. browsing. Method: In this paper, we present a semi-automatic approach that organizes the crowd knowledge available on Stack Overflow to build cookbooks for APIs. We have generated cookbooks for three APIs widely used by the software development community: SWT, LINQ and QT. We have also defined desired properties that crowd cookbooks must meet, and we conducted an evaluation of the cookbooks against these properties with human subjects. Results: The results showed that the cookbooks built using our approach, in general, meet those properties. As a highlight, most of the recipes were considered appropriate to be in the cookbooks and have self-contained information. Conclusion: We concluded that our approach is capable to produce adequate cookbooks automatically, which can be as useful as manually produced cookbooks. This opens an opportunity for API designers to enrich existent cookbooks with the different points of view from the crowd, or even to generate initial versions of new cookbooks.",API documentation | Cookbook | Crowd knowledge | Stack Overflow,Information and Software Technology,2019-07-01,Article,"Souza, Lucas B.L.;Campos, Eduardo C.;Madeiral, Fernanda;Paixão, Klérisson;Rocha, Adriano M.;Maia, Marcelo de Almeida",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85071285493,10.1016/j.infsof.2019.08.001,Impact of the conceptual model's representation format on identifying and understanding user stories,"Context: Eliciting user stories is a major challenge for agile development approaches. Conceptual models are used to support the identification of user stories and increase their understanding. In many companies, existing model documentation stored as either use cases or BPMN models is available. However, these two types of business process models might not be equally effective for elicitation tasks due to their formats. Objective: We address the effectiveness of different elicitation tasks when supported either with visual or textual conceptual model. Since the agile literature shows little attention to reusing existing BPMN documentation, we propose several hypotheses to compare it to the use of textual use case models. Method: We conducted an experiment to compare the effectiveness of the two business process formats: textual use cases and visual BPMN models. We studied their effects on three elicitation tasks: identifying user stories and understanding their execution-order and integration dependencies. Results: The subjects better understood execution-order dependencies when visual input in the form of BPMN models was provided. The performance of the other two tasks showed no statistical differences. Conclusion: We addressed an important problem of user story elicitation: which informationally equivalent model (visual BPMN or textual use case) is more effective when identifying and understanding user stories.",,Information and Software Technology,2019-12-01,Article,"Trkman, Marina;Mendling, Jan;Trkman, Peter;Krisper, Marjan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85055413129,10.1016/j.infsof.2018.10.010,API recommendation for event-driven Android application development,"Context: Software development is increasingly dependent on existing libraries. Developers need help to find suitable library APIs. Although many studies have been proposed to recommend relevant functional APIs that can be invoked for implementing a functionality, few studies have paid attention to an orthogonal need associated with event-driven programming frameworks, such as the Android framework. In addition to invoking functional APIs, Android developers need to know where to place functional code according to various events that may be triggered within the framework. Objective: This paper aims to develop an API recommendation engine for Android application development that can recommend both (1) functional APIs for implementing a functionality and (2) the event callback APIs that are to be overridden to contain the functional code. Method: We carry out an empirical study on actual Android programming questions from StackOverflow to confirm the need of recommending callbacks. Then we build Android-specific API databases to contain the correlations among various functionalities and APIs, based on customized parsing of code snippets and natural language processing of texts in Android tutorials and SDK documents, and then textual and code similarity metrics are adapted for recommending relevant APIs. Results: We have evaluated our prototype recommendation engine, named LibraryGuru, with about 1500 questions on Android programming from StackOverflow, and demonstrated that our top-5 results on recommending callbacks and functional APIs can on estimate achieve up to 43.5% and 50.9% respectively in precision, 24.6% and 32.5% respectively in mean average precision (MAP) scores, and 51.1% and 44.0% respectively in recall. Conclusion: We conclude that it is important and possible to recommend both functional APIs and callbacks for Android application development, and future work is needed to take more data sources into consideration to make more relevant recommendations for developers’ needs.",Android programming | API recommendation | Code search | Event callbacks | Information retrieval,Information and Software Technology,2019-03-01,Article,"Yuan, Weizhao;Nguyen, Hoang H.;Jiang, Lingxiao;Chen, Yuting;Zhao, Jianjun;Yu, Haibo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078087442,10.1109/APSEC48747.2019.00050,Automatic Classifying Self-Admitted Technical Debt Using N-Gram IDF,"Technical Debt (TD) introduces a quality problem and increases maintenance cost since it may require improvements in the future. Several studies show that it is possible to automatically detect TD from source code comments that developers intentionally created, so-called self-admitted technical debt (SATD). Those studies proposed to use binary classification technique to predict whether a comment shows SATD. However, SATD has different types (e.g. design SATD and requirement SATD). In this paper, we therefore propose an approach using N-gram Inverse Document Frequency (IDF) and employ a multi-class classification technique to build a model that can identify different types of SATD. From the empirical evaluation on 10 open-source projects, our approach outperforms alternative methods (e.g. using BOW and TF-IDF). Our approach also improves the prediction performance over the baseline benchmark by 33%.",Multi class Classification | N-Gram IDF | Self Admitted Technical Debt,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2019-12-01,Conference Paper,"Wattanakriengkrai, Supatsara;Srisermphoak, Napat;Sintoplertchaikul, Sahawat;Choetkiertikul, Morakot;Ragkhitwetsagul, Chaiyong;Sunetnanta, Thanwadee;Hata, Hideaki;Matsumoto, Kenichi",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85085710561,10.1109/IICSPI48186.2019.9095968,Automatic Generation of Code Comments Based on Comment Reuse and Program Parsing,"In order to solve the problem of rare comments and low quality, a method of automatically generating code comments by comment reuse and program parsing is proposed. First, the clone code is detected by Nicad, and the codes and their comments are extracted. Then, through a series of heuristic rules such as dedrying and generalization, the code and the relatively simple code comments are streamlined and optimized. For complex code comments with obvious semantic characteristics, the program parsing method is used to optimize, finally, the comments are mapped to the code to automatically generate comments for the target code. In order to verify the validity of the experiment, the code comments in the 14 target software were manually verified, the comment results were evaluated, and then the five softwares were compared with the previous methods of ours team. The experimental results show that 33.69% of the code comments are good, the generated comments are 14.13% higher than the previous experimental methods, and the quality is improved by about 5%.",automatic generation of code comment | clone detection | code comment | comment optimization | program parsing,"Proceedings - 2019 2nd International Conference on Safety Produce Informatization, IICSPI 2019",2019-11-01,Conference Paper,"Bai, Yang;Zhang, Liping;Yan, Sheng",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85073795363,10.1109/QRS.2019.00029,Characterizing Software Maintainability in Issue Summaries using a Fuzzy Classifier,"Despite the importance of software maintainability in the life cycle of software systems, accurate measurement remains difficult to achieve. Previous work has shown how bug reports can be classified by expressed quality concerns which can give insight into maintainability across domains and over time. However, the amount of manual effort required to produce such classifications limits its usage. In this paper, we build a fuzzy classifier with linguistic patterns to automatically map issue summaries into the seven subgroup SQ classifications provided in a software maintainability ontology. We investigate how long it takes to generate a stable set of rules and evaluate the performance of the rule set on both rule generating and nonrule generating projects. The results validate the generalizability of the fuzzy classifier in correctly and automatically identifying the subgroup SQ classifications from given issue summaries. This provides a building block for analyzing project maintainability on a larger scale.",natural language processing | Software maintainability | Software maintainability ontology | Software quality | Software quality measurement,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",2019-07-01,Conference Paper,"Chen, Celia;Shoga, Michael;Boehm, Barry",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073781599,10.1109/QRS.2019.00026,SMARTKT: A Search Framework to Assist Program Comprehension using Smart Knowledge Transfer,"Regardless of attempts to extract knowledge from code bases to aid in program comprehension, there is an absence of a framework to extract and integrate knowledge to provide a near-complete multifaceted understanding of a program. To bridge this gap, we propose SMARTKT (Smart Knowledge Transfer) to extract and transfer knowledge related to software development and application-specific characteristics and their interrelationships in form of a knowledge graph. For an application, the knowledge graph provides an overall understanding of the design and implementation and can be used by an intelligent natural language query system to convert the process of knowledge transfer into a developer-friendly Google-like search. For validation, we develop an analyzer to discover concurrency-related design aspects from runtime traces in a machine learning framework and obtain a precision and recall of around 97% and 95% respectively. We extract application-specific knowledge from code comments and obtain 72% match against human-Annotated ground truth.",Knowledge Graph | Knowledge Transfer | Machine Learning | Natural Language Processing | Program Comprehension,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",2019-07-01,Conference Paper,"Majumdar, Srijoni;Papdeja, Shakti;Das, Partha Pratim;Ghosh, Soumya Kanti",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85078246379,10.1109/EDOC.2019.00037,Classification of changes in API evolution,"Applications typically communicate with each other, accessing and exposing data and features by using Application Programming Interfaces (APIs). Even though API consumers expect APIs to be steady and well established, APIs are prone to continuous changes, experiencing different evolutive phases through their lifecycle. These changes are of different types, caused by different needs and are affecting consumers in different ways. In this paper, we identify and classify the changes that often happen to APIs, and investigate how all these changes are reflected in the documentation, release notes, issue tracker and API usage logs. The analysis of each step of a change, from its implementation to the impact that it has on API consumers, will help us to have a bigger picture of API evolution. Thus, we review the current state of the art in API evolution and, as a result, we define a classification framework considering both the changes that may occur to APIs and the reasons behind them. In addition, we exemplify the framework using a software platform offering a Web API, called District Health Information System (DHIS2), used collaboratively by several departments of World Health Organization (WHO).",API changes classification | API evolution | Issue tracker | Log mining,"Proceedings - 2019 IEEE 23rd International Enterprise Distributed Object Computing Conference, EDOC 2019",2019-10-01,Conference Paper,"Koci, Rediana;Franch, Xavier;Jovanovic, Petar;Abello, Alberto",Include,
10.1016/j.jss.2022.111515,2-s2.0-85064182126,10.1109/SANER.2019.8668026,A Human-As-Sensors Approach to API Documentation Integration and Its Effects on Novice Programmers,"In recent years, there has been a great interest in integrating crowdsourced API documents that are often dispersed across multiple places. Because of the complexity of natural language, however, automatically synthesized documents often fall short on quality and completeness compared to those authored by human experts. We develop a complementary 'human-Assensors' approach to document integration that generates API FAQs based on users' help-seeking behavior and history. We investigated the benefits and limitations of this approach in the context of programming education. This paper describes a prototype system called COFAQ and a controlled experiment with 18 novice programmers. The study confirms that the generated FAQs effectively fosters knowledge transfer between the programmers and significantly reduce the need for repeated search. It also discovers several difficulties novice programmers encountered when seeking API help as well as the strategies they used to seek and utilize API knowledge.",API | crowd-computing | crowdsourcing | documentation | empirical | experiment | grounded theory | human-computing | integration | user study | visualization,"SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",2019-03-15,Conference Paper,"Chen, Cong;Yang, Yulong;Yang, Lin;Zhang, Kang",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85064179160,10.1109/SANER.2019.8668034,Knowledge Graphing Git Repositories: A Preliminary Study,"Knowledge Graph, being able to connect information from a variety of sources, has become very famous in recent years since its creation in 2012 by Google. Researchers in our community have leveraged Knowledge Graph to achieve various purposes such as improving API caveats accessibilities, generating answers to developer questions, and reasoning common software weaknesses, etc. In this work, we would like to leverage the knowledge graph concept for helping developers and project managers to comprehend software repositories. To this end, we design and implement a prototype tool called GitGraph, which takes as input a Git repository and constructs automatically a knowledge graph associated with the repository. Our preliminary experimental results show that GitGraph can correctly generate knowledge graphs for Git projects and the generated graphs are also useful for users to comprehend the projects. More specifically, the knowledge graph, on one hand, provides a graphic interface that users can interactively explore the integrated artefacts such as commits and changed methods, while on the other hand, provides a convenient means for users to search for advanced relations between the different artefacts.",,"SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",2019-03-15,Conference Paper,"Zhao, Yanjie;Wang, Haoyu;Ma, Lei;Liu, Yuxin;Li, Li;Grundy, John",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85072711023,10.1109/COMPSAC.2019.00012,An empirical study on API-misuse bugs in open-source C programs,"Today, large and complex software is developed with integrated components using application programming interfaces (APIs). Correct usage of APIs in practice presents a challenge due to implicit constraints, such as call conditions or call orders. API misuse, i.e., violation of these constraints, is a well-known source of bugs, some of which can cause serious security vulnerabilities. Although researchers have developed many API-misuse detectors over the last two decades, recent studies show that API misuses are still prevalent. In this paper, we provide a comprehensive empirical study on API-misuse bugs in open-source C programs. To understand the nature of API misuses in practice, we analyze 830 API-misuse bugs from six popular programs across different domains. For all the studied bugs, we summarize their root causes, fix patterns and usage statistics. Furthermore, to understand the capabilities and limitations of state-of-the-art static analysis detectors for API-misuse detection, we develop APIMU4C, a dataset of API-misuse bugs in C code based on our empirical study results, and evaluate three widely-used detectors on it qualitatively and quantitatively. We share all the findings and present possible directions towards more powerful API-misuse detectors.",API misuse | Benchmark | Bug detection | Empirical study,Proceedings - International Computer Software and Applications Conference,2019-07-01,Conference Paper,"Gu, Zuxing;Wu, Jiecheng;Liu, Jiaxiang;Zhou, Min;Gu, Ming",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85078888728,10.1109/VLHCC.2019.8818681,The Long Tail: Understanding the Discoverability of API Functionality,"Almost all software development revolves around the discovery and use of application programming interfaces (APIs). Once a suitable API is selected, programmers must begin the process of determining what functionality in the API is relevant to a programmer's task and how to use it. Our work aims to understand how API functionality is discovered by programmers and where tooling may be appropriate. We employed a mixed-methods approach to investigate Apache Beam, a distributed data processing API, by mining Beam client code and running a lab study to see how people discover Beam's available functionality. We found that programmers' prior experience with similar APIs significantly impacted their ability to find relevant features in an API and attempting to form a top-down mental model of an API resulted in less discovery of features.",API discoverability | API learn-ability | API usability | API usage analysis,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2019-10-01,Conference Paper,"Horvath, Amber;Myers, Brad;Grover, Sachin;Dong, Sihan;Zhou, Emily;Voichick, Finn;Kery, Mary Beth;Shinju, Shwetha;Nam, Daye;Nagy, Mariann",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85078951254,10.1109/VLHCC.2019.8818816,Active Documentation: Helping Developers Follow Design Decisions,"Good documentation has long been argued to be key to helping developers write code more quickly and consistently with design decisions, but is left largely disconnected from code. We propose a method for active documentation, where design decisions are made explicit as design rules and checked against code. Developers can discover how to follow a design rule by navigating to examples in their codebase. After editing code, developers receive immediate feedback about which design rules are satisfied and which are violated, notifying developers who miss design decisions about the existence of these design decisions. We implemented our approach in a prototype tool and conducted a user study. Compared to developers using a traditional design document, developers working in an unfamiliar codebase with active documentation were faster and more successful, using active documentation to learn how to follow design decisions through examples and receive immediate feedback on their changes.",design decisions | documentation | IDE,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2019-10-01,Conference Paper,"Mehrpour, Sahar;Latoza, Thomas D.;Kindi, Rahul K.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85073440661,10.1109/EMIP.2019.00014,Design of an executable specification language using eye tracking,"Increasingly complex systems require powerful and easy to understand specification languages. In the course of the design and the extension of an executable specification language based on the Abstract State Machines formalism we recently performed a user survey. The question, how new language features are comprehended, remains unsolved. The main emphasis of this study is on eye gaze behavior of participants learning language features. Eye-tracking experiments were performed in two different countries using a Pupil Labs eye-tracking headset. An example specification and simple comprehension tasks were used as stimuli. The preliminary results of the eye-gaze behaviour analysis reveal that the new language feature was undestood well, but the new abstractions were frequently confused by participants. The foreknowledge of specific programming concepts is crucial how these abstractions are comprehended. More research is needed to infer this knowledge from viewing patterns.",Abstract state machines | Effects of language features | Executable specification language | Gaze behavior,"Proceedings - 2019 IEEE/ACM 6th International Workshop on Eye Movements in Programming, EMIP 2019",2019-05-01,Conference Paper,"Simhandl, Georg;Paulweber, Philipp;Zdun, Uwe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073417456,10.1109/EMIP.2019.00012,Synchronized analysis of eye movement and EEG during program comprehension,"Appropriate support for program comprehension contributes to efficient software development. Several previous studies used bio-information such as brain activity to classify the inner-state of programmer without interruption. In this paper, we measure programmer's brain waves and eye movement simultaneously while they comprehend the source code. In the experiment, we analyze difference of time-series brain wave features between success/failure for source code comprehension task. The result of the experiment showed the participants who success source code comprehension significantly increased power spectrum of α wave with the time passage. Also the eye movements of the succeed participants shift their focus of fixation from specification to source code in early time. Synchronized analysis of failed programmer shows similar but slow pattern of EEG and eye movement changes compared with succeed programmer.",EEG | Eye movement | Program comprehension | Synchronized analysis,"Proceedings - 2019 IEEE/ACM 6th International Workshop on Eye Movements in Programming, EMIP 2019",2019-05-01,Conference Paper,"Ishida, Toyomi;Uwano, Hidetake",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072917405,10.1109/RAISE.2019.00011,Automatic detection of latent software component relationships from online Q&A sites,"Modern software system stacks are composed of large numbers of software components. These components may include a broad range of entities such as services, libraries, and frameworks, all intended to address specific requirements. It is not only necessary that these components satisfy respective functional and non-functional concerns, but also that the combinations of selected components work well together. The space of component combinations to explore is huge. Together with the almost universal lack of formal documentation suggesting desirable combinations and cautioning against undesirable ones, this renders the proper selection of combinations very challenging. For this reason, software engineers often solicit advice and document their experience on online forums such as community Q&A sites. In this paper, we show that these Q&A sites contain valuable knowledge about inter-component relations. We develop an approach using information extraction techniques to automatically identify three different types of compatibility relations from unstructured text on Q&A site postings. Our work demonstrates that identifying such relations is valuable for the design of component-based systems and that automatic relation extraction is a promising technique to systematically harness such community knowledge.",Component Based Software Development | Deep Learning | Information Extraction | Relation Extraction,"Proceedings - 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2019",2019-05-01,Conference Paper,"Karthik, Suhrid;Medvidovic, Nenad",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85076400632,10.1109/3ICT.2019.8910273,A code summarization approach for object oriented programs,"The software maintenance process is not an easy job, especially when the source code becomes more complex and less documented. Software developers and engineers may spend plenty of time to understand the structures and features of the source code in order to maintain their software projects. Using an automated code summarization technique is a key factor to save time and cost of the maintenance process. Generating descriptive documents from the source code helps the software developers to understand their software. In this paper, a code summarization framework is proposed to document the source code. A set of software features are extracted and displayed to the developers based on mapping the target source code segments to an XML representation. The proposed framework has been conducted on an open source project to evaluate its effectiveness. The generated results showed that the proposed approach is useful in understanding the different structural aspects of the source code.",Code summarization | Method summarization | Reverse engineering | Software documentation,"2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies, 3ICT 2019",2019-09-01,Conference Paper,"Mohsin, Ali Hameed;Hammad, Mustafa",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85079240355,10.1109/ICIC48496.2019.8966682,BigData Analysis of Stack Overflow for Energy Consumption of Android Framework,"With the recent increase in the sales of Android-based smart-phones, the demand for android development from the past years has improved tremendously. The recent debacle of Samsung Galaxy Note 7 along with some other incidents have highlighted the need for better energy management in smart-phones. One source of the reported battery issues was the energy-inefficient APIs used by the Android developers. To improve reliability and better power utilization this needs to be addressed by the developers. This paper reports our analysis of this problem. In our study, we have used NLTK techniques, for analyzing energy-related posts on Stack Overflow (SO). Energy inefficient APIs have been discussed frequently by the developers on SO. We have analyzed over four million posts from SO from which thousands of posts on the topic of energy inefficient APIs are scrutinized in detail. Our study identifies the issues that are frequently related to LocationListener API, LocationManager API, Broadcastreceiver API in term of GPS location, and AlarmManager for location updation.",Android Energy Consumption | Big Data Analytics | Mining Social Networks | Part of Speech | Unsupervised Learning for Big Data,"3rd International Conference on Innovative Computing, ICIC 2019",2019-11-01,Conference Paper,"Khan, Saif Ur Rehman;Farooq, Muhammad Umer;Beg, Mirza Omer",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85083468373,10.1109/ICICAS48597.2019.00125,Study of a code comment decision method based on structural features,"Code comment is one of the most effective ways to help programmers to understand the source code. High-quality comment decisions can not only cover the core code snippets in the software system but also avoid generating redundant code comments. However, in actual development, there is no uniform comment specification, and most of the comment decisions depend on personal experience and domain knowledge. This paper has learned a common comment decision specification from a large number of code comment examples to assist programmers in making appropriate comment decisions during code development. This paper proposes a method to extract the code structure from the context code of the current code line, and use the machine learning algorithm to determine the possibility that the current code needs to add comments. The proposed method was evaluated on 8 well-known open-source Python projects in GitHub and the experimental results show the feasibility and effectiveness of the method in some code types.",Code comment | Comment decision | Imblanced data | Machine learning,"Proceedings - 2019 International Conference on Intelligent Computing, Automation and Systems, ICICAS 2019",2019-12-01,Conference Paper,"Wang, Renmin;Wang, Tao;Wang, Huaimin",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85073246693,10.1109/IJCNN.2019.8851751,Automatic Source Code Summarization with Extended Tree-LSTM,"Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially structured, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call Multi-way Tree-LSTM and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.",,Proceedings of the International Joint Conference on Neural Networks,2019-07-01,Conference Paper,"Shido, Yusuke;Kobayashi, Yasuaki;Yamamoto, Akihiro;Miyamoto, Atsushi;Matsumura, Tadayuki",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.2139/ssrn.3426281,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3358931.3358937,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85070686760,10.1016/j.cosrev.2019.05.001,Systematic mapping study of API usability evaluation methods,"An Application Programming Interface (API) provides a programmatic interface to a software component that is often offered publicly and may be used by programmers who are not the API's original designers. APIs play a key role in software reuse. By reusing high quality components and services, developers can increase their productivity and avoid costly defects. The usability of an API is a qualitative characteristic that evaluates how easy it is to use an API. Recent years have seen a considerable increase in research efforts aiming at evaluating the usability of APIs. An API usability evaluation can identify problem areas and provide recommendations for improving the API. In this systematic mapping study, we focus on 47 primary studies to identify the aim and the method of the API usability studies. We investigate which API usability factors are evaluated, at which phases of API development is the usability of API evaluated and what are the current limitations and open issues in API usability evaluation. We believe that the results of this literature review would be useful for both researchers and industry practitioners interested in investigating the usability of API and new API usability evaluation methods.",API developers | API usability | Cognitive dimensions | Usability evaluation methods | Usability factors,Computer Science Review,2019-01-01,Review,"Rauf, Irum;Troubitsyna, Elena;Porres, Ivan",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85054405342,10.1016/j.cose.2018.09.007,Why Johnny can't develop a secure application? A usability analysis of Java Secure Socket Extension API,"Lack of usability of security Application Programming Interfaces (APIs) is one of the main reasons for mistakes that programmers make that result in security vulnerabilities in software applications they develop. Especially, APIs that provide Transport Layer Security (TLS) related functionalities are sometimes too complex for programmers to learn and use. Therefore, applications are often diagnosed with vulnerable TLS implementations due to mistakes made by programmers. In this work, we evaluated the usability of Java Secure Socket Extension (JSSE) API to identify usability issues in it that persuade programmers to make mistakes while developing applications that would result in security vulnerabilities. We conducted a study with 11 programmers where each of them spent around 2 hours and attempted to develop a secure programming solution using JSSE API. From data we collected, we identified 59 usability issues that exist in JSSE API. Then we divided those usability issues into 15 cognitive dimensions and analyzed how those issues affected the experience of participant programmers. Results of our study provided useful insights about how TLS APIs and similar security APIs should be designed, developed and improved to provide a better experience for programmers who use them.",API usability | Java Secure Socket Extension | Programmer experience | Security APIs | Transport Layer Security,Computers and Security,2019-01-01,Article,"Wijayarathna, Chamila;Arachchilage, Nalin Asanka Gamagedara",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072853095,10.1007/978-3-030-30244-3_60,Improving the Classification of Q&A Content for Android Fragmentation Using Named Entity Recognition,"Despite the huge amount of high quality information available in socio-technical sites, it is still challenging to filter out relevant piece of information to a specific task in hand. Textual content classification has been used to retrieve only relevant information to solve specific problems. However, those classifiers tend to present poor performance when the target classes have similar content. We aim at developing a Named Entity Recognizer (NER) model to recognize entities related to technical elements, and to improve textual classifiers for Android fragmentation posts from Stack Overflow using the obtained NER model. The proposed NER model was trained for the entities API version, device, hardware, API element, technology and feature. The proposed classifiers were trained using the recognized entities as attributes. To evaluate the performances of these classifiers, we compared them with other three textual classifiers. The obtained results show that the constructed NER model can recognize entities efficiently, as well as discover new entities that were not present in the training data. The classifiers constructed using the NER model produced better results than the other baseline classifiers. We suggest that NER-based classifiers should be considered as a better alternative to classify technical textual context compared to generic textual classifiers.",Android fragmentation | NER | Q&A | Textual classification,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Rocha, Adriano Mendonça;de Almeida Maia, Marcelo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069448235,10.1145/3314111.3319833,Factors influencing dwell time during source code reading - A large-scale replication experiment,"The paper partially replicates and extends a previous study by Busjahn et al. [4] on the factors influencing dwell time during source code reading, where source code element type and frequency of gaze visits are studied as factors. Unlike the previous study, this study focuses on analyzing eye movement data in large open source Java projects. Five experts and thirteen novices participated in the study where the main task is to summarize methods. The results examine semantic line-level information that developers view during summarization. We find no correlation between the line length and the total duration of time spent looking on the line even though it exists between a token’s length and the total fixation time on the token reported in prior work. The first fixations inside a method are more likely to be on a method’s signature, a variable declaration, or an assignment compared to the other fixations inside a method. In addition, it is found that smaller methods tend to have shorter overall fixation duration for the entire method, but have significantly longer duration per line in the method. The analysis provides insights into how source code’s unique characteristics can help in building more robust methods for analyzing eye movements in source code and overall in building theories to support program comprehension on realistic tasks.",Eye tracking study | Source code | Visual attention distribution,Eye Tracking Research and Applications Symposium (ETRA),2019-06-25,Conference Paper,"Peterson, Cole S.;Abid, Nahla J.;Bryant, Corey A.;Maletic, Jonathan I.;Sharif, Bonita",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85070601924,10.1145/3319619.3326814,Toward human-like summaries generated from heterogeneous software artefacts,"Automatic text summarisation has drawn considerable interest in the field of software engineering. It can improve the efficiency of software developers, enhance the quality of products, and ensure timely delivery. In this paper, we present our initial work towards automatically generating human-like multi-document summaries from heterogeneous software artefacts. Our analysis of the text properties of 545 human-written summaries from 15 software engineering projects will ultimately guide heuristics searches in the automatic generation of human-like summaries.",,GECCO 2019 Companion - Proceedings of the 2019 Genetic and Evolutionary Computation Conference Companion,2019-07-13,Conference Paper,"Alghamdi, Mahfouth;Treude, Christoph;Wagner, Markus",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85064985072,10.1145/3312662.3312710,A survey on research of code comment,"Code comments are one of the effective means for assisting programmers to understand the source code. High-quality code comments play an important role in areas such as software maintenance and software reuse. Good code comments can help programmers understand the role of source code and facilitate comprehension of programs and software maintenance tasks quickly. But in reality, most programmers only pay attention to the code and ignore comments and documents, which greatly reduce the program's readability and maintainability. This paper has compiled the relevant research on code comments so far, mainly including four aspects: automatic generation of code comments, consistency of code comments, classification of code comments, and quality evaluation of code comments. By analyzing relevant methods in this research field, it provides more complete information for future research.",Automatic generation of code comments | Code comment | Code comment classification | Consistency changes | Quality evaluation | Software maintenance,ACM International Conference Proceeding Series,2019-01-12,Conference Paper,"Bai, Yang;Zhang, Liping;Zhao, Fengrong",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85064700404,10.1145/3302541.3313103,Concern-driven Reporting of Software Performance Analysis Results,"State-of-the-art approaches for reporting performance analysis results rely on charts providing insights on the performance of the system, often organized in dashboards. The insights are usually data-driven, i.e., not directly connected to the performance concern leading the users to execute the performance engineering activity, thus limiting the understandability of the provided result. A cause is that the data is presented without further explanations. To solve this problem, we propose a concern-driven approach for reporting of performance evaluation results, shaped around a performance concern stated by a stakeholder and captured by state-of-the-art declarative performance engineering specifications. Starting from the available performance analysis, the approach automatically generates a customized performance report providing a chart- and natural-language-based answer to the concern. In this paper, we introduce the general concept of concern-driven performance analysis reporting and present a first prototype implementation of the approach. We envision that, by applying our approach, reports tailored to user concerns reduce the effort to analyze performance evaluation results.",concern-driven reporting | declarative performance engineering | software performance engineering,ICPE 2019 - Companion of the 2019 ACM/SPEC International Conference on Performance Engineering,2019-04-04,Conference Paper,"Okanovi, Duan;Van Hoorn, André;Zorn, Christoph;Beck, Fabian;Ferme, Vincenzo;Walter, Jürgen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077013429,10.1109/ACCESS.2019.2931579,A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques,"As an integral part of source code files, code comments help improve program readability and comprehension. However, developers sometimes do not comment their program code adequately due to the incurred extra efforts, lack of relevant knowledge, unawareness of the importance of code commenting or some other factors. As a result, code comments can be inadequate, absent or even mismatched with source code, which affects the understanding, reusing and the maintenance of software. To solve these problems of code comments, researchers have been concerned with generating code comments automatically. In this work, we aim at conducting a survey of automatic code commenting researches. First, we generally analyze the challenges and research framework of automatic generation of program comments. Second, we present the classification of representative algorithms, the design principles, strengths and weaknesses of each category of algorithms. Meanwhile, we also provide an overview of the quality assessment of the generated comments. Finally, we summarize some future directions for advancing the techniques of automatic generation of code comments and the quality assessment of comments.",Code comment | deep learning | information retrieval | machine learning | program annotation,IEEE Access,2019-01-01,Article,"Song, Xiaotao;Sun, Hailong;Wang, Xu;Yan, Jiafei",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85067821756,10.1109/TCSS.2019.2919667,Managing QoS of Internet-of-Things Services Using Blockchain,"Owing to the exponential growth of Internet of Things (IoTs), ensure that the Quality of Service (QoS) over IoT becomes challenges at the network edge or on cloud. The traditional mechanisms for QoS measurements rely on the centralized trusted third parties who use specialized agents to collect data and measure the performances of services. However, these mechanisms are ineffective to deal with highly dynamic and distributed nature of IoT-based services. Moreover, the dynamism of QoS needs to collect, update, and access reliable quality relevant data frequently, while lacking trust becomes a major hurdle for data utilization. It is our argument that the QoS measurement of IoT-based services would be decentralized and the trusts be built from collectively trusted subnetworks. In this paper, we propose to integrate the blockchain technologies (BCT) with a multi-agent approach to warrantee the trustiness of real-time data for the measurement of QoS in the IoT environment. The proposed approach is verified by some demonstrative examples in addressing QoS specification patterns commonly found in service-based applications (SBAs), where qualitative analyses are conducted for the evaluation of the patterns.",Blockchain technologies (BCTs) | consensus | distributed systems | Internet of Things (IoTs) | Quality of Service (QoS) | service-based applications (SBAs) | smart contracts,IEEE Transactions on Computational Social Systems,2019-12-01,Article,"Viriyasitavat, Wattana;Xu, Li Da;Bi, Zhuming;Hoonsopon, Danupol;Charoenruk, Nuttirudee",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85061594809,10.1587/transinf.2018EDP7071,An empirical study of README contents for java script packages,"Contemporary software projects often utilize a README. md to share crucial information such as installation and usage examples related to their software. Furthermore, these files serve as an important source of updated and useful documentation for developers and prospective users of the software. Nonetheless, both novice and seasoned developers are sometimes unsure of what is required for a good README file. To understand the contents of README, we investigate the contents of 43,900 JavaScript packages. Results show that these packages contain common content themes (i.e., ‘usage’, ‘install’ and ‘license’). Furthermore, we find that application-specific packages more frequently included content themes such as ‘options’, while library-based packages more frequently included other specific content themes (i.e., ‘install’ and ‘license’).",Association rule mining | Documentation | JavaScript packages | README,IEICE Transactions on Information and Systems,2019-02-01,Article,"Ikeda, Shohei;Ihara, Akinori;Kula, Raula Gaikovina;Matsumoto, Kenichi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85068646192,10.4018/IJEIS.2019070105,Application programming interface (API) research: A review of the past to inform the future,"The purpose of this study is to perform a synthesis of API research. The study took stock of literature from academic journals on APIs with their associated themes, frameworks, methodologies, publication outlets and level of analysis. The authors draw on a total of 104 articles from academic journals and conferences published from 2010 to 2018. A systematic literature review was conducted on the selected articles. The findings suggest that API research is primarily atheoretical and largely focuses on the technological dimensions such as design and usage; thus, neglecting most of the social issues such as the business and managerial applications of APIs, which are equally important. Future research directions are provided concerning the gaps identified.",Application Programming Interface | Conceptual Approaches | Future Research Directions | Methodological Approaches | Software | Systematic Literature Review | Technology | Themes,International Journal of Enterprise Information Systems,2019-07-01,Review,"Ofoeda, Joshua;Boateng, Richard;Effah, John",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85065672913,10.1145/3297280.3297508,Semi-automatic ontology-driven development documentation generating documents from RDF data and DITA templates,"For a data-driven economy, digitization of product information throughout the entire product lifecycle is key to agility and efficiency of product-related processes. Documenting products and their development, e.g., creating requirement specifications, is an indispensable, time-consuming and resource-intensive activity in large organizations. A vast amount of related information often emerges across several siloing lifecycle tools, and only a portion of it is available in the post-hoc documentation. Additionally, numerous product lines and versions additionally increase the documentation effort. To tackle these issues in a research project, we developed a semi-automatic end-to-end documentation system, able to generate documents based on templates and structured data. As a use case for document generation, we employ the RDF-based lifecycle tool integration standard OSLC and add extended publishing information. In order to generate target documents, we leverage DITA, an established digital publishing standard. A pilot implementation demonstrates that the approach is able to extract distributed lifecycle data and to generate several types of documents in multiple formats. Since the method can also be used to generate documents from arbitrary RDF graphs, the results can be generalized to other domains beyond software development. We believe that the results support the change from a document-driven to a data-driven documentation paradigm in large organizations.",Digital publishing | Linked Data | Ontology | Product documentation | Tool integration,Proceedings of the ACM Symposium on Applied Computing,2019-01-01,Conference Paper,"Pikus, Yevgen;Weißenberg, Norbert;Holtkamp, Bernhard;Otto, Boris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073260344,10.1145/3350768.3350791,Does the introduction of lambda expressions improve the comprehension of Java programs?,"Background: The Java programming language version eighth introduced a number of features that encourage the functional style of programming, including the support for lambda expressions and the Stream API. Currently, there is a common wisdom that refactoring a legacy code to introduce lambda expressions, besides other potential benefits, simplifies the code and improves program comprehension. Aims: The purpose of this paper is to investigate this belief, conducting an in depth study to evaluate the effect of introducing lambda expressions on program comprehension. Method: We conduct this research using a mixed-method study. First, we quantitatively analyze 66 pairs of real code snippets, where each pair corresponds to the body of a method before and after the introduction of lambda expressions. We computed two metrics related to source code complexity (number of lines of code and cyclomatic complexity) and two metrics that estimate the readability of the source code. Second, we conduct a survey with practitioners to collect their perceptions about the benefits on program comprehension, with the introduction of lambda expressions. The practitioners evaluate a number between three and six pairs of code snippets, to answer questions about possible improvements. Results: We found contradictory results in our research. Based on the quantitative assessment, we could not find evidences that the introduction of lambda expressions improves software readability-one of the components of program comprehension. Differently, our findings of the qualitative assessment suggest that the introduction of lambda expression improves program comprehension. Implications: We argue in this paper that one can improve program comprehension when she applies particular transformations to introduce lambda expressions (e.g., replacing anonymous inner classes by lambda expressions). In addition, the opinion of the participants shine the opportunities in which a transformation for introducing lambda might be advantageous. This might support the implementation of effective tools for automatic program transformations. Finally, our results suggest that state-of-the-art models for estimating program readability are not helpful to capture the benefits of a program transformation to introduce lambda expressions.",Empirical Studies | Java Lambda Expressions | Program Comprehension,ACM International Conference Proceeding Series,2019-09-23,Conference Paper,"Lucas, Walter;Bonifácio, Rodrigo;Canedo, Edna Dias;Marcílio, Diego;Lima, Fernanda",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1007/s11219-019-09476-z,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85061189262,10.1007/s11219-018-9428-4,API trustworthiness: an ontological approach for software library adoption,"The globalization of the software industry has led to an emerging trend where software systems depend increasingly on the use of external open-source external libraries and application programming interfaces (APIs). While a significant body of research exists on identifying and recommending potentially reusable libraries to end users, very little is known on the potential direct and indirect impact of these external library recommendations on the quality and trustworthiness of a client’s project. In our research, we introduce a novel Ontological Trustworthiness Assessment Model (OntTAM), which supports (1) the automated analysis and assessment of quality attributes related to the trustworthiness of libraries and APIs in open-source systems and (2) provides developers with additional insights into the potential impact of reused libraries and APIs on the quality and trustworthiness of their project. We illustrate the applicability of our approach, by assessing the trustworthiness of libraries in terms of their API breaking changes, security vulnerabilities, and license violations and their potential impact on client projects.",API breaking changes | Code reuse | License violations | Software quality | Software security vulnerabilities | Trustworthiness,Software Quality Journal,2019-09-01,Article,"Eghan, Ellis E.;Alqahtani, Sultan S.;Forbes, Christopher;Rilling, Juergen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85075607890,10.1007/978-3-030-30952-7_64,Transferring Java Comments Based on Program Static Analysis,"In the process of software development and maintenance, code comments can help developers reduce the time of reading source code, and thus improve their work efficiency. For large Java software projects, comments tend to appear in front of the main program entry or method definition. Readers need to search and read other source files containing the comments, which is very inconvenient. For this purpose, based on program static analysis technique, this paper has realized the process of automatically transferring comments in source code to their corresponding methods’ callers. This method first processes the source code using the program static analysis tool, which identifies the method call and the variable Define-Use and other dependency relations and confirms the comment-transferring paths; then extracts the text information in the comments that are in front of method definitions. Finally, it completes the extraction and transferring of comments in Java projects. The average precision of the experiment with 14 open source Java projects is 82.76%.",Comment transferring | Program static analysis | Software maintenance,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Li, Binger;Li, Feifei;Huang, Xinlin;He, Xincheng;Xu, Lei",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85066884200,10.1145/3308558.3313632,COACOR: Code annotation for code retrieval with reinforcement learning,"To accelerate software development, much research has been performed to help people understand and reuse the huge amount of available code resources. Two important tasks have been widely studied: code retrieval, which aims to retrieve code snippets relevant to a given natural language query from a code base, and code annotation, where the goal is to annotate a code snippet with a natural language description. Despite their advancement in recent years, the two tasks are mostly explored separately. In this work, we investigate a novel perspective of Code annotation for Code retrieval (hence called “CoaCor”), where a code annotation model is trained to generate a natural language annotation that can represent the semantic meaning of a given code snippet and can be leveraged by a code retrieval model to better distinguish relevant code snippets from others. To this end, we propose an effective framework based on reinforcement learning, which explicitly encourages the code annotation model to generate annotations that can be used for the retrieval task. Through extensive experiments, we show that code annotations generated by our framework are much more detailed and more useful for code retrieval, and they can further improve the performance of existing code retrieval models significantly.",Code Annotation | Code Retrieval | Reinforcement Learning,"The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019",2019-05-13,Conference Paper,"Yao, Ziyu;Peddamail, Jayavardhan Reddy;Sun, Huan",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.4233/UUID:3D7BC400-2447-4A88-8768-3025D7B54B7F,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85074844140,10.1145/3361242.3362774,A neural-network based code summarization approach by using source code and its call dependencies,"Code summarization aims at generating natural language abstraction for source code, and it can be of great help for program comprehension and software maintenance. The current code summarization approaches have made progress with neural-network. However, most of these methods focus on learning the semantic and syntax of source code snippets, ignoring the dependency of codes. In this paper, we propose a novel method based on neural-network model using the knowledge of the call dependency between source code and its related codes. We extract call dependencies from the source code, transform it as a token sequence of method names, and leverage the Seq2Seq model for code summarization using the combination of source code and call dependency information. About 100,000 code data is collected from 1,000 open source Java proejects on github for experiment. The large-scale code experiment shows that by considering not only the code itself but also the codes it called, the code summarization model can be improved with the BLEU score to 33.08.",Call Dependency | Code Summarization | Neural Network | Open Source,ACM International Conference Proceeding Series,2019-10-28,Conference Paper,"Liu, Bohong;Wang, Tao;Zhang, Xunhui;Fan, Qiang;Yin, Gang;Deng, Jinsheng",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.2478/jec-2019-0026,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.5753/cbsoft_estendido.2019.7664,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85073537978,10.1007/s11771-019-4185-5,Why do they ask? An exploratory study of crowd discussions about Android application programming interface in stack overflow,"Nowadays, more and more Android developers prefer to seek help from Q&A website like Stack Overflow, despite the rich official documentation. Several researches have studied the limitations of the official application programming interface (API) documentations and proposed approaches to improve them. However, few of them digged into the requirements of the third-party developers to study this. In this work, we gain insight into this question from multidimensional perspectives of API developers and API users by a kind of cross-validation. We propose a hybrid approach, which combines manual inspection on artifacts and online survey on corresponding developers, to explore the different focus between these two types of stakeholders. In our work, we manually inspect 1000 posts and receive 319 questionnaires in total. Through the mutual verification of the inspection and survey process, we found that the users are more concerned with the usage of API, while the official documentation mainly provides functional description. Furthermore, we identified 9 flaws of the official documentation and summarized 12 aspects (from the content to the representation) for promotion to improve the official API documentations.",Android | API documentation | online survey | Stack Overflow,Journal of Central South University,2019-09-01,Article,"Fan, Qiang;Wang, Tao;Yang, Cheng;Yin, Gang;Yu, Yue;Wang, Huai min",Include,
10.1016/j.jss.2022.111515,2-s2.0-85075930046,10.1007/s12046-019-1223-9,Natural language processing in mining unstructured data from software repositories: a review,"With the increasing popularity of open-source platforms, software data is easily available from various open-source tools like GitHub, CVS, SVN, etc. More than 80 percent of the data present in them is unstructured. Mining data from these repositories helps project managers, developers and businesses, in getting interesting insights. Most of the software artefacts present in these repositories are in the natural language form, which makes natural language processing (NLP) an important part of mining to get the useful results. The paper reviews the application of NLP techniques in the field of Mining Software Repositories (MSR). The paper mainly focuses on sentiment analysis, summarization, traceability, norms mining and mobile analytics. The paper presents the major NLP works performed in this area by surveying the research papers from 2000 to 2018. The paper firstly describes the major artefacts present in the software repositories where the NLP techniques have been applied. Next, the paper presents some popular open-source NLP tools that have been used to perform NLP tasks. Later the paper discusses, in brief, the research state of NLP in MSR field. The paper also lists down the various challenges along with the pointers for future work in this field of research and finally the conclusion.",mining unstructured data | mobile analytics | Natural language processing | software artefacts | software repositories,Sadhana - Academy Proceedings in Engineering Sciences,2019-12-01,Review,"Gupta, Som;Gupta, S. K.",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85074826983,10.1145/3361242.3361246,Migrating deprecated API to documented replacement: Patterns and tool,"Deprecation is commonly leveraged to preserve the backwards compatibility in API clients during API evolution. Client developers then usually encounter migration tasks when relying on a new version of API. Manually migrating a deprecated API to its replacement may be tedious and time-consuming. Existing research has focused on the requirements of automated migration however has limitations on the consistency and granularity of the generated migration pathway. In this paper, we propose an approach for migrating deprecated APIs to their replacements which are explicitly documented in the code documentation. A migration pattern including the migration type and the migration strategy is first introduced to describe the difference and the code change from a deprecated API to its replacement. An automated process is then designed and implemented to construct the pattern for a specific deprecated API by reviewing the code documentation and analyzing the API source code. Based on the patterns, a tool named DAAMT is developed to locate deprecated APIs and to provide migration suggestions for API clients. An experimental study on open-source Java projects and a user study is performing utilizing the approach. The results show that three-fourth of the deprecated APIs with documented replacements involved in the projects can be automatically migrated. In addition, the DAAMT tool can improve the efficiency and accuracy of the tasks compared with the manual migration.",API Migration | Deprecated API | Migration Pattern,ACM International Conference Proceeding Series,2019-10-28,Conference Paper,"Xi, Yaoguo;Shen, Liwei;Gui, Yukun;Zhao, Wenyun",Exclude,
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85053642631,10.1109/TSC.2016.2592909,ROSF: Leveraging Information Retrieval and Supervised Learning for Recommending Code Snippets,"When implementing unfamiliar programming tasks, developers commonly search code examples and learn usage patterns of APIs from the code examples or reuse them by copy-pasting and modifying. For providing high-quality code examples, previous studies present several methods to recommend code snippets mainly based on information retrieval. In this paper, to provide better recommendation results, we propose ROSF, Recommending code Snippets with multi-aspect Features, a novel method combining both information retrieval and supervised learning. In our method, we recommend Top-K code snippets for a given free-form query based on two stages, i.e., coarse-grained searching and fine-grained re-ranking. First, we generate a code snippet candidate set by searching a code snippet corpus using an information retrieval method. Second, we predict probability values of the code snippets for different relevance scores in the candidate set by the learned prediction model from a training set, re-rank these candidate code snippets according to the probability values, and recommend the final results to developers. We conduct several experiments to evaluate our method in a large-scale corpus containing 921,713 real-world code snippets. The results show that ROSF is an effective method for code snippets recommendation and outperforms the-state-of-the-art methods by 20-41percent in Precision and 13-33 percent in NDCG.",Code snippets recommendation | feature | information retrieval | supervised learning | topic model,IEEE Transactions on Services Computing,2019-01-01,Article,"Jiang, He;Nie, Liming;Sun, Zeyi;Ren, Zhilei;Kong, Weiqiang;Zhang, Tao;Luo, Xiapu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85076552941,10.1007/978-3-030-35333-9_16,Technical Debt and Waste in Non-functional Requirements Documentation: An Exploratory Study,"Background: To adequately attend to non-functional requirements (NFRs), they must be documented; otherwise, developers would not know about their existence. However, the documentation of NFRs may be subject to Technical Debt and Waste, as any other software artefact. Aims: The goal is to explore indicators of potential Technical Debt and Waste in NFRs documentation. Method: Based on a subset of data acquired from the most recent NaPiRE (Naming the Pain in Requirements Engineering) survey, we calculate, for a standard set of NFR types, how often respondents state they document a specific type of NFR when they also state that it is important. This allows us to quantify the occurrence of potential Technical Debt and Waste. Results: Based on 398 survey responses, four NFR types (Maintainability, Reliability, Usability, and Performance) are labelled as important but they are not documented by more than 22% of the respondents. We interpret that these NFR types have a higher risk of Technical Debt than other NFR types. Regarding Waste, 15% of the respondents state they document NFRs related to Security and they do not consider it important. Conclusions: There is a clear indication that there is a risk of Technical Debt for a fixed set of NFRs since there is a lack of documentation of important NFRs. The potential risk of incurring Waste is also present but to a lesser extent.",Non functional requirements | Technical Debt | Waste,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Robiolo, Gabriela;Scott, Ezequiel;Matalonga, Santiago;Felderer, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85091908476,10.1145/3387904.3389258,A human study of comprehension and code summarization,"Software developers spend a great deal of time reading and understanding code that is poorly-documented, written by other developers, or developed using differing styles. During the past decade,researchers have investigated techniques for automatically documenting code to improve comprehensibility. In particular, recentadvances in deep learning have led to sophisticated summary generation techniques that convert functions or methods to simple English strings that succinctly describe that code's behavior. However,automatic summarization techniques are assessed using internalmetrics such as BLEU scores, which measure natural language properties in translational models, or ROUGE scores, which measureoverlap with human-written text. Unfortunately, these metrics donot necessarily capture how machine-generated code summariesactually affect human comprehension or developer productivity.We conducted a human study involving both university studentsand professional developers (n = 45). Participants reviewed Javamethods and summaries and answered established program comprehension questions. In addition, participants completed codingtasks given summaries as specifications. Critically, the experimentcontrolled the source of the summaries: for a given method, someparticipants were shown human-written text and some were shownmachine-generated text.We found that participants performed significantly better (p =0.029) using human-written summaries versus machine-generatedsummaries. However, we found no evidence to support that participants perceive human-and machine-generated summaries to havedifferent qualities. In addition, participants' performance showedno correlation with the BLEU and ROUGE scores often used toassess the quality of machine-generated summaries. These resultssuggest a need for revised metrics to assess and guide automaticsummarization techniques.",,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Stapleton, Sean;Gambhir, Yashmeet;LeClair, Alexander;Eberhart, Zachary;Weimer, Westley;Leach, Kevin;Huang, Yu",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85091950217,10.1145/3387904.3389261,A self-attentional neural architecture for code completion with multi-task learning,"Code completion, one of the most useful features in the IntegratedDevelopment Environments (IDEs), can accelerate software development by suggesting the libraries, APIs, and method namesin real-time. Recent studies have shown that statistical languagemodels can improve the performance of code completion toolsthrough learning from large-scale software repositories. However,these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilizedin the program's representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networksbased language models are not sufficient to model the long-termdependency. c) Existing approaches perform a specific task in onemodel, which leads to the underuse of the information from relatedtasks. To address these challenges, in this paper, we propose a selfattentional neural architecture for code completion with multi-tasklearning. To utilize the hierarchical structural information of theprograms, we present a novel method that considers the path fromthe predicting node to the root node. To capture the long-termdependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable theknowledge sharing between related tasks, we creatively propose aMulti-Task Learning (MTL) framework to learn two related tasks incode completion jointly. Experiments on three real-world datasetsdemonstrate the effectiveness of our model when compared withstate-of-the-art methods.",Code completion | Hierarchical structure | Multi-task learning | Selfattention,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Liu, Fang;Li, Ge;Wei, Bolin;Xia, Xin;Fu, Zhiyi;Jin, Zhi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85091903308,10.1145/3387904.3389272,BugSum: Deep context understanding for bug repor summarization,"During collaborative software development, bug reports are dynamically maintained and evolved as a part of a software project. For a historical bug report with complicated discussions, an accurate and concise summary can enable stakeholders to reduce the time effort perusing the entire content. Existing studies on bug report summarization, based on whether supervised or unsupervised techniques, are limited due to their lack of consideration of the redundant information and disapproved standpoints among developers' comments. Accordingly, in this paper, we propose a novel unsupervised approach based on deep learning network, called Bug-Sum. Our approach integrates an auto-encoder network for feature extraction with a novel metric (believability) to measure the degree to which a sentence is approved or disapproved within discussions. In addition, a dynamic selection strategy is employed to optimize the comprehensiveness of the auto-generated summary represented by limited words. Extensive experiments show that our approach outperforms 8 comparative approaches over two public datasets. In particular, the probability of adding controversial sentences that are clearly disapproved by other developers during the discussion, into the summary is reduced by up to 69.6%.",Bug Report Summarization | Deep Learning | Mining Software Repositories | Software Maintenance,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Liu, Haoran;Yu, Yue;Li, Shanshan;Guo, Yong;Wang, Deze;Mao, Xiaoguang",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85091911897,10.1145/3387904.3389282,Deep-diving into documentation to develop improved java-to-swift API mapping,"Application program interface (API) mapping is the key to the success of code migration. Leveraging API documentation to map APIshas been explored by previous studies, and recently, code-basedlearning approaches have become the mainstream approach andshown better results. However, learning approaches often requirea large amount of training data (e.g., projects implemented usingmultiple languages or API mapping datasets), which are not widelyavailable. In contrast, API documentation is usually available, butwe have observed that much information in API documentation hasbeen underexploited. Therefore, we develop a deep-dive approachto extensively explore API documentation to create improved APImapping methods. Our documentation exploration approach involves analyzing the functional description of APIs, and also considers the parameters and return values. The results of this analysis canbe used to generate not only one-to-one API mapping, but also compatible API sequences, thereby enabling one-to-many API mapping.In addition, parameter-mapping relationships, which have oftenbeen ignored in previous approaches, can be produced. We applythis approach to map APIs from Java to Swift, and the experimentalresults indicate that our deep-dive analysis of API documentationleads to API mapping results that are superior to those generatedby existing approaches.",API mapping | Code migration | Document process,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Zhang, Zejun;Pan, Minxue;Zhang, Tian;Zhou, Xinyu;Li, Xuandong",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85091930441,10.1145/3387904.3389274,How graduate computing students search when using an unfamiliar programming language,"Developers and computing students are usually expected to mastermultiple programming languages. To learn a new language, developers often turn to online search to find information and codeexamples. However, insights on how learners perform code searchwhen working with an unfamiliar language are lacking. Understanding how learners search and the challenges they encounterwhen using an unfamiliar language can motivate future tools andtechniques to better support subsequent language learners.Research on code search behavior typically involves monitoringdevelopers during search activities through logs or in situ surveys.We conducted a study on how computing students search for codein an unfamiliar programming language with 18 graduate studentsworking on VBA tasks in a lab environment. Our surveys explicitly asked about search success and query reformulation to gatherreliable data on those metrics. By analyzing the combination ofsearch logs and survey responses, we found that students typicallysearch to explore APIs or find example code. Approximately 50% ofqueries that precede clicks on documentation or tutorials successfully solved the problem. Students frequently borrowed terms fromlanguages with which they are familiar when searching for examples in an unfamiliar language, but term borrowing did not impedesearch success. Edit distances between reformulated queries andnon-reformulated queries were nearly the same. These results haveimplications for code search research, especially on reformulation,and for research on supporting programmers when learning a newlanguage.",Code search | Learning new programming languages | VBA,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Bai, Gina R.;Kayani, Joshua;Stolee, Kathryn T.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85091891388,10.1145/3387904.3389268,Improved code summarization via a graph neural network,"Automatic source code summarization is the task of generatingnatural language descriptions for source code. Automatic code summarization is a rapidly expanding research area, especially as thecommunity has taken greater advantage of advances in neural network and AI technologies. In general, source code summarizationtechniques use the source code as input and outputs a natural language description. Yet a strong consensus is developing that usingstructural information as input leads to improved performance. Thefirst approaches to use structural information flattened the AST intoa sequence. Recently, more complex approaches based on randomAST paths or graph neural networks have improved on the modelsusing flattened ASTs. However, the literature still does not describethe using a graph neural network together with source code sequence as separate inputs to a model. Therefore, in this paper, wepresent an approach that uses a graph-based neural architecturethat better matches the default structure of the AST to generatethese summaries. We evaluate our technique using a data set of2.1 million Java method-comment pairs and show improvementover four baseline techniques, two from the software engineeringliterature, and two from machine learning literature.",Artificial intelligence | Automatic documentation | Deep learning | Neural networks,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"LeClair, Alexander;Haque, Sakib;Wu, Lingfei;McMillan, Collin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85091891733,10.1145/3387904.3389259,The secret life of commented-out source code,"Source code commenting is a common practice to improve codecomprehension in software development. While comments oftenconsist of descriptive natural language, surprisingly, there exists anon-trivial portion of comments that are actually code statements,i.e., commented-out code (CO code), even in well-maintained software systems. Commented-out code practice is rarely studied andoften excluded in prior studies on comments due to its irrelevanceto natural language. When being openly discussed, CO practiceis generally considered a bad practice. However, there is no priorwork to assess the nature (prevalence, evolution, motivation, andnecessity of utilization) of CO code practice.In this paper, we perform the first study to understand CO codepractice. Inspired by prior works in comment analysis, we developautomated solutions to identify CO code and track its evolution indevelopment history. Through analyzing six open-source projectsof different sizes and from diverse domains, we find that CO codepractice is non-trivial in software development, especially in theearly phase of development history, e.g., up to 20% of the commitsinvolve CO code practice. We observe common evolution patternsof CO code and find that developers may uncomment and comment code more frequently than expected, e.g., 10% of the CO codepractices have been uncommented at least once. Through a manualanalysis, we identify the common reasons that developers adoptCO code practices and reveal maintenance challenges associatedwith CO code practices.",Comment analysis | Comment/code evolution | Commented-out code,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Pham, Tri Minh Triet;Yang, Jinqiu",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85091898402,10.1145/3387904.3389286,Detecting code comment inconsistency using siamese recurrent network,"Comments are the internal documentation of corresponding codeblocks, which are essential to understand and maintain a software.In large scale software development, developers need to analyzeexisting codes, where comments assist better readability. In practice, developers commonly ignore comments' updating with respectto changing codes, which leads the code comment inconsistency.Traditionally researchers detect these inconsistencies based on codecomment tokens. However, sequence ordering in codecomments isignored in existing solution, as a result inconsistencies for invalid sequences of codes and comments are neglected. This paper solvesthese inconsistencies using siamese recurrent network which usesword tokens in codes and comments as well as their sequences incorresponding codes or comments. Proposed approach has beenevaluated with a benchmark dataset, along with the ability of detecting invalid code comment sequence is examined.",Code comment refactoring | Deep learning | Program comprehension | Siamese neural network,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Rabbi, Fazle;Siddik, Md Saeed",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85091945160,10.1145/3387904.3389288,Linguistic documentation of software history,"Open Source Software (OSS) projects start with an initial vocabulary, often determined by the first generation of developers. Thisvocabulary, embedded in code identifier names and internal codecomments, goes through multiple rounds of change, influenced bythe interrelated patterns of human (e.g., developers joining anddeparting) and system (e.g., maintenance activities) interactions.Capturing the dynamics of this change is crucial for understandingand synthesizing code changes over time. However, existing codeevolution analysis tools, available in modern version control systems such as GitHub and SourceForge, often overlook the linguisticaspects of code evolution. To bridge this gap, in this paper, wepropose to study code evolution in OSS projects through the lensof developers' language, also known as code lexicon. Our analysisis conducted using 32 OSS projects sampled from a broad range ofapplication domains. Our results show that different maintenanceactivities impact code lexicon differently. These insights lay outa preliminary foundation for modeling the linguistic history ofOSS projects. In the long run, this foundation will be utilized toprovide support for basic program comprehension tasks and helpresearchers gain new insights into the complex interplay betweenlinguistic change and various system and human aspects of OSSdevelopment.",Code Lexicon | Linguistic Change | Open Source Software,IEEE International Conference on Program Comprehension,2020-07-13,Conference Paper,"Tushev, Miroslav;Mahmoud, Anas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094326342,10.1145/3377811.3380390,Extracting taint specifications for javascript libraries,"Modern JavaScript applications extensively depend on third-party libraries. Especially for the Node.js platform, vulnerabilities can have severe consequences to the security of applications, resulting in, e.g., cross-site scripting and command injection attacks. Existing static analysis tools that have been developed to automatically detect such issues are either too coarse-grained, looking only at package dependency structure while ignoring dataflow, or rely on manually written taint specifications for the most popular libraries to ensure analysis scalability. In this work, we propose a technique for automatically extracting taint specifications for JavaScript libraries, based on a dynamic analysis that leverages the existing test suites of the libraries and their available clients in the npm repository. Due to the dynamic nature of JavaScript, mapping observations from dynamic analysis to taint specifications that fit into a static analysis is non-trivial. Our main insight is that this challenge can be addressed by a combination of an access path mechanism that identifies entry and exit points, and the use of membranes around the libraries of interest. We show that our approach is effective at inferring useful taint specifications at scale. Our prototype tool automatically extracts 146 additional taint sinks and 7 840 propagation summaries spanning 1 393 npm modules. By integrating the extracted specifications into a commercial, state-of-the-art static analysis, 136 new alerts are produced, many of which correspond to likely security vulnerabilities. Moreover, many important specifications that were originally manually written are among the ones that our tool can now extract automatically.",Dynamic analysis | Static analysis | Taint analysis,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Staicu, Cristian Alexandru;Torp, Martin Toldam;Schafer, Max;Moller, Anders;Pradel, Michael",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094324000,10.1145/3377811.3380407,Slacc: Simion-based language agnostic code clones,"Successful cross-language clone detection could enable researchers and developers to create robust language migration tools, facilitate learning additional programming languages once one is mastered, and promote reuse of code snippets over a broader codebase. However, identifying cross-language clones presents special challenges to the clone detection problem. A lack of common underlying representation between arbitrary languages means detecting clones requires one of the following solutions: 1) a static analysis framework replicated across each targeted language with annotations matching language features across all languages, or 2) a dynamic analysis framework that detects clones based on runtime behavior. In thiswork,we demonstrate the feasibility of the latter solution, a dynamic analysis approach called SLACC for cross-language clone detection. Like prior clone detection techniques, we use input/output behavior to match clones, though we overcome limitations of prior work by amplifying the number of inputs and covering more data types; and as a result, achieve better clusters than prior attempts. Since clusters are generated based on input/output behavior, SLACC supports cross-language clone detection. As an added challenge, we target a static typed language, Java, and a dynamic typed language, Python. Compared to HitoshiIO, a recent clone detection tool for Java, SLACC retrieves 6 times as many clusters and has higher precision (86.7% vs. 30.7%). This is the rst work to perform clone detection for dynamic typed languages (precision = 87.3%) and the first to perform clone detection across languages that lack a common underlying representation (precision = 94.1%). It provides a first step towards the larger goal of scalable language migration tools.",Cross-language analysis | Semantic code clone detection,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Mathew, George;Parnin, Chris;Stolee, Kathryn T.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094320633,10.1145/3377811.3380323,"SAVER: Scalable, precise, and safe memory-error repair","We present SAVER, a new memory-error repair technique for C programs. Memory errors such as memory leak, double-free, and use-after-free are highly prevalent and fixing them requires significant effort. Automated program repair techniques hold the promise of reducing this burden but the state-of-the-art is still unsatisfactory. In particular, no existing techniques are able to fix those errors in a scalable, precise, and safe way, all of which are required for a truly practical tool. SAVER aims to address these shortcomings. To this end, we propose a method based on a novel representation of the program called object flow graph, which summarizes the program's heap-related behavior using static analysis. We show that fixing memory errors can be formulated as a graph labeling problem over object flow graph and present an efficient algorithm. We evaluated SAVER in combination with Infer, an industrial-strength static bug-finder, and show that 74% of the reported errors can be fixed automatically for a range of open-source C programs.",Debugging | Memory errors | Program analysis | Program repair,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Hong, Seongjoon;Lee, Junhee;Lee, Jeongsoo;Oh, Hakjoo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094322255,10.1145/3377811.3380434,HARP: Holistic analysis for refactoring python-based analytics programs,"Modern machine learning programs are often written in Python, with the main computations specified through calls to some highly optimized libraries (e.g., TensorFlow, PyTorch). How to maximize the computing efficiency of such programs is essential for many application domains, which has drawn lots of recent attention. This work points out a common limitation in existing efforts: they focus their views only on the static computation graphs specified by library APIs, but leave the influence from the hosting Python code largely unconsidered. The limitation often causes them to miss the big picture and hence many important optimization opportunities. This work proposes a new approach named HARP to address the problem. HARP enables holistic analysis that spans across computation graphs and their hosting Python code. HARP achieves it through a set of novel techniques: analytics-conscious speculative analysis to circumvent Python complexities, a unified representation augmented computation graphs to capture all dimensions of knowledge related with the holistic analysis, and conditioned feedback mechanism to allow risk-controlled aggressive analysis. Refactoring based on HARP gives 1.3 3X and 2.07X average speedups on a set of TensorFlow and PyTorch programs.",Computation graph | Dynamic language | Machine learning program | Program analysis,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Zhou, Weijie;Zhao, Yue;Zhang, Guoqiang;Shen, Xipeng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094323088,10.1145/3377811.3380405,Software documentation: The practitioners' perspective,"In theory, (good) documentation is an invaluable asset to any software project, as it helps stakeholders to use, understand, maintain, and evolve a system. In practice, however, documentation is generally affected by numerous shortcomings and issues, such as insufficient and inadequate content and obsolete, ambiguous information. To counter this, researchers are investigating the development of advanced recommender systems that automatically suggest highquality documentation, useful for a given task. A crucial first step is to understand what quality means for practitioners and what information is actually needed for specific tasks. We present two surveys performed with 146 practitioners to investigate (i) the documentation issues they perceive as more relevant together with solutions they apply when these issues arise; and (ii) the types of documentation considered as important in different tasks. Our findings can help researchers in designing the next generation of documentation recommender systems.",Documentation | Empirical study,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Aghajani, Emad;Nagy, Csaba;Linares-Vasquez, Mario;Moreno, Laura;Bavota, Gabriele;Lanza, Michele;Shepherd, David C.",Include,
10.1016/j.jss.2022.111515,2-s2.0-85091923254,10.1145/3377811.3380352,Herewe go again: Why is it difficult for developers to learn another programming language,"Once a programmer knows one language, they can leverage concepts and knowledge already learned, and easily pick up another programming language. But is that always the case To understand if programmers have difficulty learning additional programming languages, we conducted an empirical study of Stack Overflow questions across 18 different programming languages. We hypothesized that previous knowledge could potentially interfere with learning a new programming language. From our inspection of 450 Stack Overflow questions, we found 276 instances of interference that occurred due to faulty assumptions originating from knowledge about a different language. To understand why these difficulties occurred, we conducted semi-structured interviews with 16 professional programmers. The interviews revealed that programmers make failed attempts to relate a new programming language with what they already know. Our findings inform design implications for technical authors, toolsmiths, and language designers, such as designing documentation and automated tools that reduce interference, anticipating uncommon language transitions during language design, and welcoming programmers not just into a language, but its entire ecosystem.",Interference theory | Learning | Program comprehension | Programming environments | Programming languages,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Shrestha, Nischal;Botta, Colton;Barik, Titus;Parnin, Chris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094318678,10.1145/3377811.3380343,Tailoring programs for static analysis via program transformation,"Static analysis is a proven technique for catching bugs during software development. However, analysis tooling must approximate, both theoretically and in the interest of practicality. False positives are a pervading manifestation of such approximations-tool configuration and customization is therefore crucial for usability and directing analysis behavior. To suppress false positives, developers readily disable bug checks or insert comments that suppress spurious bug reports. Existing work shows that these mechanisms fall short of developer needs and present a significant pain point for using or adopting analyses. We draw on the insight that an analysis user always has one notable ability to influence analysis behavior regardless of analyzer options and implementation: modifying their program. We present a new technique for automated, generic, and temporary code changes that tailor to suppress spurious analysis errors. We adopt a rule-based approach where simple, declarative templates describe general syntactic changes for code patterns that are known to be problematic for the analyzer. Our technique promotes program transformation as a general primitive for improving the fidelity of analysis reports (we treat any given analyzer as a black box). We evaluate using five different static analyzers supporting three different languages (C, Java, and PHP) on large, real world programs (up to 800KLOC). We show that our approach is effective in sidestepping long-standing and complex issues in analysis implementations.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Tonder, Rijnard Vel;Goues, Claire Le",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094324658,10.1145/3377811.3380425,Pipelining bottom-up data flow analysis,"Bottom-up program analysis has been traditionally easy to parallelize because functions without caller-callee relations can be analyzed independently. However, such function-level parallelism is significantly limited by the calling dependence - functions with caller-callee relations have to be analyzed sequentially because the analysis of a function depends on the analysis results, a.k.a., function summaries, of its callees.We observe that the calling dependence can be relaxed in many cases and, as a result, the parallelism can be improved. In this paper, we present Coyote, a framework of bottom-up data flow analysis, in which the analysis task of each function is elaborately partitioned into multiple sub-tasks to generate pipelineable function summaries. These sub-tasks are pipelined and run in parallel, even though the calling dependence exists. We formalize our idea under the IFDS/IDE framework and have implemented an application to checking null-dereference bugs and taint issues in C/C++ programs. We evaluate Coyote on a series of standard benchmark programs and open-source software systems, which demonstrates significant speedup over a conventional parallel design.",Bottomup analysis | Compositional program analysis | Data flow analysis | Ifds/ide. | Modular program analysis,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Shi, Qingkai;Zhang, Charles",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094314820,10.1145/3377811.3380357,How android developers handle evolution-induced api compatibility issues: A large-scale study,"As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them. In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in countermeasure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.",Android app analysis | Api evolution | Compatibility issues,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Xia, Hao;Zhang, Yuan;Zhou, Yingtian;Chen, Xiaoting;Wang, Yang;Zhang, Xiangyu;Cui, Shuaishuai;Hong, Geng;Zhang, Xiaohan;Yang, Min;Yang, Zhemin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094313772,10.1145/3377811.3380922,An empirical study on api parameter rules,"Developers build programs based on software libraries to reduce coding effort. If a program inappropriately sets an API parameter, the program may exhibit unexpected runtime behaviors. To help developers correctly use library APIs, researchers built tools to mine API parameter rules. However, it is still unknown (1) what types of parameter rules there are, and (2) how these rules distribute inside documents and source files. In this paper,we conducted an empirical study to investigate the above-mentioned questions. To analyze as many parameter rules as possible, we took a hybrid approach that combines automatic localization of constrained parameters with manual inspection. Our automatic approach-PaRu-locates parameters that have constraints either documented in Javadoc (i.e., document rules) or implied by source code (i.e., code rules). Our manual inspection (1) identifies and categorizes rules for the located parameters, and (2) establishes mapping between document and code rules. By applying PaRu to 9 widely used libraries, we located 5,334 parameters with either document or code rules. Interestingly, there are only 187 parameters that have both types of rules, and 79 pairs of these parameter rules are unmatched. Additionally, PaRu extracted 1,688 rule sentences from Javadoc and code.We manually classified these sentences into six categories, two of which are overlooked by prior approaches.We found that 86.2% of parameters have only code rules; 10.3% of parameters have only document rules; and only 3.5% of parameters have both document and code rules. Our research reveals the challenges for automating parameter rule extraction. Based on our findings, we discuss the potentials of prior approaches and present our insights for future tool design.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Zhong, Hao;Meng, Na;Li, Zexuan;Jia, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094108815,10.1145/3377811.3380433,When apis are intentionally bypassed: An exploratory study of apiworkarounds,"Application programming interfaces (APIs) have become ubiquitous in software development. However, external APIs are not guaranteed to contain every desirable feature, nor are they immune to software defects. Therefore, API users will sometimes be faced with situations where a current API does not satisfy all of their requirements, but migrating to another API is costly. In these cases, due to the lack of communication channels between API developers and users, API users may intentionally bypass an existing API after inquiring into workarounds for their API problems with online communities. This mechanism takes the API developer out of the conversation, potentially leaving API defects unreported and desirable API features undiscovered. In this paper we explore API workaround inquiries from API users on Stack Overflow. We uncover general reasons why API users inquire about API workarounds, and general solutions to API workaround requests. Furthermore, using workaround implementations in Stack Overflow answers, we develop three API workaround implementation patterns. We identify instances of these patterns in real-life open source projects and determine their value for API developers from their responses to feature requests based on the identified API workarounds.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Lamothe, Maxime;Shang, Weiyi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094321670,10.1145/3377811.3380430,"Demystify official api usage directives with crowdsourced api misuse scenarios, erroneous code examples and patches","API usage directives in official API documentation describe the contracts, constraints and guidelines for using APIs in natural language. Through the investigation of API misuse scenarios on Stack Overflow, we identify three barriers that hinder the understanding of the API usage directives, i.e., lack of specific usage context, indirect relationships to cooperative APIs, and confusing APIs with subtle differences. To overcome these barriers, we develop a text mining approach to discover the crowdsourced API misuse scenarios on Stack Overflow and extract from these scenarios erroneous code examples and patches, as well as related API and confusing APIs to construct demystification reports to help developers understand the official API usage directives described in natural language. We apply our approach to API usage directives in official Android API documentation and android-tagged discussion threads on Stack Overflow. We extract 159,116 API misuse scenarios for 23,969 API usage directives of 3138 classes and 7471 methods, from which we generate the demystification reports. Our manual examination confirms that the extracted information in the generated demystification reports are of high accuracy. By a user study of 14 developers on 8 API-misuse related error scenarios, we show that our demystification reports help developer understand and debug API-misuse related program errors faster and more accurately, compared with reading only plain API usage-directive sentences.",Api misuse | Api usage directive | Open information extraction | Stack overflow,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Ren, Xiaoxue;Sun, Jiamou;Xing, Zhenchang;Xia, Xin;Sun, Jianling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094325173,10.1145/3377811.3380435,BCFA: Bespoke control flow analysis for cfa at scale,"Many data-driven software engineering tasks such as discovering programming patterns, mining API specifications, etc., perform source code analysis over control flow graphs (CFGs) at scale. Analyzing millions of CFGs can be expensive and performance of the analysis heavily depends on the underlying CFG traversal strategy. State-of-the-art analysis frameworks use a fixed traversal strategy. We argue that a single traversal strategy does not fit all kinds of analyses and CFGs and propose bespoke control flow analysis (BCFA). Given a control flow analysis (CFA) and a large number of CFGs, BCFA selects the most efficient traversal strategy for each CFG. BCFA extracts a set of properties of the CFA by analyzing the code of the CFA and combines it with properties of the CFG, such as branching factor and cyclicity, for selecting the optimal traversal strategy. We have implemented BCFA in Boa, and evaluated BCFA using a set of representative static analyses that mainly involve traversing CFGs and two large datasets containing 287 thousand and 162 million CFGs. Our results show that BCFA can speedup the large scale analyses by 1%-28%. Further, BCFA has low overheads; less than 0.2%, and low misprediction rate; less than 0.01%.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Ramu, Ramanathan;Upadhyaya, Ganesha B.;Nguyen, Hoan Anh;Rajan, Hridesh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094148542,10.1145/3377811.3380342,Big code != big vocabulary: Open-vocabulary models for source code,"Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported. All datasets, code, and trained models used in this work are publicly available.",Byte-pair encoding | Naturalness of code | Neural language models,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Karampatsis, Rafael Michael;Babii, Hlib;Robbes, Romain;Sutton, Charles;Janes, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3377811.3380375,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094321455,10.1145/3377811.3380362,An empirical study on program failures of deep learning jobs,"Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O. This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.",Deep learning jobs | Empirical study | Program failures,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Zhang, Ru;Xiao, Wencong;Zhang, Hongyu;Liu, Yu;Lin, Haoxiang;Yang, Mao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094321524,10.1145/3377811.3380392,"Accessibility issues in android apps: State of affairs, sentiments, andways forward","Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps.We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Alshayban, Abdulaziz;Ahmed, Iftekhar;Malek, Sam",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094316824,10.1145/3377811.3380427,CPC: Automatically classifying and propagating natural language comments via program analysis,"Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, speciication inference, and code synthesis. However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, reine, and propagate comments. For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments. Developers usually comment on diferent aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties. To more efectively utilize comments, a ine-grained and elaborated taxonomy of comments and a reliable classiier to automatically categorize a comment are needed. In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy. Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code. Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been conirmed and ixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments. This demonstrates the efectiveness of our approach. Our user study conirms propagated comments align well with existing comments in terms of quality.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Zhai, Juan;Xu, Xiangzhe;Shi, Yu;Tao, Guanhong;Pan, Minxue;Ma, Shiqing;Xu, Lei;Zhang, Weifeng;Tan, Lin;Zhang, Xiangyu",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.1145/3377811.3380926,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094321003,10.1145/3377811.3380341,Verifying object construction,"In object-oriented languages, constructors often have a combination of required and optional formal parameters. It is tedious and inconvenient for programmers to write a constructor by hand for each combination. The multitude of constructors is error-prone for clients, and client code is difficult to read due to the large number of constructor arguments. Therefore, programmers often use design patterns that enable more flexible object construction-the builder pattern, dependency injection, or factory methods. However, these design patterns can be too flexible: not all combinations of logical parameters lead to the construction of wellformed objects. When a client uses the builder pattern to construct an object, the compiler does not check that a valid set of values was provided. Incorrect use of builders can lead to security vulnerabilities, run-time crashes, and other problems. This work shows how to statically verify uses of object construction, such as the builder pattern. Using a simple specification language, programmers specify which combinations of logical arguments are permitted. Our compile-time analysis detects client code that may construct objects unsafely. Our analysis is based on a novel special case of typestate checking, accumulation analysis, that modularly reasons about accumulations of method calls. Because accumulation analysis does not require precise aliasing information for soundness, our analysis scales to industrial programs. We evaluated it on over 9 million lines of code, discovering defects which included previously-unknown security vulnerabilities and potential null-pointer violations in heavily-used open-source codebases. Our analysis has a low false positive rate and low annotation burden. Our implementation and experimental data are publicly available.",Ami sniping | Autovalue | Builder pattern | Lightweight verification | Lombok | Pluggable type systems,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Kellogg, Martin;Ran, Manli;Sridharan, Manu;Schaf, Martin;Ernst, Michael D.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093643636,10.1145/3377811.3380324,Co-Evolving code with evolving metamodels,"Metamodels play a significant role to describe and analyze the relations between domain concepts. They are also cornerstone to build a software language (SL) for a domain and its associated tooling. Metamodel definition generally drives code generation of a core API. The latter is further enriched by developers with additional code implementing advanced functionalities, e.g., checkers, recommenders, etc. When a SL is evolved to the next version, the metamodels are evolved as well before to re-generate the core API code. As a result, the developers added code both in the core API and the SL toolings may be impacted and thus may need to be co-evolved accordingly. Many approaches support the co-evolution of various artifacts when metamodels evolve. However, not the co-evolution of code. This paper fills this gap. We propose a semi-automatic co-evolution approach based on change propagation. The premise is that knowledge of the metamodel evolution changes can be propagated by means of resolutions to drive the code co-evolution. Our approach leverages on the abstraction level of metamodels where a given metamodel element has often different usages in the code. It supports alternative co-evaluations to meet different developers needs. Our work is evaluated on three Eclipse SL implementations, namely OCL, Modisco, and Papyrus over several evolved versions of metamodels and code. In response to five different evolved metamodels, we co-evolved 976 impacts over 18 projects.A comparison of our co-evolved code with the versioned ones shows the usefulness of our approach. Our approach was able to reach a weighted average of 87.4% and 88.9% respectively of precision and recall while supporting useful alternative co-evolution that developers have manually performed.",,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Khelladi, Djamel Eddine;Combemale, Benoit;Acher, Mathieu;Barais, Olivier;Jezequel, Jean Marc",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094323417,10.1145/3377811.3380404,Interpreting cloud computer vision pain-points: A mining study of stack overflow,"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour-the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.",Computer vision | Documentation | Empirical study | Intelligent services | Pain points | Stack overflow,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Cummaudo, Alex;Vasa, Rajesh;Barnett, Scott;Grundy, John;Abdelrazek, Mohamed",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093657954,10.1145/3379597.3387468,A Study on the Accuracy of OCR Engines for Source Code Transcription from Programming Screencasts,"Programming screencasts can be a rich source of documentation for developers. However, despite the availability of such videos, the information available in them, and especially the source code being displayed is not easy to find, search, or reuse by programmers. Recent work has identified this challenge and proposed solutions that identify and extract source code from video tutorials in order to make it readily available to developers or other tools. A crucial component in these approaches is the Optical Character Recognition (OCR) engine used to transcribe the source code shown on screen. Previous work has simply chosen one OCR engine, without consideration for its accuracy or that of other engines on source code recognition. In this paper, we present an empirical study on the accuracy of six OCR engines for the extraction of source code from screencasts and code images. Our results show that the transcription accuracy varies greatly from one OCR engine to another and that the most widely chosen OCR engine in previous studies is by far not the best choice. We also show how other factors, such as font type and size can impact the results of some of the engines. We conclude by offering guidelines for programming screencast creators on which fonts to use to enable a better OCR recognition of their source code, as well as advice on OCR choice for researchers aiming to analyze source code in screencasts.",Code Extraction | Optical Character Recognition | Programming video tutorials | Software documentation | Video mining,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Khormi, Abdulkarim;Alahmadi, Mohammad;Haiduc, Sonia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093687976,10.1145/3379597.3387464,An Empirical Study on Regular Expression Bugs,"Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice. This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3%). The remaining root causes are incorrect API usage (9.3%) and other code issues that require regular expression changes in the fix (29.5%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.",bug fixes | pull requests | Regular expression bug characteristics,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Wang, Peipei;Brown, Chris;Jennings, Jamie A.;Stolee, Kathryn T.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093696448,10.1145/3379597.3387459,Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems,"Self-admitted technical debt (SATD) is a particular case of Technical Debt (TD) where developers explicitly acknowledge their sub-optimal implementation decisions. Previous studies mine SATD by searching for specific TD-related terms in source code comments. By contrast, in this paper we argue that developers can admit technical debt by other means, e.g., by creating issues in tracking systems and labelling them as referring to TD. We refer to this type of SATD as issue-based SATD or just SATD-I. We study a sample of 286 SATD-I instances collected from five open source projects, including Microsoft Visual Studio and GitLab Community Edition. We show that only 29% of the studied SATD-I instances can be tracked to source code comments. We also show that SATD-I issues take more time to be closed, compared to other issues, although they are not more complex in terms of code churn. Besides, in 45% of the studied issues TD was introduced to ship earlier, and in almost 60% it refers to DESIGN flaws. Finally, we report that most developers pay SATD-I to reduce its costs or interests (66%). Our findings suggest that there is space for designing novel tools to support technical debt management, particularly tools that encourage developers to create and label issues containing TD concerns.",,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Xavier, Laerte;Ferreira, Fabio;Brito, Rodrigo;Valente, Marco Tulio",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093690882,10.1145/3379597.3387478,Detecting and Characterizing Bots that Commit Code,"Background: Some developer activity traditionally performed manually, such as making code commits, opening, managing, or closing issues is increasingly subject to automation in many OSS projects. Specifically, such activity is often performed by tools that react to events or run at specific times. We refer to such automation tools as bots and, in many software mining scenarios related to developer productivity or code quality, it is desirable to identify bots in order to separate their actions from actions of individuals. Aim: Find an automated way of identifying bots and code committed by these bots, and to characterize the types of bots based on their activity patterns. Method and Result: We propose BIMAN, a systematic approach to detect bots using author names, commit messages, files modified by the commit, and projects associated with the commits. For our test data, the value for AUC-ROC was 0.9. We also characterized these bots based on the time patterns of their code commits and the types of files modified, and found that they primarily work with documentation files and web pages, and these files are most prevalent in HTML and JavaScript ecosystems. We have compiled a shareable dataset containing detailed information about 461 bots we found (all of which have more than 1000 commits) and 13,762,430 commits they created.",automated commits | bots | ensemble model | random forest | social coding platforms | software engineering,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Dey, Tapajit;Mousavi, Sara;Ponce, Eduardo;Fry, Tanner;Vasilescu, Bogdan;Filippova, Anna;Mockus, Audris",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093665458,10.1145/3379597.3387442,"What constitutes Software?: An Empirical, Descriptive Study of Artifacts","The term software is ubiquitous, however, it does not seem as if we as a community have a clear understanding of what software actually is. Imprecise definitions of software do not help other professions, in particular those acquiring and sourcing software from third-parties, when deciding what precisely are potential deliverables. In this paper we investigate which artifacts constitute software by analyzing 23 715 repositories from Github, we categorize the found artifacts into high-level categories, such as, code, data, and documentation (and into 19 more concrete categories) and we can confirm the notion of others that software is more than just source code or programs, for which the term is often used synonymously. With this work we provide an empirical study of more than 13 million artifacts, we provide a taxonomy of artifact categories, and we can conclude that software most often consists of variously distributed amounts of code in different forms, such as source code, binary code, scripts, etc., data, such as configuration files, images, databases, etc., and documentation, such as user documentation, licenses, etc.",artifacts | empirical study | software,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Pfeiffer, Rolf Helge",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093678645,10.1145/3379597.3387487,20-MAD: 20 Years of Issues and Commits of Mozilla and Apache Development,"Data of long-lived and high profile projects is valuable for research on successful software engineering in the wild. Having a dataset with different linked software repositories of such projects, enables deeper diving investigations. This paper presents 20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects. It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue comments, and its compressed size is over 6 GB. The data contains all the typical information about source code commits (e.g., lines added and removed, message and commit time) and issues (status, severity, votes, and summary). The issue comments have been pre-processed for natural language processing and sentiment analysis. This includes emoticons and valence and arousal scores. Linking code repository and issue tracker information, allows studying individuals in two types of repositories and provide more accurate time zone information for issue trackers as well. To our knowledge, this the largest linked dataset in size and in project lifetime that is not based on GitHub.",,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Claes, Maëlick;Mäntylä, Mika V.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093651575,10.1145/3379597.3387501,A C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries,"We collected a large C/C++ code vulnerability dataset from open-source Github projects, namely Big-Vul. We crawled the public Common Vulnerabilities and Exposures (CVE) database and CVE-related source code repositories. Specifically, we collected the descriptive information of the vulnerabilities from the CVE database, e.g., CVE IDs, CVE severity scores, and CVE summaries. With the CVE information and its related published Github code repository links, we downloaded all of the code repositories and extracted vulnerability related code changes. In total, Big-Vul contains 3,754 code vulnerabilities spanning 91 different vulnerability types. All these code vulnerabilities are extracted from 348 Github projects. All information is stored in the CSV format. We linked the code changes with the CVE descriptive information. Thus, our Big-Vul can be used for various research topics, e.g., detecting and fixing vulnerabilities, analyzing the vulnerability related code changes. Big-Vul is publicly available on Github.",C/C++ Code | Code Changes | Common Vulnerabilities and Exposures,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Fan, Jiahao;Li, Yi;Wang, Shaohua;Nguyen, Tien N.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093700105,10.1145/3379597.3387486,Dataset of Video Game Development Problems,"Different from traditional software development, there is little information about the software-engineering process and techniques in video-game development. One popular way to share knowledge among the video-game developers' community is the publishing of postmortems, which are documents summarizing what happened during the video-game development project. However, these documents are written without formal structure and often providing disparate information. Through this paper, we provide developers and researchers with grounded dataset describing software-engineering problems in video-game development extracted from postmortems. We created the dataset using an iterative method through which we manually coded more than 200 postmortems spanning 20 years (1998 to 2018) and extracted 1,035 problems related to software engineering while maintaining traceability links to the postmortems. We grouped the problems in 20 different types. This dataset is useful to understand the problems faced by developers during video-game development, providing researchers and practitioners a starting point to study video-game development in the context of software engineering.",Dataset | development problems | postmortem | video game,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Politowski, Cristiano;Petrillo, Fabio;Ullmann, Gabriel Cavalheiro;De Andrade Werly, Josias;Guéhéneuc, Yann Gaël",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093687740,10.1145/3379597.3387497,Hall-of-Apps: The Top Android Apps Metadata Archive,"The amount of Android apps available for download is constantly increasing, exerting a continuous pressure on developers to publish outstanding apps. Google Play (GP) is the default distribution channel for Android apps, which provides mobile app users with metrics to identify and report apps quality such as rating, amount of downloads, previous users comments, etc. In addition to those metrics, GP presents a set of top charts that highlight the outstanding apps in different categories. Both metrics and top app charts help developers to identify whether their development decisions are well valued by the community. Therefore, app presence in these top charts is a valuable information when understanding the features of top-apps. In this paper we present Hall-of-Apps, a dataset containing top charts' apps metadata extracted (weekly) from GP, for 4 different countries, during 30 weeks. The data is presented as (i) raw HTML files, (ii) a MongoDB database with all the information contained in app's HTML files (e.g., app description, category, general rating, etc.), and (iii) data visualizations built with the D3.js framework. A first characterization of the data along with the urls to retrieve it can be found in our online appendix: https://bit.ly/2uIkXs8. Dataset: https://bit.ly/2Wu9qsn","Android, Metadata | Google Play | Top Chart Apps","Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Bello-Jiménez, Laura;Escobar-Velásquez, Camilo;Mojica-Hanke, Anamaria;Cortés-Fernández, Santiago;Linares-Vásquez, Mario",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086829725,10.1145/3385412.3385997,Typilus: Neural type hints,"Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program's structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace - a continuous relaxation of the discrete space of types - and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.",Deep learning | Graph neural networks | Meta-learning | Structured learning | Type inference,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2020-06-11,Conference Paper,"Allamanis, Miltiadis;Barr, Earl T.;Ducousso, Soline;Gao, Zheng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086824027,10.1145/3385412.3385973,Repairing and mechanising the JavaScript relaxed memory model,"Modern JavaScript includes the SharedArrayBuffer feature, which provides access to true shared memory concurrency. SharedArrayBuffers are simple linear buffers of bytes, and the JavaScript specification defines an axiomatic relaxed memory model to describe their behaviour. While this model is heavily based on the C/C++11 model, it diverges in some key areas. JavaScript chooses to give a well-defined semantics to data-races, unlike the ""undefined behaviour"" of C/C++11. Moreover, the JavaScript model is mixed-size. This means that its accesses are not to discrete locations, but to (possibly overlapping) ranges of bytes. We show that the model, in violation of the design intention, does not support a compilation scheme to ARMv8 which is used in practice. We propose a correction, which also incorporates a previously proposed fix for a failure of the model to provide Sequential Consistency of Data-Race-Free programs (SC-DRF), an important correctness condition. We use model checking, in Alloy, to generate small counter-examples for these deficiencies, and investigate our correction. To accomplish this, we also develop a mixed-size extension to the existing ARMv8 axiomatic model. Guided by our Alloy experimentation, we mechanise (in Coq) the JavaScript model (corrected and uncorrected), our ARMv8 model, and, for the corrected JavaScript model, a ""model-internal"" SC-DRF proof and a compilation scheme correctness proof to ARMv8. In addition, we investigate a non-mixed-size subset of the corrected JavaScript model, and give proofs of compilation correctness for this subset to x86-TSO, Power, RISC-V, ARMv7, and (again) ARMv8, via the Intermediate Memory Model (IMM). As a result of our work, the JavaScript standards body (ECMA TC39) will include fixes for both issues in an upcoming edition of the specification.",Alloy | ARMv8 | Coq | Weak memory | Web worker,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2020-06-11,Conference Paper,"Watt, Conrad;Pulte, Christopher;Podkopaev, Anton;Barbier, Guillaume;Dolan, Stephen;Flur, Shaked;Pichon-Pharabod, Jean;Guo, Shu Yu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086824248,10.1145/3385412.3386001,Semantic code search via equational reasoning,"We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle's Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.",Code search | Equational reasoning,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2020-06-11,Conference Paper,"Premtoon, Varot;Koppel, James;Solar-Lezama, Armando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086820556,10.1145/3385412.3386027,Reconciling enumerative and deductive program synthesis,"Syntax-guided synthesis (SyGuS) aims to find a program satisfying semantic specification as well as user-provided structural hypotheses. There are two main synthesis approaches: enumerative synthesis, which repeatedly enumerates possible candidate programs and checks their correctness, and deductive synthesis, which leverages a symbolic procedure to construct implementations from specifications. Neither approach is strictly better than the other: automated deductive synthesis is usually very efficient but only works for special grammars or applications; enumerative synthesis is very generally applicable but limited in scalability. In this paper, we propose a cooperative synthesis technique for SyGuS problems with the conditional linear integer arithmetic (CLIA) background theory, as a novel integration of the two approaches, combining the best of the two worlds. The technique exploits several novel divide-and-conquer strategies to split a large synthesis problem to smaller subproblems. The subproblems are solved separately and their solutions are combined to form a final solution. The technique integrates two synthesis engines: a pure deductive component that can efficiently solve some problems, and a height-based enumeration algorithm that can handle arbitrary grammar. We implemented the cooperative synthesis technique, and evaluated it on a wide range of benchmarks. Experiments showed that our technique can solve many challenging synthesis problems not possible before, and tends to be more scalable than state-of-the-art synthesis algorithms.",Deductive synthesis | Divide-and-Conquer | Enumerative synthesis | Syntax-guided synthesis,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2020-06-11,Conference Paper,"Huang, Kangjing;Qiu, Xiaokang;Shen, Peiyuan;Wang, Yanjun",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099250609,10.1145/3324884.3416558,Broadening Horizons of Multilingual Static Analysis: Semantic Summary Extraction from C Code for JNI Program Analysis,"Most programming languages support foreign language interoperation that allows developers to integrate multiple modules implemented in different languages into a single multilingual program. While utilizing various features from multiple languages expands expressivity, differences in language semantics require developers to understand the semantics of multiple languages and their inter-operation. Because current compilers do not support compile-time checking for interoperation, they do not help developers avoid in-teroperation bugs. Similarly, active research on static analysis and bug detection has been focusing on programs written in a single language. In this paper, we propose a novel approach to analyze multilingual programs statically. Unlike existing approaches that extend a static analyzer for a host language to support analysis of foreign function calls, our approach extracts semantic summaries from programs written in guest languages using a modular analysis technique, and performs a whole-program analysis with the extracted semantic summaries. To show practicality of our approach, we design and implement a static analyzer for multilingual programs, which analyzes JNI interoperation between Java and C. Our empirical evaluation shows that the analyzer is scalable in that it can construct call graphs for large programs that use JNI interoperation, and useful in that it found 74 genuine interoperation bugs in real-world Android JNI applications.",Java Native Interface | Language Interoperability | Multilingual Program Analysis,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Lee, Sungho;Lee, Hyogun;Ryu, Sukyoung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099244937,10.1145/3324884.3416626,Demystifying Loops in Smart Contracts,"This paper aims to shed light on how loops are used in smart contracts. Towards this goal, we study various syntactic and semantic characteristics of loops used in over 20,000 Solidity contracts deployed on the Ethereum blockchain, with the goal of informing future research on program analysis for smart contracts. Based on our findings, we propose a small domain-specific language (DSL) that can be used to summarize common looping patterns in Solidity. To evaluate what percentage of smart contract loops can be expressed in our proposed DSL, we also design and implement a program synthesis toolchain called Solis that can synthesize loop summaries in our DSL. Our evaluation shows that at least 56% of the analyzed loops can be summarized in our DSL, and 81% of these summaries are exactly equivalent to the original loop.",loop summarization | program synthesis | smart contracts,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Mariano, Benjamin;Chen, Yanju;Feng, Yu;Lahiri, Shuvendu K.;Dillig, Isil",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099196841,10.1145/3324884.3416639,ER Catcher: A Static Analysis Framework for Accurate and Scalable Event-Race Detection in Android,"Android platform provisions a number of sophisticated concurrency mechanisms for the development of apps. The concurrency mechanisms, while powerful, are quite difficult to properly master by mobile developers. In fact, prior studies have shown concurrency issues, such as event-race defects, to be prevalent among real-world Android apps. In this paper, we propose a flow-, context-, and thread-sensitive static analysis framework, called ER Catcher, for detection of event-race defects in Android apps. ER Catcher introduces a new type of summary function aimed at modeling the concurrent behavior of methods in both Android apps and libraries. In addition, it leverages a novel, statically constructed Vector Clock for rapid analysis of happens-before relations. Altogether, these design choices enable ER Catcher to not only detect event-race defects with a substantially higher degree of accuracy, but also in a fraction of time compared to the existing state-of-the-art technique.",Android | Concurrency | Event-Race Detection | Program Analysis,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Salehnamadi, Navid;Alshayban, Abdulaziz;Ahmed, Iftekhar;Malek, Sam",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099225955,10.1145/3324884.3416627,A Deep Multitask Learning Approach for Requirements Discovery and Annotation from Open Forum,"The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91 % and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.",Deep learning | Multitask learning | Requirements annotation | Requirements discovery,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Li, Mingyang;Shi, Lin;Yang, Ye;Wang, Qing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099180545,10.1145/3324884.3416538,Stay Professional and Efficient: Automatically Generate Titles for Your Bug Reports,"Bug reports in a repository are generally organized line by line in a list-view, with their titles and other meta-data displayed. In this list-view, a concise and precise title plays an important role that enables project practitioners to quickly and correctly digest the core idea of the bug, without carefully reading the corresponding details. However, the quality of bug report titles varies in open-source communities, which may be due to the limited time and unprofessionalism of authors. To help report authors efficiently draft good-quality titles, we propose a method, named iTAPE, to automatically generate titles for their bug reports. iTAPE formulates title generation into a one-sentence summarization task. By properly tackling two domain-specific challenges (i.e. lacking off-the-shelf dataset and handling the low-frequency human-named tokens), iTAPE then generates titles using a Seq2Seq-based model. A comprehensive experimental study shows that iTAPE can obtain fairly satisfactory results, in terms of the comparison with three latest one-sentence summarization works, as well as the feedback from human evaluation.",bug report quality | issue title generation | low-frequency token handling | one-sentence summarization,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Chen, Songqiang;Xie, Xiaoyuan;Yin, Bangguo;Ji, Yuanxiang;Chen, Lin;Xu, Baowen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099183853,10.1145/3324884.3416551,API-Misuse Detection Driven by Fine-Grained API-Constraint Knowledge Graph,"API misuses cause significant problem in software development. Existing methods detect API misuses against frequent API usage patterns mined from codebase. They make a naive assumption that API usage that deviates from the most-frequent API usage is a misuse. However, there is a big knowledge gap between API usage patterns and API usage caveats in terms of comprehensiveness, explainability and best practices. In this work, we propose a novel approach that detects API misuses directly against the API caveat knowledge, rather than API usage patterns. We develop open information extraction methods to construct a novel API-constraint knowledge graph from API reference documentation. This knowledge graph explicitly models two types of API-constraint relations (call-order and condition-checking) and enriches return and throw relations with return conditions and exception triggers. It empowers the detection of three types of frequent API misuses - missing calls, missing condition checking and missing exception handling, while existing detectors mostly focus on only missing calls. As a proof-of-concept, we apply our approach to Java SDK API Specification. Our evaluation confirms the high accuracy of the extracted API-constraint relations. Our knowledge-driven API misuse detector achieves 0.60 (68/113) precision and 0.28 (68/239) recall for detecting Java API misuses in the API misuse benchmark MuBench. This performance is significantly higher than that of existing pattern-based API misused detectors. A pilot user study with 12 developers shows that our knowledge-driven API misuse detection is very promising in helping developers avoid API misuses and debug the bugs caused by API misuses.",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Ren, Xiaoxue;Ye, Xinyuan;Xing, Zhenchang;Xia, Xin;Xu, Xiwei;Zhu, Liming;Sun, Jianling",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099192253,10.1145/3324884.3416632,JISET: JavaScript IR-based Semantics Extraction Toolchain,"JavaScript was initially designed for client-side programming in web browsers, but its engine is now embedded in various kinds of host software. Despite the popularity, since the JavaScript semantics is complex especially due to its dynamic nature, understanding and reasoning about JavaScript programs are challenging tasks. Thus, researchers have proposed several attempts to define the formal semantics of JavaScript based on ECMAScript, the official JavaScript specification. However, the existing approaches are manual, labor-intensive, and error-prone and all of their formal semantics target ECMAScript 5.1 (ES5.1, 2011) or its former versions. Therefore, they are not suitable for understanding modern JavaScript language features introduced since ECMAScript 6 (ES6, 2015). Moreover, ECMAScript has been annually updated since ES6, which already made five releases after ES5.1. To alleviate the problem, we propose JISET, a JavaScript IR-based Semantics Extraction Toolchain. It is the first tool that automatically synthesizes parsers and AST-IR translators directly from a given language specification, ECMAScript. For syntax, we develop a parser generation technique with lookahead parsing for BNFES, a variant of the extended BNF used in ECMAScript. For semantics, JISET synthesizes AST-IR translators using forward compatible rule-based compilation. Compile rules describe how to convert each step of abstract algorithms written in a structured natural language into IRES, an Intermediate Representation that we designed for ECMAScript. For the four most recent ECMAScript versions, JISET automatically synthesized parsers for all versions, and compiled 95.03% of the algorithm steps on average. After we complete the missing parts manually, the extracted core semantics of the latest ECMAScript (ES10, 2019) passed all 18, 064 applicable tests. Using this first formal semantics of modern JavaScript, we found nine specification errors in ES10, which were all confirmed by the Ecma Technical Committee 39. Furthermore, we showed that JISET is forward compatible by applying it to nine feature proposals ready for inclusion in the next ECMAScript, which let us find three errors in the BigInt proposal.",JavaScript | mechanized formal semantics | program synthesis,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Park, Jihyeok;Park, Jihee;An, Seungmin;Ryu, Sukyoung",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099248414,10.1145/3324884.3416640,Team Discussions and Dynamics during DevOps Tool Adoptions in OSS Projects,"In Open Source Software (OSS) projects, pre-built tools dominate DevOps-oriented pipelines. In practice, a multitude of configuration management, cloud-based continuous integration, and automated deployment tools exist, and often more than one for each task. Tools are adopted (and given up) by OSS projects regularly. Prior work has shown that some tool adoptions are preceded by discussions, and that tool adoptions can result in benefits to the project. But important questions remain: how do teams decide to adopt a tool? What is discussed before the adoption and for how long? And, what team characteristics are determinant of the adoption?In this paper, we employ a large-scale empirical study in order to characterize the team discussions and to discern the team-level determinants of tool adoption into OSS projects' development pipelines. Guided by theories of team and individual motivations and dynamics, we perform exploratory data analyses, do deep-dive case studies, and develop regression models to learn the determinants of adoption and discussion length, and the direction of their effect on the adoption. From data of commit and comment traces of large-scale GitHub projects, our models find that prior exposure to a tool and member involvement are positively associated with the tool adoption, while longer discussions and the number of newer team members associate negatively. These results can provide guidance beyond the technical appropriateness for the timeliness of tool adoptions in diverse programmer teams. Our data and code is available at https://github.com/lkyin/tool_adoptions. In this paper, we employ a large-scale empirical study in order to characterize the team discussions and to discern the team-level determinants of tool adoption into OSS projects' development pipelines. Guided by theories of team and individual motivations and dynamics, we perform exploratory data analyses, do deep-dive case studies, and develop regression models to learn the determinants of adoption and discussion length, and the direction of their effect on the adoption. From data of commit and comment traces of large-scale GitHub projects, our models find that prior exposure to a tool and member involvement are positively associated with the tool adoption, while longer discussions and the number of newer team members associate negatively. These results can provide guidance beyond the technical appropriateness for the timeliness of tool adoptions in diverse programmer teams. Our data and code is available at https://github.com/lkyin/tool_adoptions.",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Yin, Likang;Filkov, Vladimir",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099246480,10.1145/3324884.3416634,Scaling Client-Specific Equivalence Checking via Impact Boundary Search,"Client-specific equivalence checking (CSEC) is a technique proposed previously to perform impact analysis of changes to downstream components (libraries) from the perspective of an unchanged system (client). Existing analysis techniques, whether general (re-gression verification, equivalence checking) or special-purpose, when applied to CSEC, either require users to provide specifications, or do not scale. We propose a novel solution to the CSEC problem, called 2clever, that is based on searching the control-flow of a program for impact boundaries. We evaluate a prototype implementation of 2clever on a comprehensive set of benchmarks and conclude that our prototype performs well compared to the state-of-the-art.",Formal methods | Maintenance and evolution | Software analysis | Testing | validation | verification,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Feng, Nick;Mora, Federico;Hui, Vincent;Chechik, Marsha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099210573,10.1145/3324884.3416546,"Code to Comment 'Translation': Data, Metrics, Baselining Evaluation","The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task-specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CODENN, DEEPCOM, FUNCOM, and Docstring. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art natural language translators. We found some interesting differences between the code-comment data and the WMT19 natural language data. Next, we describe and conduct some studies to calibrate BLEU (which is commonly used as a measure of comment quality). using 'affinity pairs' of methods, from different projects, in the same project, in the same class, etc; Our study suggests that the current performance on some datasets might need to be improved substantially. We also argue that fairly naive information retrieval (IR) methods do well enough at this task to be considered a reasonable baseline. Finally, we make some suggestions on how our findings might be used in future research in this area.",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Gros, David;Sezhiyan, Hariharan;Devanbu, Prem;Yu, Zhou",Include,
10.1016/j.jss.2022.111515,2-s2.0-85096721907,10.1145/3324884.3416628,Generating Concept based API Element Comparison Using a Knowledge Graph,"Developers are concerned with the comparison of similar APIs in terms of their commonalities and (often subtle) differences. Our empirical study of Stack Overflow questions and API documentation confirms that API comparison questions are common and can often be answered by knowledge contained in API reference documentation. Our study also identifies eight types of API statements that are useful for API comparison. Based on these findings, we propose a knowledge graph based approach APIComp that automatically extracts API knowledge from API reference documentation to support the comparison of a pair of API classes or methods from different aspects. Our approach includes an offline phase for constructing an API knowledge graph, and an online phase for generating an API comparison result for a given pair of API elements. Our evaluation shows that the quality of different kinds of extracted knowledge in the API knowledge graph is generally high. Furthermore, the comparison results generated by APIComp are significantly better than those generated by a baseline approach based on heuristic rules and text similarity, and our generated API comparison results are useful for helping developers in API selection tasks.",API | Documentation | Knowledge Extraction | Knowledge Graph,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Liu, Yang;Liu, Mingwei;Peng, Xin;Treude, Christoph;Xing, Zhenchang;Zhang, Xiaoxin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099237019,10.1145/3324884.3416543,Understanding Performance Concerns in the API Documentation of Data Science Libraries,"The development of efficient data science applications is often impeded by unbearably long execution time and rapid RAM exhaustion. Since API documentation is the primary information source for troubleshooting, we investigate how performance concerns are documented in popular data science libraries. Our quantitative results reveal the prevalence of data science APIs that are documented in performance-related context and the infrequent maintenance activities on such documentation. Our qualitative analyses further reveal that crowd documentation like Stack Overflow and GitHub are highly complementary to official documentation in terms of the API coverage, the knowledge distribution, as well as the specific information conveyed through performance-related content. Data science practitioners could benefit from our findings by learning a more targeted search strategy for resolving performance issues. Researchers can be more assured of the advantages of integrating both the official and the crowd documentation to achieve a holistic view on the performance concerns in data science development.",API documentation | data science | empirical study | performance,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Tao, Yida;Jiang, Jiefang;Liu, Yepang;Xu, Zhiwu;Qin, Shengchao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099211873,10.1145/3324884.3416582,Automated Third-Party Library Detection for Android Applications: Are We There Yet?,"Third-party libraries (TPLs) have become a significant part of the Android ecosystem. Developers can employ various TPLs with different functionalities to facilitate their app development. Unfortunately, the popularity of TPLs also brings new challenges and even threats. TPLs may carry malicious or vulnerable code, which can infect popular apps to pose threats to mobile users. Besides, the code of third-party libraries could constitute noises in some downstream tasks (e.g., malware and repackaged app detection). Thus, researchers have developed various tools to identify TPLs. However, no existing work has studied these TPL detection tools in detail; different tools focus on different applications with performance differences, but little is known about them. To better understand existing TPL detection tools and dissect TPL detection techniques, we conduct a comprehensive empirical study to fill the gap by evaluating and comparing all publicly available TPL detection tools based on four criteria: effectiveness, efficiency, code obfuscation-resilience capability, and ease of use. We reveal their advantages and disadvantages based on a systematic and thorough empirical study. Furthermore, we also conduct a user study to evaluate the usability of each tool. The results showthat LibScout outperforms others regarding effectiveness, LibRadar takes less time than others and is also regarded as the most easy-to-use one, and LibPecker performs the best in defending against code obfuscation techniques. We further summarize the lessons learned from different perspectives, including users, tool implementation, and researchers. Besides, we enhance these open-sourced tools by fixing their limitations to improve their detection ability. We also build an extensible framework that integrates all existing available TPL detection tools, providing online service for the research community. We make publicly available the evaluation dataset and enhanced tools. We believe our work provides a clear picture of existing TPL detection techniques and also give a road-map for future directions.",Android | Empirical study | Library detection | Third-party library,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Zhan, Xian;Fan, Lingling;Liu, Tianming;Chen, Sen;Li, Li;Wang, Haoyu;Xu, Yifei;Luo, Xiapu;Liu, Yang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099264368,10.1145/3324884.3416625,Towards Generating Thread-Safe Classes Automatically,"The existing concurrency model for Java (or C) requires programmers to design and implement thread-safe classes by explicitly acquiring locks and releasing locks. Such a model is error-prone and is the reason for many concurrency bugs. While there are alternative models like transactional memory, manually writing locks remains prevalent in practice. In this work, we propose AutoLock, which aims to solve the problem by fully automatically generating thread-safe classes. Given a class which is assumed to be correct with sequential clients, AutoLock automatically generates a thread-safe class which is linearizable, and does it in a way without requiring a specification of the class. AutoLock takes three steps: (1) infer access annotations (i.e., abstract information on how variables are accessed and aliased), (2) synthesize a locking policy based on the access annotations, and (3) consistently implement the locking policy. AutoLock has been evaluated on a set of benchmark programs and the results show that AutoLock generates thread-safe classes effectively and could have prevented existing concurrency bugs.",concurrency | locking policy | thread safe class,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Wang, Haichi;Wang, Zan;Sun, Jun;Liu, Shuang;Sadiq, Ayesha;Li, Yuan Fang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099193166,10.1145/3324884.3416579,Scalable Multiple-View Analysis of Reactive Systems via Bidirectional Model Transformations,"Systematic model-driven design and early validation enable engineers to verify that a reactive system does not violate its requirements before actually implementing it. Requirements may come from multiple stakeholders, who are often concerned with different facets - design typically involves different experts having different concerns and views of the system. Engineers start from a specification which may be sourced from some domain model, while validation is often done on state-transition structures that support model checking. Two computationally expensive steps may work against scalability: transformation from specification to state-transition structures, and model checking. We propose a technique that makes the former efficient and also makes the resulting transition systems small enough to be efficiently verified. The technique automatically projects the specification into submodels depending on a property sought to be evaluated, which captures some stakeholder's viewpoint. The resulting reactive system submodel is then transformed into a state-transition structure and verified. The technique achieves cone-of-influence reduction, by slicing at the specification model level. Submodels are analysis-equivalent to the corresponding full model. If stakeholders propose a change to a submodel based on their own view, changes are automatically propagated to the specification model and other views affected. Automated reflection is achieved thanks to bidirectional model transformations, ensuring correctness. We cast our proposal in the context of graph-based reactive systems whose dynamics is described by rewriting rules. We demonstrate our view-based framework in practice on a case study within cyber-physical systems.",Model checking | model transformations | reactive systems | software engineering,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Tsigkanos, Christos;Li, Nianyu;Jin, Zhi;Hu, Zhenjiang;Ghezzi, Carlo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85098140202,10.1145/3324884.3416631,Trace-Checking Signal-based Temporal Properties: A Model-Driven Approach,"Signal-based temporal properties (SBTPs) characterize the behavior of a system when its inputs and outputs are signals over time; they are very common for the requirements specification of cyber-physical systems. Although there exist several specification languages for expressing SBTPs, such languages either do not easily allow the specification of important types of properties (such as spike or oscillatory behaviors), or are not supported by (efficient) trace-checking procedures. In this paper, we propose SB-TemPsy, a novel model-driven trace-checking approach for SBTPs. SB-TemPsy provides (i) SB-TemPsy-DSL, a domain-specific language that allows the specification of SBTPs covering the most frequent requirement types in cyber-physical systems, and (ii) SB-TemPsy-Check, an efficient, model-driven trace-checking procedure. This procedure reduces the problem of checking an SB-TemPsy-DSL property over an execution trace to the problem of evaluating an Object Constraint Language constraint on a model of the execution trace. We evaluated our contributions by assessing the expressiveness of SB-TemPsy-DSL and the applicability of SB-TemPsy-Check using a representative industrial case study in the satellite domain. SB-TemPsy-DSL could express 97% of the requirements of our case study and SB-TemPsy-Check yielded a trace-checking verdict in 87% of the cases, with an average checking time of 48.7 s. From a practical standpoint and compared to state-of-the-art alternatives, our approach strikes a better trade-off between expressiveness and performance as it supports a large set of property types that can be checked, in most cases, within practical time limits.",cyber-physical systems | model-driven | run-time verification | signals | specification patterns | temporal properties | trace checking,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Boufaied, Chaima;Menghi, Claudio;Bianculli, Domenico;Briand, Lionel;Parache, Yago Isasi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099195358,10.1145/3324884.3416553,Cross-Contract Static Analysis for Detecting Practical Reentrancy Vulnerabilities in Smart Contracts,"Reentrancy bugs, one of the most severe vulnerabilities in smart contracts, have caused huge financial loss in recent years. Researchers have proposed many approaches to detecting them. However, empirical studies have shown that these approaches suffer from undesirable false positives and false negatives, when the code under detection involves the interaction between multiple smart contracts. In this paper, we propose an accurate and efficient cross-contract reentrancy detection approach in practice. Rather than design rule-of-thumb heuristics, we conduct a large empirical study of 11714 real-world contracts from Etherscan against three well-known general-purpose security tools for reentrancy detection. We manually summarized the reentrancy scenarios where the state-of-the-art approaches cannot address. Based on the empirical evidence, we present Clairvoyance, a cross-function and cross-contract static analysis to detect reentrancy vulnerabilities in real world with significantly higher accuracy. To reduce false negatives, we enable, for the first time, a cross-contract call chain analysis by tracking possibly tainted paths. To reduce false positives, we systematically summarized five major path protective techniques (PPTs) to support fast yet precise path feasibility checking. We implemented our approach and compared Clairvoyance with five state-of-the-art tools on 17770 real-worlds contracts. The results show that Clairvoyance yields the best detection accuracy among all the five tools and also finds 101 unknown reentrancy vulnerabilities.",cross-contract analysis | reentrancy vulnerabilities | smart contracts | static taint analysis,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Xue, Yinxing;Ma, Mingliang;Lin, Yun;Sui, Yulei;Ye, Jiaming;Peng, Tianyong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099209638,10.1145/3324884.3416630,Synthesis-Based Resolution of Feature Interactions in Cyber-Physical Systems,"The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.",Cyber physical systems | Feature interactions | Signal temporal logic,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Gafford, Benjamin;Durschmid, Tobias;Moreno, Gabriel A.;Kang, Eunsuk",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099191406,10.1145/3324884.3416646,Summary-Based Symbolic Evaluation for Smart Contracts,"This paper presents Solar, a system for automatic synthesis of adversarial contracts that exploit vulnerabilities in a victim smart contract. To make the synthesis tractable, we introduce a query language as well as summary-based symbolic evaluation, which significantly reduces the number of instructions that our synthesizer needs to evaluate symbolically, without compromising the precision of the vulnerability query. We encoded common vulnerabilities of smart contracts and evaluated Solar on the entire data set from Etherscan. Our experiments demonstrate the benefits of summary-based symbolic evaluation and show that Solar outperforms state-of-the-art smart contracts analyzers, TEETHER, Mythril, and Contract Fuzzer, in terms of running time and precision.",n/a,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Feng, Yu;Torlak, Emina;Bodik, Rastislav",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099256456,10.1145/3324884.3415287,Applying Learning Techniques to Oracle Synthesis,"Software reliability is a primary concern in the construction of software, and thus a fundamental component in the definition of software quality. Analyzing software reliability requires a specification of the intended behavior of the software under analysis. Unfortunately, software many times lacks such specifications. This issue seriously diminishes the analyzability of software with respect to its reliability. Thus, finding novel techniques to capture the intended software behavior in the form of specifications would allow us to exploit them for automated reliability analysis. Our research focuses on the application of learning techniques to automatically distinguish correct from incorrect software behavior. The aim here is to decrease the developer's effort in specifying oracles, and instead generating them from actual software behaviors.",evolutionary computation | neural networks | oracle synthesis | program analysis,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Molina, Facundo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099221956,10.1145/3324884.3415285,SAT-Based Arithmetic Support for Alloy,"Formal specifications in Alloy are organized around user-defined data domains, associated with signatures, with almost no support for built-in datatypes. This minimality in the built-in datatypes provided by the language is one of its main features, as it contributes to the automated analyzability of models. One of the few built-in datatypes available in Alloy specifications are integers, whose SAT-based treatment allows only for small bit-widths. In many contexts, where relational datatypes dominate, the use of integers may be auxiliary, e.g., in the use of cardinality constraints and other features. However, as the applications of Alloy are increased, e.g., with the use of the language and its tool support as backend engine for different analysis tasks, the provision of efficient support for numerical datatypes becomes a need. In this work, we present our current preliminary approach to providing an efficient, scalable and user-friendly extension to Alloy, with arithmetic support for numerical datatypes. Our implementation allows for arithmetic with varying precisions, and is implemented via standard Alloy constructions, thus resorting to SAT solving for resolving arithmetic constraints in models.",alloy | sat solving,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Cornejo, Cesar",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099199885,10.1145/3324884.3421837,Automatic Generation of IFTTT Mashup Infrastructures,"In recent years, IF-This-Then-That (IFTTT) services are becoming more and more popular. Many platforms such as Zapier, IFTTT.com, and Workato provide such services, which allow users to create workflows with 'triggers' and 'actions' by using Web Application Programming Interfaces (APIs). However, the number of IFTTT recipes in the above platforms increases much slower than the growth of Web APIs. This is because human efforts are still largely required to build and deploy IFTTT recipes in the above platforms. To address this problem, in this paper, we present an automation tool to automatically generate the IFTTT mashup infrastructure. The proposed tool provides 5 REST APIs, which can automatically generate triggers, rules, and actions in AWS, and create a workflow XML to describe an IFTTT mashup by connecting the triggers, rules, and actions. This workflow XML is automatically sent to Fujitsu RunMyProcess (RMP) to set up and execute IFTTT mashup. The proposed tool, together with its associated method and procedure, enables an end-to-end solution for automatically creating, deploying, and executing IFTTT mashups in a few seconds, which can greatly reduce the development cycle and cost for new IFTTT mashups.",Automation | AWS | IFTTT | RMP | Web APIs,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Liu, Lei;Bahrami, Mehdi;Chen, Wei Peng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099242927,10.1145/3324884.3418910,"The Symptom, Cause and Repair of Workaround","In software development, issue tracker systems are widely used to manage bug reports. In such a system, a bug report can be filed, diagnosed, assigned, and fixed. In the standard process, a bug can be resolved as fixed, invalid, duplicated or won't fix. Although the above resolutions are well-defined and easy to understand, a bug report can end with a less known resolution, i.e., workaround. Compared with other resolutions, the definition of workarounds is more ambiguous. Besides the problem that is reported in a bug report, the resolution of a workaround raises more questions. Some questions are important for users, especially those programmers who build their projects upon others (e.g., libraries). Although some early studies have been conducted to analyze API workarounds, many research questions on workarounds are still open. For example, which bugs are resolved as workarounds? Why is a bug report resolved as workarounds? What are the repairs of workarounds? In this experience paper, we conduct the first empirical study to explore the above research questions. In particular, we analyzed 221 real workarounds that were collected from Apache projects. Our results lead to some interesting and useful answers to all the above questions. For example, we find that most bug reports are resolved as workarounds, because their problems reside in libraries (24.43%), settings (18.55%), and clients (10.41%). Among them, many bugs are difficult to be fixed fully and perfectly. As a late breaking result, we can only briefly introduce our study, but we present a detailed plan to extend it to a full paper.ACM Reference Format: Daohan Song, Hao Zhong, and Li Jia. 2020. The Symptom, Cause and Repair of Workaround. In 35th IEEE/ACM International Conference on Automated Software Engineering (ASE '20), September 21-25, 2020, Virtual Event, Australia. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3324884.3418910.",bug report and repair | empirical study | workaround,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Song, Daohan;Zhong, Hao;Jia, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099207011,10.1145/3324884.3415290,FILO: FIx-LOcus Localization for Backward Incompatibilities Caused by Android Framework Upgrades,"Mobile operating systems evolve quickly, frequently updating the APIs that app developers use to build their apps. Unfortunately, API updates do not always guarantee backward compatibility, causing apps to not longer work properly or even crash when running with an updated system. This paper presents FILO, a tool that assists Android developers in resolving backward compatibility issues introduced by API upgrades. FILO both suggests the method that needs to be modified in the app in order to adapt the app to an upgraded API, and reports key symptoms observed in the failed execution to facilitate the fixing activity. Results obtained with the analysis of 12 actual upgrade problems and the feedback produced by early tool adopters show that FILO can practically support Android developers. FILO can be downloaded from https://gitlab.com/learnERC/filo, and its video demonstration is available at https://youtu.be/WDvkKj-wnlQ.",Android | API upgrades | Debugging | Fault localization,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Mobilio, Marco;Riganelli, Oliviero;Micucci, Daniela;Mariani, Leonardo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099228142,10.1145/3324884.3415305,RepoSkillMiner: Identifying software expertise from GitHub repositories using Natural Language Processing,"A GitHub profile is becoming an essential part of a developer's resume enabling HR departments to extract someone's expertise, through automated analysis of his/her contribution to open-source projects. At the same time, having clear insights on the technologies used in a project can be very beneficial for resource allocation and project maintainability planning. In the literature, one can identify various approaches for identifying expertise on programming languages, based on the projects that developer contributed to. In this paper, we move one step further and introduce an approach (ac-companied by a tool) to identify low-level expertise on particular software frameworks and technologies apart, relying solely on GitHub data, using the GitHub API and Natural Language Processing (NLP)-using the Microsoft Language Understanding Intelligent Service (LUIS). In particular, we developed an NLP model in LUIS for named-entity recognition for three (3). NET technologies and two (2) front-end frameworks. Our analysis is based upon specific commit contents, in terms of the exact code chunks, which the committer added or changed. We evaluate the precision, recall and f-measure for the derived technologies/frameworks, by conducting a batch test in LUIS and report the results. The proposed approach is demonstrated through a fully functional web application named RepoSkillMiner.",Expertise | Frameworks | GitHub | Natural Language Processing | Software Project Management,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Kourtzanidis, Stratos;Chatzigeorgiou, Alexander;Ampatzoglou, Apostolos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85099189206,10.1145/3324884.3418913,The Classification and Propagation of Program Comments,"Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.",comment | programm analysis,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Xu, Xiangzhe",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85089216036,10.1007/978-3-030-53288-8_6,Formal Analysis and Redesign of a Neural Network-Based Aircraft Taxiing System with VerifAI,"We demonstrate a unified approach to rigorous design of safety-critical autonomous systems using the VerifAI toolkit for formal analysis of AI-based systems. VerifAI provides an integrated toolchain for tasks spanning the design process, including modeling, falsification, debugging, and ML component retraining. We evaluate all of these applications in an industrial case study on an experimental autonomous aircraft taxiing system developed by Boeing, which uses a neural network to track the centerline of a runway. We define runway scenarios using the Scenic probabilistic programming language, and use them to drive tests in the X-Plane flight simulator. We first perform falsification, automatically finding environment conditions causing the system to violate its specification by deviating significantly from the centerline (or even leaving the runway entirely). Next, we use counterexample analysis to identify distinct failure cases, and confirm their root causes with specialized testing. Finally, we use the results of falsification and debugging to retrain the network, eliminating several failure cases and improving the overall performance of the closed-loop system.",Automated testing | Autonomous systems | Debugging | Falsification | Machine learning | Simulation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Fremont, Daniel J.;Chiu, Johnathan;Margineantu, Dragos D.;Osipychev, Denis;Seshia, Sanjit A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089226412,10.1007/978-3-030-53288-8_9,Stratified Abstraction of Access Control Policies,"The shift to cloud-based APIs has made application security critically depend on understanding and reasoning about policies that regulate access to cloud resources. We present stratified predicate abstraction, a new approach that summarizes complex security policies into a compact set of positive and declarative statements that precisely state who has access to a resource. We have implemented stratified abstraction and deployed it as the engine powering AWS’s IAM Access Analyzer service, and hence, demonstrate how formal methods and SMT can be used for security policy explanation.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Backes, John;Berrueco, Ulises;Bray, Tyler;Brim, Daniel;Cook, Byron;Gacek, Andrew;Jhala, Ranjit;Luckow, Kasper;McLaughlin, Sean;Menon, Madhav;Peebles, Daniel;Pugalia, Ujjwal;Rungta, Neha;Schlesinger, Cole;Schodde, Adam;Tanuku, Anvesh;Varming, Carsten;Viswanathan, Deepa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089228669,10.1007/978-3-030-53288-8_13,"Semantics, Specification, and Bounded Verification of Concurrent Libraries in Replicated Systems","Geo-replicated systems provide a number of desirable properties such as globally low latency, high availability, scalability, and built-in fault tolerance. Unfortunately, programming correct applications on top of such systems has proven to be very challenging, in large part because of the weak consistency guarantees they offer. These complexities are exacerbated when we try to adapt existing highly-performant concurrent libraries developed for shared-memory environments to this setting. The use of these libraries, developed with performance and scalability in mind, is highly desirable. But, identifying a suitable notion of correctness to check their validity under a weakly consistent execution model has not been well-studied, in large part because it is problematic to naïvely transplant criteria such as linearizability that has a useful interpretation in a shared-memory context to a distributed one where the cost of imposing a (logical) global ordering on all actions is prohibitive. In this paper, we tackle these issues by proposing appropriate semantics and specifications for highly-concurrent libraries in a weakly-consistent, replicated setting. We use these specifications to develop a static analysis framework that can automatically detect correctness violations of library implementations parameterized with respect to the different consistency policies provided by the underlying system. We use our framework to analyze the behavior of a number of highly non-trivial library implementations of stacks, queues, and exchangers. Our results provide the first demonstration that automated correctness checking of concurrent libraries in a weakly geo-replicated setting is both feasible and practical.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Nagar, Kartik;Mukherjee, Prasita;Jagannathan, Suresh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089226889,10.1007/978-3-030-53288-8_16,Hampa: Solver-Aided Recency-Aware Replication,"Replication is a common technique to build reliable and scalable systems. Traditional strong consistency maintains the same total order of operations across replicas. This total order is the source of multiple desirable consistency properties: integrity, convergence and recency. However, maintaining the total order has proven to inhibit availability and performance. Weaker notions exhibit responsiveness and scalability; however, they forfeit the total order and hence its favorable properties. This project revives these properties with as little coordination as possible. It presents a tool called that given a sequential object with the declaration of its integrity and recency requirements, automatically synthesizes a correct-by-construction replicated object that simultaneously guarantees the three properties. It features a relational object specification language and a syntax-directed analysis that infers optimum staleness bounds. Further, it defines coordination-avoidance conditions and the operational semantics of replicated systems that provably guarantees the three properties. It characterizes the computational power and presents a protocol for recency-aware objects. uses automatic solvers statically and embeds them in the runtime to dynamically decide the validity of coordination-avoidance conditions. The experiments show that recency-aware objects reduce coordination and response time.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Li, Xiao;Houshmand, Farzin;Lesani, Mohsen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089219940,10.1007/978-3-030-53291-8_3,RTLola Cleared for Take-Off: Monitoring Autonomous Aircraft,"The autonomous control of unmanned aircraft is a highly safety-critical domain with great economic potential in a wide range of application areas, including logistics, agriculture, civil engineering, and disaster recovery. We report on the development of a dynamic monitoring framework for the DLR ARTIS (Autonomous Rotorcraft Testbed for Intelligent Systems) family of unmanned aircraft based on the formal specification language RTLola. RTLola is a stream-based specification language for real-time properties. An RTLola specification of hazardous situations and system failures is statically analyzed in terms of consistency and resource usage and then automatically translated into an FPGA-based monitor. Our approach leads to highly efficient, parallelized monitors with formal guarantees on the noninterference of the monitor with the normal operation of the autonomous system.",Autonomous aircraft | FPGA | Runtime verification | Stream monitoring,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Baumeister, Jan;Finkbeiner, Bernd;Schirmer, Sebastian;Schwenger, Maximilian;Torens, Christoph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089226737,10.1007/978-3-030-53291-8_16,Certifying Certainty and Uncertainty in Approximate Membership Query Structures,"Approximate Membership Query structures (AMQs) rely on randomisation for time- and space-efficiency, while introducing a possibility of false positive and false negative answers. Correctness proofs of such structures involve subtle reasoning about bounds on probabilities of getting certain outcomes. Because of these subtleties, a number of unsound arguments in such proofs have been made over the years. In this work, we address the challenge of building rigorous and reusable computer-assisted proofs about probabilistic specifications of AMQs. We describe the framework for systematic decomposition of AMQs and their properties into a series of interfaces and reusable components. We implement our framework as a library in the Coq proof assistant and showcase it by encoding in it a number of non-trivial AMQs, such as Bloom filters, counting filters, quotient filters and blocked constructions, and mechanising the proofs of their probabilistic specifications. We demonstrate how AMQs encoded in our framework guarantee the absence of false negatives by construction. We also show how the proofs about probabilities of false positives for complex AMQs can be obtained by means of verified reduction to the implementations of their simpler counterparts. Finally, we provide a library of domain-specific theorems and tactics that allow a high degree of automation in probabilistic proofs.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Gopinathan, Kiran;Sergey, Ilya",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089217718,10.1007/978-3-030-53291-8_24,AMYTISS: Parallelized Automated Controller Synthesis for Large-Scale Stochastic Systems,"In this paper, we propose a software tool, called AMYTISS, implemented in C++/OpenCL, for designing correct-by-construction controllers for large-scale discrete-time stochastic systems. This tool is employed to (i) build finite Markov decision processes (MDPs) as finite abstractions of given original systems, and (ii) synthesize controllers for the constructed finite MDPs satisfying bounded-time high-level properties including safety, reachability and reach-avoid specifications. In AMYTISS, scalable parallel algorithms are designed such that they support the parallel execution within CPUs, GPUs and hardware accelerators (HWAs). Unlike all existing tools for stochastic systems, AMYTISS can utilize high-performance computing (HPC) platforms and cloud-computing services to mitigate the effects of the state-explosion problem, which is always present in analyzing large-scale stochastic systems. We benchmark AMYTISS against the most recent tools in the literature using several physical case studies including robot examples, room temperature and road traffic networks. We also apply our algorithms to a 3-dimensional autonomous vehicle and 7-dimensional nonlinear model of a BMW 320i car by synthesizing an autonomous parking controller.",Automated controller synthesis | Discrete-time stochastic systems | Finite MDPs | High performance computing platform | Parallel algorithms,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Lavaei, Abolfazl;Khaled, Mahmoud;Soudjani, Sadegh;Zamani, Majid",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090838745,10.1145/3383219.3383229,Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line,"In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ˜ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (˜ 80%), and (3) machine learning algorithms perform differently.",Feature Extraction | Feature Terms Identification | Requirement Documents | Software Product Lines,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Li, Yang;Schulze, Sandro;Xu, Jiahua",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090822686,10.1145/3383219.3383240,Mining Decision-Making Processes in Open Source Software Development: A Study of Python Enhancement Proposals (PEPs) using Email Repositories,"Open source software (OSS) communities are often able to produce high quality software comparable to proprietary software. The success of an open source software development (OSSD) community is often attributed to the underlying governance model, and a key component of these models is the decision-making (DM) process. While there have been studies on the decision-making processes publicized by OSS communities (e.g., through published process diagrams), little has been done to study decision-making processes that can be extracted using a bottom-up, data-driven approach, which can then be used to assess whether the publicized processes conform to the extracted processes. To bridge this gap, we undertook a large-scale data-driven study to understand how decisions are made in an OSSD community, using the case study of Python Enhancement Proposals (PEPs), which embody decisions made during the evolution of the Python language. Our main contributions are: (a) the design and development of a framework using information retrieval and natural language processing techniques to analyze the Python email archives (comprising 1.48 million emails), and (b) the extraction of decision-making processes that reveal activities that are neither explicitly mentioned in documentation published by the Python community nor identified in prior research work. Our results provide insights into the actual decision-making process employed by the Python community.","decision-making | Mining repositories | Open Source software development (OSSD) | Python, process extraction, process mining",ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Sharma, Pankajeshwara;Savarimuthu, Bastin Tony Roy;Stanger, Nigel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090789536,10.1145/3383219.3383241,Surveying Software Practitioners on Technical Debt Payment Practices and Reasons for not Paying off Debt Items,"Background: Little is known about the practices used for technical debt (TD) payment. The study of payment practices, as well as the reasons for not applying them, can help practitioners to control and manage TD items. Aims: To investigate, from the point of view of software practitioners, if TD items have been paid off in software projects, the practices that have been used to pay off TD and the reasons that hamper the implementation of these practices. Method: We analyzed-both quantitatively and qualitatively-a corpus of responses from a survey of 432 practitioners, from four countries, about the possibility of TD payment. Results: We found that, for most of the cases, TD items have not been eliminated from software projects. The main reasons for not paying off TD are lack of organizational interest, low priority on the debt, focus on short-term goals, cost, and lack of time. On the other hand, we identified that code refactoring, design refactoring, and update system documentation are the most used practices for TD payment. Practitioners also cited practices related to the prevention, prioritization, and creation of a favorable setting as part of TD payment initiatives. Conclusion: This paper summarizes the identified practices and reasons for not paying off debt items in a map. Our map reveals that the majority of payment practices are of a technical nature while the majority of reasons for not paying off debts are associated with non-technical issues.",InsighTD | Technical debt | Technical debt management | Technical debt payment,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Freire, Sávio;Rios, Nicolli;Gutierrez, Boris;Torres, Darío;Mendonça, Manoel;Izurieta, Clemente;Seaman, Carolyn;Spínola, Rodrigo O.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090833156,10.1145/3383219.3383245,Documentation of Quality Requirements in Agile Software Development,"Context: Quality requirements (QRs) have a significant role in the success of software projects. In agile software development (ASD), where working software is valued over comprehensive documentation, QRs are often under-specified or not documented. Consequently, they may be handled improperly and result in degraded software quality and increased maintenance costs. Investigating the documentation of QRs in ASD, would provide evidence on existing practices, tools and aspects considered in ASD that other practitioners might utilize to improve documentation and management of QRs in ASD. Although there are some studies examining documentation in ASD, those that specifically investigate the documentation of QRs in depth are lacking. Method: we conducted a multiple case study by interviewing 15 practitioners of four ASD cases, to provide empirical evidence on documentation of QRs in ASD. We also run workshops with two of the cases, to identify important aspects that ASD practitioners consider when documenting QRs in requirements management repositories. Result and conclusions: ASD companies approach documentation of QRs to fit the needs of their context. They used tools, backlogs, iterative prototypes, and artifacts such as epic, and stories to document QRs, or utilized face-face communication without documenting QRs. We observed that documentation of QRs in ASD is affected by factors such as context (e.g. product domain, and size) and the experience of practitioners. Some tools used to document QRs also enhanced customer collaboration, enabling customers report and document QRs. Aspects such as levels of abstraction, the traceability of QRs, optimal details of information of QRs and verification and validation are deemed important when documenting QRs in ASD requirements management repositories.",agile software development | documentation | non-functional requirements | Quality requirement,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Behutiye, Woubshet;Seppänen, Pertti;Rodríguez, Pilar;Oivo, Markku",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85090823297,10.1145/3383219.3383248,Tricking Johnny into Granting Web Permissions,"We studied the web permission API dialog box in popular mobile and desktop browsers, and found that it typically lacks measures to protect users from unwittingly granting web permission when clicking too fast. We developed a game that exploits this issue, and tricks users into granting webcam permission. We conducted three experiments, each with 40 different participants, on both desktop and mobile browsers. The results indicate that in the absence of a prevention mechanism, we achieve a considerably high success rate in tricking 95% and 72% of participants on mobile and desktop browsers, respectively. Interestingly, we also tricked 47% of participants on a desktop browser where a prevention mechanism exists.",Browser security | Clickjacking | UI security | Web permission,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Hazhirpasand, Mohammadreza;Ghafari, Mohammad;Nierstrasz, Oscar",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85090849190,10.1145/3383219.3383249,An Empirical Investigation on Software Practices in Growth Phase Startups,"Context: Software startups are software-intensive early-stage companies with high growth rates. We notice little evidence in the literature concerning engineering practices when startups transition to the growth phase. Aim: Our goal is to evaluate how software startups embrace software engineering practices. Methodology: We conduct a survey guided by semi-structured interviews as an initial step, to be followed by field questionnaires as part of a future exploratory study. We use open coding to identify patterns leading to themes we use to state our hypotheses. To identify our samples, we use purposive sampling. Results: Specifically, we analyze seven startup cases during the first qualitative phase. We obtain five anti-patterns (no-documentation, no-agile, no-code intellectual property protection, cowboy programming, no-automated testing) and corresponding patterns (readable code, ad-hoc project management, private code repositories, paired and individual programming, ad-hoc testing) in adopting software engineering practices. We state 10 corresponding hypotheses we intend to corroborate by surveying a more significant number of software startups. Contribution: This study, throughout its recommendations, provides an initial road map for software startups in the growth phase, allowing future researchers and practitioners to make educated recommendations.",Empirical Investigation | Growth Phase | Software Engineering Practices | Software Startups,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Cico, Orges;Duc, Anh Nguyen;Jaccheri, Letizia",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85090845271,10.1145/3383219.3383267,"Achieving ""good Enough"" Software Security: The Role of Objectivity","Today's software development projects need to consider security as one of the qualities the software should possess. However, overspending on security will imply that the software will become more expensive and often also delayed. This paper discusses the role of objectivity in assessing and researching the goal of good enough security. Different understandings of objectivity are introduced, and the paper explores how these can guide the way forward in improving judgements on what level of security is good enough. The paper recommends adopting and improving upon methods that include different perspectives, support the building of interactive expertise, and support confirmability by keeping documentation of the basis on which judgements were made.",agile software development | good enough security | objectivity | security level | security priorities | software security,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Tøndel, Inger Anne;Cruzes, Daniela Soares;Jaatun, Martin Gilje",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087382093,10.1145/3383219.3383277,On the Evaluation of the Security Usability of Bitcoin's APIs,"Bitcoin is a peer-to-peer software system that is primarily used as digital money. There exist many software libraries supporting various programming languages that allow access to the Bitcoin system via an Application Programming Interface (API). APIs that are inappropriately used would lead to bugs and security vulnerabilities, which are hard to discover resulting in serious zero-day attacks. Making APIs usable is, therefore, an essential aspect related to the quality and robustness of software. This paper surveys the general academic literature concerning API usability and usable security. Furthermore, it evaluates the API usability of libbitcoin, a well-known C++ implementation of the bitcoin system, and assesses how the findings of this evaluation could affect the applications that use libbitcoin. In addition, the paper proposes two static analysis tools for such a purpose. The findings of this research have improved libbitcoin in many places as will be shown in the paper.",API Usability | Bitcoin | Privacy | Security,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Tschannen, Philipp;Ahmed, Ali",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090828171,10.1145/3383219.3383284,Software Security Specifications and Design: How Software Engineers and Practitioners Are Mixing Things up,"Huge numbers of worldwide-deployed software suffer from poor quality and possess vulnerabilities with serious impact. Meanwhile, people are using such software to save and manage their valuable information including their monetary data. This has increased the hackers' appetite to attack software. Henceforth, researchers and practitioners are convinced that software security is not an added value or a gold-plating need. Consequently, security requirements specification and implementation become vital during the software development process. Unfortunately, researchers and practitioners are doing so in a rush. This has made them mix concepts and practices up in a way that can terribly make the problem of delivering software overdue more chronic which will result in a security and technical debt. This research represents a corrective study that sheds light on what has been achieved in analyzing and designing secure software and what are the problems committed and how to handle them.",Security | Software Architecture | Software Design | Software Development Management | Software Engineering,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Zarour, Mohammad;Alenezi, Mamdouh;Alsarayrah, Khalid",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090867247,10.1145/3383219.3383290,The State of the Art on Secure Software Engineering: A Systematic Mapping Study,"Secure Software Development (SSD) is becoming a major challenge, due to the increasing complexity, openness and extensibility of Information and Communication Technologies (ICTs). These make the overall security requirements analysis very difficult. Many techniques have been theoretically developed, however, there is a lack of empirical evidence of its application in building secure software system. A Systematic Mapping Study (SMS) has been conducted in this paper to examine the existence of software security frameworks, models and methods. In total, we selected 116 primary studies. After examining the selected studies, we identified 37 Secure Software Engineering (SSE) paradigms/frameworks/models. The results show that the most frequently used SSE frameworks/models are ""Microsoft Software Development Life Cycle (MS-SDL)"", ""Misuse case modeling"", ""Abuse case modeling"", ""Knowledge Acquisition for Automated Specification"", ""System Security Engineering-Capability Maturity Model (SSE-CMM)""and ""Secure Tropos Methodology"". This work will help organizations in the development of software to better understand existing security initiatives used in the development of secure software. It can also provide researchers with a basis for designing and developing new methods of software security and identifying new axis of research.",Software Development Life Cycle | Software Engineering | Software Security | Systematic mapping study,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Khan, Rafiq A.;Khan, Siffat U.;Ilyas, Muhammad;Idris, Mohd Y.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090830321,10.1145/3383219.3383293,What Norwegian Developers Want and Need from Security-Directed Program Analysis Tools: A Survey,"Code enforcing access control policies often has high inherent complexity, making it challenging to test using only classical review and testing techniques. To more thoroughly test such code, it is strategic to also use program analysis tools, which often can find subtle, critical bugs going unnoticed to humans. These powerful tools are however rarely used in software consultancy practice, due to factors such as bad usability or unsatisfactory non-functional characteristics. To encourage wider adoption of such tools, more must be learned about how to design them to the preferences of software consultants. Towards this goal, we conducted a survey of Norwegian software consultants. Among our findings is a positive relation between preference for soundness over completeness in tools and preference for annotation-based over automated tools. 51% of the developers surveyed prefer soundness over completeness when detecting access control vulnerabilities, while only 37.5% view completeness as the more important characteristic. Qualitative responses illuminate concerns regarding usability, soundness, completeness, and performance.",access control vulnerabilities | consultants | program analysis | static analysis | survey,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Sørensen, Elias Brattli;Karlsen, Edvard Kristoffer;Li, Jingyue",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096682608,10.1109/ICSME46990.2020.00013,Haste Makes Waste: An Empirical Study of Fast Answers in Stack Overflow,"Modern programming question answer (QA) sites such as Stack Overflow (SO) employ gamified mechanisms to stimulate volunteers' contributions. To maximize the chances of winning gamification rewards such as reputation and badges, a portion of users race to post answers as quickly as possible (i.e., fast answers or FAs), which makes SO the fastest QA site; however, this behavior may affect the contribution quality as well. In this paper, we report on a large-scale, mixed-methods empirical study of the gamification-influenced FA phenomenon in SO. We first quantitatively investigate the popularity of the phenomenon and user behaviors regarding FAs. Then, we study the quality of FAs by using regression modeling and qualitatively analyzing 300 instances of FAs. Our main findings reveal that more than 70% and 90% of FAs are not edited by the answerers and other users, respectively, and that later incoming answers have lower chances of being voted on and accepted. Notably, we find that the answer length, code snippets length, and readability of FAs are significantly lower than those of non-fast answers. Although FAs have higher crowd assessment scores, they have no relationship with acceptance from the perspective of asker assessment, and a considerable portion of FAs solve the problem by interacting with the asker in the comments. These results help us better understand the effects of reward-based gamification on crowdsourced software engineering communitites and provide implications for designers of gamified systems.",fast answer | Gamification | QA community | Stack Overflow,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Lu, Yao;Mao, Xinjun;Zhou, Minghui;Zhang, Yang;Wang, Tao;Li, Zude",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096672772,10.1109/ICSME46990.2020.00014,"An Empirical Study of Usages, Updates and Risks of Third-Party Libraries in Java Projects","Third-party libraries play a key role in software development as they can relieve developers of the heavy burden of re-implementing common functionalities. However, third-party libraries and client projects evolve asynchronously. As a result, out-dated third-party libraries might be used in client projects while developers are not aware of the potential risk (e.g., security bug). Outdated third-party libraries may be updated in client projects in a delayed way, and developers may be less aware of the potential risk (e.g., API incompatibility) in updates. Developers of third-party libraries may be unaware of how their third-party libraries are used or updated in client projects. Therefore, a quantitative and holistic study on usages, updates and risks of third-party libraries in open-source projects can provide concrete evidences on these problems, and practical insights to improve the ecosystem.In this paper, we contribute such a study in Java ecosystem. In particular, we conduct a library usage analysis (e.g., usage intensity and outdatedness) and library update analysis (e.g., update intensity and delay) on 806 open-source projects and 13,565 third-party libraries. Then, we carry out a library risk analysis (e.g., usage risk and update risk) on 806 open-source projects and 544 security bugs. These analyses aim to quantify the usage and update practices and the potential risk of using and updating outdated third-party libraries with respect to security bugs from two holistic perspectives (i.e., open-source projects and third-party libraries). Our findings suggest practical implications to developers and researchers on problems and potential solutions in maintaining third-party libraries (e.g., smart alerting and automated updating of outdated third-party libraries). To indicate the usefulness of our findings, we design a smart alerting system for assisting developers to make confident decisions when updating third-party libraries. 33 and 24 open-source projects have confirmed and updated third-party libraries after receiving our alerts.",outdated libraries | security bugs,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Wang, Ying;Chen, Bihuan;Huang, Kaifeng;Shi, Bowen;Xu, Congying;Peng, Xin;Wu, Yijian;Liu, Yang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096705571,10.1109/ICSME46990.2020.00016,Database-Access Performance Antipatterns in Database-Backed Web Applications,"Database-backed web applications are prone to performance bugs related to database accesses. While much work has been conducted on database-access antipatterns with some recent work focusing on performance impact, there still lacks a comprehensive view of database-access performance antipatterns in database-backed web applications. To date, no existing work systematically reports known antipatterns in the literature, and no existing work has studied database-access performance bugs in major types of web applications that access databases differently.To address this issue, we first summarize all known database-access performance antipatterns found through our literature survey, and we report all of them in this paper. We further collect database-access performance bugs from web applications that access databases through language-provided SQL interfaces, which have been largely ignored by recent work, to check how extensively the known antipatterns can cover these bugs. For bugs not covered by the known antipatterns, we extract new database-access performance antipatterns based on real-world performance bugs from such web applications. Our study in total reports 24 known and 10 new database-access performance antipatterns. Our results can guide future work to develop effective tool support for different types of web applications.",characteristic study | database-backed web applications | performance antipatterns | performance bugs,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Shao, Shudi;Qiu, Zhengyi;Yu, Xiao;Yang, Wei;Jin, Guoliang;Xie, Tao;Wu, Xintao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096691094,10.1109/ICSME46990.2020.00017,Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?,"Extensive research has been conducted on sentiment analysis for software engineering (SA4SE). Researchers have invested much effort in developing customized tools (e.g., SentiStrength-SE, SentiCR) to classify the sentiment polarity for Software Engineering (SE) specific contents (e.g., discussions in Stack Overflow and code review comments). Even so, there is still much room for improvement. Recently, pre-trained Transformer-based models (e.g., BERT, XLNet) have brought considerable breakthroughs in the field of natural language processing (NLP). In this work, we conducted a systematic evaluation of five existing SA4SE tools and variants of four state-of-the-art pre-trained Transformer-based models on six SE datasets. Our work is the first to fine-tune pre-trained Transformer-based models for the SA4SE task. Empirically, across all six datasets, our fine-tuned pre-trained Transformer-based models outperform the existing SA4SE tools by 6.5-35.6% in terms of macro/micro-averaged F1 scores.",Natural Language Processing | Pre-trained Models | Sentiment Analysis | Software Mining,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Zhang, Ting;Xu, Bowen;Thung, Ferdian;Haryono, Stefanus Agus;Lo, David;Jiang, Lingxiao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096655109,10.1109/ICSME46990.2020.00022,Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction,"Software quality evolution and predictive models to support decisions about resource distribution in software quality assurance tasks are an important part of software engineering research. Recently, a fine-grained just-in-time defect prediction approach was proposed which has the ability to find bug-inducing files within changes instead of only complete changes. In this work, we utilize this approach and improve it in multiple places: data collection, labeling and features. We include manually validated issue types, an improved SZZ algorithm which discards comments, whitespaces and refactorings. Additionally, we include static source code metrics as well as static analysis warnings and warning density derived metrics as features. To assess whether we can save cost we incorporate a specialized defect prediction cost model. To evaluate our proposed improvements of the fine-grained just-in-time defect prediction approach we conduct a case study that encompasses 38 Java projects, 492,241 file changes in 73,598 commits and spans 15 years. We find that static source code metrics and static analysis warnings are correlated with bugs and that they can improve the quality and cost saving potential of just-in-time defect prediction models.",Software metrics | Software quality,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Trautsch, Alexander;Herbold, Steffen;Grabowski, Jens",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096663154,10.1109/ICSME46990.2020.00024,A First Look at the Deprecation of RESTful APIs: An Empirical Study,"REpresentational State Transfer (REST) is considered as one standard software architectural style to build web APIs that can integrate software systems over the internet. However, while connecting systems, RESTful APIs might also break the dependent applications that rely on their services when they introduce breaking changes, e.g., an older version of the API is no longer supported. To warn developers promptly and thus prevent critical impact on downstream applications, a deprecated-removed model should be followed, and deprecation-related information such as alternative approaches should also be listed. While API deprecation analysis as a theme is not new, most existing work focuses on non-web APIs, such as the ones provided by Java and Android.To investigate RESTful API deprecation, we propose a framework called RADA (RESTful API Deprecation Analyzer). RADA is capable of automatically identifying deprecated API elements and analyzing impacted operations from an OpenAPI specification, a machine-readable profile for describing RESTful web service. We apply RADA on 2,224 OpenAPI specifications of 1,368 RESTful APIs collected from APIs.guru, the largest directory of OpenAPI specifications. Based on the data mined by RADA, we perform an empirical study to investigate how the deprecated-removed protocol is followed in RESTful APIs and characterize practices in RESTful API deprecation. The results of our study reveal several severe deprecation-related problems in existing RESTful APIs. Our implementation of RADA and detailed empirical results are publicly available for future intelligent tools that could automatically identify and migrate usage of deprecated RESTful API operations in client code.",API Deprecation | Evolution of Web APIs | OpenAPI Specification | RESTful API | Web API,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Yasmin, Jerin;Tian, Yuan;Yang, Jinqiu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096644035,10.1109/ICSME46990.2020.00025,Achieving Reliable Sentiment Analysis in the Software Engineering Domain using BERT,"Researchers have shown that sentiment analysis of software artifacts can potentially improve various software engineering tools, including API and library recommendation systems, code suggestion tools, and tools for improving communication among software developers. However, sentiment analysis techniques applied to software artifacts still have not yet yielded very high accuracy. Recent adaptations of sentiment analysis tools to the software domain have reported some improvements, but the f-measures for the positive and negative sentences still remain in the 0.4-0.64 range, which deters their practical usefulness for software engineering tools.In this paper, we explore the potential effectiveness of customizing BERT, a language representation model, which has recently achieved very good results on various Natural Language Processing tasks on English texts, for the task of sentiment analysis of software artifacts. We describe our application of BERT to analyzing sentiments of sentences in Stack Overflow posts and compare the impact of a BERT sentiment classifier to state-of-the-art sentiment analysis techniques when used on a domain-specific data set created from Stack Overflow posts. We also investigate how the performance of sentiment analysis changes when using a much (3 times) larger data set than previous studies. Our results show that the BERT classifier achieves reliable performance for sentiment analysis of software engineering texts. BERT combined with the larger data set achieves an overall f-measure of 0.87, with the f-measures for the negative and positive sentences reaching 0.91 and 0.78 respectively, a significant improvement over the state-of-the-art.",BERT | Sentiment Analysis | Software Engineering,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Biswas, Eeshita;Karabulut, Mehmet Efruz;Pollock, Lori;Vijay-Shanker, K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096658977,10.1109/ICSME46990.2020.00026,Detecting Semantic Conflicts via Automated Behavior Change Detection,"Branching and merging are common practices in collaborative software development. They increase developer productivity by fostering teamwork, allowing developers to independently contribute to a software project. Despite such benefits, branching and merging comes at a cost-the need to merge software and to resolve merge conflicts, which often occur in practice. While modern merge techniques, such as 3-way or structured merge, can resolve many such conflicts automatically, they fail when the conflict arises not at the syntactic, but the semantic level. Detecting such conflicts requires understanding the behavior of the software, which is beyond the capabilities of most existing merge tools. As such, semantic conflicts can only be identified and fixed with significant effort and knowledge of the changes to be merged. While semantic merge tools have been proposed, they are usually heavyweight, based on static analysis, and need explicit specifications of program behavior. In this work, we take a different route and explore the automated creation of unit tests as partial specifications to detect unwanted behavior changes (conflicts) when merging software.We systematically explore the detection of semantic conflicts through unit-test generation. Relying on a ground-truth dataset of 38 software merge scenarios, which we extracted from GitHub, we manually analyzed them and investigated whether semantic conflicts exist. Next, we apply test-generation tools to study their detection rates. We propose improvements (code transformations) and study their effectiveness, as well as we qualitatively analyze the detection results and propose future improvements. For example, we analyze the generated test suites for false-negative cases to understand why the conflict was not detected. Our results evidence the feasibility of using test-case generation to detect semantic conflicts as a method that is versatile and requires only limited deployment effort in practice, as well as it does not require explicit behavior specifications.",Behavior Change Detection | Differential Testing | Semantic Conflicts,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Silva, Leuson Da;Borba, Paulo;Mahmood, Wardah;Berger, Thorsten;Moisakis, Joao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096658547,10.1109/ICSME46990.2020.00029,Automated Extraction of Requirement Entities by Leveraging LSTM-CRF and Transfer Learning,"Requirement entities, ""explicit specification of concepts that define the primary function objects"", play an important role in requirement analysis for software development and maintenance. It is a labor-intensive activity to extract requirement entities from textual requirements, which is typically done manually. A few existing studies propose automated methods to support key requirement concept extraction. However, they face two main challenges: lack of domain-specific natural language processing techniques and expensive labeling effort. To address the challenges, this study presents a novel approach named RENE, which employs LSTM-CRF model for requirement entity extraction and introduces the general knowledge to reduce the demands for labeled data. It consists of four phases: 1) Model construction, where RENE builds LSTM-CRF model and an isomorphic LSTM language model for transfer learning; 2) LSTM language model training, where RENE captures general knowledge and adapt to requirement context; 3) LSTM-CRF training, where RENE trains the LSTM-CRF model with the transferred layers; 4) Requirement entity extraction, where RENE applies the trained LSTM-CRF model to a new-coming requirement, and automatically extracts its requirement entities. RENE is evaluated using two methods: evaluation on historical dataset and user study. The evaluation on the historical dataset shows that RENE could achieve 79% precision, 81% recall, and 80% F1. The evaluation results from the user study also suggest that RENE could produce more accurate and comprehensive requirement entities, compared with those produced by engineers.",LSTM-CRF | Requirement Entity | Sequence Tagging | Transfer Learning,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Li, Mingyang;Yang, Ye;Shi, Lin;Wang, Qing;Hu, Jun;Peng, Xinhua;Liao, Weimin;Pi, Guizhen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096655684,10.1109/ICSME46990.2020.00041,Evaluating Code Readability and Legibility: An Examination of Human-centric Studies,"Reading code is an essential activity in software maintenance and evolution. Several studies with human subjects have investigated how different factors, such as the employed programming constructs and naming conventions, can impact code readability, i.e., what makes a program easier or harder to read and apprehend by developers, and code legibility, i.e., what influences the ease of identifying elements of a program. These studies evaluate readability and legibility by means of different comprehension tasks and response variables. In this paper, we examine these tasks and variables in studies that compare programming constructs, coding idioms, naming conventions, and formatting guidelines, e.g., recursive vs. iterative code. To that end, we have conducted a systematic literature review where we found 54 relevant papers. Most of these studies evaluate code readability and legibility by measuring the correctness of the subjects' results (83.3%) or simply asking their opinions (55.6%). Some studies (16.7%) rely exclusively on the latter variable. There are still few studies that monitor subjects' physical signs, such as brain activation regions (5%). Moreover, our study shows that some variables are multi-faceted. For instance, correctness can be measured as the ability to predict the output of a program, answer questions about its behavior, or recall parts of it. These results make it clear that different evaluation approaches require different competencies from subjects, e.g., tracing the program vs. summarizing its goal vs. memorizing its text. To assist researchers in the design of new studies and improve our comprehension of existing ones, we model program comprehension as a learning activity by adapting a preexisting learning taxonomy. This adaptation indicates that some competencies, e.g., tracing, are often exercised in these evaluations whereas others, e.g., relating similar code snippets, are rarely targeted.",code legibility | Code readability | code understandability | code understanding | program comprehension,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Oliveira, Delano;Bruno, Reydne;Madeiral, Fernanda;Castor, Fernando",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096668318,10.1109/ICSME46990.2020.00057,Lifting the Curtain on Merge Conflict Resolution: A Sensemaking Perspective,"Merge conflicts are an inevitable, but painful part of collaborative software development. Merge conflict resolution is nontrivial because it requires gathering information from disparate sources (e.g., a codebase, diffs between versions, history of changes, documentation etc.) and then piecing together not only this information, but the rationale and context behind the conflicting changes. Current tools offer inadequate support to developers trying to understand the context and impact of the conflicting changes. They also offer little support in evaluating potential resolutions. To improve conflict resolution tools, we first need to understand the information needs of conflict resolution and its underlying sensemaking process. In this paper, through in-situ observations of 10 conflict resolutions, we qualitatively investigate how developers collect and sensemake different conflict-related information and how they reach a resolution. We identified eight behaviour patterns that developers use. These patterns have implications for tool builders and developers alike. We identify specific areas that need improved tool support. We also highlight the sensemaking traps practitioners may fall into. Index Terms-Merge conflicts, software development practices, sensemaking","Merge conflicts, software development practices, sensemaking","Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Brindescu, Caius;Ramirez, Yenifer;Sarma, Anita;Jensen, Carlos",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096670427,10.1109/ICSME46990.2020.00067,Score-Based Automatic Detection and Resolution of Syntactic Ambiguity in Natural Language Requirements,"The quality of a delivered product relies heavily upon the quality of its requirements. Across many disciplines and domains, system and software requirements are mostly specified in natural language (NL). However, natural language is inherently ambiguous and inconsistent. Such intrinsic challenges can lead to misinterpretations and errors that propagate to the subsequent phases of the system development. Pattern-based natural language processing (NLP) techniques have been proposed to detect the ambiguity in requirements specifications. However, such approaches typically address specific cases or patterns and lack the versatility essential to detecting different cases and forms of ambiguity. In this paper, we propose an efficient and versatile automatic syntactic ambiguity detection technique for NL requirements. The proposed technique relies on filtering the possible scored interpretations of a given sentence obtained via Stanford CoreNLP library. In addition, it provides feedback to the user with the possible correct interpretations to resolve the ambiguity. Our approach incorporates four filtering pipelines on the input NL-requirements working in conjunction with the CoreNLP library to provide the most likely possible correct interpretations of a requirement. We evaluated our approach on a suite of datasets of 126 requirements and achieved 65% precision and 99% recall on average.",Ambiguity | Quality checking | Requirements analysis | Requirements specification,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Osama, Mohamed;Zaki-Ismail, Aya;Abdelrazek, Mohamed;Grundy, John;Ibrahim, Amani",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096716162,10.1109/ICSME46990.2020.00070,Who (Self) Admits Technical Debt?,"Self-Admitted Technical Debt (SATD) are comments, left by developers in the source code or elsewhere, aimed at describing the presence of TD, i.e., source code ""not ready yet"". Although this was never stated in the original paper by Potdar and Shihab, the term SATD might suggest that it refers to a ""self-admission""by whoever has written or changed the source code. This paper empirically investigates, using a curated SATD dataset from five Java open-source projects, (i) the extent to which SATD comments are introduced by authors different from those who have done last changes to the related source code, and (ii) when this happens, what is the level of ownership those developers have about the commented source code. Results of the study indicate that, depending on the project, the percentage of SATD admissions introduced or changed without modifying the related source code varies between 0% and 16%, and therefore represent a small, yet not negligible, phenomenon. The level of ownership of those developers is not particularly low, with a median value per project between 10% and 42%. This indicates the possible use of SATD as a different way to perform code review, although this behavior should be considered sub-optimal to the use of more traditional tools, which entail suitable notification mechanisms.",Code review | Empirical study | Self-admitted technical debt,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Fucci, Gianmarco;Zampetti, Fiorella;Serebrenik, Alexander;Di Penta, Massimiliano",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85096710228,10.1109/ICSME46990.2020.00101,QScored: An Open Platform for Code Quality Ranking and Visualization,"Though abundant source code repositories are available on code repository hosting platforms, their detailed code quality information is not available readily. Software engineering researchers often need to select a set of high-quality repositories. Despite the code quality is an important concern for repository selection, the lack of this information makes researchers depend on alternatives such as the number of issues and the number of stars associated with repositories. Furthermore, practitioners expect user-friendly visual ways to assess the quality of their projects during the evolution of the codebase without putting a considerable effort. We propose an open platform QScored to fill the gap for both researchers and practitioners. The platform hosts detailed code quality analysis information for a large number of repositories (currently more than twelve thousand containing more than 200 million LOC), computes quality score and assigns relative ranking of the hosted repositories based on detected architecture, design, and implementation smells, as well as offers a comprehensive set of visualization aids for code quality aspects. Furthermore, the platform provides REST APIs to search repositories based on their code quality scores and ranking of hosted software projects.Video of the demo: https://youtu.be/-IgvjGV-2X0",Code quality | quality badge | quality ranking | quality score | visualization,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Thakur, Vishvajeet;Kessentini, Marouane;Sharma, Tushar",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85096666300,10.1109/ICSME46990.2020.00109,Mobile App Energy Consumption: A Study of Known Energy Issues in Mobile Applications and their Classification Schemes-Summary Plan,"Work in Mobile App Energy Consumption (MAEC) has drawn participants from a broad range of communities such as systems, networking, hardware, testing, analysis and design. This diversity has enriched and informed the research from many different angles. For example, knowledge about the physical properties of batteries (e.g., that their performance is temperature dependent) is necessary to control for con-founding variables during experiments. However, it has also led to a confusing and conflicting mix of terms, names, and expressions as researchers from different domains each attempt to apply their existing terminology or invent new terms to describe various kinds of energy related issues.","energy issues, energy consumption, mobile, apps, software engineering, efficiency, classifications","Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Alotaibi, Ali;Clause, James;Halfond, William G.J.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85096653752,10.1109/ICSME46990.2020.00114,Source Code based On-demand Class Documentation Generation,"In this paper, we present OpenAPIDocGen2, a tool that generates on-demand class documentation based on source code and documentation analysis. For a given class, OpenAPIDocGen2 generates a combined documentation for it, which includes functionality descriptions, directives, domain concepts, usage examples, class/method roles, key methods, relevant classes/methods, characteristics and concepts classification, and usage scenarios.",Documentation Generation | Information Seeking | Knowledge Extraction,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Liu, Mingwei;Peng, Xin;Meng, Xiujie;Xu, Huanjun;Xing, Shuangshuang;Wang, Xin;Liu, Yang;Lv, Gang",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85096717498,10.1109/ICSME46990.2020.00115,Learning based and Context Aware Non-Informative Comment Detection,"This report introduces the approach that we have designed and implemented for the DeClutter challenge of Doc-Gen2, which detects non-informative code comments. The approach combines both comment based text classification and code context based prediction. Based on the approach, our ""fduse""team achieved the best F1 score (0.847) in the competition.",API Documentation | Deep Learning | Text Classification,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Liu, Mingwei;Yang, Yanjun;Peng, Xin;Wang, Chong;Zhao, Chengyuan;Wang, Xin;Xing, Shuangshuang",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85095816338,10.1145/3382494.3410671,A survey on the interplay between software engineering and systems engineering during SoS architecting,"Background: The Systems Engineering and Software Engineering disciplines are highly intertwined in most modern Systems of Systems (SoS), and particularly so in industries such as defense, transportation, energy and health care. However, the combination of these disciplines during the architecting of SoS seems to be especially challenging; the literature suggests that major integration and operational issues are often linked to ambiguities and gaps between system-level and software-level architectures. Aims: The objective of this paper is to empirically investigate: 1) the state of practice on the interplay between these two disciplines in the architecting process of systems with SoS characteristics; 2) the problems perceived due to this interplay during said architecting process; and 3) the problems arising due to the particular characteristics of SoS systems. Method:We conducted a questionnaire-based online survey among practitioners from industries in the aforementioned domains, having a background on Systems Engineering, Software Engineering or both, and experience in the architecting of systems with SoS characteristics. The survey combined multiple-choice and openended questions, and the data collected from the 60 respondents were analyzed using quantitative and qualitative methods. Results: We found that although in most cases the software architecting process is governed by system-level requirements, the way requirements were specified by systems engineers, and the lack of domain-knowledge of software engineers, often lead to misinterpretations at software level. Furthermore, we found that unclear and/or incomplete specifications could be a common cause of technical debt in SoS projects, which is caused, in part, by insufficient interface definitions. It also appears that while the SoS concept has been adopted by some practitioners in the field, the same is not true about the existing and growing body of knowledge on the subject in Software Engineering resulting in recurring problems with system integration. Finally, while not directly related to the interplay of the two disciplines, the survey also indicates that low-level hardware components, despite being identified as the root cause of undesired emergent behavior, are often not considered when modeling or simulating the system. Conclusions: The survey indicates the need for tighter collaboration between the two disciplines, structured around concrete guidelines and practices for reconciling their differences. A number of open issues identified by this study require further investigation.",Architecting | Practitioners survey | Systems of systems,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Cadavid, Héctor;Andrikopoulos, Vasilios;Avgeriou, Paris;Klein, John",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85095840684,10.1145/3382494.3410692,An empirical study of software exceptions in the field using search logs,"Background: Software engineers spend a substantial amount of time using Web search to accomplish software engineering tasks. Such search tasks include finding code snippets, API documentation, seeking help with debugging, etc. While debugging a bug or crash, one of the common practices of software engineers is to search for information about the associated error or exception traces on the internet. Aims: In this paper, we analyze query logs from Bing to carry out a large scale study of software exceptions. To the best of our knowledge, this is the first large scale study to analyze how Web search is used to find information about exceptions. Method: We analyzed about 1 million exception related search queries from a random sample of 5 billion web search queries. To extract exceptions from unstructured query text, we built a novel machine learning model. With the model, we extracted exceptions from raw queries and performed popularity, effort, success, query characteristic and web domain analysis.We also performed programming language-specific analysis to give a better view of the exception search behavior. Results: Using the model with an F1-score of 0.82, our study identifies most frequent, most effort-intensive, or less successful exceptions and popularity of community Q&A sites. Conclusion: These techniques can help improve existing methods, documentation and tools for exception analysis and prediction. Further, similar techniques can be applied for APIs, frameworks, etc.",Debugging | Machine learning | Software engineering | Web search,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Hassan, Foyzul;Bansal, Chetan;Nagappan, Nachiappan;Zimmermann, Thomas;Awadallah, Ahmed Hassan",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85095843161,10.1145/3382494.3410636,An empirical validation of cognitive complexity as a measure of source code understandability,"Background: Developers spend a lot of their time on understanding source code. Static code analysis tools can draw attention to code that is difficult for developers to understand. However, most of the findings are based on non-validated metrics, which can lead to confusion and code that is hard to understand not being identified. Aims: In thiswork,we validate a metric called Cognitive Complexity which was explicitly designed to measure code understandability and which is already widely used due to its integration in wellknown static code analysis tools. Method:We conducted a systematic literature search to obtain data sets from studies which measured code understandability. This way we obtained about 24,000 understandability evaluations of 427 code snippets. We calculated the correlations of these measurements with the corresponding metric values and statistically summarized the correlation coefficients through a meta-analysis. Results: Cognitive Complexity positively correlates with comprehension time and subjective ratings of understandability. The metric showed mixed results for the correlation with the correctness of comprehension tasks and with physiological measures. Conclusions: It is the first validated and solely code-based metric which is able to reflect at least some aspects of code understandability. Moreover, due to its methodology, this work shows that code understanding is currently measured in many different ways, which we also do not know how they are related. This makes it difficult to compare the results of individual studies as well as to develop a metric that measures code understanding in all its facets.",Cognitive complexity | Meta-analysis | Software metrics | Source code comprehension | Source code understandability,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Barón, Marvin Muñoz;Wyrich, Marvin;Wagner, Stefan",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85095843117,10.1145/3382494.3410688,Characterizing energy consumption of third-party API libraries using API utilization profiles,"Background: Third-party software libraries often serve as fundamental building blocks for developing applications. However, depending on such libraries for development raises a new concern, energy consumption, which has become of increased interest for the software engineering community in recent years. Understanding the energy implications of software design choices is an ongoing research challenge, especially when working with mobile devices that are constrained by limited battery life. Aims: Our goal is to research approaches, which will support software developers to better comprehend the energy implications of software design choices on Android. For this study, we particularly focus on APIs from thirdparty libraries for which we research methods that will enable estimating the energy consumption without the need for specific measurement hardware and laborious measurements. Method: To achieve the stipulated goal we introduce System API Utilization Profiles (uAPI) which are based on the general assumption that the actual energy consumption of a library is directly tied to its utilization of the underlying System API. We provide a formal definition and implementation of the proposed uAPI profiles that are calculated based on dynamic call graphs obtained from a library under test. To further show the connection between our proposed uAPI profiles and energy consumption, we empirically examined their correlation using two experiments. The first one is dedicated to Android I/O operations, the second one examines uAPI profiles based on a popular open-source library for JSON document processing. Results: In our empirical evaluation, we collected 1052 individual call graphs which we used as an input to evaluate the our model and compare it with the actual energy consumption measured. For each call graph, we measured the energy consumption with special hardware and attributed the measurements to individual methods. Based on that data, we examined a strong linear correlation between the proposed uAPI profiles and the actual measured energy consumption. Conclusions: uApi profiles serve as a lightweight and feasible approach to characterize the energy characteristics of thirdparty libraries. Their computation does not involve any special hardware, and there are no alterations on the target mobile device required. For future work, we investigate possibilities to use the proposed uAPI profiles as a foundation for a regression model, which would allow us to predict the energy consumption development of a library over time.",Dynamic program analysis | Energy consumption | Software energy profiling,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Schuler, Andreas;Anderst-Kotsis, Gabriele",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85095859565,10.1145/3382494.3410678,On reducing the energy consumption of software: From hurdles to requirements,"Background. As software took control over hardware in many domains, the question of the energy footprint induced by the software is becoming critical for our society, as the resources powering the underlying infrastructure are finite. Yet, beyond this growing interest, energy consumption remains a difficult concept to master for a developer. Aims. The purpose of this study is to better understand the root causes that prevent the issue of software energy consumption to be more widely considered by developers and companies. Method. To investigate this issue, this paper reports on a qualitative study we conducted in an industrial context. We applied an in-depth analysis of the interviews of 10 experienced developers and summarized a set of implications. Results.We argue that our study delivers i) insightful feedback on how green software design is considered among the interviewed developers and ii) a set of findings to build helpful tools, motivate further research, and establish better development strategies to promote green software design. Conclusion. This paper covers an industrial case study of developers' awareness of green software design and howto promote it within the company. While it might not be generalizable for any company, we believe our results deliver a common body of knowledge with implications to be considered for similar cases and further researches.",,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Ournani, Zakaria;Rouvoy, Romain;Rust, Pierre;Penhoat, Joel",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85095851601,10.1145/3382494.3410684,Spectrum-based log diagnosis,"Background: Continuous Engineering practices are increasingly adopted in modern software development. However, a frequently reported need is for more effective methods to analyze the massive amounts of data resulting from the numerous build and test runs. Aims: We present and evaluate Spectrum-Based Log Diagnosis (SBLD), a method to help developers quickly diagnose problems found in complex integration and deployment runs. Inspired by Spectrum-Based Fault Localization, SBLD leverages the differences in event occurrences between logs for failing and passing runs, to highlight events that are stronger associated with failing runs. Method: Using data provided by Cisco Norway, we empirically investigate the following questions: (i) How well does SBLD reduce the effort needed to identify all failure-relevant events in the log for a failing run? (ii) How is the performance of SBLD affected by available data? (iii) How does SBLD compare to searching for simple textual patterns that often occur in failure-relevant events? We answer (i) and (ii) using summary statistics and heatmap visualizations, and for (iii) we compare three configurations of SBLD (with resp. minimum, median and maximum data) against a textual search using Wilcoxon signed-rank tests and the Vargha-Delaney measure of stochastic superiority. Results: Our evaluation shows that (i) SBLD achieves a significant effort reduction for the dataset used, (ii) SBLD benefits from additional logs for passing runs in general, and it benefits from additional logs for failing runs when there is a proportional amount of logs for passing runs in the data. Finally, (iii) SBLD and textual search are roughly equally effective at effort-reduction, while textual search has slightly better recall. We investigate the cause, and discuss how it is due to characteristics of a specific part of our data. Conclusions: We conclude that SBLD shows promise as a method for diagnosing failing runs, that its performance is positively affected by additional data, but that it does not outperform textual search on the dataset considered. Future work includes investigating SBLD's generalizability on additional datasets.",Continuous engineering | Failure diagnosis | Log analysis | Log mining,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Rosenberg, Carl Martin;Moonen, Leon",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85095831308,10.1145/3382494.3410672,What industry wants from requirements engineers in China? An exploratory and comparative study on RE job ads,"[Background] Publications on the professional occupation of Requirements Engineering (RE) reported on market demands for both RE and non-RE qualifications and indicated the state-of-the-practice of RE roles in industry. However, prior research was not from the perspective of the RE area in the Software Engineering Body of Knowledge (SWEBOK). Nor, they shed light on the industry needs of RE professionals in China. [Aims] This paper focused on RE-specific tasks and skills sought after in China, from the perspective of RE activities elaborated in SWEBOK. [Method] Using an empirical qualitative research method, we selected and analyzed 535 job ads from China's two largest job portals. Job titles and descriptions of these ads were analyzed to uncover RErelevant responsibilities in the categories of RE activities in SWEBOK as well as RE skills. [Results] We identified the qualifications, experience and skills demanded by Chinese employers. Specifically, we reported 23 RE tasks demanded in the 535 job ads, from the perspective of SWEBOK RE activities. [Conclusion] Our findings reveal that in China's job market, 'requirements engineer' is explicitly used as a title of job ads. Plus, around 78% of the selected job positions want the employees to perform tasks in requirements elicitation. Editing requirements specification is the most in-demand task in RE activities. In addition, employers placed more emphasis on both RE-specific and broad industry experience.",Exploratory Study | Job Ads | RE Job Market | Requirements Engineer | Requirements Engineering Practice,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Wang, Chong;Tang, Yaqian;Liang, Peng;Daneva, Maya;Van Sinderen, Marten",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85095841232,10.1145/3382494.3422171,Web frameworks for desktop apps: An exploratory study,"Background: Novel frameworks for the development of desktop applications with web technologies have become popular. These desktop web app frameworks allow developers to reuse existing code and knowledge of web applications for the creation of crossplatform apps integrated with native APIs. Aims: To date, desktop web app frameworks have not been studied empirically. In this paper, we aim to fill this gap by characterizing the usage of web frameworks and providing evidence on how beneficial are their pros and how impactful are their cons. Method: We conducted an empirical study, collecting and analyzing 453 desktop web apps publicly available on GitHub. We performed qualitative and quantitative analyses to uncover the traits and issues of desktop web apps. Results: We found that desktop web app frameworks enable the development of cross-platform applications even for teams of limited dimensions, taking advantage of the abundant number of available web libraries. However, at the same time, bugs deriving from platform compatibility issues are common. Conclusions: Our study provides concrete evidence on some disadvantages associated with desktop web app frameworks. Future work is required to assess their impact on the required development and maintenance effort, and to investigate other aspects not considered in this first research.",Cross-platform | Desktop apps | Web technologies,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Scoccia, Gian Luca;Autili, Marco",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85095808766,10.1145/3382494.3422166,Java cryptography uses in the wild,"[Background] Previous research has shown that developers commonly misuse cryptography APIs. [Aim] We have conducted an exploratory study to find out how crypto APIs are used in open-source Java projects, what types of misuses exist, and why developers make such mistakes. [Method] We used a static analysis tool to analyze hundreds of open-source Java projects that rely on Java Cryptography Architecture, and manually inspected half of the analysis results to assess the tool results. We also contacted the maintainers of these projects by creating an issue on the GitHub repository of each project, and discussed the misuses with developers. [Results] We learned that 85% of Cryptography APIs are misused, however, not every misuse has severe consequences. Developer feedback showed that security caveats in the documentation of crypto APIs are rare, developers may overlook misuses that originate in thirdparty code, and the context where a Crypto API is used should be taken into account. [Conclusion] We conclude that using Crypto APIs is still problematic for developers but blindly blaming them for such misuses may lead to erroneous conclusions.",Empirical study | Java cryptography | Security,International Symposium on Empirical Software Engineering and Measurement,2020-10-05,Conference Paper,"Hazhirpasand, Mohammadreza;Ghafari, Mohammad;Nierstrasz, Oscar",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097343546,10.1109/ISSRE5003.2020.00016,Understanding merge conflicts and resolutions in git rebases,"Software merging is an important activity during software development. Merge conflicts may arise and degrade the software quality. Empirical studies on software merging are helpful to understand developers' needs and the challenges of detecting and resolving conflicts. Existing studies collect merges by identifying commits that have more than one parent commit. Different from these explicit merges, rebasing branches is used to merge other changes but rewrites the evolutionary history. Hence, existing studies fail to identify implicit merges performed by rebasing branches. Consequently, the results of these studies may fail to provide comprehensive insights on software merging. In our study, we leverage the recently updated APIs of GitHub to study rebase activities in the pull requests. Our study shows that rebasing is widely used in pull requests. And our results indicate that, to resolve textual conflicts, developers adopt similar strategies shown in existing studies on explicit merges. However, in 34.2% of non-conflict rebase scenarios, developers add new changes during the rebase process. And this indicates that there are some new challenges of validating rebases. Our results provide useful insights for improving the state-of-the-art techniques on resolving conflicts and validating rebases.",Merge conflicts | Software evolution | Software merging,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Ji, Tao;Chen, Liqian;Yi, Xin;Mao, Xiaoguang",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097348956,10.1109/ISSRE5003.2020.00025,An exploratory study of bugs in extended reality applications on the web,"Extended Reality (XR) technologies are becoming increasingly popular in recent years. To help developers deploy XR applications on the Web, W3C released the WebXR Device API in 2019, which enable users to interact with browsers using XR devices. Given the convenience brought byWebXR, a growing number of WebXR projects have been deployed in practice. However, many WebXR applications are insufficiently tested before being released. They suffer from various bugs that can degrade user experience or cause undesirable consequences. Yet, the community has limited understanding towards the bugs in the WebXR ecosystem, which impedes the advance of techniques for assuring the reliability of WebXR applications. To bridge this gap, we conducted the first empirical study of WebXR bugs. We collected 368 real bugs from 33 WebXR projects hosted on GitHub. Via a seven-round manual analysis of these bugs, we built a taxonomy of WebXR bugs according to their symptoms and root causes. Furthermore, to understand the uniqueness of WebXR bugs, we compared them with bugs in conventional JavaScript programs and web applications. We believe that our findings can inspire future researches on relevant topics and we released our bug dataset to facilitate follow-up studies.",Bug taxonomy | Empirical study | WebXR,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Li, Shuqing;Wu, Yechang;Liu, Yi;Wang, Dinghua;Wen, Ming;Tao, Yida;Sui, Yulei;Liu, Yepang",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097351850,10.1109/ISSRE5003.2020.00026,Deep learning based valid bug reports determination and explanation,"Bug reports are widely used by developers to fix bugs. Due to the lack of experience, reporters may submit numerous invalid bug reports. Manually determining valid bug reports is a laborious task. Automatically identifying valid bug reports can save time and effort for bug analysis. In this paper, we propose a deep learning-based approach to determine and explain valid bug reports using only textual information i.e., summaries and descriptions of bug reports. Convolutional neural network (CNN) is applied to capture their contextual and semantic features. Moreover, by analyzing the spatial structure of CNN, we backtrack the trained CNN model to get phrases that can explain valid bug reports determination. After inspecting the phrases manually, we summarize some valid bug report patterns. We evaluate our approach on five large-scale open-source projects containing a total of 540491 bug reports. On average, across the five projects, our approach achieves 0.85, 0.80, 0.69 and improves the state-of-the-art approach by 8.97%, 9.59%, 9.52% in terms of AUC, F1-score for valid bug reports, and F1-score for invalid bug reports, respectively. From the summarized patterns, we can find that determining valid bug reports is mainly due to three categories of patterns: Attachment, Environment, and Reproduce.",Deep Learning | Model Explainability | Software Quality Assurance | Valid Bug Report,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"He, Jianjun;Xu, Ling;Fan, Yuanrui;Xu, Zhou;Yan, Meng;Lei, Yan",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097331754,10.1109/ISSRE5003.2020.00027,HINDBR: Heterogeneous information network based duplicate bug report prediction,"Duplicate bug reports often exist in bug tracking systems (BTSs). Almost all the existing approaches for automatically detecting duplicate bug reports are based on text similarity. A recent study found that such approaches may become ineffective in detecting duplicates in bug reports submitted after the justin- time (JIT) retrieval, which is now a built-in feature of modern BTSs (e.g., Bugzilla). This is mainly because the embedded JIT feature suggests possible duplicates in a bug database when a bug reporter types in the new summary field, therefore minimizing the submission of textually similar reports. Although JIT filtering seems effective, a number of bug report duplicates remain undetected. Our hypothesis is that we can detect them using a semantic similarity-based approach. This paper presents HINDBR, a novel deep neural network (DNN) that accurately detects semantically similar duplicate bug reports using a heterogeneous information network (HIN). Instead of matching text similarity alone, HINDBR embeds semantic relations of bug reports into a low-dimensional embedding space where two duplicate bug reports represented by two vectors are close to each other in the latent space. Results show that HINDBR is effective.",Deep learning | Duplicate bug report prediction | Heterogeneous information network,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Xiao, Guanping;Du, Xiaoting;Sui, Yulei;Yue, Tao",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097332315,10.1109/ISSRE5003.2020.00031,Correlating ui contexts with sensitive api calls: Dynamic semantic extraction and analysis,"The Android framework provides sensitive APIs for Android apps to access the user's private information, e.g., SMS, call logs and locations. Whether a sensitive API call in an app is legitimate or not depends on whether the app has provided enough natural-language semantics to reflect the need for the permission. The prior efforts on analyzing description-topermission fidelity in an app are all static. Some check whether the permissions requested (or sensitive APIs used) by the app are consistent with the functionalities described by the app. These app-level techniques are too coarse-grained, as they cannot tell if a sensitive API call under a certain runtime context, such as a UI state, is legitimate or not. Others attempt to establish this connection by performing a data-flow analysis, but such finegrained API-level static analyses are too imprecise to handle a variety of dynamic language features used in Android apps, including dynamic class loading, reflection and code obfuscation. We introduce APICOG, an automated fine-grained APIlevel approach, representing the first dynamic description-topermission fidelity analysis for an Android app that can check if a sensitive API call is legitimate or not under a given runtime context. APICOG relates each sensitive API call with a UI state, called its UI context, under which the call is made via dynamic analysis and then extracts the text-based semantics for each UI context from its associated text- and image-typed attributes by applying a natural language processing (NLP) technique. Finally, APICOG relies on machine-learning to deduce if a sensitive API call under a UI context is legitimate or not. We have evaluated APICOG with thousands of Android apps drawn from a thirdparty market and a malware dataset, achieving an accuracy of 97.7%, a precision of 94.1% and a recall of 92.8% overall, outperforming the prior art in all the three metrics.",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Liu, Jie;He, Dongjie;Wu, Diyu;Xue, Jingling",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097347004,10.1109/ISSRE5003.2020.00043,SACC - A property driven approach to expose undesired behaviors among system's components,"In recent years, there has been an increase in automation of safety critical systems such as self-driving cars, caretaking robots, or rescue drones. With increase in automation, the risk of systems behaving in an undesired manner has also risen. Most safety analysis approaches predominantly concentrate on identifying or assessing how the failure of a component can affect the behavior of a system based on the system's requirements. Yet, undesired behaviors can also occur when multiple components concurrently exert opposite effects on a shared resource. To identify safety-critical issues due to undesired concurrent component behaviors, we propose a property-driven approach called safety assessment for concurrent components (SACC). SACC uses a combinatorial technique that considers the requirements specification of a system, expressed as the states and properties of the system's components for identifying undesired combinations of component behaviors. To evaluate SACC, we performed a study using a requirements document on a caretaking robot. Our results show that SACC identified 38%-80% more undesired system behaviors when compared to the control techniques.",Component States | SACC | Safety-Critical Component Combination | Undesired Behavior,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Madala, Kaushik;Ye Hang, Ke;Do, Hyunsook;Tenbergen, Bastian",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097333059,10.1109/ISSRE5003.2020.00046,Model-driven fault injection in java source code,"The injection of software faults in source code requires accurate knowledge of the programming language, both to craft faults and to identify injection locations. As such, fault injection and code mutation tools are typically tailored for a specific language and have limited extensibility. In this paper we present a model-driven approach to craft and inject software faults in source code. While its concrete application is presented for Java, the workflow we propose does not depend on a specific programming language. Following Model-Driven Engineering principles, the faults and the criteria to select injection locations are described using structured, machinereadable specifications based on a domain-specific language. Then, automated transformations craft artifacts based on OCL and Java, which represent the faults to be injected and are able to select the candidate injection locations. Finally, artifacts are executed against the target source code, performing the injection in the desired locations. We devise a supporting tool and exercise the approach injecting 13 different kinds of software faults in the Java source code of six different projects.",Code patterns | Fault libraries | Java | Metamodel | OCL | Software faults,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Rodrigues, Elder;Montecchi, Leonardo;Ceccarelli, Andrea",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097645475,10.1109/SCAM51674.2020.00007,Compositional Information Flow Analysis for WebAssembly Programs,"WebAssembly is a new W3C standard, providing a portable target for compilation for various languages. All major browsers can run WebAssembly programs, and its use extends beyond the web: There is interest in compiling cross-platform desktop applications, server applications, IoT and embedded applications to WebAssembly because of the performance and security guarantees it aims to provide. Indeed, WebAssembly has been carefully designed with security in mind. In particular, WebAssembly applications are sandboxed from their host environment. However, recent works have brought to light several limitations that expose WebAssembly to traditional attack vectors. Visitors of websites using WebAssembly have been exposed to malicious code as a result.In this paper, we propose an automated static program analysis to address these security concerns. Our analysis is focused on information flow and is compositional. For every WebAssembly function, it first computes a summary that describes in a sound manner where the information from its parameters and the global program state can flow to. These summaries can then be applied during the subsequent analysis of function calls. Through a classical fixed-point formulation, one obtains an approximation of the information flow in the WebAssembly program. This results in the first compositional static analysis for WebAssembly. On a set of 34 benchmark programs spanning 196kLOC of WebAssembly, we compute at least 64% of the function summaries precisely in less than a minute in total.",security | Static program analysis | WebAssembly,"Proceedings - 20th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2020",2020-09-01,Conference Paper,"Stievenart, Quentin;Roover, Coen De",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097651306,10.1109/SCAM51674.2020.00015,Understanding and Characterizing Changes in Bugs Priority: The Practitioners' Perceptive,"Assigning appropriate priority to bugs is critical for timely addressing important software maintenance issues. An underlying aspect is the effectiveness of assigning priorities: if the priorities of a fair number of bugs are changed, it indicates delays in fixing critical bugs. There has been little prior work on understanding the dynamics of changing bug priorities. In this paper, we performed an empirical study to observe and understand the changes in bugs' priority to build a 3-W model on Why and When bug priorities change, and Who performs the change. We conducted interviews and a survey with practitioners as well as performed a quantitative analysis containing 225, 000 bug reports, developers' comments, and source code changes from 24 open-source systems. The interviews with 11 developers from industry aim to establish an initial model to characterize the changes in bugs priority. The survey with an additional 38 developers was to understand their experience in why and when bug priorities change, and who performs the change. Then, we conducted a manual inspection of the collected data on open-source projects to compare our final bugs priority change model with changes identified in practice. Our quantitative results confirmed the outcomes of our interviews and surveys. For instance, we observed frequent changes in bug priorities and their impact on delaying critical bug fixes especially just before shipping a new release. Our findings can enable 1) researchers to build automated tools for checking and validating requests for bug priority changes, 2) practitioners to use a standard format in documenting and approving bug priority changes, and 3) educators to teach the better management of bug priorities.",Bug Report | Bugs Priority | Empirical Study | Software Bugs,"Proceedings - 20th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2020",2020-09-01,Conference Paper,"Almhana, Rafi;Ferreira, Thiago;Kessentini, Marouane;Sharma, Tushar",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097644581,10.1109/SCAM51674.2020.00020,Annotation practices in Android apps,"Understanding the adoption and usage of any programming language feature is crucial for improving it. Existing studies indicate that Java annotations are widely used by developers. However, there is currently no empirical data on annotation usage in Android apps. Android apps are often smaller than general Java applications and typically use Android APIs or specific libraries catered to the mobile environment. Therefore, it is not clear if the results of existing Java studies hold for Android apps. In this paper, we investigate annotation practices in Android apps through an empirical study of 1, 141 open-source apps. Using previously studied metrics, we first compare annotation usage in Android apps to existing results from general Java applications. Then, for the first time, we study why developers declare custom annotations. Our results show that the density of annotations and the values of various other annotation metrics are notably less in Android apps than in Java projects. Additionally, the types of annotations used in Android apps are different than those in Java, with many Android-specific annotations. These results imply that researchers may need to distinguish mobile apps while performing studies on programming language features. However, we also found examples of extreme usage of annotations with, for example, a large number of attributes, as well as a low adoption rate for most annotations. By looking at such results, annotation designers can assess adoption patterns and take various improvement measures, such as modularizing their offered annotations or cleaning up unused ones. Finally, we find that developers declare custom annotations in different apps but with the same purpose, which presents an opportunity for annotation designers to create new annotations.",Android annotations | Android apps | annotations | custom annotations | Java annotations,"Proceedings - 20th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2020",2020-09-01,Conference Paper,"Jha, Ajay Kumar;Nadi, Sarah",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85097643109,10.1109/SCAM51674.2020.00030,Engineering a Converter between Two Domain-Specific Languages for Sorting,"Part of the ecosystem of applications running on mainframe computers is the DFSORT program. It is responsible for sorting and reformatting data (amongst other functionalities) and is configured by specifications written in a Domain-Specific Language (DSL). When migrating such sort workloads off from the mainframe, the SyncSort product is an attractive alternative. It is also configured by specifications written in a DSL but this language is structured in a radically different way. Whereas the DFSORT DSL uses an explicit fixed pipeline for processing, the SyncSort DSL does not. To allow DFSORT workloads to run on SyncSort we have therefore built a source-To-source translator from the DFSORT DSL to the SyncSort DSL. Our language converter performs abstract interpretation of the DFSORT specification, considering the different steps in the DFSORT pipeline at translation time. This is done by building a graph of objects and key to the construction of this graph is the reification of the records being sorted. In this paper we report on the design and implementation of the converter, describing how it treats the DFSORT pipeline. We also show how its design allowed for the straightforward implementation of unexpected changes in requirements for the generated output.",Domain-Specific Languages | Language Conversion | Legacy Systems,"Proceedings - 20th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2020",2020-09-01,Conference Paper,"Fabry, Johan;Jaradin, Ynes;Gul, Aynel",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85097650670,10.1109/SCAM51674.2020.00034,MUTAMA: An Automated Multi-label Tagging Approach for Software Libraries on Maven,"Recent studies show that the Maven ecosystem alone already contains over 2 million library artefacts including their source code, byte code, and documentation. To help developers cope with this information, several websites overlay configurable views on the ecosystem. For instance, views in which similar libraries are grouped into categories or views showing all libraries that have been tagged with tags corresponding to coarse-grained library features. The MVNRepository overlay website offers both category-based and tag-based views. Unfortunately, several libraries have not been categorised or are missing relevant tags. Some initial approaches to the automated categorisation of Maven libraries have already been proposed. However, no such approach exists for the problem of tagging of libraries in a multi-label setting.This paper proposes MUTAMA, a multi-label classification approach to the Maven library tagging problem based on information extracted from the byte code of each library. We analysed 4088 randomly selected libraries from the Maven software ecosystem. MUTAMA trains and deploys five multi-label classifiers using feature vectors obtained from class and method names of the tagged libraries. Our results indicate that classifiers based on ensemble methods achieve the best performances. Finally, we propose directions to follow in this area.",classification | libraries | multi-label | software ecosystem,"Proceedings - 20th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2020",2020-09-01,Conference Paper,"Velazquez-Rodriguez, Camilo;De Roover, Coen",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85099445073,10.1109/VISSOFT51673.2020.00009,Visualizing Interaction Data Inside Outside the IDE to Characterize Developer Productivity,"Work fragmentation is a common phenomenon in the workspace, and is detrimental to the actual work taking place. To measure and study the impact of work fragmentation in software development, several studies exploited interaction data, i.e., the data generated by the events performed by the developers in the IDE. However, the absence of information on activities performed outside the IDE could lead to a misclassification of development time. In fact, sometimes leaving the IDE is not an interruption of the task at hand, e.g., when consulting API documentation, or when discussing with colleagues in ad-hoc collaboration applications.In this paper, we propose Ferax, a data analytics platform that developers can leverage for retrospection and possibly to improve their productivity. The capabilities of Ferax are twofold: First, it extends Tako, a profiler to record IDE interaction data for Visual Studio Code, with information about which applications were used and which websites were visited. Second, to enable the understanding of productivity and interruptions on developer sessions, Ferax provides interactive visualizations that show the detailed sequence of events inside and outside the IDE, the switches the developer performs by classifying them as productive or possible interruptions, and the time distribution for application usage.As a preliminary evaluation of Ferax we have collected and analyzed real development sessions from a set of master students and two professional developers. We illustrate how a developer can leverage Ferax to characterize her usual habits, to elicit the impact of interruptions, and to better characterize sessions which were only apparently unproductive.",development productivity | interaction data,"Proceedings - 8th IEEE Working Conference on Software Visualization, VISSOFT 2020",2020-09-01,Conference Paper,"Di Rosa, Gabriele;Mocci, Andrea;D'Ambros, Marco",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85099468369,10.1109/VISSOFT51673.2020.00010,Enhanced Visualization of Method Invocations by Extending Reverse-engineered Sequence Diagrams,"Software} maintainers employ reverse-engineered sequence diagrams to visually understand software behavior, especially when software documentation is absent or outdated. Much research has studied the adoption of reverse-engineered sequence diagrams to visualize program interactions. However, due to the forward-engineering nature of sequence diagrams, visualizing more complex programming scenarios can be challenging. In particular, sequence diagrams represent method invocations as unidirectional arrows. However, in practice, source code may contain compound method invocations that share values/objects implicitly. For example, method invocations can be nested, e.g., fun (foo ()), or chained, e.g., fun (). foo (). The standard notation of sequence diagrams does not have enough expressive power to precisely represent compound scenarios of method invocations. Understanding the flow of information between method invocations simplifies debugging, inspection, and exception handling operations for software maintainers. Despite the research invested to address the limitations of UML sequence diagrams, previous approaches fail to visualize compound scenarios of method invocations. In this paper, we propose sequence diagram extensions to enhance the visualization of (i) three widely used types of compound method invocations in practice (i.e., nested, chained, and recursive) and (ii) lifelines of objects returned from method invocations. We aim through our extensions to increase the level of abstraction and expressiveness of method invocation code. We develop a tool to reverse engineer compound method invocations and generate the corresponding extended sequence diagrams. We evaluate how our proposed extensions can improve the understandability of program interactions using a controlled experiment. We find that program interactions are significantly more comprehensible when visualized using our extensions.",controlled experiment | extended notation | method invocation | program comprehension | Sequence diagram,"Proceedings - 8th IEEE Working Conference on Software Visualization, VISSOFT 2020",2020-09-01,Conference Paper,"Ghaleb, Taher Ahmed;Aljasser, Khalid;Alturki, Musab A.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85099478745,10.1109/VISSOFT51673.2020.00011,REM: Visualizing the Ripple Effect on Dependencies Using Metrics of Health,"In recent years, free and open source software (FOSS) components have become common dependencies in the development of software, both open source and proprietary. As the complexity of software increases, so does the number of components they depend upon; in addition, components are also depending on other components. Thus, their dependency graphs are growing in size and complexity. One of the current challenges in software development is that it is not trivial to know the full dependency graph of an application. Developers are usually aware of the direct dependencies their application requires, but might not be fully aware of the dependencies that those dependencies require (the transitive dependencies). Unfortunately, transitive dependencies can break any software application; therefore, project developers need tools, methods and visualizations to inspect the health of these transitive dependencies and their potential impact.In this work, we propose the Ripple Effect of Metrics (REM) dependency graphs, a visualization of dependency graphs that leverages metrics of the health of dependencies. The two main features of REM dependency graph are: first, to display, and potentially summarize, the full dependency graph of an application based on the health of each of its dependencies; and second, to evaluate the ripple effect of potentially risky dependencies on the rest of the dependency graph. The REM helps application developers inspect the health of all of its dependencies, and also the impact that some of these dependencies might have. By showcasing two examples of popular NPM JavaScript application, we demonstrate that the combination of the ripple effect on the dependency graph using health metrics activity can be beneficial to developers. The advantages of REM graphs are: 1) the metric of health annotation is useful for evaluating the health of dependencies, and 2) the ripple effect of a vulnerability provides an easy method to identify potential risk in a dependency chain and 3) the summarizing mechanisms of the REM help reduce the size and complexity of the large dependency graphs, while focusing in specific aspects of the health of the dependency graph.",Dependency graph | Health | Metrics | Software components,"Proceedings - 8th IEEE Working Conference on Software Visualization, VISSOFT 2020",2020-09-01,Conference Paper,"Chen, Zhe;German, Daniel M.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85099433669,10.1109/VISSOFT51673.2020.00013,Interactive Visualization for OSGi-based Projects,"Big software projects often use architectural frameworks for a consistent structure. OSGi is such a framework to create modular Java applications. The architecture of individual projects, however, is often hidden in configuration files.We propose to visualize projects in a modular framework such as OSGi with an approach to allow users to comprehend the connections within a system. We assist this comprehension using filtering and automatically generated, interactive views of the project. We extend the notion of interactive views with a concept to reproduce configured views for arbitrary system revisions to enhance up-To-date documentation.We have implemented this proposal in the publicly available KIELER project, and have validated it with a large software project in the railway domain.","OSGi, visualization, interactive, software architecture, documentation","Proceedings - 8th IEEE Working Conference on Software Visualization, VISSOFT 2020",2020-09-01,Conference Paper,"Rentz, Niklas;Dams, Christian;Hanxleden, Reinhard Von",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85079408292,10.1145/3355181,"On the monitoring of decentralized specifications: Semantics, properties, analysis, and simulation","We introduce two complementary approaches to monitor decentralized systems. The first approach relies on systems with a centralized specification, i.e., when the specification is written for the behavior of the entire system. To do so, our approach introduces a data structure that (i) keeps track of the execution of an automaton (ii) has predictable parameters and size, and (iii) guarantees strong eventual consistency. The second approach defines decentralized specifications wherein multiple specifications are provided for separate parts of the system.We study two properties of decentralized specifications pertaining to monitorability and compatibility between specification and architecture. We also present a general algorithm for monitoring decentralized specifications. We map three existing algorithms to our approaches and provide a framework for analyzing their behavior. Furthermore, we present THEMIS, a framework for designing such decentralized algorithms and simulating their behavior. We demonstrate the usage of THEMIS to compare multiple algorithms and validate the trends predicted by the analysis in two scenarios: a synthetic benchmark and the Chiron user interface.",Automata | Decentralized monitoring | Eventual consistency | Monitoring | Runtime verification | Simulation,ACM Transactions on Software Engineering and Methodology,2020-01-25,Article,"El-Hokayem, Antoine;Falcone, Yliès",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3365664,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088269950,10.1145/3387909,Monotone Precision and Recall Measures for Comparing Executions and Specifications of Dynamic Systems,"The behavioural comparison of systems is an important concern of software engineering research. For example, the areas of specification discovery and specification mining are concerned with measuring the consistency between a collection of execution traces and a program specification. This problem is also tackled in process mining with the help of measures that describe the quality of a process specification automatically discovered from execution logs. Though various measures have been proposed, it was recently demonstrated that they neither fulfil essential properties, such as monotonicity, nor can they handle infinite behaviour. In this article, we address this research problem by introducing a new framework for the definition of behavioural quotients. We prove that corresponding quotients guarantee desired properties that existing measures have failed to support. We demonstrate the application of the quotients for capturing precision and recall measures between a collection of recorded executions and a system specification. We use a prototypical implementation of these measures to contrast their monotonic assessment with measures that have been defined in prior research.",behavioural analysis | behavioural comparison | conformance checking | coverage | entropy | fitness | precision | process mining | recall | System comparison,ACM Transactions on Software Engineering and Methodology,2020-07-09,Article,"Polyvyanyy, Artem;Solti, Andreas;Weidlich, Matthias;Ciccio, Claudio Di;Mendling, Jan",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088298066,10.1145/3392093,Psc2code: Denoising Code Extraction from Programming Screencasts,"Programming screencasts have become a pervasive resource on the Internet, which help developers learn new programming technologies or skills. The source code in programming screencasts is an important and valuable information for developers. But the streaming nature of programming screencasts (i.e., a sequence of screen-captured images) limits the ways that developers can interact with the source code in the screencasts. Many studies use the Optical Character Recognition (OCR) technique to convert screen images (also referred to as video frames) into textual content, which can then be indexed and searched easily. However, noisy screen images significantly affect the quality of source code extracted by OCR, for example, no-code frames (e.g., PowerPoint slides, web pages of API specification), non-code regions (e.g., Package Explorer view, Console view), and noisy code regions with code in completion suggestion popups. Furthermore, due to the code characteristics (e.g., long compound identifiers like ItemListener), even professional OCR tools cannot extract source code without errors from screen images. The noisy OCRed source code will negatively affect the downstream applications, such as the effective search and navigation of the source code content in programming screencasts. In this article, we propose an approach named psc2code to denoise the process of extracting source code from programming screencasts. First, psc2code leverages the Convolutional Neural Network (CNN) based image classification to remove non-code and noisy-code frames. Then, psc2code performs edge detection and clustering-based image segmentation to detect sub-windows in a code frame, and based on the detected sub-windows, it identifies and crops the screen region that is most likely to be a code editor. Finally, psc2code calls the API of a professional OCR tool to extract source code from the cropped code regions and leverages the OCRed cross-frame information in the programming screencast and the statistical language model of a large corpus of source code to correct errors in the OCRed source code. We conduct an experiment on 1,142 programming screencasts from YouTube. We find that our CNN-based image classification technique can effectively remove the non-code and noisy-code frames, which achieves an F1-score of 0.95 on the valid code frames. We also find that psc2code can significantly improve the quality of the OCRed source code by truly correcting about half of incorrectly OCRed words. Based on the source code denoised by psc2code, we implement two applications: (1) a programming screencast search engine; (2) an interaction-enhanced programming screencast watching tool. Based on the source code extracted from the 1,142 collected programming screencasts, our experiments show that our programming screencast search engine achieves the precision@5, 10, and 20 of 0.93, 0.81, and 0.63, respectively. We also conduct a user study of our interaction-enhanced programming screencast watching tool with 10 participants. This user study shows that our interaction-enhanced watching tool can help participants learn the knowledge in the programming video more efficiently and effectively.",code search | deep learning | Programming videos,ACM Transactions on Software Engineering and Methodology,2020-07-09,Article,"Bao, Lingfeng;Xing, Zhenchang;Xia, Xin;Lo, David;Wu, Minghui;Yang, Xiaohu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85092710127,10.1145/3391533,Handling SQL Databases in Automated System Test Generation,"Automated system test generation for web/enterprise systems requires either a sequence of actions on a GUI (e.g., clicking on HTML links and form buttons) or direct HTTP calls when dealing with web services (e.g., REST and SOAP). When doing white-box testing of such systems, their code can be analyzed, and the same type of heuristics (e.g., the branch distance) used in search-based unit testing can be employed to improve performance. However, web/enterprise systems do often interact with a database. To obtain higher coverage and find new faults, the state of the databases needs to be taken into account when generating white-box tests. In this work, we present a novel heuristic to enhance search-based software testing of web/enterprise systems, which takes into account the state of the accessed databases. Furthermore, we enable the generation of SQL data directly from the test cases. This is useful when it is too difficult or time consuming to generate the right sequence of events to put the database in the right state. Also, it is useful when dealing with databases that are ""read-only""for the system under test, and the actual data are generated by other services. We implemented our technique as an extension of EVOMASTER, where system tests are generated in the JUnit format. Experiments on six RESTful APIs (five open-source and one industrial) show that our novel techniques improve coverage significantly (up to +16.5%), finding seven new faults in those systems.",automated test generation | database | REST | SBST | SQL | system testing | web service,ACM Transactions on Software Engineering and Methodology,2020-10-01,Article,"Arcuri, Andrea;Galeotti, Juan P.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85092728971,10.1145/3401026,Generating Question Titles for Stack Overflow from Mined Code Snippets,"Stack Overflow has been heavily used by software developers as a popular way to seek programming-related information from peers via the internet. The Stack Overflow community recommends users to provide the related code snippet when they are creating a question to help others better understand it and offer their help. Previous studies have shown that a significant number of these questions are of low-quality and not attractive to other potential experts in Stack Overflow. These poorly asked questions are less likely to receive useful answers and hinder the overall knowledge generation and sharing process. Considering one of the reasons for introducing low-quality questions in SO is that many developers may not be able to clarify and summarize the key problems behind their presented code snippets due to their lack of knowledge and terminology related to the problem, and/or their poor writing skills, in this study we propose an approach to assist developers in writing high-quality questions by automatically generating question titles for a code snippet using a deep sequence-to-sequence learning approach. Our approach is fully data-driven and uses an attention mechanism to perform better content selection, a copy mechanism to handle the rare-words problem and a coverage mechanism to eliminate word repetition problem. We evaluate our approach on Stack Overflow datasets over a variety of programming languages (e.g., Python, Java, Javascript, C# and SQL) and our experimental results show that our approach significantly outperforms several state-of-the-art baselines in both automatic and human evaluation. We have released our code and datasets to facilitate other researchers to verify their ideas and inspire the follow up work.",question generation | question quality | sequence-to-sequence | Stack overflow,ACM Transactions on Software Engineering and Methodology,2020-10-01,Article,"Gao, Zhipeng;Xia, Xin;Grundy, John;Lo, David;Li, Yuan Fang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85047195749,10.1109/TSE.2018.2838131,Use and Misuse of Continuous Integration Features: An Empirical Study of Projects That (Mis)Use Travis CI,"Continuous Integration (CI) is a popular practice where software systems are automatically compiled and tested as changes appear in the version control system of a project. Like other software artifacts, CI specifications require maintenance effort. Although there are several service providers like Travis CI offering various CI features, it is unclear which features are being (mis)used. In this paper, we present a study of feature use and misuse in 9,312 open source systems that use Travis CI. Analysis of the features that are adopted by projects reveals that explicit deployment code is rare - 48.16 percent of the studied Travis CI specification code is instead associated with configuring job processing nodes. To analyze feature misuse, we propose Hansel - an anti-pattern detection tool for Travis CI specifications. We define four anti-patterns and Hansel detects anti-patterns in the Travis CI specifications of 894 projects in the corpus (9.60 percent), and achieves a recall of 82.76 percent in a sample of 100 projects. Furthermore, we propose Gretel - an anti-pattern removal tool for Travis CI specifications, which can remove 69.60 percent of the most frequently occurring anti-pattern automatically. Using Gretel, we have produced 36 accepted pull requests that remove Travis CI anti-patterns automatically.",anti-patterns | Continuous integration | mining software repositories,IEEE Transactions on Software Engineering,2020-01-01,Article,"Gallaba, Keheliya;McIntosh, Shane",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85050622208,10.1109/TSE.2018.2859926,"On the Understandability of Temporal Properties Formalized in Linear Temporal Logic, Property Specification Patterns and Event Processing Language","Temporal properties are important in a wide variety of domains for different purposes. For example, they can be used to avoid architectural drift in software engineering or to support the regulatory compliance of business processes. In this work, we study the understandability of three major temporal property representations: (1) Linear Temporal Logic (LTL) is a formal and well-established logic that offers temporal operators to describe temporal properties; (2) Property Specification Patterns (PSP) are a collection of recurring temporal properties that abstract underlying formal and technical representations; (3) Event Processing Language (EPL) can be used for runtime monitoring of event streams using Complex Event Processing. We conducted two controlled experiments with 216 participants in total to study the understandability of those approaches using a completely randomized design with one alternative per experimental unit. We hypothesized that PSP, as a highly abstracting pattern language, is easier to understand than LTL and EPL, and that EPL, due to separation of concerns (as one or more queries can be used to explicitly define the truth value change that an observed event pattern causes), is easier to understand than LTL. We found evidence supporting our hypotheses which was statistically significant and reproducible.",complex event processing | Controlled experiment | event processing language | linear temporal logic | property specification patterns | temporal property | understandability,IEEE Transactions on Software Engineering,2020-01-01,Article,"Czepa, Christoph;Zdun, Uwe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049676251,10.1109/TSE.2018.2853726,A Framework for Quantitative Modeling and Analysis of Highly (Re)configurable Systems,"This paper presents our approach to the quantitative modeling and analysis of highly (re)configurable systems, such as software product lines. Different combinations of the optional features of such a system give rise to combinatorially many individual system variants. We use a formal modeling language that allows us to model systems with probabilistic behavior, possibly subject to quantitative feature constraints, and able to dynamically install, remove or replace features. More precisely, our models are defined in the probabilistic feature-oriented language QFLan, a rich domain specific language (DSL) for systems with variability defined in terms of features. QFLan specifications are automatically encoded in terms of a process algebra whose operational behavior interacts with a store of constraints, and hence allows to separate system configuration from system behavior. The resulting probabilistic configurations and behavior converge seamlessly in a semantics based on discrete-time Markov chains, thus enabling quantitative analysis. Our analysis is based on statistical model checking techniques, which allow us to scale to larger models with respect to precise probabilistic analysis techniques. The analyses we can conduct range from the likelihood of specific behavior to the expected average cost, in terms of feature attributes, of specific system variants. Our approach is supported by a novel Eclipse-based tool which includes state-of-the-art DSL utilities for QFLan based on the Xtext framework as well as analysis plug-ins to seamlessly run statistical model checking analyses. We provide a number of case studies that have driven and validated the development of our framework.",formal methods | probabilistic modeling | quantitative constraints | Software product lines | statistical model checking,IEEE Transactions on Software Engineering,2020-03-01,Article,"Ter Beek, Maurice H.;Legay, Axel;Lluch Lafuente, Alberto;Vandin, Andrea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85049859538,10.1109/TSE.2018.2854716,Requirements engineering for safety-critical systems: An interview study with industry practitioners,"We have conducted in-depth interviews with experienced practitioners in the Safety-Critical Systems (SCS) domain in order to investigate several aspects related to requirements specification and safety analysis for SCS. We interviewed 19 practitioners from eleven SCS companies in different domains with the intention of verifying which approaches they use day-to-day, and what their perceptions are in relation to the approaches used to elicit, analyze, specify and validate safety requirements. The aim of this study is to obtain an in-depth understanding of how requirements engineering is carried out in companies that develop SCS.",Requirements | requirements engineering | safety critical systems | SCS | software and system safety | software engineering | specification,IEEE Transactions on Software Engineering,2020-04-01,Article,"Martins, Luiz Eduardo G.;Gorschek, Tony",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85054418798,10.1109/TSE.2018.2872971,Automatic Detection and Repair Recommendation of Directive Defects in Java API Documentation,"Application Programming Interfaces (APIs) represent key tools for software developers to build complex software systems. However, several studies have revealed that even major API providers tend to have incomplete or inconsistent API documentation. This can severely hamper the API comprehension and, as a consequence, the quality of the software built on them. In this paper, we propose DRONE (Detect and Repair of dOcumentatioN dEfects), a framework to automatically detect and repair defects from API documents by leveraging techniques from program analysis, natural language processing, and constraint solving. Specifically, we target at the directives of API documents, which are related to parameter constraints and exception handling declarations. Furthermore, in presence of defects, we also provide a prototypical repair recommendation system. We evaluate our approach on parts of the well-documented APIs of JDK 1.8 APIs (including javaFX) and Android 7.0 (level 24). Across the two empirical studies, our approach can detect API defects with an average F-measure of 79.9, 71.7, and 81.4 percent, respectively. The API repairing capability has also been evaluated on the generated recommendations in a further experiment. User judgments indicate that the constraint information is addressed correctly and concisely in the rendered directives.",API documentation | directive defects | natural language processing | repair recommendation,IEEE Transactions on Software Engineering,2020-09-01,Article,"Zhou, Yu;Wang, Changzhi;Yan, Xin;Chen, Taolue;Panichella, Sebastiano;Gall, Harald",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85055057262,10.1109/TSE.2018.2876006,Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding,"Developers increasingly rely on text matching tools to analyze the relation between natural language words and APIs. However, semantic gaps, namely textual mismatches between words and APIs, negatively affect these tools. Previous studies have transformed words or APIs into low-dimensional vectors for matching; however, inaccurate results were obtained due to the failure of modeling words and APIs simultaneously. To resolve this problem, two main challenges are to be addressed: the acquisition of massive words and APIs for mining and the alignment of words and APIs for modeling. Therefore, this study proposes Word2API to effectively estimate relatedness of words and APIs. Word2API collects millions of commonly used words and APIs from code repositories to address the acquisition challenge. Then, a shuffling strategy is used to transform related words and APIs into tuples to address the alignment challenge. Using these tuples, Word2API models words and APIs simultaneously. Word2API outperforms baselines by 10-49.6 percent of relatedness estimation in terms of precision and NDCG. Word2API is also effective on solving typical software tasks, e.g., query expansion and API documents linking. A simple system with Word2API-expanded queries recommends up to 21.4 percent more related APIs for developers. Meanwhile, Word2API improves comparison algorithms by 7.9-17.4 percent in linking questions in Question&Answer communities to API documents.",API documents linking | query expansion | Relatedness estimation | word embedding | word2Vec,IEEE Transactions on Software Engineering,2020-10-01,Article,"Li, Xiaochen;Jiang, He;Kamei, Yasutaka;Chen, Xin",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85055051429,10.1109/TSE.2018.2876340,Automating Intention Mining,"Developers frequently discuss aspects of the systems they are developing online. The comments they post to discussions form a rich information source about the system. Intention mining, a process introduced by Di Sorbo et al., classifies sentences in developer discussions to enable further analysis. As one example of use, intention mining has been used to help build various recommenders for software developers. The technique introduced by Di Sorbo et al. to categorize sentences is based on linguistic patterns derived from two projects. The limited number of data sources used in this earlier work introduces questions about the comprehensiveness of intention categories and whether the linguistic patterns used to identify the categories are generalizable to developer discussion recorded in other kinds of software artifacts (e.g., issue reports). To assess the comprehensiveness of the previously identified intention categories and the generalizability of the linguistic patterns for category identification, we manually created a new dataset, categorizing 5, 408 sentences from issue reports of four projects in GitHub. Based on this manual effort, we refined the previous categories. We assess Di Sorbo et al.'s patterns on this dataset, finding that the accuracy rate achieved is low (0.31). To address the deficiencies of Di Sorbo et al.'s patterns, we propose and investigate a convolution neural network (CNN)-based approach to automatically classify sentences into different categories of intentions. Our approach optimizes CNN by integrating batch normalization to accelerate the training speed, and an automatic hyperparameter tuning approach to tune appropriate hyperparameters of CNN. Our approach achieves an accuracy of 0.84 on the new dataset, improving Di Sorbo et al.'s approach by 171 percent. We also apply our approach to improve an automated software engineering task, in which we use our proposed approach to rectify misclassified issue reports, thus reducing the bias introduced by such data to other studies. A case study on four open source projects with 2, 076 issue reports shows that our approach achieves an average AUC score of 0.687, which improves other baselines by at least 16 percent.",Deep Learning | Empirical Study | Intention | Issue Report,IEEE Transactions on Software Engineering,2020-10-01,Article,"Huang, Qiao;Xia, Xin;Lo, David;Murphy, Gail C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85055038692,10.1109/TSE.2018.2876439,Understanding and Detecting FragmentationInduced Compatibility Issues for Android Apps,"Android ecosystem is heavily fragmented. The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps, and thus various compatibility issues arise. Unfortunately, little is known on the characteristics of such fragmentation-induced compatibility issues. No mature tools exist to help developers quickly diagnose and fix these issues. To bridge the gap, we conducted an empirical study on 220 real-world compatibility issues collected from five popular open-source Android apps. We further interviewed Android practitioners and conducted an online survey to gain insights from real practices. Via the studies, we characterized compatibility issues, investigated common practices to handle compatibility issues, and disclosed that these issues exhibit common patterns. With these findings, we propose a technique, FICFINDER, to automatically detect compatibility issues in Android apps. FICFINDER performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues can be triggered. FICFINDER reports actionable debugging information to developers when it detects potential issues. We evaluated FICFINDER with 53 large-scale open-source Android apps. The results show that FICFINDER can precisely detect compatibility issues in these apps and uncover previously-unknown issues.",android applications | android fragmentation | compatibility issues | empirical study | Mobile applications | program analysis,IEEE Transactions on Software Engineering,2020-11-01,Article,"Wei, Lili;Liu, Yepang;Cheung, Shing Chi;Huang, Huaxun;Lu, Xuan;Liu, Xuanzhe",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85094941434,10.1145/3380851.3416741,Writing for human and machine translation: Best practices for technical writers,"The Red Hat Localization Services team translates technical product documentation from English to other languages required for Red Hat customers. Translators from Localization Services collaborate with Red Hat technical writers to clarify statements in the documentation and answer technical questions related to the documentation as they translate content. This industry insight report examines the team's translation workflow and explores how the machine translation technology used by the Localization Services team impacts the translation progress, for better and for worse. We take a closer look at how this intersection of technology and design both limits and enhances the documentation design and translation processes. This report also examines the role of the technical writer in the translation process, how the writer's documentation design decisions impact the machine translation process, and how the writer should be influenced by the translation process as a whole. This report discusses several methods that can be used by technical writers to write clear and concise content for both human and machine translation. Taking a proactive approach to writing for a global audience makes the translations process more efficient and increases content quality.",Localization | Machine translation | Product documentation | Technical communication,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Hardin, Ashley R.;Ito, Junko;Sasaki, Aiko",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85094954064,10.1145/3380851.3416742,Designing an analytics approach for technical content,"Working on an enterprise cloud product, my documentation team rethought our approach to content analytics. Despite a variety of tools and awareness of industry best practices, my team felt stuck using analytics only in annual or on-demand reports to management, instead of to produce value for our end users. We employed Design Thinking practices to guide a multifaceted user research project that led to changes in the way that we created documentation and automated quality content checks. Key takeaways include to involve the technical documentation team in identifying not only what metrics to collect, but also how to collect, report, and use the metrics in order to increase buy-in and the likelihood that data analytics about content leads to meaningful change within the content itself.",Content strategy | Data analytics | Design thinking,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Berger, Arthur",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85094982197,10.1145/3380851.3416744,Advanced manufacturing documentation: Process-based and practical considerations,"This paper examines documentation procedures and rhetorical contexts navigated by technical communicators in postindustrial, advanced manufacturing environments. Also addressed are the challenges of structure and authority faced by the technical writer in postindustrial hierarchies, particularly those of a STEM-dominated field. In reference to produced documentation, the need for technical communicators to explore alternative formats is such that the traditional operating manual may not be sufficient to fully account for the complexities of automated and digitally-augmented manufacturing systems. Documentation processes, then, must account for collaborative, distributed work approaches, as well as those of technology transfer in the composition and distribution of documentation in support of postindustrial, advanced manufacturing.",Advanced manufacturing | Distributed work | Postindustrial manufacturing environments | Technical communication | Technological rhetoric | Technology transfer,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Stucker, David H.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85094963144,10.1145/3380851.3416746,Creating a framework for home ownership documentation: Style guides as technology and design,"This study of Rural Studio's style guide uses artifacts and interview findings to examine the role of the style guide in an organization that promotes home ownership to mitigate rural poverty. To address these challenges, the team is developing a suite of communication products. As we conducted the study, we came to understand the style guide as a technology, a dynamic element of the genre ecology that is shaped by and in turn shapes the narratives about both individuals and the problem as a whole.",Genre ecology | Narrative | Style guide,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Sidler, Michelle;Youngblood, Susan A.;Butts-Ball, Natalie",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85094963020,10.1145/3380851.3416749,"Addressing the speculative ""you"": Contextualizing the readers of documentation","The poster presents a corpus analysis of a stylistic feature of topic-based documentation: the speculative ""you.""The feature signals important information to help readers adapt the content for their situated uses. The feature is illustrated with examples and the author offers recommendations for amplifying this information.",Corpus Analysis | Documentation | Structured Authoring | Style,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Swarts, Jason",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85094963557,10.1145/3380851.3416759,Optimizing API documentation: Some guidelines and effects,"The growing importance of APIs creates a need to support developers with effective documentation. Prior research has generated important findings regarding information needs of developers and expectations they form towards API documentation. Several guidelines have been proposed on the basis of these findings, but evidence is lacking whether such guidelines actually lead to better documentation. This paper contributes the results of an empirical test that compared the performance of two groups of developers working on a set of pre-defined tasks with an API they were unfamiliar with. One group had access to documentation which was optimized following guidelines for API documentation design proposed in the literature whereas the other group used non-optimized documentation. Results show that developers working with optimized documentation made fewer errors on the test tasks and were faster in planning and executing the tasks. We conclude that the guidelines used in our study do have the intended effect and effectively support initial interactions with an API.",API documentation | Information design | Usability | User studies,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Meng, Michael;Steinhardt, Stephanie M.;Schubert, Andreas",Include,
10.1016/j.jss.2022.111515,2-s2.0-85094963818,10.1145/3380851.3416786,Infrastructures of accessibility: Designing open science projects to incorporate multiple audiences,"Infrastructure is a crucial consideration for creating code that can be reused and repurposed. Infrastructures of data are highly rhetorical, because they structure how others interact with information and reflect hierarchies of information, people, and things. Those creating open source and open science projects, in particular, must consider how they structure their data according to principles of accessibility, reusability, and replicability. Failure to consider the design of publicly available code and insufficient user documentation lead to issues with transparency, learnability, and usability within open source science projects. This paper explores the decision-making processes of two open science researchers as they design their data according to the principles of open science. Using interviews and document analysis, I analyze the rhetorical coding choices of researchers and how these choices encourage or discourage accessibility for different audiences. Ultimately, I examine what accessibility means for these researchers working under the umbrella of open science.",Accessibility | Infrastructure | Open access | Open science,SIGDOC 2020 - Proceedings of the 38th ACM International Conference on Design of Communication,2020-10-03,Conference Paper,"Roberts, Laura",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1145/3291044,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065743149,10.1145/3311950,Machine learning for smart building applications: Review and taxonomy,"The use of machine learning (ML) in smart building applications is reviewed in this article. We split existing solutions into two main classes: occupant-centric versus energy/devices-centric. The first class groups solutions that use ML for aspects related to the occupants, including (1) occupancy estimation and identification, (2) activity recognition, and (3) estimating preferences and behavior. The second class groups solutions that use ML to estimate aspects related either to energy or devices. They are divided into three categories: (1) energy profiling and demand estimation, (2) appliances profiling and fault detection, and (3) inference on sensors. Solutions in each category are presented, discussed, and compared; open perspectives and research trends are discussed as well. Compared to related state-of-the-art survey papers, the contribution herein is to provide a comprehensive and holistic review from the ML perspectives rather than architectural and technical aspects of existing building management systems. This is by considering all types of ML tools, buildings, and several categories of applications, and by structuring the taxonomy accordingly. The article ends with a summary discussion of the presented works, with focus on lessons learned, challenges, open and future directions of research in this field.",Internet of Things | Smart buildings | Smart cities,ACM Computing Surveys,2020-03-31,Article,"Djenouri, Djamel;Laidi, Roufaida;Djenouri, Youcef;Balasingham, Ilangko",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85063352955,10.1145/3300148,Quality evaluation of solution sets in multiobjective optimisation: A survey,"Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.",Evolutionary algorithms | Exact method | Heuristic | Indicator | Measure | Metaheuristic | Metric | Multi-criteria optimisation | Multobjective optimisation | Performance assessment | Quality evaluation,ACM Computing Surveys,2020-03-31,Article,"Li, Miqing;Yao, Xin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074863709,10.1145/3284748,Countermeasures against worm spreading: A new challenge for vehicular networks,"Vehicular ad hoc networks (VANETs) are essential components of the intelligent transport systems. They are attracting an increasing amount of interest in research and industrial sectors. Vehicular nodes are capable of transporting, sensing, processing information, and wireless communication, which makes them more vulnerable to worm infections than conventional hosts. This survey provides an overview on worm spreading over VANETs. We first briefly introduce the computer worms. Then the V2X communication and applications are discussed from malware and worms propagation perspective to show the indispensability of studying the characteristics of worm propagating on VANETs. The recent literature on worm spreading and containment on VANETs are categorized based on their research methods. The improvements and limitations of the existing studies are discussed. Next, the main factors influencing worm spreading in vehicular networks are discussed followed by a summary of countermeasure strategies designed to deal with these worms.",Information diffusion | LTE-vehicle | Malware | VANETs | Worm containment | Worms,ACM Computing Surveys,2020-03-31,Article,"Boukerche, Azzedine;Zhang, Qi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073893749,10.1145/3299769,Indoor positioning based on visible light communication: A performance-based survey of real-world prototypes,"The emergent context-aware applications in ubiquitous computing demands for obtaining accurate location information of humans or objects in real-time. Indoor location-based services can be delivered through implementing different types of technology, among which is a recent approach that utilizes LED lighting as a medium for Visible Light Communication (VLC). The ongoing development of solid-state lighting (SSL) is resulting in the wide increase of using LED lights and thereby building the ground for a ubiquitous wireless communication network from lighting systems. Considering the recent advances in implementing Visible Light Positioning (VLP) systems, this article presents a review of VLP systems and focuses on the performance evaluation of experimental achievements on location sensing through LED lights. We have outlined the performance evaluation of different prototypes by introducing new performance metrics, their underlying principles, and their notable findings. Furthermore, the study synthesizes the fundamental characteristics of VLC-based positioning systems that need to be considered, presents several technology gaps based on the current state-of-the-art for future research endeavors, and summarizes our lessons learned towards the standardization of the performance evaluation.",Indoor positioning system | Localization | Performance | Visible light communication | Visible light positioning,ACM Computing Surveys,2020-03-31,Review,"Afzalan, Milad;Jazizadeh, Farrokh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85065717075,10.1145/3309550,"A survey on modality characteristics, performance evaluation metrics, and security for traditional and wearable biometric systems","Biometric research is directed increasingly toward Wearable Biometric Systems (WBS) for user authentication and identification. However, prior to engaging in WBS research, how their operational dynamics and design considerations differ from those of Traditional Biometric Systems (TBS) must be understood. While the current literature is cognizant of those differences, there is no effective work that summarizes the factors where TBS and WBS differ, namely, their modality characteristics, performance, security, and privacy. To bridge the gap, this article accordingly reviews and compares the key characteristics of modalities, contrasts the metrics used to evaluate system performance, and highlights the divergence in critical vulnerabilities, attacks, and defenses for TBS and WBS. It further discusses how these factors affect the design considerations for WBS, the open challenges, and future directions of research in these areas. In doing so, the article provides a big-picture overview of the important avenues of challenges and potential solutions that researchers entering the field should be aware of. Hence, this survey aims to be a starting point for researchers in comprehending the fundamental differences between TBS and WBS before understanding the core challenges associated with WBS and its design.",Attacks | Biometrics | Metrics | Threats | Vulnerabilities | WBAN | Wearables,ACM Computing Surveys,2020-03-31,Review,"Sundararajan, Aditya;Sarwat, Arif I.;Pons, Alexander",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072400606,10.1145/3342101,Computational mining of social media to curb terrorism,"In the ever-connected social networking era, terrorists exploit social media platforms via sophisticated approaches. To curb these activities, a rich collection of computational methods was developed. This article surveys the use of social media by terrorists, followed by a temporal classification framework that overviews computational countermeasures at four major stages, including inception of an attack, immediately before an attack, onset of an attack, and after an attack. The literature surveyed was organized around the four temporal stages. The resulting survey is summarized in a table with the main technology used in each stage based on the time of the attack.",Computational analytics | Counterterrorism | Social media mining | Terrorism,ACM Computing Surveys,2020-09-30,Article,"Almoqbel, Mashael;Xu, Songhua",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072397978,10.1145/3344548,Extraction and analysis of fictional character networks: A survey,"A character network is a graph extracted from a narrative in which vertices represent characters and edges correspond to interactions between them. A number of narrative-related problems can be addressed automatically through the analysis of character networks, such as summarization, classification, or role detection. Character networks are particularly relevant when considering works of fiction (e.g., novels, plays, movies, TV series), as their exploitation allows developing information retrieval and recommendation systems. However, works of fiction possess specific properties that make these tasks harder. This survey aims at presenting and organizing the scientific literature related to the extraction of character networks from works of fiction, as well as their analysis. We first describe the extraction process in a generic way and explain how its constituting steps are implemented in practice, depending on the medium of the narrative, the goal of the network analysis, and other factors. We then review the descriptive tools used to characterize character networks, with a focus on the way they are interpreted in this context. We illustrate the relevance of character networks by also providing a review of applications derived from their analysis. Finally, we identify the limitations of the existing approaches and the most promising perspectives.",Character network | Graph analysis | Graph extraction | Information retrieval | Narrative | Work of fiction,ACM Computing Surveys,2020-09-30,Article,"Labatut, Vincent;Bost, Xavier",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072372563,10.1145/3329168,A survey on food computing,"Food is essential for human life and it is fundamental to the human experience. Food-related study may support multifarious applications and services, such as guiding human behavior, improving human health, and understanding the culinary culture. With the rapid development of social networks, mobile networks, and Internet of Things (IoT), people commonly upload, share, and record food images, recipes, cooking videos, and food diaries, leading to large-scale food data. Large-scale food data offers rich knowledge about food and can help tackle many central issues of human society. Therefore, it is time to group several disparate issues related to food computing. Food computing acquires and analyzes heterogenous food data from different sources for perception, recognition, retrieval, recommendation, and monitoring of food. In food computing, computational approaches are applied to address food-related issues in medicine, biology, gastronomy, and agronomy. Both large-scale food data and recent breakthroughs in computer science are transforming the way we analyze food data. Therefore, a series of works has been conducted in the food area, targeting different food-oriented tasks and applications. However, there are very few systematic reviews that shape this area well and provide a comprehensive and in-depth summary of current efforts or detail open problems in this area. In this article, we formalize food computing and present such a comprehensive overview of various emerging concepts, methods, and tasks. We summarize key challenges and future directions ahead for food computing. This is the first comprehensive survey that targets the study of computing technology for the food area and also offers a collection of research studies and technologies to benefit researchers and practitioners working in different food-related fields.",Food computing | Food perception | Food recognition | Food retrieval | Health | Monitoring | Recipe analysis | Recipe recommendation | Survey,ACM Computing Surveys,2020-09-30,Article,"Min, Weiqing;Jiang, Shuqiang;Liu, Linhu;Rui, Yong;Jain, Ramesh",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072380854,10.1145/3341145,Machine learning methods for reliable resource provisioning in edge-cloud computing: A survey,"Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use. This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article.",Autoscaling | Cloud computing | Consolidation | Distributed systems | Edge computing | Machine learning | Optimization | Placement | Reliability | Remediation,ACM Computing Surveys,2020-09-30,Article,"Le Duc, Thang;Leiva, Rafael García;Casari, Paolo;Östberg, Per Olov",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072386123,10.1145/3347712,Video skimming: Taxonomy and comprehensive survey,"Video skimming, also known as dynamic video summarization, generates a temporally abridged version of a given video. Skimming can be achieved by identifying significant components either in uni-modal or multi-modal features extracted from the video. Being dynamic in nature, video skimming, through temporal connectivity, allows better understanding of the video from its summary. Having this obvious advantage, recently, video skimming has drawn the focus of many researchers benefiting from the easy availability of the required computing resources. In this article, we provide a comprehensive survey on video skimming focusing on the substantial amount of literature from the past decade. We present a taxonomy of video skimming approaches and discuss their evolution highlighting key advances. We also provide a study on the components required for the evaluation of a video skimming performance.",Affective content | Attention model | Deep learning | Dynamic video summarization/video skimming | Machine learning | Semantic concept,ACM Computing Surveys,2020-09-30,Article,"Vivekraj, V. K.;Debashis, S. E.N.;Raman, Balasubramanian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074130170,10.1145/3345317,Academic plagiarism detection: A systematic literature review,"This article summarizes the research on computational methods to detect academic plagiarism by systematically reviewing 239 research papers published between 2013 and 2018. To structure the presentation of the research contributions, we propose novel technically oriented typologies for plagiarism prevention and detection efforts, the forms of academic plagiarism, and computational plagiarism detection methods. We show that academic plagiarism detection is a highly active research field. Over the period we review, the field has seen major advances regarding the automated detection of strongly obfuscated and thus hard-to-identify forms of academic plagiarism. These improvements mainly originate from better semantic text analysis methods, the investigation of non-textual content features, and the application of machine learning. We identify a research gap in the lack of methodologically thorough performance evaluations of plagiarism detection systems. Concluding from our analysis, we see the integration of heterogeneous analysis methods for textual and non-textual content features using machine learning as the most promising area for future research contributions to improve the detection of academic plagiarism further.",Literature review | Machine learning | Plagiarism detection | Semantic analysis | Text-matching software,ACM Computing Surveys,2020-11-30,Review,"Foltýnek, Tomáš;Meuschke, Norman;Gipp, Bela",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074128687,10.1145/3355398,Survey of compressed domain video summarization techniques,"Video summarization is the method of extracting key frames or clips from a video to generate a synopsis of the content of the video. Generally, video is compressed before storing or transmitting it in most of the practical applications. Traditional techniques require the videos to be decoded to summarize them, which is a tedious job. Instead, compressed domain video processing can be used for summarizing videos by partially decoding them. A classification and analysis of various summarization techniques are presented in this article with special focus on compressed domain techniques along with a discussion on machine-learning-based techniques that can be applied to summarize the videos.",Compressed domain | Machine learning | Video abstraction | Video processing,ACM Computing Surveys,2020-11-30,Review,"Basavarajaiah, Madhushree;Sharma, Priyanka",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072862533,10.1145/3357231,New opportunities for integrated formal methods,"Formal methods have provided approaches for investigating software engineering fundamentals and also have high potential to improve current practices in dependability assurance. In this article, we summarise known strengths and weaknesses of formal methods. From the perspective of the assurance of robots and autonomous systems (RAS), we highlight new opportunities for integrated formal methods and identify threats to the adoption of such methods. Based on these opportunities and threats, we develop an agenda for fundamental and empirical research on integrated formal methods and for successful transfer of validated research to RAS assurance. Furthermore, we outline our expectations on useful outcomes of such an agenda.",Autonomous systems | Challenges | Formal methods | Integration | Opportunities | Research agenda | Robots | Strengths | SWOT | Threats | Unification | Weaknesses,ACM Computing Surveys,2020-11-30,Article,"Gleirscher, Mario;Foster, Simon;Woodcock, Jim",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074131823,10.1145/3360498,A survey on interdependent privacy,"The privacy of individuals does not only depend on their own actions and data but may also be affected by the privacy decisions and by the data shared by other individuals. This interdependence is an essential aspect of privacy and ignoring it can lead to serious privacy violations. In this survey, we summarize and analyze research on interdependent privacy risks and on the associated (cooperative and non-cooperative) solutions. We also demonstrate that interdependent privacy has been studied in isolation in different research communities. By doing so, we systematize knowledge on interdependent privacy research and provide insights on how this research should be conducted and which challenges it should address.",Game theory | Interdependent privacy | Statistical learning | User studies,ACM Computing Surveys,2020-11-30,Review,"Humbert, Mathias;Trubert, Benjamin;Huguenin, Kévin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077741213,10.1145/3365756,"A systematic review on literature-based discovery: General overview, methodology, & statistical analysis","The vast nature of scientific publications brings out the importance of Literature-Based Discovery (LBD) research that is highly beneficial to accelerate knowledge acquisition and the research development process. LBD is a knowledge discovery workflow that automatically detects significant, implicit knowledge associations hidden in fragmented knowledge areas by analysing existing scientific literature. Therefore, the LBD output not only assists in formulating scientifically sensible, novel research hypotheses but also encourages the development of cross-disciplinary research. In this systematic review, we provide an in-depth analysis of the computational techniques used in the LBD process using a novel, up-to-date, and detailed classification. Moreover, we also summarise the key milestones of the discipline through a timeline of topics. To provide a general overview of the discipline, the review outlines LBD validation checks, major LBD tools, application areas, domains, and generalisability of LBD methodologies. We also outline the insights gathered through our statistical analysis that capture the trends in LBD literature. To conclude, we discuss the prevailing research deficiencies in the discipline by highlighting the challenges and opportunities of future LBD research.",Hypotheses generation | Knowledge discovery | LBD | Literature mining | Literature-based discovery | Systematic review | Text mining,ACM Computing Surveys,2020-11-30,Review,"Thilakaratne, Menasha;Falkner, Katrina;Atapattu, Thushari",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073805876,10.1007/s10664-019-09771-0,Why reinventing the wheels? An empirical study on library reuse and re-implementation,"Nowadays, with the rapid growth of open source software (OSS), library reuse becomes more and more popular since a large amount of third- party libraries are available to download and reuse. A deeper understanding on why developers reuse a library (i.e., replacing self-implemented code with an external library) or re-implement a library (i.e., replacing an imported external library with self-implemented code) could help researchers better understand the factors that developers are concerned with when reusing code. This understanding can then be used to improve existing libraries and API recommendation tools for researchers and practitioners by using the developers concerns identified in this study as design criteria. In this work, we investigated the reasons behind library reuse and re-implementation. To achieve this goal, we first crawled data from two popular sources, F-Droid and GitHub. Then, potential instances of library reuse and re-implementation were found automatically based on certain heuristics. Next, for each instance, we further manually identified whether it is valid or not. For library re-implementation, we obtained 82 instances which are distributed in 75 repositories. We then conducted two types of surveys (i.e., individual survey to corresponding developers of the validated instances and another open survey) for library reuse and re-implementation. For library reuse individual survey, we received 36 responses out of 139 contacted developers. For re-implementation individual survey, we received 13 responses out of 71 contacted developers. In addition, we received 56 responses from the open survey. Finally, we perform qualitative and quantitative analysis on the survey responses and commit logs of the validated instances. The results suggest that library reuse occurs mainly because developers were initially unaware of the library or the library had not been introduced. Re-implementation occurs mainly because the used library method is only a small part of the library, the library dependencies are too complicated, or the library method is deprecated. Finally, based on all findings obtained from analyzing the surveys and commit messages, we provided a few suggestions to improve the current library recommendation systems: tailored recommendation according to users’ preferences, detection of external code that is similar to a part of the users’ code (to avoid duplication or re-implementation), grouping similar recommendations for developers to compare and select the one they prefer, and disrecommendation of poor-quality libraries.",Code re-implementation | Code reuse | Library recommendation systems,Empirical Software Engineering,2020-01-01,Article,"Xu, Bowen;An, Le;Thung, Ferdian;Khomh, Foutse;Lo, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074344807,10.1007/s10664-019-09775-w,SIEVE: Helping developers sift wheat from chaff via cross-platform analysis,"Software developers have benefited from various sources of knowledge such as forums, question-and-answer sites, and social media platforms to help them in various tasks. Extracting software-related knowledge from different platforms involves many challenges. In this paper, we propose an approach to improve the effectiveness of knowledge extraction tasks by performing cross-platform analysis. Our approach is based on transfer representation learning and word embedding, leveraging information extracted from a source platform which contains rich domain-related content. The information extracted is then used to solve tasks in another platform (considered as target platform) with less domain-related content. We first build a word embedding model as a representation learned from the source platform, and use the model to improve the performance of knowledge extraction tasks in the target platform. We experiment with Software Engineering Stack Exchange and Stack Overflow as source platforms, and two different target platforms, i.e., Twitter and YouTube. Our experiments show that our approach improves performance of existing work for the tasks of identifying software-related tweets and helpful YouTube comments.",Software engineering | Transfer representation learning | Word embedding,Empirical Software Engineering,2020-01-01,Article,"Sulistya, Agus;Prana, Gede Artha Azriadi;Sharma, Abhishek;Lo, David;Treude, Christoph",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078984030,10.1007/s10664-020-09804-z,Meshing agile and plan-driven development in safety-critical software: a case study,"Organizations developing safety-critical software are increasingly seeking to create better practices by meshing agile and plan-driven development processes. Significant differences between the agile and the plan-driven processes make meshing difficult, and very little empirical evidence on using agile processes for safety-critical software development exists. There are four areas of concern, in particular, for meshing the development of safety-critical software concerning: documentation, requirements, life cycle and testing. We report on a case study of a pharmaceutical organization in which a Scrum process was implemented to support agile software development in a plan-driven safety-critical project. The purpose was to answer the following research question: For safety-critical software, what can a software team do to mesh agile and plan-driven processes effectively? The main contribution of the paper is an elaborated understanding of meshing in the four areas of concern and how the conditions for safety-critical software influence them. We discuss how meshing within the four areas of concern is a contribution to existing research.",Agile software development | FDA | Plan-driven software development | Process | Safety-critical software meshing compliance scrum,Empirical Software Engineering,2020-03-01,Article,"Heeager, Lise Tordrup;Nielsen, Peter Axel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079467477,10.1007/s10664-019-09781-y,How bugs are born: a model to identify how bugs are introduced in software components,"When identifying the origin of software bugs, many studies assume that “a bug was introduced by the lines of code that were modified to fix it”. However, this assumption does not always hold and at least in some cases, these modified lines are not responsible for introducing the bug. For example, when the bug was caused by a change in an external API. The lack of empirical evidence makes it impossible to assess how important these cases are and therefore, to which extent the assumption is valid. To advance in this direction, and better understand how bugs “are born”, we propose a model for defining criteria to identify the first snapshot of an evolving software system that exhibits a bug. This model, based on the perfect test idea, decides whether a bug is observed after a change to the software. Furthermore, we studied the model’s criteria by carefully analyzing how 116 bugs were introduced in two different open source software projects. The manual analysis helped classify the root cause of those bugs and created manually curated datasets with bug-introducing changes and with bugs that were not introduced by any change in the source code. Finally, we used these datasets to evaluate the performance of four existing SZZ-based algorithms for detecting bug-introducing changes. We found that SZZ-based algorithms are not very accurate, especially when multiple commits are found; the F-Score varies from 0.44 to 0.77, while the percentage of true positives does not exceed 63%. Our results show empirical evidence that the prevalent assumption, “a bug was introduced by the lines of code that were modified to fix it”, is just one case of how bugs are introduced in a software system. Finding what introduced a bug is not trivial: bugs can be introduced by the developers and be in the code, or be created irrespective of the code. Thus, further research towards a better understanding of the origin of bugs in software projects could help to improve design integration tests and to design other procedures to make software development more robust.",Bug origins | Bug-introducing changes | Extrinsic bugs | First-failing change | Intrinsic bugs | SZZ algorithm,Empirical Software Engineering,2020-03-01,Article,"Rodríguez-Pérez, Gema;Robles, Gregorio;Serebrenik, Alexander;Zaidman, Andy;Germán, Daniel M.;Gonzalez-Barahona, Jesus M.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079647298,10.1007/s10664-020-09812-z,"Guest editorial: special section on software analysis, evolution, and reengineering: Selected papers from the SANER 2018 conference",,,Empirical Software Engineering,2020-03-01,Editorial,"Di Penta, Massimiliano;Shepherd, David C.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85080090966,10.1007/s10664-019-09793-8,A systemic framework for crowdsourced test report quality assessment,"In crowdsourced mobile application testing, crowd workers perform test tasks for developers and submit test reports to report the observed abnormal behaviors. These test reports usually provide important information to improve the quality of software. However, due to the poor expertise of workers and the inconvenience of editing on mobile devices, some test reports usually lack necessary information for understanding and reproducing the revealed bugs. Sometimes developers have to spend a significant part of available resources to handle the low-quality test reports, thus severely reducing the inspection efficiency. In this paper, to help developers determine whether a test report should be selected for inspection within limited resources, we issue a new problem of test report quality assessment. Aiming to model the quality of test reports, we propose a new framework named TERQAF. First, we systematically summarize some desirable properties to characterize expected test reports and define a set of measurable indicators to quantify these properties. Then, we determine the numerical values of indicators according to the contained contents of test reports. Finally, we train a classifier by using logistic regression to predict the quality of test reports. To validate the effectiveness of TERQAF, we conduct extensive experiments over five crowdsourced test report datasets. Experimental results show that TERQAF can achieve 85.18% in terms of Macro-average Precision (MacroP), 75.87% in terms of Macro-average Recall (MacroR), and 80.01% in terms of Macro-average F-measure (MacroF) on average in test report quality assessment. Meanwhile, the empirical results also demonstrate that test report quality assessment can help developers handle test reports more efficiently.",Crowdsourced testing | Desirable properties | Natural language processing | Quality indicators | Test report quality,Empirical Software Engineering,2020-03-01,Article,"Chen, Xin;Jiang, He;Li, Xiaochen;Nie, Liming;Yu, Dongjin;He, Tieke;Chen, Zhenyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85075914487,10.1007/s10664-019-09756-z,You broke my code: understanding the motivations for breaking changes in APIs,"As most software systems, libraries and frameworks also evolve, which may break existing clients. However, the main reasons to introduce breaking changes in APIs are unclear. Therefore, in this paper, we first report the results of an almost 4-month long field study with popular Java libraries and frameworks. We configured an infrastructure to observe all changes in these libraries and to detect breaking changes shortly after their introduction in the code. We detected possible breaking changes in 61 projects. After identifying breaking changes, we asked the developers to explain the reasons behind their decision to change the APIs. By analyzing the developers’ answers, we report that breaking changes are mostly motivated by the need to implement new features, by the desire to make the APIs simpler and with fewer elements, and to improve maintainability. To complement this first study, we describe a second study, including the analysis of 110 Stack Overflow posts related to breaking changes. We reveal that breaking changes have an important impact on clients, since 45% of the questions are from clients asking how to overcome specific breaking changes; they are also common in other ecosystems—JavaScript,.NET, etc. We conclude by providing suggestions to language designers, tool builders, software engineering researchers, and API developers.",API evolution | Breaking changes | Firehouse interviews | Stack Overflow,Empirical Software Engineering,2020-03-01,Article,"Brito, Aline;Valente, Marco Tulio;Xavier, Laerte;Hora, Andre",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85069501006,10.1007/s10664-019-09743-4,CAPS: a supervised technique for classifying Stack Overflow posts concerning API issues,"The design and maintenance of APIs (Application Programming Interfaces) are complex tasks due to the constantly changing requirements of their users. Despite the efforts of their designers, APIs may suffer from a number of issues (such as incomplete or erroneous documentation, poor performance, and backward incompatibility). To maintain a healthy client base, API designers must learn these issues to fix them. Question answering sites, such as Stack Overflow (SO), have become a popular place for discussing API issues. These posts about API issues are invaluable to API designers, not only because they can help to learn more about the problem but also because they can facilitate learning the requirements of API users. However, the unstructured nature of posts and the abundance of non-issue posts make the task of detecting SO posts concerning API issues difficult and challenging. In this paper, we first develop a supervised learning approach using a Conditional Random Field (CRF), a statistical modeling method, to identify API issue-related sentences. We use the above information together with different features collected from posts, the experience of users, readability metrics and centrality measures of collaboration network to build a technique, called CAPS, that can classify SO posts concerning API issues. In total, we consider 34 features along eight different dimensions. Evaluation of CAPS using carefully curated SO posts on three popular API types reveals that the technique outperforms all three baseline approaches we consider in this study. We then conduct studies to find important features and also evaluate the performance of the CRF-based technique for classifying issue sentences. Comparison with two other baseline approaches shows that the technique has high potential. We also test the generalizability of CAPS results, evaluate the effectiveness of different classifiers, and identify the impact of different feature sets.",API issue | Feature extraction | Stack Overflow | Text classification | Unstructured data mining,Empirical Software Engineering,2020-03-01,Article,"Ahasanuzzaman, Md;Asaduzzaman, Muhammad;Roy, Chanchal K.;Schneider, Kevin A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081884178,10.1007/s10664-019-09796-5,An exploratory study of smart contracts in the Ethereum blockchain platform,"Ethereum is a blockchain platform that supports smart contracts. Smart contracts are pieces of code that perform general-purpose computations. For instance, smart contracts have been used to implement crowdfunding initiatives that raised a total of US$6.2 billion from January to June of 2018. In this paper, we conduct an exploratory study of smart contracts. Differently from prior studies that focused on particular aspects of a subset of smart contracts, our goal is to have a broader understanding of all contracts that are currently deployed in Ethereum. In particular, we elucidate how frequently used the contracts are (activity level), what they do (category), and how complex they are (source code complexity). To conduct this study, we mined and cross-linked data from four sources: Ethereum dataset on the Google BigQuery platform, Etherscan, State of the DApps, and CoinMarketCap. Our study period runs from July 2015 (inception of Ethereum) until September 2018. With regards to activity level, we notice that it is concentrated on a very small subset of the contracts. More specifically, only 0.05% of the smart contracts are the target of 80% of the transactions that are sent to contracts. New solutions to cope with Ethereum’s limited scalability should take such an activity imbalance into consideration. With regards to categories, we highlight that the new and widely advertised rich programming model of smart contracts is currently being used to develop very simple applications that tend to be token-centric (e.g., ICOs, Crowdsales, etc). Finally, with regards to code complexity, we observe that the source code of high-activity verified contracts is small, with at most 211 instructions in 80% of the cases. These contracts also commonly include at least two subcontracts and libraries in their source code. The comment ratio of these contracts is also significantly higher than that of GitHub top-starred projects written in Java, C++, and C#. Hence, the source code of high-activity verified smart contracts exhibit particular complexity characteristics compared to other popular programming languages. Further studies are necessary to uncover the actual reasons behind such differences. Finally, based on our findings, we propose an open research agenda to drive and foster future studies in the area.",Blockchain | Ethereum | Smart contracts,Empirical Software Engineering,2020-05-01,Article,"Oliva, Gustavo A.;Hassan, Ahmed E.;Jiang, Zhen Ming (Jack)",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081611315,10.1007/s10664-020-09802-1,Boosting crash-inducing change localization with rank-performance-based feature subset selection,"Given a bucket of crash reports, it would be helpful for developers to find and fix the corresponding defects if the crash-inducing software changes can be automatically located. Recently, an approach called ChangeLocator was proposed, which used ten change-level features to train a supervised model based on the data from the historical fixed crashes. It was reported that ChangeLocator achieved a good performance in terms of Recall@1, MAP, and MRR, when all the ten features were combined together. However, in ChangeLocator, the redundancy between features are neglected, which may degrade the localization effectiveness. In this paper, we propose an improved approach ChangeRanker with a rank-performance-based feature selection technology (Rfs) to boost the effectiveness of crash-inducing change localization. Our experimental results on NetBeans show that ChangeRanker can achieve an improvement of 35.9%, 17.4%, and 15.3% over ChangeLocator in terms of Recall@1, MRR, and MAP, respectively. Furthermore, compared with three popular feature selection approaches, Rfs is able to select more informative features to boost localization effectiveness. In order to assess the real generalization capability of the proposed extension, we adapt ChangeRanker and ChangeLocator to locate bug-inducing changes on three additional data sets. Again, we observe that, on average, ChangeRanker achieves an improvement of 115.3%, 37.6%, and 41.2% in terms of Recall@1, MRR, and MAP, respectively. This indicates that our proposed rank-performance-based feature selection method has a good generalization capability. In summary, our work provides an easy-to-use approach to boosting the performance of the state-of-the-art crash-inducing change localization approach.",Bug localization | Crash stack | Crash-inducing change | Feature subset selection | Software crash,Empirical Software Engineering,2020-05-01,Article,"Guo, Zhaoqiang;Li, Yanhui;Ma, Wanwangying;Zhou, Yuming;Lu, Hongmin;Chen, Lin;Xu, Baowen",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85082068815,10.1007/s10664-020-09817-8,Guest editorial: Mining software repositories 2018,,,Empirical Software Engineering,2020-05-01,Editorial,"Kamei, Yasutaka;Zaidman, Andy",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85077704500,10.1007/s10664-019-09764-z,CDA: Characterising Deprecated Android APIs,"Because of functionality evolution, or security and performance-related changes, some APIs eventually become unnecessary in a software system and thus need to be cleaned to ensure proper maintainability. Those APIs are typically marked first as deprecated APIs and, as recommended, follow through a deprecated-replace-remove cycle, giving an opportunity to client application developers to smoothly adapt their code in next updates. Such a mechanism is adopted in the Android framework development where thousands of reusable APIs are made available to Android app developers. In this work, we present a research-based prototype tool called CDA and apply it to different revisions (i.e., releases or tags) of the Android framework code for characterising deprecated APIs. Based on the data mined by CDA, we then perform an empirical study on API deprecation in the Android ecosystem and the associated challenges for maintaining quality apps. In particular, we investigate the prevalence of deprecated APIs, their annotations and documentation, their removal and consequences, their replacement messages, developer reactions to API deprecation, as well as the evolution of the usage of deprecated APIs. Experimental results reveal several findings that further provide promising insights related to deprecated Android APIs. Notably, by mining the source code of the Android framework base, we have identified three bugs related to deprecated APIs. These bugs have been quickly assigned and positively appreciated by the framework maintainers, who claim that these issues will be updated in future releases.",Android | CDA | Deprecated APIs,Empirical Software Engineering,2020-05-01,Article,"Li, Li;Gao, Jun;Bissyandé, Tegawendé F.;Ma, Lei;Xia, Xin;Klein, Jacques",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85083303197,10.1007/s10664-020-09806-x,Preface to the special issue on program comprehension,,,Empirical Software Engineering,2020-05-01,Editorial,"Siegmund, Janet;Roy, Chanchal K.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85067895545,10.1007/s10664-019-09730-9,Deep code comment generation with hybrid lexical and syntactical information,"During software maintenance, developers spend a lot of time understanding the source code. Existing studies show that code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named Hybrid-DeepCom to automatically generate code comments for the functional units of Java language, namely, Java methods. The generated comments aim to help developers understand the functionality of Java methods. Hybrid-DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. It formulates the comment generation task as the machine translation problem. Hybrid-DeepCom exploits a deep neural network that combines the lexical and structure information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects on GitHub. We evaluate the experimental results on both machine translation metrics and information retrieval metrics. Experimental results demonstrate that our method Hybrid-DeepCom outperforms the state-of-the-art by a substantial margin. In addition, we evaluate the influence of out-of-vocabulary tokens on comment generation. The results show that reducing the out-of-vocabulary tokens improves the accuracy effectively.",Comment generation | Deep learning | Program comprehension,Empirical Software Engineering,2020-05-01,Article,"Hu, Xing;Li, Ge;Xia, Xin;Lo, David;Jin, Zhi",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85074539137,10.1007/s10664-019-09765-y,Every build you break: developer-oriented assistance for build failure resolution,"Continuous integration is an agile software development practice. Instead of integrating features right before a release, they are constantly being integrated into an automated build process. This shortens the release cycle, improves software quality, and reduces time to market. However, the whole process will come to a halt when a commit breaks the build, which can happen for several reasons, e.g., compilation errors or test failures, and fixing the build suddenly becomes a top priority. Developers not only have to find the cause of the build break and fix it, but they have to be quick in all of it to avoid a delay for others. Unfortunately, these steps require deep knowledge and are often time-consuming. To support developers in fixing a build break, we propose Bart, a tool that summarizes the reasons for Maven build failures and suggests possible solutions found on the internet. We will show in a case study with 17 participants that developers find Bart useful to understand build breaks and that using Bart substantially reduces the time to fix a build break, on average by 37%. We have also conducted a qualitative study to better understand the workflows and information needs when fixing builds. We found that typical workflows differ substantially between various error categories, and that several uncommon build errors are both very hard to investigate and to fix. These findings will be useful to inform future research in this area.",Agile software development | Build break | Error recovery | Software development tools | Software engineering | Summarization,Empirical Software Engineering,2020-05-01,Article,"Vassallo, Carmine;Proksch, Sebastian;Zemp, Timothy;Gall, Harald C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85072032337,10.1007/s10664-019-09758-x,What kind of questions do developers ask on Stack Overflow? A comparison of automated approaches to classify posts into question categories,"On question and answer sites, such as Stack Overflow (SO), developers use tags to label the content of a post and to support developers in question searching and browsing. However, these tags mainly refer to technological aspects instead of the purpose of the question. Tagging questions with their purpose can add a new dimension to the identification of discussed topics in posts on SO. In this paper, we aim at automating the classification of SO question posts into seven question categories. As a first step, we harmonized existing taxonomies of question categories and then, we manually classified 1,000 SO questions according to our new taxonomy. Additionally to the question category, we marked the phrases that indicate a question category for each of the posts. We then use this data set to automate the classification of posts using two approaches. For the first approach, we manually analyzed the phrases to find patterns. Based on regular expressions, we implemented a classifier, for each of the categories, that determines whether a post belongs to a category. These regular expressions are derived by analyzing patterns in the phrases. In the second approach, we use the curated data set to train classification models of supervised machine learning algorithms (Random Forest and Support Vector Machines). For the machine learning algorithms, we experimented with 1,312 different configurations regarding the preprocessing of the text and the representation of the input data. Then, we compared the performance of the regex approach with the performance of the best configuration that uses machine learning algorithms on a validation set of 110 posts. The results show that using the regular expression approach, we can classify posts into the correct question category with an average precision and recall of 0.90, and an MCC of 0.68. Additionally, we applied the regex approach on all questions of SO that deal with Android app development and investigated the co-occurrence of question categories in posts. We found that the categories API usage, Conceptual, and Discrepancy are the most frequently assigned question categories and that they also occur together frequently. Our approach can be used to support developers in browsing SO discussions or researchers in building recommender systems based on SO.",Android | Machine learning | Question categories | Stack Overflow,Empirical Software Engineering,2020-05-01,Article,"Beyer, Stefanie;Macho, Christian;Di Penta, Massimiliano;Pinzger, Martin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073928721,10.1007/s10664-019-09757-y,Guiding log revisions by learning from software evolution history,"Despite the importance of log statements in postmortem debugging, developers are difficult to establish good logging practices. There are mainly two reasons. First, there are no rigorous specifications or systematic processes to instruct logging practices. Second, logging code evolves with bug fixes or feature updates. Without considering the impact of software evolution, previous works on log enhancement can partially release the first problem but are hard to solve the latter. To fill this gap, this paper proposes to guide log revisions by learning from evolution history. Motivated by code clones, we assume that logging code with similar context is pervasive and deserves similar modifications and conduct an empirical study on 12 open-source projects to validate our assumption. Upon this, we design and implement LogTracker, an automatic tool that learns log revision rules by mining the correlation between logging context and modifications and recommends candidate log revisions by applying these rules. With an enhanced modeling of logging context, LogTracker can instruct more intricate log revisions that cannot be covered by existing tools. Our experiments show that LogTracker can detect 369 instances of candidates when applied to the latest versions of software. So far, we have reported 79 of them, and 52 have been accepted.",Empirical study | Failure diagnose | Log revision | Software evolution,Empirical Software Engineering,2020-05-01,Article,"Li, Shanshan;Niu, Xu;Jia, Zhouyang;Liao, Xiangke;Wang, Ji;Li, Tao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85084081553,10.1007/s10664-020-09818-7,How software engineering research aligns with design science: a review,"Background: Assessing and communicating software engineering research can be challenging. Design science is recognized as an appropriate research paradigm for applied research, but is rarely explicitly used as a way to present planned or achieved research contributions in software engineering. Applying the design science lens to software engineering research may improve the assessment and communication of research contributions. Aim: The aim of this study is 1) to understand whether the design science lens helps summarize and assess software engineering research contributions, and 2) to characterize different types of design science contributions in the software engineering literature. Method: In previous research, we developed a visual abstract template, summarizing the core constructs of the design science paradigm. In this study, we use this template in a review of a set of 38 award winning software engineering publications to extract, analyze and characterize their design science contributions. Results: We identified five clusters of papers, classifying them according to their different types of design science contributions. Conclusions: The design science lens helps emphasize the theoretical contribution of research output—in terms of technological rules—and reflect on the practical relevance, novelty and rigor of the rules proposed by the research.",Design science | Empirical software engineering | Research review,Empirical Software Engineering,2020-07-01,Article,"Engström, Emelie;Storey, Margaret Anne;Runeson, Per;Höst, Martin;Baldassarre, Maria Teresa",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85085089656,10.1007/s10664-020-09815-w,What am I testing and where? Comparing testing procedures based on lightweight requirements annotations,"Context: The testing of software-intensive systems is performed in different test stages each having a large number of test cases. These test cases are commonly derived from requirements. Each test stages exhibits specific demands and constraints with respect to their degree of detail and what can be tested. Therefore, specific test suites are defined for each test stage. In this paper, the focus is on the domain of embedded systems, where, among others, typical test stages are Software- and Hardware-in-the-loop. Objective: Monitoring and controlling which requirements are verified in which detail and in which test stage is a challenge for engineers. However, this information is necessary to assure a certain test coverage, to minimize redundant testing procedures, and to avoid inconsistencies between test stages. In addition, engineers are reluctant to state their requirements in terms of structured languages or models that would facilitate the relation of requirements to test executions. Method: With our approach, we close the gap between requirements specifications and test executions. Previously, we have proposed a lightweight markup language for requirements which provides a set of annotations that can be applied to natural language requirements. The annotations are mapped to events and signals in test executions. As a result, meaningful insights from a set of test executions can be directly related to artifacts in the requirements specification. In this paper, we use the markup language to compare different test stages with one another. Results: We annotate 443 natural language requirements of a driver assistance system with the means of our lightweight markup language. The annotations are then linked to 1300 test executions from a simulation environment and 53 test executions from test drives with human drivers. Based on the annotations, we are able to analyze how similar the test stages are and how well test stages and test cases are aligned with the requirements. Further, we highlight the general applicability of our approach through this extensive experimental evaluation. Conclusion: With our approach, the results of several test levels are linked to the requirements and enable the evaluation of complex test executions. By this means, practitioners can easily evaluate how well a systems performs with regards to its specification and, additionally, can reason about the expressiveness of the applied test stage.",Markup language | Requirements modeling | Simulation | Test stage comparison | Test stage evaluation,Empirical Software Engineering,2020-07-01,Article,"Pudlitz, Florian;Brokhausen, Florian;Vogelsang, Andreas",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086159251,10.1007/s10664-020-09827-6,An empirical investigation of performance overhead in cross-platform mobile development frameworks,"The heterogeneity of the leading mobile platforms in terms of user interfaces, user experience, programming language, and ecosystem have made cross-platform development frameworks popular. These aid the creation of mobile applications – apps – that can be executed across the target platforms (typically Android and iOS) with minimal to no platform-specific code. Due to the cost- and time-saving possibilities introduced through adopting such a framework, researchers and practitioners alike have taken an interest in the underlying technologies. Examining the body of knowledge, we, nonetheless, frequently encounter discussions on the drawbacks of these frameworks, especially with regard to the performance of the apps they generate. Motivated by the ongoing discourse and a lack of empirical evidence, we scrutinised the essential piece of the cross-platform frameworks: the bridge enabling cross-platform code to communicate with the underlying operating system and device hardware APIs. The study we present in the article benchmarks and measures the performance of this bridge to reveal its associated overhead in Android apps. The development of the artifacts for this experiment was conducted using five cross-platform development frameworks to generate Android apps, in addition to a baseline native Android app implementation. Our results indicate that – for Android apps – the use of cross-platform frameworks for the development of mobile apps may lead to decreased performance compared to the native development approach. Nevertheless, certain cross-platform frameworks can perform equally well or even better than native on certain metrics which highlights the importance of well-defined technical requirements and specifications for deliberate selection of a cross-platform framework or overall development approach.",Cross-platform development | Development approaches | Mobile app | Performance benchmark,Empirical Software Engineering,2020-07-01,Article,"Biørn-Hansen, Andreas;Rieger, Christoph;Grønli, Tor Morten;Majchrzak, Tim A.;Ghinea, Gheorghita",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089744564,10.1007/s10664-020-09840-9,An empirical study of the characteristics of popular Minecraft mods,"It is becoming increasingly difficult for game developers to manage the cost of developing a game, while meeting the high expectations of gamers. One way to balance the increasing gamer expectation and development stress is to build an active modding community around the game. There exist several examples of games with an extremely active and successful modding community, with the Minecraft game being one of the most notable ones. This paper reports on an empirical study of 1,114 popular and 1,114 unpopular Minecraft mods from the CurseForge mod distribution platform, one of the largest distribution platforms for Minecraft mods. We analyzed the relationship between 33 features across 5 dimensions of mod characteristics and the popularity of mods (i.e., mod category, mod documentation, environmental context of the mod, remuneration for the mod, and community contribution for the mod), to understand the characteristics of popular Minecraft mods. We firstly verify that the studied dimensions have significant explanatory power in distinguishing the popularity of the studied mods. Then we evaluated the contribution of each of the 33 features across the 5 dimensions. We observed that popular mods tend to have a high quality description and promote community contribution.",CurseForge | Minecraft | Mod development | Mods,Empirical Software Engineering,2020-09-01,Article,"Lee, Daniel;Rajbahadur, Gopi Krishnan;Lin, Dayi;Sayagh, Mohammed;Bezemer, Cor Paul;Hassan, Ahmed E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088872515,10.1007/s10664-020-09854-3,Wait for it: identifying “On-Hold” self-admitted technical debt,"Self-admitted technical debt refers to situations where a software developer knows that their current implementation is not optimal and indicates this using a source code comment. In this work, we hypothesize that it is possible to develop automated techniques to understand a subset of these comments in more detail, and to propose tool support that can help developers manage self-admitted technical debt more effectively. Based on a qualitative study of 333 comments indicating self-admitted technical debt, we first identify one particular class of debt amenable to automated management: on-hold self-admitted technical debt (on-hold SATD), i.e., debt which contains a condition to indicate that a developer is waiting for a certain event or an updated functionality having been implemented elsewhere. We then design and evaluate an automated classifier which can identify these on-hold instances with an area under the receiver operating characteristic curve (AUC) of 0.98 as well as detect the specific conditions that developers are waiting for. Our work presents a first step towards automated tool support that is able to indicate when certain instances of self-admitted technical debt are ready to be addressed.",Classification | Qualitative study | Self-admitted technical debt,Empirical Software Engineering,2020-09-01,Article,"Maipradit, Rungroj;Treude, Christoph;Hata, Hideaki;Matsumoto, Kenichi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089859907,10.1007/s10664-020-09857-0,Information correspondence between types of documentation for APIs,"Documentation for programming languages and their APIs takes many forms, such as reference documentation, blog posts or other textual and visual media. Prior research has suggested that developers switch between reference and tutorial-like documentation while learning a new API. Documentation creation and maintenance is also an effort-intensive process that requires its creators to carefully inspect and organize information, while ensuring consistency across different sources. This article reports on the relationship between information in tutorials and in API reference documentation of three libraries on the topics: regular expressions, unified resource location and Input/Output in the two programming languages, Java and Python. Our investigation reveals that about half of the sentences in the tutorials studied describe API Information, i.e. syntax, behaviour, usage and performance of the API, that could be found in the reference documentation. The remaining are tutorial specific use-cases and examples. We also elicited and analyzed six types of correspondences between sentences in tutorials and reference documentation, ranging from identical to implied. Based on our findings, we propose a general information reuse pattern as a structured abstraction to represent the systematic integration of information from the reference documentation into a tutorial. We report on the distribution of 38 instances of this pattern, and on the impact of applying the pattern automatically on the existing tutorials. This work lays a foundation for understanding the nature of information correspondence across different documentation types to inform and assist documentation generation and maintenance.",Application programming interface | Exploratory study | Qualitative analysis | Software documentation,Empirical Software Engineering,2020-09-01,Article,"Arya, Deeksha M.;Guo, Jin L.C.;Robillard, Martin P.",Include,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85090856184,10.1007/s10664-020-09870-3,An Empirical Investigation of Relevant Changes and Automation Needs in Modern Code Review,"Recent research has shown that available tools for Modern Code Review (MCR) are still far from meeting the current expectations of developers. The objective of this paper is to investigate the approaches and tools that, from a developer’s point of view, are still needed to facilitate MCR activities. To that end, we first empirically elicited a taxonomy of recurrent review change types that characterize MCR. The taxonomy was designed by performing three steps: (i) we generated an initial version of the taxonomy by qualitatively and quantitatively analyzing 211 review changes/commits and 648 review comments of ten open-source projects; then (ii) we integrated into this initial taxonomy, topics, and MCR change types of an existing taxonomy available from the literature; finally, (iii) we surveyed 52 developers to integrate eventually missing change types in the taxonomy. Results of our study highlight that the availability of new emerging development technologies (e.g., Cloud-based technologies) and practices (e.g., Continuous delivery) has pushed developers to perform additional activities during MCR and that additional types of feedback are expected by reviewers. Our participants provided recommendations, specified techniques to employ, and highlighted the data to analyze for building recommender systems able to automate the code review activities composing our taxonomy. We surveyed 14 additional participants (12 developers and 2 researchers), not involved in the previous survey, to qualitatively assess the relevance and completeness of the identified MCR change types as well as assess how critical and feasible to implement are some of the identified techniques to support MCR activities. Thus, with a study involving 21 additional developers, we qualitatively assess the feasibility and usefulness of leveraging natural language feedback (automation considered critical/feasible to implement) in supporting developers during MCR activities. In summary, this study sheds some more light on the approaches and tools that are still needed to facilitate MCR activities, confirming the feasibility and usefulness of using summarization techniques during MCR activities. We believe that the results of our work represent an essential step for meeting the expectations of developers and supporting the vision of full or partial automation in MCR.",Automated software engineering | Code review process and practices | Empirical study,Empirical Software Engineering,2020-11-01,Article,"Panichella, Sebastiano;Zaugg, Nik",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85092255217,10.1007/s10664-020-09877-w,API compatibility issues in Android: Causes and effectiveness of data-driven detection techniques,"Android fragmentation is a well-known issue referring to the adoption of different versions in the multitude of devices supporting such an operating system. Each Android version features a set of APIs provided to developers. These APIs are subject to changes and may cause compatibility issues. To support app developers, approaches have been proposed to automatically identify API compatibility issues. CiD, the state-of-the-art approach, is a data-driven solution learning how to detect those issues by analyzing the change history of Android APIs (“API side” learning). In this paper (extension of our MSR 2019 paper), we present an alternative data-driven approach, named ACRyL. ACRyL learns from changes implemented in apps in response to API changes (“client side” learning). When comparing these two solutions on 668 apps, for a total of 11,863 snapshots, we found that there is no clear winner, since the two techniques are highly complementary, and none of them provides a comprehensive support in detecting API compatibility issues: ACRyL achieves a precision of 7.0% (28.0%, when considering only the severe warnings), while CiD achieves a precision of 18.4%. This calls for more research in this field, and led us to run a second empirical study in which we manually analyze 500 pull-requests likely related to the fixing of compatibility issues, documenting the root cause behind the fixed issue. The most common causes are related to changes in the Android APIs (∼ 87%), while about 13% of the issues are related to external causes, such as build and distribution, dependencies, and the app itself. The provided empirical knowledge can inform the building of better tools for the detection of API compatibility issues.",Android | API compatibility issues | Empirical study | Taxonomy,Empirical Software Engineering,2020-11-01,Article,"Scalabrino, Simone;Bavota, Gabriele;Linares-Vásquez, Mario;Piantadosi, Valentina;Lanza, Michele;Oliveto, Rocco",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073942145,10.1016/j.jss.2019.07.088,StaDART: Addressing the problem of dynamic code updates in the security analysis of android applications,"Dynamic code update techniques (Android Studio – support for dynamic delivery), such as dynamic class loading and reflection, enable Android apps to extend their functionality at runtime. At the same time, these techniques are misused by malware developers to transform a seemingly benign app into a malware, once installed on a real device. Among the corpus of evasive techniques used in modern real-world malware, evasive usage of dynamic code updates plays a key role. First, we demonstrate the ineffectiveness of existing tools to analyze apps in the presence of dynamic code updates using our test apps, i.e., Reflection-Bench and InboxArchiver. Second, we present StaDART, combining static and dynamic analysis of Android apps to reveal the concealed behavior of malware. StaDART performs dynamic code interposition using a vtable tampering technique for API hooking to avoid modifications to the Android framework. Furthermore, we integrate it with a triggering solution, DroidBot, to make it more scalable and fully automated. We present our evaluation results with a dataset of 2000 real world apps; containing 1000 legitimate apps and 1000 malware samples. The evaluation results with this dataset and Reflection-Bench show that StaDART reveals suspicious behavior that is otherwise hidden to static analysis tools.",Android | Dynamic class loading | Dynamic code updates | Reflection | Security analysis,Journal of Systems and Software,2020-01-01,Article,"Ahmad, Maqsood;Costamagna, Valerio;Crispo, Bruno;Bergadano, Francesco;Zhauniarovich, Yury",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073994062,10.1016/j.jss.2019.110434,Affect recognition in code review: An in-situ biometric study of reviewer's affect,"Code review in software development is an important practice that increases team productivity and improves product quality. Code review is also an example of remote, computer-mediated asynchronous communication prone to the loss of affective information. Since positive affect has been linked to productivity in software development, prior research has focused on sentiment analysis in source codes. Although the methods of sentiment analysis have advanced, there are remaining challenges due to numerous domain oriented expressions, subtle nuances, and indications of sentiment. Here we explore the potentials of (1) nonverbal behavioral signals such as conventional typing, and (2) indirect physiology (eye gaze, GSR, touch pressure) that reflect genuine affective states in the in-situ code review in a large software company. Nonverbal behavioral signals of 33 professional software developers were unobtrusively recorded while they worked on their daily code review. Using Linear Mixed Effect Models, we observed that affect presented in the written comments was associated with prolonged typing duration. Using physiological features, a trained Random Forest classifier could predict post-task valence with 90.0% accuracy (F1-score = 0.937) and arousal with 83.9% accuracy (F1-score = 0.856). The results presents potentials for intelligent affect-aware interfaces for code review in-situ.",Affective computing | Code review | CSCW | Physiological signals,Journal of Systems and Software,2020-01-01,Article,"Vrzakova, Hana;Begel, Andrew;Mehtätalo, Lauri;Bednarik, Roman",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074537486,10.1016/j.jss.2019.110450,A survey on the use of access permission-based specifications for program verification,"Verifying the correctness and reliability of imperative and object-oriented programs is one of the grand challenges in computer science. In imperative programming models, programmers introduce concurrency manually by using explicit concurrency constructs such as multi-threading. Multi-threaded programs are prone to synchronization problems such as data races and dead-locks, and verifying API protocols in object-oriented programs is a non-trivial task due to improper and unexpected state transition at run time. This is in part due to the unexpected sharing of program states in such programs. With these considerations in mind, access permissions have been investigated as a means to reasoning about the correctness of such programs. Access permissions are abstract capabilities that characterize the way a shared resource can be accessed by multiple references. This paper provides a comprehensive survey of existing access permission-based verification approaches. We describe different categories of permissions and permission-based contracts. We elaborate how permission-based specifications have been used to ensure compliance of API protocols and to avoid synchronization problems in concurrent programs. We compare existing approaches based on permission usage, analysis performed, language and/or tool supported, and properties being verified. Finally, we provide insight into the research challenges posed by existing approaches and suggest future directions.",Access permissions | Concurrency | Permission inference | Program verification | Protocol verification | Survey,Journal of Systems and Software,2020-01-01,Article,"Sadiq, Ayesha;Li, Yuan Fang;Ling, Sea",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85074504797,10.1016/j.jss.2019.110454,Finding help with programming errors: An exploratory study of novice software engineers’ focus in stack overflow posts,"Monthly, 50 million users visit Stack Overflow, a popular Q&A forum used by software developers, to share and gather knowledge and help with coding problems. Although Q&A forums serve as a good resource for seeking help from developers beyond the local team, the abundance of information can cause developers, especially novice software engineers, to spend considerable time in identifying relevant answers and suitable suggested fixes. This exploratory study aims to understand how novice software engineers direct their efforts and what kinds of information they focus on within a post selected from the results returned in response to a search query on Stack Overflow. The results can be leveraged to improve the Q&A forum interface, guide tools for mining forums, and potentially improve granularity of traceability mappings involving forum posts. We qualitatively analyze the novice software engineers’ perceptions from a survey as well as their annotations of a set of Stack Overflow posts. Our results indicate that novice software engineers pay attention to only 27% of code and 15–21% of text in a Stack Overflow post to understand and determine how to apply the relevant information to their context. Our results also discern the kinds of information prominent in that focus.",Empirical study | Mining software repositories | Software developer behavior | Stack overflow,Journal of Systems and Software,2020-01-01,Article,"Chatterjee, Preetha;Kong, Minji;Pollock, Lori",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086430077,10.1016/j.jss.2020.110659,The impact factors on the performance of machine learning-based vulnerability detection: A comparative study,"Machine learning-based Vulnerability detection is an active research topic in software security. Different traditional machine learning-based and deep learning-based vulnerability detection methods have been proposed. To our best knowledge, we are the first to identify four impact factors and conduct a comparative study to investigate the performance influence of these factors. In particular, the quality of datasets, classification models and vectorization methods can directly affect the detection performance, in contrast function/variable name replacement can affect the features of vulnerability detection and indirectly affect the performance. We collect three different vulnerability code datasets from two various sources (i.e., NVD and SARD). These datasets can correspond to different types of vulnerabilities. Moreover, we extract and analyze the features of vulnerability code datasets to explain some experimental results. Our findings based on the experimental results can be summarized as follows: (1) Deep learning models can achieve better performance than traditional machine learning models. Of all the models, BLSTM can achieve the best performance. (2) CountVectorizer can significantly improve the performance of traditional machine learning models. (3) Features generated by the random forest algorithm include system-related functions, syntax keywords, and user-defined names. Different vulnerability types and code sources will generate different features. (4) Datasets with user-defined variable and function name replacement will decrease the performance of vulnerability detection. (5) As the proportion of code from SARD increases, the performance of vulnerability detection will increase.",Comparative study | Deep learning | Feature extraction | Machine learning | Vulnerability detection,Journal of Systems and Software,2020-10-01,Article,"Zheng, Wei;Gao, Jialiang;Wu, Xiaoxue;Liu, Fengyu;Xun, Yuxing;Liu, Guoliang;Chen, Xiang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85085566995,10.1016/j.jss.2020.110663,Descriptions of issues and comments for predicting issue success in software projects,"Software development tasks must be performed successfully to achieve software quality and customer satisfaction. Knowing whether software tasks are likely to fail is essential to ensure the success of software projects. Issue Tracking Systems store information of software tasks (issues) and comments, which can be useful to predict issue success; however; almost no research on this topic exists. This work studies the usefulness of textual descriptions of issues and comments for predicting whether issues will be resolved successfully or not. Issues and comments of 588 software projects were extracted from four popular Issue Tracking Systems. Seven machine learning classifiers were trained on 30k issues and more than 120k comments, and more than 6000 experiments were performed to predict the success of three types of issues: bugs, improvements and new features. The results provided evidence that descriptions of issues and comments are useful for predicting issue success with more than 85% of accuracy and precision, and that the predictions of issue success vary over time. Words related to software development were particularly relevant for predicting issue success. Other communication aspects and their relationship to the success of software projects must be researched in detail using data from software tools.",Issue success | Issue Tracking System | Machine learning | Software development | Software project,Journal of Systems and Software,2020-10-01,Article,"Ramírez-Mora, Sandra L.;Oktaba, Hanna;Gómez-Adorno, Helena",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087512124,10.1016/j.jss.2020.110720,Towards the adoption of OMG standards in the development of SOA-based IoT systems,"A common feature of the Internet of Things (IoT) is the high heterogeneity, regarding network protocols, data formats, hardware and software platforms. Aiming to deal with such a degree of heterogeneity, several frameworks have applied the Model-Driven Development (MDD) to build IoT applications. On the software architecture viewpoint, the literature has shown that the Service-Oriented Architecture (SOA) is a promising style to address the interoperability of entities composing these solutions. Some features of IoT make it challenging to analyze the impact of design decisions on the SOA-based IoT applications behavior. Thus, it is a key requirement to simulate the model to verify whether the system performs as expected before its implementation. Although the literature has identified that the SOA style is suitable for addressing the interoperability, existing modeling languages do not consider SOA elements as first-class citizens when designing IoT applications. Furthermore, although existing MDD frameworks provide modeling languages comprising well-defined syntax, they lack execution semantics, thus, are not suitable for model execution and analysis. This work aims at addressing these issues by introducing IoTDraw. The framework provides a fully OMG-compliant executable modeling language for SOA-based IoT systems; thus, its specifications can be implemented by any tool implementing OMG standards.",Application | Internet of Things | Model-Driven Development | Service-Oriented Architecture,Journal of Systems and Software,2020-11-01,Article,"Costa, Bruno;Pires, Paulo F.;Delicato, Flávia C.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087591981,10.1016/j.jss.2020.110728,From API to NLI: A new interface for library reuse,"Developers frequently reuse APIs from existing libraries to implement certain functionality. However, learning APIs is difficult due to their large scale and complexity. In this paper, we design an abstract framework NLI2CODE to ease the reuse process. Under the framework, users can reuse library functionalities with a high-level, automatically-generated NLI (Natural Language Interface) instead of the detailed API elements. The framework consists of three components: a functional feature extractor to summarize the frequently-used library functions in natural language form, a code pattern miner to give a code template for each functional feature, and a synthesizer to complete code patterns into well-typed snippets. From the perspective of a user, a reuse task under NLI2CODE starts from choosing a functional feature and our framework will guide the user to synthesize the desired solution. We instantiated the framework as a tool to reuse Java libraries. The evaluation shows our tool can generate a high-quality natural language interface and save half of the coding time for newcomers to solve real-world programming tasks.",Code pattern | Library reuse | Program synthesis,Journal of Systems and Software,2020-11-01,Article,"Shen, Qi;Wu, Shijun;Zou, Yanzhen;Zhu, Zixiao;Xie, Bing",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088384495,10.1016/j.jss.2020.110708,Function-as-a-Service performance evaluation: A multivocal literature review,"Function-as-a-Service (FaaS) is one form of the serverless cloud computing paradigm and is defined through FaaS platforms (e.g., AWS Lambda) executing event-triggered code snippets (i.e., functions). Many studies that empirically evaluate the performance of such FaaS platforms have started to appear but we are currently lacking a comprehensive understanding of the overall domain. To address this gap, we conducted a multivocal literature review (MLR) covering 112 studies from academic (51) and grey (61) literature. We find that existing work mainly studies the AWS Lambda platform and focuses on micro-benchmarks using simple functions to measure CPU speed and FaaS platform overhead (i.e., container cold starts). Further, we discover a mismatch between academic and industrial sources on tested platform configurations, find that function triggers remain insufficiently studied, and identify HTTP API gateways and cloud storages as the most used external service integrations. Following existing guidelines on experimentation in cloud systems, we discover many flaws threatening the reproducibility of experiments presented in the surveyed studies. We conclude with a discussion of gaps in literature and highlight methodological suggestions that may serve to improve future FaaS performance evaluation studies.",Benchmarking | Cloud computing | Function-as-a-Service | Multivocal literature review | Performance | Serverless,Journal of Systems and Software,2020-12-01,Article,"Scheuner, Joel;Leitner, Philipp",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088222392,10.1016/j.jss.2020.110724,Studying the Relationship Between the Usage of APIs Discussed in the Crowd and Post-Release Defects,"Software development nowadays is heavily based on libraries, frameworks and their proposed Application Programming Interfaces (APIs). However, due to challenges such as the complexity and the lack of documentation, these APIs may introduce various obstacles for developers and common defects in software systems. To resolve these issues, developers usually utilize Question and Answer (Q&A) websites such as Stack Overflow by asking their questions and finding proper solutions for their problems on APIs. Therefore, these websites have become inevitable sources of knowledge for developers, which is also known as the crowd knowledge. However, the relation of this knowledge to the software quality has never been adequately explored before. In this paper, we study whether using APIs which are challenging according to the discussions of the Stack Overflow is related to code quality defined in terms of post-release defects. To this purpose, we define the concept of challenge of an API, which denotes how much the API is discussed in high-quality posts on Stack Overflow. Then, using this concept, we propose a set of products and process metrics. We empirically study the statistical correlation between our metrics and post-release defects as well as added explanatory and predictive power to traditional models through a case study on five open source projects including Spring, Elastic Search, Jenkins, K-8 Mail Android Client, and OwnCloud Android client. Our findings reveal that our metrics have a positive correlation with post-release defects which is comparable to known high-performance traditional process metrics, such as code churn and number of pre-release defects. Furthermore, our proposed metrics can provide additional explanatory and predictive power for software quality when added to the models based on existing products and process metrics. Our results suggest that software developers should consider allocating more resources on reviewing and improving external API usages to prevent further defects.",API | Crowd knowledge | Defect prediction | Stack overflow,Journal of Systems and Software,2020-12-01,Article,"Tahmooresi, Hamed;Heydarnoori, Abbas;Nadri, Reza",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087859453,10.1016/j.jss.2020.110732,Scrum versus Rational Unified Process in facing the main challenges of product configuration systems development,"Product configuration systems (PCSs) are software applications that enable companies to customise configurable products by facilitating the automation of sales and engineering. Widely used in various industries, PCSs can bring substantial benefits and constitute a fundamental tool for mass customisation. However, serious challenges in PCS development have been reported. Software engineering approaches, such as the rational unified process (RUP) and Scrum, have been adopted to realise high-quality PCSs, but research insights on their use in PCS development are very limited, and their different capabilities to address PCS challenges are almost totally unexplored. This article illustrates the application of RUP and Scrum in PCS development and compares their contributions to addressing PCS development challenges. To perform this comparison, four PCS projects in a company that moved from RUP to Scrum are analysed. The evidence provided suggests that moving from RUP to Scrum has a positive effect in facing organisational,​ IT-related and resource constraint challenges. The results also highlight worsening knowledge management and documentation, product modelling and visualisation. The findings suggest the adaptation of Scrum for PCS development to reinforce Scrum's knowledge-related capabilities.",Agile | Product configuration systems (PCSs) | Rational unified process (RUP) | Scrum,Journal of Systems and Software,2020-12-01,Article,"Shafiee, Sara;Wautelet, Yves;Hvam, Lars;Sandrin, Enrico;Forza, Cipriano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087940377,10.1016/j.jss.2020.110738,Implementation relations and testing for cyclic systems with refusals and discrete time,"We present a formalism to represent cyclic models and study different semantic frameworks that support testing. These models combine sequences of observable actions and the passing of (discrete) time and can be used to specify a number of classes of reactive systems, an example being robotic systems. We use implementation relations in order to formally define a notion of correctness of a system under test (SUT) with respect to a specification. As usual, the aim is to devise an extension of the classical ioco implementation relation but available timed variants of ioco are not suitable for cyclic models. This paper thus defines new implementation relations that encapsulate the discrete nature of time and take into account not only the actions that models can perform but also the ones that they can refuse. In addition to defining these relations, we study a number of their properties and provide alternative characterisations, showing that the relations are appropriate conservative extensions of trace containment. Finally, we give test derivation algorithms and prove that they are sound and also are complete in the limit.",Cyclic systems | Implementation relations | Model-based testing,Journal of Systems and Software,2020-12-01,Article,"Lefticaru, Raluca;Hierons, Robert M.;Núñez, Manuel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087986700,10.1016/j.jss.2020.110740,"On the generation, structure, and semantics of grammar patterns in source code identifiers","Identifiers make up a majority of the text in code. They are one of the most basic mediums through which developers describe the code they create and understand the code that others create. Therefore, understanding the patterns latent in identifier naming practices and how accurately we are able to automatically model these patterns is vital if researchers are to support developers and automated analysis approaches in comprehending and creating identifiers correctly and optimally. This paper investigates identifiers by studying sequences of part-of-speech annotations, referred to as grammar patterns. This work advances our understanding of these patterns and our ability to model them by (1) establishing common naming patterns in different types of identifiers, such as class and attribute names; (2) analyzing how different patterns influence comprehension; and (3) studying the accuracy of state-of-the-art techniques for part-of-speech annotations, which are vital in automatically modeling identifier naming patterns, in order to establish their limits and paths toward improvement. To do this, we manually annotate a dataset of 1,335 identifiers from 20 open-source systems and use this dataset to study naming patterns, semantics, and tagger accuracy.",Identifier naming | Part-of-speech tagging | Program comprehension | Software maintenance | Source code analysis,Journal of Systems and Software,2020-12-01,Article,"Newman, Christian D.;AlSuhaibani, Reem S.;Decker, Michael J.;Peruma, Anthony;Kaushik, Dishant;Mkaouer, Mohamed Wiem;Hill, Emily",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088924536,10.1016/j.jss.2020.110754,CommtPst: Deep learning source code for commenting positions prediction,"Existing techniques for automatic code commenting assume that the code snippet to be commented has been identified, thus requiring users to provide the code snippet in advance. A smarter commenting approach is desired to first self-determine where to comment in a given source code and then generate comments for the code snippets that need comments. To achieve the first step of this goal, we propose a novel method, CommtPst, to automatically find the appropriate commenting positions in the source code. Since commenting is closely related to the code syntax and semantics, we adopt neural language model (word embeddings) to capture the code semantic information, and analyze the abstract syntax trees to capture code syntactic information. Then, we employ LSTM (long short term memory) to model the long-term logical dependency of code statements over the fused semantic and syntactic information and learn the commenting patterns on the code sequence. We evaluated CommtPst using large data sets from dozens of open-source software systems in GitHub. The experimental results show that the precision, recall and F-Measure values achieved by CommtPst are 0.792, 0.602 and 0.684, respectively, which outperforms the traditional machine learning method with 11.4% improvement on F-measure.",Code semantics | Code syntax | Comment generation | Comment position | LSTM,Journal of Systems and Software,2020-12-01,Article,"Huang, Yuan;Hu, Xinyu;Jia, Nan;Chen, Xiangping;Zheng, Zibin;Luo, Xiapu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088921900,10.1016/j.jss.2020.110767,Black-box adversarial sample generation based on differential evolution,"Deep Neural Networks (DNNs) are being used in various daily tasks such as object detection, speech processing, and machine translation. However, it is known that DNNs suffer from robustness problems — perturbed inputs called adversarial samples leading to misbehaviors of DNNs. In this paper, we propose a black-box technique called Black-box Momentum Iterative Fast Gradient Sign Method (BMI-FGSM) to test the robustness of DNN models. The technique does not require any knowledge of the structure or weights of the target DNN. Compared to existing white-box testing techniques that require accessing model internal information such as gradients, our technique approximates gradients through Differential Evolution and uses approximated gradients to construct adversarial samples. Experimental results show that our technique can achieve 100% success in generating adversarial samples to trigger misclassification, and over 95% success in generating samples to trigger misclassification to a specific target output label. It also demonstrates better perturbation distance and better transferability. Compared to the state-of-the-art black-box technique, our technique is more efficient. Furthermore, we conduct testing on the commercial Aliyun API and successfully trigger its misbehavior within a limited number of queries, demonstrating the feasibility of real-world black-box attack.",Adversarial samples | Black-box testing | Deep Neural Network | Differential evolution,Journal of Systems and Software,2020-12-01,Article,"Lin, Junyu;Xu, Lei;Liu, Yingqi;Zhang, Xiangyu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089806921,10.1016/j.jss.2020.110775,A Large Scale Analysis of Android — Web Hybridization,"Many Android applications embed webpages via WebView components and execute JavaScript code within Android. Hybrid applications leverage dedicated APIs to load a resource and render it in a WebView. Furthermore, Android objects can be shared with the JavaScript world. However, bridging the interfaces of the Android and JavaScript world might also incur severe security threats: Potentially untrusted webpages and their JavaScript might interfere with the Android environment and its access to native features. No general analysis is currently available to assess the implications of such hybrid apps bridging the two worlds. To understand the semantics and effects of hybrid apps, we perform a large-scale study on the usage of the hybridization APIs in the wild. We analyze and categorize the parameters to hybridization APIs for 7,500 randomly selected and the 196 most popular applications from the Google Playstore as well as 1000 malware samples. Our results advance the general understanding of hybrid applications, as well as implications for potential program analyses, and the current security situation: We discovered thousands of flows of sensitive data from Android to JavaScript, the vast majority of which could flow to potentially untrustworthy code. Our analysis identified numerous web pages embedding vulnerabilities, which we exemplarily exploited. Additionally, we discovered a multitude of applications in which potentially untrusted JavaScript code may interfere with (trusted) Android objects, both in benign and malign applications.",Android Hybrid Apps | Information Flow Control | Static Analysis,Journal of Systems and Software,2020-12-01,Article,"Tiwari, Abhishek;Prakash, Jyoti;Groß, Sascha;Hammer, Christian",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85089832552,10.1016/j.jss.2020.110800,Generating summaries for methods of event-driven programs: An Android case study,"The lack of proper documentation makes program comprehension a cumbersome process for developers. Source code summarization is one of the existing solutions to this problem. Many approaches have been proposed to summarize source code in recent years. A prevalent weakness of these solutions is that they do not pay much attention to interactions among elements of software. An element is simply a callable code snippet such as a method or even a clickable button. As a result, these approaches cannot be applied to event-driven programs, such as Android applications, because they have specific features such as numerous interactions between their elements. To tackle this problem, we propose a novel approach based on deep neural networks and dynamic call graphs to generate summaries for methods of event-driven programs. First, we collect a set of comment/code pairs from Github and train a deep neural network on the set. Afterward, by exploiting a dynamic call graph, the Pagerank algorithm, and the pre-trained deep neural network, we generate summaries. An empirical evaluation with 14 real-world Android applications and 42 participants indicates 32.3% BLEU4 which is a definite improvement compared to the existing state-of-the-art techniques. We also assessed the informativeness and naturalness of our generated summaries from developers’ perspectives and showed they are sufficiently understandable and informative.",Deep learning | Event-driven programs | Neural machine translation | Source code summarization,Journal of Systems and Software,2020-12-01,Article,"Aghamohammadi, Alireza;Izadi, Maliheh;Heydarnoori, Abbas",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85075887207,10.1016/j.jss.2019.110478,An OSLC-based environment for system-level functional testing of ERTMS/ETCS controllers,"Product and application life-cycle management (PLM/ALM) are the processes that govern a product and a software system, respectively, encompassing the creation, deployment and operation of a system from the beginning to the end of its life. As both PLM and ALM require cross-discipline collaboration and cooperation, tools integration and inter-operation are necessary to enable the efficient and effective usage of tool suites supporting the management of the entire system life-cycle and overcome the limitations of all-in-one solutions from one tool vendor. In this context, the Open Services for Life-cycle Collaboration (OSLC) initiative proposes a set of specifications to allow a seamless integration based on linked data. This paper describes the work performed within the ARTEMIS JU project CRYSTAL to develop an environment for the functional system-level testing of railway controllers, relying on OSLC to enable inter-operation with existing PLM/ALM tools. A concrete realization of the proposed architecture is described also discussing some design and implementation choices. A real industrial case study is used to exemplify the features and the usage of the environment in testing one of the functionalities of the Radio Block Centre, the vital core of the European Rail Traffic Management System/European Train Control System (ERTMS/ETCS) Control System.",Critical systems | Life-cycle collaboration | Model based testing | OSLC | Testing automation,Journal of Systems and Software,2020-03-01,Article,"Nardone, Roberto;Marrone, Stefano;Gentile, Ugo;Amato, Aniello;Barberio, Gregorio;Benerecetti, Massimo;De Guglielmo, Renato;Di Martino, Beniamino;Mazzocca, Nicola;Peron, Adriano;Pisani, Gaetano;Velardi, Luigi;Vittorini, Valeria",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85076588834,10.1016/j.jss.2019.110480,Performance evaluation of web service response time probability distribution models for business process cycle time simulation,"Context: The adoption of Business Process Management (BPM) is enabling companies to improve the pace of building new capabilities, enhancing existing ones, and measuring process performance to identify bottlenecks. It is essential to compute the cycle time of the process to assess the performance of a business process. The cycle time typically forms part of service level agreements (SLAs) and is a crucial contributor to the overall user experience and productivity. The simulation technique is versatile and has broad applicability for determining realistic cycle time using historical data of web service response time. BPM tools offer inadequate support for modeling input data used in simulation in the form of descriptive statistics or standard probability distributions like normal, lognormal, which results in inaccurate simulation results. Objective: We evaluate the effectiveness of different parametric and non-parametric probability distributions for modeling data of web service response time. We further assess how the choice of probability distribution impacts the accuracy of the simulated cycle time of a business process. The work is the first of such a study using real-world data for encouraging Business Process Simulation Specification (BPSim) standard setters and BPM tools to enhance their support for such distributions in their simulation engine. Method: We consider several parametric and non-parametric distributions and explore how well these distributions fit web service response time from extensive public and a real-world dataset. The cycle time of the business process of a real-world system is simulated using the identified distributions to model the underlying web service data. Results: Our results show that kernel distribution is the most suitable choice, followed by Burr. Kernel outperforms Burr by 86.63% for the public and 84.21% for the real-world dataset. The choice of distribution affects the percentile ranks like 90 and above than the median. The use of single-point values underestimates cycle time values at higher percentiles. Conclusion: Based on our empirical results, we recommend the addition of kernel and Burr to the current list of distributions supported by BPSim and BPM tools.",Cycle time | Non-parametric distributions | Parametric distributions | Performance evaluation | Simulation input modeling | Web service response time,Journal of Systems and Software,2020-03-01,Article,"Ramakrishnan, Raghu;Kaur, Arvinder",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85077321850,10.1016/j.jss.2019.110491,A universal cross language software similarity detector for open source software categorization,"While there are novel approaches for detecting and categorizing similar software applications, previous research focused on detecting similarity in applications written in the same programming language and not on detecting similarity in applications written in different programming languages. Cross-language software similarity detection is inherently more challenging due to variations in language, application structures, support libraries used, and naming conventions. In this paper we propose a novel model, CroLSim, to detect similar software applications across different programming languages. We define a semantic relationship among cross-language libraries and API methods (both local and third party) using functional descriptions and a word-vector learning model. Our experiments show that CroLSim can successfully detect cross-language similar software applications, which outperforms all existing approaches (mean average precision rate of 0.65, confidence rate of 3.6, and 75% highly rated successful queries). Furthermore, we applied CroLSim to a source code repository to see whether our model can recommend cross-language source code fragments if queried directly with source code. From our experiments we found that CroLSim can recommend cross-language functional similar source code when source code is directly used as a query (average precision=0.28, recall=0.85, and F-Measure=0.40).",API Calls | Cross-Language software similarity detection | Doc2Vec | Singular value decomposition,Journal of Systems and Software,2020-04-01,Article,"Nafi, Kawser Wazed;Roy, Banani;Roy, Chanchal K.;Schneider, Kevin A.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079002933,10.1016/j.jss.2020.110533,An Android application risk evaluation framework based on minimum permission set identification,"Android utilizes a security mechanism that requires apps to request permission for accessing sensitive user data, e.g., contacts and SMSs, or certain system features, e.g., camera and Internet access. However, Android apps tend to be overprivileged, i.e., they often request more permissions than necessary. This raises the security problem of overprivilege. To alleviate the overprivilege problem, this paper proposes MPDroid, an approach that combines static analysis and collaborative filtering to identify the minimum permissions for an Android app based on its app description and API usage. Given an app, MPDroid first employs collaborative filtering to identify the initial minimum permissions for the app. Then, through static analysis, the final minimum permissions that an app really needs are identified. Finally, it evaluates the overprivilege risk by inspecting the app's extra privileges, i.e., the unnecessary permissions requested by the app. Experiments are conducted on 16,343 popular apps collected from Google Play. The results show that MPDroid outperforms the state-of-the-art approach significantly.",App risk evaluation | Collaborative filtering | Minimum permissions | Permission overprivilege | Static analysis,Journal of Systems and Software,2020-05-01,Article,"Xiao, Jianmao;Chen, Shizhan;He, Qiang;Feng, Zhiyong;Xue, Xiao",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079383470,10.1016/j.jss.2020.110547,Modeling programs hierarchically with stack-augmented LSTM,"Programming language modeling has attracted extensive attention in recent years, and it plays an essential role in program processing fields. Statistical language models, which are initially designed for natural languages, have been generally used for modeling programming languages. However, different from natural languages, programming languages contain explicit and hierarchical structure that is hard to learn by traditional statistical language models. To address this challenge, we propose a novel Stack-Augmented LSTM neural network for programming language modeling. Adding a stack memory component into the LSTM network enables our model to capture the hierarchical information of programs through the PUSH and POP operations, which further allows our model capturing the long-term dependency in the programs. We evaluate the proposed model on three program analysis tasks, i.e., code completion, program classification, and code summarization. Evaluation results show that our proposed model outperforms baseline models in all the three tasks, indicating that by capturing the structural information of programs with a stack, our proposed model can represent programs more precisely.",Deep learning | Hierarchical structure | Programming language modeling | Software engineering,Journal of Systems and Software,2020-06-01,Article,"Liu, Fang;Zhang, Lu;Jin, Zhi",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85080064786,10.1016/j.jss.2020.110558,MSL: A pattern language for engineering self-adaptive systems,"In architecture-based self-adaptation of decentralized systems, design patterns have been introduced to ease the design of complex adaptation solutions that usually require the interaction of different MAPE-K (Monitor-Analyze-Plan-Execute over a shared Knowledge) control loops, each dealing with an adaptation concern of the managed system. Such MAPE patterns have been proposed by means of a graphical notation, but without a well-defined way to document them and to express the semantics of components interactions. In this paper, we propose an approach to overcome these limitations. We present a domain-specific language, called MSL for MAPE Specification Language, to define and instantiate MAPE patterns and to give semantics to some semantic variation points of the equivalent graphical notation for MAPE pattern. We also provide a formal semantics of the language by means of self-adaptive Abstract State Machines, an extension of the Abstract State Machines (ASMs) formalism to model self-adaptation. Such semantics definition comes with an automatic transformation of MSL models into formal executable models, and opens to the possibility of performing rigorous analysis (validation w.r.t. the adaptation requirements and verification of adaptation properties) of MSL models. Moreover, we present our current results toward a (long-term) realization of an MSL-centric framework, where MSL is the notation of a modeling front-end, on top of richer and more specific modeling, analysis, and implementation back-end frameworks. As proof of concept of our approach, we show the application of MSL and its formal support to a running case study in the field of home automation, by modeling an adaptive control of a virtual smart home developed with the OpenHAB runtime platform.",Adaptive smart home systems | Architecture-based self-adaptation | MAPE-K pattern loops | Pattern-oriented modeling | Self-adaptive ASMs,Journal of Systems and Software,2020-06-01,Article,"Arcaini, Paolo;Mirandola, Raffaela;Riccobene, Elvinia;Scandurra, Patrizia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081591086,10.1016/j.jss.2020.110565,Systematic literature review of empirical studies on mental representations of programs,"Programmers are frequently tasked with modifying, enhancing, and extending applications. To perform these tasks, programmers must understand existing code by forming mental representations. Empirical research is required to determine the mental representations constructed during program comprehension to inform the development of programming languages, instructional practices, and tools. To make recommendations for future work a systematic literature review was conducted that summarizes the empirical research on mental representations formed during program comprehension, how the methods and tasks have changed over time, and the research contributions. The data items included in the systematic review are empirical studies of programmers that investigated the comprehension and internal representation of code written in a formal programming language. The eligibility criteria used in the review were meant to extract studies with a focus on knowledge representation as opposed to knowledge utilization. The results revealed a lack of incremental research and a dramatic decline in the research meaning that newly developed or popularized languages and paradigms have not been a part of the research reviewed. Accordingly, we argue that there needs to be a resurgence of empirical research on the psychology of programming to inform the design of tools and languages, especially in new and emerging paradigms.",Mental representations | Program comprehension | Systematic literature review,Journal of Systems and Software,2020-07-01,Article,"Bidlake, Leah;Aubanel, Eric;Voyer, Daniel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081157908,10.1016/j.jss.2020.110566,An ontology-based learning approach for automatically classifying security requirements,"Although academia has recognized the importance of explicitly specifying security requirements in early stages of system developments for years, in reality, many projects mix security requirements with other types of requirements. Thus, there is a strong need for precisely and efficiently classifying such security requirements from other requirements in requirement specifications. Existing studies leverage lexical evidence to build probabilistic classifiers, which are domain-dependent by design and cannot effectively classify security requirements from different application domains. In this paper, we propose an ontology-driven learning approach to automatically classify security requirements. Our approach consists of a conceptual layer and a linguistic layer, which understands security requirements based on not only lexical evidence but also conceptual domain knowledge. In particular, we apply a systematic approach to identify linguistic features of security requirements based on an extended security requirements ontology and linguistic knowledge, connecting the conceptual layer with the linguistic layer. Such linguistic features are then used to train domain-independent security requirements classifiers by using machine learning techniques. We have carried out a series of experiments to evaluate the performance and generalization ability of our proposal against existing approaches. The results of the experiments show that the proposed approach outperforms existing approaches with a significant increase of F1 score (0.63 VS. 0.44) when the training dataset and the testing dataset come from different application domains, i.e., the classifiers trained by our approach can be generalized to classify security requirements from different domains.",linguistic pattern | machine learning | natural language processing | security requirements classification | security requirements ontology,Journal of Systems and Software,2020-07-01,Article,"Li, Tong;Chen, Zhishuai",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081654312,10.1016/j.jss.2020.110568,Enhancing example-based code search with functional semantics,"As the quality and quantity of open source code increase, effective and efficient search for code implementing certain semantics, or semantics-based code search, has become an emerging need for software developers to retrieve and reuse existing source code. Previous techniques in semantics-based code search encode the semantics of loop-free Java code snippets as constraints and utilize an SMT solver to find encoded snippets that match an input/output (IO) query. We present in this article the Quebio approach to semantics-based search for Java methods. Quebio advances the state-of-the-art by supporting important language features like invocation to library APIs and enabling the search to handle more data types like array/List, Set, and Map. Compared with existing approaches, Quebio also integrates a customized keyword-based search that uses as the input a textual, behavioral summary of the desired methods to quickly prune the methods to be checked against the IO examples. To evaluate the effectiveness and efficiency of Quebio, we constructed a repository of 14,792 methods from 723 open source Java projects hosted on GitHub and applied the approach to resolve 47 queries extracted from StackOverflow. Quebio was able to find methods correctly implementing the specified IO behaviors for 43 of the queries, significantly outperforming the existing semantics-based code search techniques. The average search time with Quebio was 213.2 seconds for each query.",Semantics-based code search | SMT solver | Symbolic analysis,Journal of Systems and Software,2020-07-01,Article,"Chen, Zhengzhao;Jiang, Renhe;Zhang, Zejun;Pei, Yu;Pan, Minxue;Zhang, Tian;Li, Xuandong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85083814774,10.1016/j.jss.2020.110599,Achieving agility and quality in product development - an empirical study of hardware startups,"Context: Startups aim at scaling their business, often by developing innovative products with limited human and financial resources. The development of software products in the startup context is known as opportunistic, agility-driven, and with high tolerance for technical debt. The special context of hardware startups calls for a better understanding of state-of-the-practice of hardware startups’ activities. Objective: This study aimed to identify whether and how startups can achieve product quality while maintaining focus on agility. Method: We conducted an exploratory study with 13 hardware startups, collecting data through semi-structured interviews and analysis of documentation. We proposed an integrative model of agility and quality in hardware startups. Results: Agility in hardware startups is complex and not achieved through adoption of fast-paced development practices alone. Hardware startups follow a quality-driven approach for development of core components, where frequent user testing is a measure for early debt management. Hardware startups often lack mindset and strategies for achieving long-term quality in early stages. Conclusions: Hardware startups need attention to hardware quality to allow for evolutionary prototyping and speed. Future research should focus on defining quality-driven practices that contribute to agility, and strategies and mindsets to support long-term quality in the hardware startup context.",Empirical research | Hardware startup | Product development | Software engineering | Startup,Journal of Systems and Software,2020-09-01,Article,"Berg, Vebjørn;Birkeland, Jørgen;Nguyen-Duc, Anh;Pappas, Ilias O.;Jaccheri, Letizia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85085256428,10.1016/j.infsof.2020.106313,"Early prediction of quality of service using interface-level metrics, code-level metrics, and antipatterns","Context: With the current high trends of deploying and using web services in practice, effective techniques for maintaining high quality of Service are becoming critical for both service providers and subscribers/users. Service providers want to predict the quality of service during early stages of development before releasing them to customers. Service clients consider the quality of service when selecting the best one satisfying their preferences in terms of price/budget and quality between the services offering the same features. The majority of existing studies for the prediction of quality of service are based on clustering algorithms to classify a set of services based on their collected quality attributes. Then, the user can select the best service based on his expectations both in terms of quality and features. However, this assumption requires the deployment of the services before being able to make the prediction and it can be time-consuming to collect the required data of running web services during a period of time. Furthermore, the clustering is only based on well-known quality attributes related to the services performance after deployment. Objective: In this paper, we start from the hypothesis that the quality of the source code and interface design can be used as indicators to predict the quality of service attributes without the need to deploy or run the services by the subscribers. Method: We collected training data of 707 web services and we used machine learning to generate association rules that predict the quality of service based on the interface and code quality metrics, and antipatterns. Results: The empirical validation of our prediction techniques shows that the generated association rules have strong support and high confidence which confirms our hypothesis that source code and interface quality metrics/antipatterns are correlated with web service quality attributes which are response time, availability, throughput, successability, reliability, compliance, best practices, latency, and documentation. Conclusion: To the best of our knowledge, this paper represents the first study to validate the correlation between interface metrics, source code metrics, antipatterns and quality of service. Another contribution of our work consists of generating association rules between the code/interface metrics and quality of service that can be used for prediction purposes before deploying new releases.",Anti-patterns | Code quality | Interface metrics | Performance prediction | Quality of service | Web services,Information and Software Technology,2020-10-01,Article,"Abid, Chaima;Kessentini, Marouane;Wang, Hanzhang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85162019623,10.1016/j.infsof.2019.106214,A systematic literature review of machine learning techniques for software maintainability prediction,"Context: Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process. Objective: The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models. Method: The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018. Results: We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely. Conclusion: Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.",Dataset | Machine learning | Metric | Software maintainability prediction | Systematic literature review,Information and Software Technology,2020-03-01,Article,"Alsolai, Hadeel;Roper, Marc",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087339146,10.1016/j.infsof.2020.106371,Automated model-based performance analysis of software product lines under uncertainty,"Context: A Software Product Line (SPL) can express the variability of a system through the specification of configuration options. Evaluating performance characteristics, such as the system response time and resource utilization, of a software product is challenging, even more so in the presence of uncertain values of the attributes. Objective: The goal of this paper is to automate the generation of performance models for software products derived from the feature model by selection heuristics. We aim at obtaining model-based predictive results to quantify the correlation between the features, along with their uncertainties, and the system performance. This way, software engineers can be informed on the performance characteristics before implementing the system. Method: We propose a tool-supported framework that, starting from a feature model annotated with performance-related characteristics, derives Queueing Network (QN) performance models for all the products of the SPL. Model-based performance analysis is carried out on the models obtained by selecting the products that show the maximum and minimum performance-based costs. Results: We applied our approach to almost seven thousand feature models including more than one hundred and seventy features. The generation of QN models is automatically performed in much less than one second, whereas their model-based performance analysis embeds simulation delays and requires about six minutes on average. Conclusion: The experimental results confirm that our approach can be effective on a variety of systems for which software engineers may be provided with early insights on the system performance in reasonably short times. Software engineers are supported in the task of understanding the performance bounds that may encounter when (de)selecting different configuration options, along with their uncertainties.",Attributed feature models | Queueing networks | Software performance engineering | Software product lines | Uncertainty,Information and Software Technology,2020-11-01,Article,"Arcaini, Paolo;Inverso, Omar;Trubiani, Catia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073249376,10.1016/j.infsof.2019.106198,Automatic extraction of product line architecture and feature models from UML class diagram variants,"Context: Software Product Lines (SPLs) are families of related products developed for specific domains. SPLs commonly emerge from existing variants when their individual maintenance and/or evolution become complex. Even though there exists a vast research literature on SPL extraction, the majority of the approaches have only focused on source code, are partially automated, or do not reflect domain constraints. Such limitations can make more difficult the extraction, management, documentation and generation of some important SPL artifacts such as the product line architecture, a fact that can impact negatively the evolution and maintenance of SPLs. Objective: To tackle these limitations, this work presents ModelVars2SPL (Model Variants to SPL Core Assets), an automated approach to aid the development of SPLs from existing system variants. Method: The input for ModelVars2SPL is a set of Unified Modeling Language (UML) class diagrams and the list of features they implement. The approach extracts two main assets: (i) Feature Model (FM), which represents the combinations of features, and (ii) a Product Line Architecture (PLA), which represents a global structure of the variants. ModelVars2SPL is composed of four automated steps. We conducted a thorough evaluation of ModelVars2SPL to analyze the artefacts it generates and its performance. Results: The results show that the FMs well-represent the features organization, providing useful information to define and manage commonalities and variabilities. The PLAs show a global structure of current variants, facilitating the understanding of existing implementations of all variants. Conclusions: An advantage of ModelVars2SPL is to exploit the use of UML design models, that is, it is independent of the programming language, and supports the re-engineering process in the design level, allowing practitioners to have a broader view of the SPL.",Feature model | Model merging | Search-based techniques | SPL architecture,Information and Software Technology,2020-01-01,Article,"Assunção, Wesley K.G.;Vergilio, Silvia R.;Lopez-Herrejon, Roberto E.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85090111773,10.1016/j.infsof.2020.106389,Empirical software product line engineering: A systematic literature review,"Context: The adoption of Software Product Line Engineering (SPLE) is usually only based on its theoretical benefits instead of empirical evidences. In fact, there is no work that synthesizes the empirical studies on SPLE. This makes it difficult for researchers to base their contributions on previous works validated with an empirical strategy. Objective: The objective of this work is to discover and summarize the studies that have used empirical evidences in SPLE limited to those ones with the intervention of humans. This will allow evaluating the quality and to know the scope of these studies over time. Doing so, research opportunities can arise Methods: A systematic literature review was conducted. The scope of the work focuses on those studies in which there is human intervention and were published between 2000 and 2018. We considered peer-reviewed papers from journals and top software engineering conferences. Results: Out of a total of 1880 studies in the initial set, a total of 62 primary studies were selected after applying a series of inclusion and exclusion criteria. We found that, approximately 56% of the studies used the empirical case study strategy while the rest used experimental strategies. Around 86% of the case studies were performed in an industrial environment showing the penetration of SPLE in industry. Conclusion: The interest of empirical studies has been growing since 2008. Around 95.16% of the studies address aspects related to domain engineering while application engineering received less attention. Most of the experiments and case study evaluated showed an acceptable level of quality. The first study found dates from 2005 and since then, the interest in the empirical SPLE has increased.",Case study | Empirical strategies | Experiment | Software product lines | Systematic literature review,Information and Software Technology,2020-12-01,Article,"Chacón-Luna, Ana Eva;Gutiérrez, Antonio Manuel;Galindo, José A.;Benavides, David",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087589181,10.1016/j.infsof.2020.106372,"The Symposium on Search-Based Software Eengineering: Past, Present and Future","Context: Search-Based Software Engineering (SBSE) is the research field where Software Engineering (SE) problems are modelled as search problems to be solved by search-based techniques. The Symposium on Search Based Software Engineering (SSBSE) is the premier event on SBSE, which had its 11th edition in 2019. Objective: In order to better understand the characteristics and evolution of papers published at SSBSE, this work reports results from a mapping study targeting the proceedings of all SSBSE editions. Despite the existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups. Method: A systematic mapping study was conducted with a set of four research questions, in which 134 studies published in all editions of SSBSE, dated from 2009 to 2019, were evaluated. In a fifth question, 32 papers published in the challenge track were summarised. Results: Throughout the years, 290 authors from 25 countries have contributed to the main track of the symposium, with the collaboration of at least two institutions in 46.3% of the papers. SSBSE papers have got substantial external visibility, as most citations are from different venues. The SE tasks addressed by SSBSE are mostly related to software testing, software debugging, software design, and maintenance. Evolutionary algorithms are present in 75% of the papers, being the most common search technique. The evaluation of the SBSE approaches usually includes industrial systems. Conclusions: SSBSE has helped increase the popularity of SBSE in the SE research community and has played an important role on making SBSE mature. There are still problems and challenges to be addressed in the SBSE field, which can be tackled by SSBSE authors in further studies.",Bibliometric analysis | Search-based software engineering | Systematic mapping,Information and Software Technology,2020-11-01,Conference Paper,"Colanzi, Thelma Elita;Assunção, Wesley K.G.;Vergilio, Silvia R.;Farah, Paulo Roberto;Guizzo, Giovani",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85078871135,10.1016/j.infsof.2020.106270,Identifying self-admitted technical debt through code comment analysis with a contextualized vocabulary,"Context: Previous work has shown that one can explore code comments to detect Self-Admitted Technical Debt (SATD) using a contextualized vocabulary. However, current detection strategies still return a large number of false positives items. Moreover, those strategies do not allow the automatic identification of the type of debt of the identified items. Objective: This work applies, evaluates, and improves a set of contextualized patterns we built to detect self-admitted technical debt using code comment analysis. We refer to this set of patterns as the self-admitted technical debt identification vocabulary. Method: We carry out three empirical studies. Firstly, 23 participants analyze the patterns of a previously defined contextualized vocabulary and register their level of importance in identifying SATD items. Secondly, we perform a qualitative analysis to investigate the relation between each pattern and types of debt. Finally, we perform a feasibility study using a new vocabulary, improved based on the results of the previous empirical studies, to automatically identify self-admitted technical debt items, and types of debt, that exist in three open source projects. Results: More than half of the new patterns were considered decisive or very decisive to detect technical debt items. The new vocabulary was able to find items associated to code, design, defect, documentation, and requirement debt. Thus, the result of the work is an improved vocabulary that considers the level of importance of each pattern and the relationship between patterns and debt types to support the identification and classification of SATD items. Conclusion: The studies allowed us to improve a vocabulary to identify self-admitted technical debt items through code comments analysis. The results show that the use of pattern-based code comment analysis can contribute to improve existing methods, or create new ones, for automatically identifying and classifying technical debt items.",Code comment analysis | Self-admitted technical debt | Technical debt | Technical debt identification,Information and Software Technology,2020-05-01,Article,"Farias, Mário André de Freitas;Neto, Manoel Gomes de Mendonça;Kalinowski, Marcos;Spínola, Rodrigo Oliveira",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85083274270,10.1016/j.infsof.2020.106293,Software product line applied to the internet of things: A systematic literature review,"Context: Internet of Things (IoT) is a promising paradigm due to the growing number of devices that may be connected, defined as “things”. Managing these “things” is still considered a challenge. One way to overcome this challenge may be by adopting the software product line (SPL) paradigm and the variability management (VM) activity. SPL engineering consists of mechanisms that provide identification, representation, and traceability, which may be helpful to “things” management supported by VM organizational and technical activities. Objective: This research aims to investigate how SPL engineering has been applied along with the IoT paradigm, as well as how VM is being carried out. Method: A systematic literature review (SLR) was conducted considering papers available until March 2019. This systematic review identified 1039 papers. After eliminating the duplicated titles and the ones not related to the review, 112 papers remained. The number of papers was narrowed to 56 after applying the exclusion criteria. Results: The results provide evidence on the diversity of proposed SPLs used to specify approaches for managing IoT systems. However, most SPLs and research developed for IoT lack a systematic and detailed specification to ensure their quality, as well as tailoring guidelines for further use.",Families of systems | Internet of things | Product family engineering | Software product line | Variability management,Information and Software Technology,2020-08-01,Review,"Geraldi, Ricardo Theis;Reinehr, Sheila;Malucelli, Andreia",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087525711,10.1016/j.infsof.2020.106373,Towards automatically generating block comments for code snippets,"Code commenting is a common programming practice of practical importance to help developers review and comprehend source code. There are two main types of code comments for a method: header comments that summarize the method functionality located before a method, and block comments that describe the functionality of the code snippets within a method. Inspired by the effectiveness of deep learning techniques in the NLP field, many studies focus on using the machine translation model to automatically generate comment for the source code. Because the data set of block comments is difficult to collect, current studies focus more on the automatic generation of header comments than that of block comments. However, block comments are important for program comprehension due to their explanation role for the code snippets in a method. To fill the gap, we have proposed an approach that combines heuristic rules and learning-based method to collect a large number of comment-code pairs from 1,032 open source projects in our previous study. In this paper, we propose a reinforcement learning-based method, RL-BlockCom, to automatically generate block comments for code snippets based on the collected comment-code pairs. Specifically, we utilize the abstract syntax tree (i.e., AST) of a code snippet to generate a token sequence with a statement-based traversal way. Then we propose a composite learning model, which combines the actor-critic algorithm of reinforcement learning with the encoder-decoder algorithm, to generate block comments. On the data set of the comment-code pairs, the BLEU-4 score of our method is 24.28, which outperforms the baselines and state-of-the-art in comment generation.",Automatic comment generation | Code comment scope | Reinforcement learning | Source code summarization,Information and Software Technology,2020-11-01,Article,"Huang, Yuan;Huang, Shaohao;Chen, Huanchao;Chen, Xiangping;Zheng, Zibin;Luo, Xiapu;Jia, Nan;Hu, Xinyu;Zhou, Xiaocong",Include,
10.1016/j.jss.2022.111515,2-s2.0-85078985017,10.1016/j.infsof.2020.106257,Time pressure in software engineering: A systematic review,"Context: Large project overruns and overtime work have been reported in the software industry, resulting in additional expense for companies and personal issues for developers. Experiments and case studies have investigated the relationship between time pressure and software quality and productivity. Objective: The present work aims to provide an overview of studies related to time pressure in software engineering; specifically, existing definitions, possible causes, and metrics relevant to time pressure were collected, and a mapping of the studies to software processes and approaches was performed. Moreover, we synthesize results of existing quantitative studies on the effects of time pressure on software development, and offer practical takeaways for practitioners and researchers, based on empirical evidence. Method: Our search strategy examined 5414 sources, found through repository searches and snowballing. Applying inclusion and exclusion criteria resulted in the selection of 102 papers, which made relevant contributions related to time pressure in software engineering. Results: The majority of high quality studies report increased productivity and decreased quality under time pressure. The most frequent categories of studies focus on quality assurance, cost estimation, and process simulation. It appears that time pressure is usually caused by errors in cost estimation. The effect of time pressure is most often identified during software quality assurance. Conclusions: The majority of empirical studies report increased productivity under time pressure, while the most cost estimation and process simulation models assume that compressing the schedule increases the total needed hours. We also find evidence of the mediating effect of knowledge on the effects of time pressure, and that tight deadlines impact tasks with an algorithmic nature more severely. Future research should better contextualize quantitative studies to account for the existing conflicting results and to provide an understanding of situations when time pressure is either beneficial or harmful.",,Information and Software Technology,2020-05-01,Review,"Kuutila, Miikka;Mäntylä, Mika;Farooq, Umar;Claes, Maëlick",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85076243410,10.1016/j.infsof.2019.106239,Generative software module development for domain-driven design with annotation-based domain specific language,"Context: Object-oriented domain-driven design (DDD) aims to iteratively develop software around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of recent work in DDD has been on using a form of annotation-based domain specific language (aDSL), internal to an object-oriented programming language, to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development. Objective: In this paper, we tackle software module development with the DDD method by adopting a generative approach that uses aDSL. To achieve this, we first extend a previous work on module-based software architecture with three enhancements that make it amenable to generative development. We then treat module configurations as first-class objects and define an aDSL, named MCCL, to express module configuration classes. To improve productivity, we define function MCCGEN to automatically generate each configuration class from the module's domain class. Method: We define our method as a refinement of an aDSL-based software development method from a previous work. We apply meta-modelling with UML/OCL to define MCCL and implement MCCL in a Java software framework. We evaluate the applicability of our method using a case study and formally define an evaluation framework for module generativity. We also analyse the correctness and performance of function MCCGEN. Results: MCCL is an aDSL for module configurations. Our evaluation shows MCCL is applicable to complex problem domains. Further, the MCCs and software modules can be generated with a high and quantifiable degree of automation. Conclusion: Our method bridges an important gap in DDD with a software module development method that uses a novel aDSL with a module-based software architecture and a generative technique for module configuration.",Attribute-oriented programming (AtOP) | Domain-driven design (DDD) | Domain-specific language (DSL) | Module-based architecture | Object-oriented programming language (OOPL) | UML-based domain modelling,Information and Software Technology,2020-04-01,Article,"Le, Duc Minh;Dang, Duc Hanh;Nguyen, Viet Ha",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85084680367,10.1016/j.infsof.2020.106320,Semantically find similar binary codes with mixed key instruction sequence,"Context: Software similarity comparison has always been a common technique for software reuse detection, plagiarism detection, and defect detection. Objective: Considering the role of API calls and arithmetic operations in software execution, a semantic-based dynamic software analysis method–mixed key instruction sequence (MKIS) is proposed. Method: MKIS embeds key value sets into a vector and constructs a novel software execution sequence that contains API calls and arithmetic operations during software execution. To determine the location of key values, a key-value equivalent matching algorithm is proposed, combined with the longest common subsequence algorithm to optimize the software execution sequence. Results: Experiments show that MKIS can accurately compare the similarity of binary programs without obtaining the software source code, and has better resiliency and credibility. Conclusion: Moreover, in the case when the software source code is changed with some main function-independent modification and code obfuscator, software reuse can be successfully detected.",Binary code similarity comparison | Code obfuscators | Dynamic software analysis | Mixed key instruction sequence | Software birthmark,Information and Software Technology,2020-09-01,Article,"Li, Yuancheng;Wang, Boyan;Hu, Baiji",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073971564,10.1016/j.infsof.2019.106196,Assisting engineers extracting requirements on components from domain documents,"Context: When entering an unfamiliar domain, organizations usually have to invest significant time and effort performing domain analysis with the purpose of acquiring system requirements. This process usually involves collecting domain documents extensively, retrieving and reviewing the related ones carefully, searching for the requirements knowledge, then extracting and specifying system requirements. Furthermore, the task must often be performed repeatedly throughout the early phases of projects. Depending on the nature of the domain and the availability of documentation, this process is extremely time-consuming and may require non-trivial human effort. Objective: In order to assist engineers identifying requirements knowledge from a collect of domain documents, previously we proposed an approach MaRK in the Conference RE’16 which ranks the domain documents by their relevance to components and highlights the content that are likely to contain component-related information. Experiments showed MaRK can almost identify the top and bottom documents in the reference list. However, it tends to underestimate the relevance of the domain documents that have a number of sections with medium knowledge density. Method: We improve the ranking algorithm in MaRK and propose MaRK-II. In addition, to assist engineers locating the relevant information in lengthy documents, we preserve the highlighting work in MaRK and strengthen MaRK-II by extracting the summary of component-related text. MaRK-II is evaluated with the documents in three domains. Results: We found that MaRK-II significantly outperforms MaRK and VSM on ranking the documents by their relevance. And a user study showed that MaRK-II is indeed helpful for engineers to extract requirements on components. Conclusions: Our approach provides three mechanisms including documents ranking, pertinent content highlighting and summarizing to help engineers obtaining requirements from a collection of domain documents.",Documents ranking | Domain reference model | Extractive text summarization | Requirements discovery,Information and Software Technology,2020-02-01,Article,"Lian, Xiaoli;Liu, Wenchuang;Zhang, Li",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85073167492,10.1016/j.infsof.2019.106194,Requirements specification for developers in agile projects: Evaluation by two industrial case studies,"Context: An inadequate requirements specification activity acts as a catalyst to other problems, such as low team productivity and difficulty in maintaining software. Although Agile Software Development (ASD) has grown in recent years, research pointed out several limitations concerning its requirements engineering activities, such as Software Requirements Specification (SRS) provided in high level and targeted to the customer, lack of information required to perform design activities and low availability of the customer. To overcome these issues, the RSD (Requirements Specification for Developers) approach was proposed to create an SRS that provides information closer to development needs. In addition, existing literature reviews identify a demand for more empirical studies on the requirements specification activity in ASD. Objective: Face to this, this work presents the evaluation of the RSD approach with respect to how it affects the teamwork and to identify its strengths and limitations. Methods: This evaluation was performed by means of two industrial case studies conducted using a multiple-case design, focusing on software engineers as the analysis unit. Data were collected during 15 months from documents, observations, and interviews. They were triangulated, analyzed, and synthesized using techniques of grounded theory. Results: The findings pointed out that the readability of SRS was compromised when several requirements are specified in the same RSD artifact. Evaluation also indicated the need of prioritization and categorization of the acceptance criteria, a tool for creating, searching and tracing the artifacts, and obtaining acceptance tests from acceptance criteria. On the other hand, the findings showed that the practices used to specify requirements using the RSD approach have the potential to produce a more objective SRS, tailored for the development team. Conclusion: As a consequence, the structure of the RSD artifact was considered as a factor that improved the team performance in the two case studies.",Agile software development | Empirical software engineering | Software requirements specification,Information and Software Technology,2020-01-01,Article,"Medeiros, Juliana;Vasconcelos, Alexandre;Silva, Carla;Goulão, Miguel",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85087337419,10.1016/j.infsof.2020.106367,PostFinder: Mining Stack Overflow posts to support software developers,"Context – During the development of complex software systems, programmers look for external resources to understand better how to use specific APIs and to get advice related to their current tasks. Stack Overflow provides developers with a broader insight into API usage as well as useful code examples. Given the circumstances, tools and techniques for mining Stack Overflow are highly desirable. Objective – In this paper, we introduce PostFinder, an approach that analyzes the project under development to extract suitable context, and allows developers to retrieve messages from Stack Overflow being relevant to the API function calls that have already been invoked. Method – PostFinder augments posts with additional data to make them more exposed to queries. On the client side, it boosts the context code with various factors to construct a query containing information needed for matching against the stored indexes. Multiple facets of the data available are used to optimize the search process, with the ultimate aim of recommending highly relevant SO posts. Results – The approach has been validated utilizing a user study involving a group of 12 developers to evaluate 500 posts for 50 contexts. Experimental results indicate the suitability of PostFinder to recommend relevant Stack Overflow posts and concurrently show that the tool outperforms a well-established baseline. Conclusions – We conclude that PostFinder can be deployed to assist developers in selecting relevant Stack Overflow posts while they are programming as well as to replace the module for searching posts in a code-to-code search engine.",Indexing posts | Mining Stack Overflow posts | Recommender systems,Information and Software Technology,2020-11-01,Article,"Rubei, Riccardo;Di Sipio, Claudio;Nguyen, Phuong T.;Di Rocco, Juri;Di Ruscio, Davide",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086145575,10.1016/j.infsof.2020.106361,Requirements elicitation methods based on interviews in comparison: A family of experiments,"Context: There are several methods to elicit requirements through interviews between an end-user and a team of software developers. The choice of the best method in this context is usually on subjective developers’ preferences instead of objective reasons. There is a lack of empirical evaluations of methods to elicit requirements that help developers to choose the most suitable one. Objective: This paper designs and conducts a family of experiments to compare three methods to elicit requirements: Unstructured Interviews, where there is no specific protocol or artifacts; Joint Application Design (JAD), where each member of the development team has a specific role; Paper Prototyping, where developers contrast the requirements with the end-user through prototypes. Method: The experiment is a between-subjects design with next response variables: number of requirements, time, diversity, completeness, quality and performance. The experiment consists of a maximum of 4 rounds of interviews between students that play the role of developers and an instructor that plays the role of client. Subjects had to elaborate a requirements specification document as results of the interviews. We recruited 167 subjects in 4 replications in 3 years. Subjects were gathered in development teams of 6 developers at most, and each team was an experimental unit. Results: We found some significant differences. Paper Prototyping yields the best results to elicit as many requirements as possible, JAD requires the highest time to report the requirements and the least overlapping, and Unstructured Interviews yields the highest overlapping and the lowest time to report the requirements. Conclusions: Paper Prototyping is the most suitable for eliciting functional requirements, JAD is the most suitable for non-functional requirements and to avoid overlapping, Unstructured Interviews is the fastest but with poor quality in the results.",Empirical software engineering | Joint application design | Prototyping | Requirements elicitation,Information and Software Technology,2020-10-01,Article,"Rueda, Silvia;Panach, Jose Ignacio;Distante, Damiano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079275079,10.1016/j.infsof.2020.106273,Detection of malicious software by analyzing the behavioral artifacts using machine learning algorithms,"Malicious software deliberately affects the computer systems. Malware are analyzed using static or dynamic analysis techniques. Using these techniques, unique patterns are extracted to detect malware correctly. In this paper, a behavior-based malware detection technique is proposed. Various runtime features are extracted by setting up a dynamic analysis environment using the Cuckoo sandbox. Three primary features are processed for developing malware classifier. Firstly, printable strings are processed word by word using text mining techniques which produced a very high dimension matrix of the string features. Then we apply the singular value decomposition technique for reducing dimensions of string features. Secondly, Shannon entropy is computed over the printable strings and API calls to consider the randomness of API and PSI features. In addition to these features, behavioral features regarding file operations, registry key modification and network activities are used in malware detection. Finally, all features are integrated in the training feature set to develop the malware classifiers using the machine learning algorithms. The proposed technique is validated with 16489 malware and 8422 benign files. Our experimental results show the accuracy of 99.54% in malware detection using ensemble machine learning algorithms. Moreover, it aims to develop a behavior-based malware detection technique of high accuracy by processing the runtime features in a new way.",Dynamic analysis | Machine learning algorithms | Malware | Random Forest | Static analysis,Information and Software Technology,2020-05-01,Article,"Singh, Jagsir;Singh, Jaswinder",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079894992,10.1016/j.infsof.2020.106290,Identifying security issues for mobile applications based on user review summarization,"Context: With the development of mobile apps, public concerns about security issues are continually rising. From the user's perspective, it is crucial to be aware of the security issues of apps. Reviews serve as an important channel for users to discover the diverse issues of apps. However, previous works rarely rely on existing reviews to provide a detailed summarization of the app's security issues. Objective: To provide a detailed overview of apps’ security issues for users, this paper introduces SRR-Miner, a novel review summarization approach that automatically summarizes security issues and users’ sentiments. Method: SRR-Miner follows a keyword-based approach to extracting security-related review sentences. It summarizes security issues and users’ sentiments with <misbehavior-aspect-opinion> triples, which makes full use of the deep analysis of sentence structures. SRR-Miner also provides visualized review summarization through a radar chart. Results: The evaluation on 17 mobile apps shows that SRR-Miner achieves higher F1-score and MCC than Machine Learning-based classification approaches in extracting security-related review sentences. It also accurately identifies misbehaviors, aspects and opinions from review sentences. A qualitative study shows that SRR-Miner outperforms two state-of-the-art approaches (AR-Miner and SUR-Miner) in terms of summarizing security issues and users’ sentiments. A further user survey indicates the usefulness of the summarization of SRR-Miner. Conclusion: SRR-Miner is capable of automatically extracting security-related review sentences based on keywords, and summarizing misbehaviors, aspects and opinions of review sentences with a deep analysis of the sentence structures.",Mobile app review summarization | Natural language processing | Security and privacy,Information and Software Technology,2020-06-01,Article,"Tao, Chuanqi;Guo, Hongjing;Huang, Zhiqiu",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85079356173,10.1016/j.infsof.2020.106277,Mining API usage scenarios from stack overflow,"Context: APIs play a central role in software development. The seminal research of Carroll et al. [15] on minimal manual and subsequent studies by Shull et al. [79] showed that developers prefer task-based API documentation instead of traditional hierarchical official documentation (e.g., Javadoc). The Q&A format in Stack Overflow offers developers an interface to ask and answer questions related to their development tasks. Objective: With a view to produce API documentation, we study automated techniques to mine API usage scenarios from Stack Overflow. Method: We propose a framework to mine API usage scenarios from Stack Overflow. Each task consists of a code example, the task description, and the reactions of developers towards the code example. First, we present an algorithm to automatically link a code example in a forum post to an API mentioned in the textual contents of the forum post. Second, we generate a natural language description of the task by summarizing the discussions around the code example. Third, we automatically associate developers reactions (i.e., positive and negative opinions) towards the code example to offer information about code quality. Results: We evaluate the algorithms using three benchmarks. We compared the algorithms against seven baselines. Our algorithms outperformed each baseline. We developed an online tool by automatically mining API usage scenarios from Stack Overflow. A user study of 31 software developers shows that the participants preferred the mined usage scenarios in Opiner over API official documentation. The tool is available online at: http://opiner.polymtl.ca/. Conclusion: With a view to produce API documentation, we propose a framework to automatically mine API usage scenarios from Stack Overflow, supported by three novel algorithms. We evaluated the algorithms against a total of eight state of the art baselines. We implement and deploy the framework in our proof-of-concept online tool, Opiner.",API | Documentation | Mining | Usage,Information and Software Technology,2020-06-01,Article,"Uddin, Gias;Khomh, Foutse;Roy, Chanchal K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85082699535,10.1016/j.infsof.2020.106311,Demystifying the adoption of behavior-driven development in open source projects,"Context:Behavior-Driven Development (BDD) features the capability, through appropriate domain-specific languages, of specifying acceptance test cases and making them executable. The availability of frameworks such as Cucumber or RSpec makes the application of BDD possible in practice. However, it is unclear to what extent developers use such frameworks, and whether they use them for actually performing BDD, or, instead, for other purposes such as unit testing. Objective:In this paper, we conduct an empirical investigation about the use of BDD tools in open source, and how, when a BDD tool is in place, BDD specifications co-evolve with source code. Method:Our investigation includes three different phases: (i) a large-scale analysis to understand the extent to which BDD frameworks are used in 50,000 popular open-source projects written in five programming languages; (ii) a study on the co-evolution of scenarios, fixtures and production code in a sample of 20 Ruby projects, through the Granger's causality test, and (iii) a survey with 31 developers to understand how they use BDD frameworks. Results:Results of the study indicate that ≃ 27% of the sampled projects use BDD frameworks, with a prevalence in Ruby projects (68%). In about 37% of the cases, we found a co-evolution between scenarios/fixtures and production code. Specifically, changes to scenarios and fixtures often happen together or after changes to source code. Moreover, survey respondents indicate that, while they understand the intended purpose of BDD frameworks, most of them write tests while/after coding rather than strictly applying BDD. Conclusions:Even if the BDD frameworks usage is widespread among open source projects, in many cases they are used for different purposes such as unit testing activities. This mainly happens because developers felt BDD remains quite effort-prone, and its application goes beyond the simple adoption of a BDD framework.",Acceptance testing | Behavior-driven development | Co-evolution | Empirical study,Information and Software Technology,2020-07-01,Article,"Zampetti, Fiorella;Di Sorbo, Andrea;Visaggio, Corrado Aaron;Canfora, Gerardo;Di Penta, Massimiliano",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85093089917,10.1109/ICCSE49874.2020.9201846,Automated summarization of bug reports to speed-up software development/maintenance process by using natural language processing (NLP),"Bug reports can provide a great deal of assistance for developers during the process of development. But due to the large size of bug repositories, it is sometimes difficult to take advantage of these artifacts in the available time. One way of helping developers to provide summaries of these reports and provide relevant details only. Once it's decided that this is the required report then one can study the details. As text mining technology advances, many substantial approaches have been proposed to generate optimized summaries for bug reports. In this paper, we have proposed an extractive based methodology for the generation of summaries of bug reports by using the sentence embedding. We achieved improved rouge-1 and rouge- 2 results than the previous state of the art systems for the bug report summary generation.",Bug reports | Machine Learning | Natural Language Processing | Software Artifacts | Summarization,"15th International Conference on Computer Science and Education, ICCSE 2020",2020-08-01,Conference Paper,"Nawaz Tarar, M. Irtaza;Ahmed, Faizan;Butt, Wasi Haider",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85102349293,10.1109/APSEC51365.2020.00027,A knowledge graph-based sensitive feature selection for android malware classification,"The rapid increase in Android malware has brought great challenges to malware analysis. To deal with such a severe situation, it has been proposed an effective way which groups malware with common behaviors into the same malware family. Although there are many methods for malware family classification, the most critical and primary step is always the definition of sensitive behavior in an application, which will be beneficial for the later classification task. Much existing literature has manually selected sensitive features, such as permission, or even designed graph-based features via the control flow graph. They heavily depend on expert knowledge and time-consuming malware application analysis, which means it has to focus on the mal ware itself to dig out valuable security knowledge at first. However, the zooming malware overwhelms such expensive feature definition methods. To overcome such a problem, we adopt a knowledge graph-based sensitive feature selection method for Android mal ware classification. Based on the Android Developer documentation, an Android API knowledge graph is constructed at first. We can obtain not only permission but also related critical API from this graph. Note that both hyperlink relation and similarity relation are used to find out the critical API. With the knowledge graph-based sensitive features, we represent each Android malware as a boolean feature vector and send it in to a machine learning classifier for malware classification. We evaluate our proposed methods on three well-known Android malware datasets, such as Genome, Drebin, and AMD. The experimental results show that: 1) our proposed sensitive API is advantageous for malware detection; 2) API chosen by similarity relation can marginally improve performance; 3) different permission groups also make an influence for classification.",Android malware family | Knowledge graph | Machine learning,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2020-12-01,Conference Paper,"Ma, Duoyuan;Bai, Yude;Xing, Zhenchang;Sun, Lintan;Li, Xiaohong",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85096567372,10.1109/SEAA51224.2020.00069,"Prevalence, Contents and Automatic Detection of KL-SATD","When developers use different keywords such as TODO and FIXME in source code comments to describe self-admitted technical debt (SATD), we refer it as Keyword-Labeled SATD (KL-SATD). We study KL-SATD from 33 software repositories with 13,588 KL-SATD comments. We find that the median percentage of KL-SATD comments among all comments is only 1,52%. We find that KL-SATD comment contents include words expressing code changes and uncertainty, such as remove, fix, maybe and probably. This makes them different compared to other comments. KL-SATD comment contents are similar to manually labeled SATD comments of prior work. Our machine learning classifier using logistic Lasso regression has good performance in detecting KL-SATD comments (AUC-ROC 0.88). Finally, we demonstrate that using machine learning we can identify comments that are currently missing but which should have a SATD keyword in them. Automating SATD identification of comments that lack SATD keywords can save time and effort by replacing manual identification of comments. Using KL-SATD offers a potential to bootstrap a complete SATD detector.",data mining | Natural language processing | self-admitted technical debt,"Proceedings - 46th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2020",2020-08-01,Conference Paper,"Rantala, Leevi;Mantyla, Mika;Lo, David",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85096553178,10.1109/SEAA51224.2020.00021,A Data-Driven Approach to Measure the Usability of Web APIs,"Application Programming Interfaces (APIs) are means of communication between applications, hence they can be seen as user interfaces, just with different kind of users, i.e., software or computers. However, the very first consumers of the APIs are humans, namely programmers. Based on the available documentation and the ""ease of use""perception (sometimes led by corporate decisions and/or restrictions) they decide to use or not a specific API. In this paper, we propose a data-driven approach to measure web API usability, expressed through the predicted error rate. Following the reviewed state of the art in API usability, we identify a set of usability attributes, and for each of them we propose indicators that web API providers should refer to when developing usable web APIs. Our focus in this paper is on those indicators that can be quantified using the API logs, which indeed reflect the actual behaviour of programmers. Next, we define metrics for the aforementioned indicators, and exemplify them in our use case, applying them on the logs from the web API of District Health Information System (DHIS2) used at World Health Organization (WHO). Using these metrics as features, we build a classifier model to predict the error rate of API endpoints. Besides finding usability issues, we also drill down into the usage logs and investigate the potential causes of these errors.",API logs | API usability | log mining | web API,"Proceedings - 46th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2020",2020-08-01,Conference Paper,"Koci, Rediana;Franch, Xavier;Jovanovic, Petar;Abello, Alberto",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85096569317,10.1109/SEAA51224.2020.00025,How agile software development practitioners perceive the need for documenting quality requirements: A multiple case study,"Agile software development (ASD) promotes minimal documentation and often prioritizes functional requirements over quality requirements (QRs). This may be beneficial in reducing the time to market of software. When considering QRs in ASD, the minimal documentation practice may be seen as a concern since QRs determine the success of software projects and are as well not easy to specify and document. Nevertheless, what do practitioners think of the necessity of documenting QRs in ASD? How do they perceive factors that may affect documentation of QRs in ASD? We conducted a multiple case study of three cases applying ASD, involving 12 participants. ASD practitioners identify that it is important to document QRs, and perceive that it contributes to ensuring quality, clarifying QRs, and helping in decision making. Time constraint, QR awareness and communication gaps on QRs influence the documentation of QRs in ASD. ASD teams may align their documentation practices to fit the sprint duration. The influence of QR awareness on documentation was dependent on project context and roles. Communication gaps can create confusion on QRs. Missing and outdated QR documentation may result in accruing technical debt, and lack of common understanding on QRs. The study synthesizes empirical evidence on the significance of documenting QRs in ASD and provides an insight into factors affecting documentation of QRs in ASD.",agile software development | documentation | non-functional requriements | quality requirements,"Proceedings - 46th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2020",2020-08-01,Conference Paper,"Behutiye, Woubshet;Rodriguez, Pilar;Oivo, Markku;Aaramaa, Sanja;Partanen, Jari;Abherve, Antonin",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85083567063,10.1109/SANER48275.2020.9054848,Studying Developer Reading Behavior on Stack Overflow during API Summarization Tasks,"Stack Overflow is commonly used by software developers to help solve problems they face while working on software tasks such as fixing bugs or building new features. Recent research has explored how the content of Stack Overflow posts affects attraction and how the reputation of users attracts more visitors. However, there is very little evidence on the effect that visual attractors and content quantity have on directing gaze toward parts of a post, and which parts hold the attention of a user longer. Moreover, little is known about how these attractors help developers (students and professionals) answer comprehension questions. This paper presents an eye tracking study on thirty developers constrained to reading only Stack Overflow posts while summarizing four open source methods or classes. Results indicate that on average paragraphs and code snippets were fixated upon most often and longest. When ranking pages by number of appearance of code blocks and paragraphs, we found that while the presence of more code blocks did not affect number of fixations, the presence of increasing numbers of plain text paragraphs significantly drove down the fixations on comments. SO posts that were looked at only by students had longer fixation times on code elements within the first ten fixations. We found that 16 developer summaries contained 5 or more meaningful terms from SO posts they viewed. We discuss how our observations of reading behavior could benefit how users structure their posts.",API summarization | controlled experiment | eye tracking | reading behavior | Stack Overflow,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",2020-02-01,Conference Paper,"Saddler, Jonathan A.;Peterson, Cole S.;Sama, Sanjana;Nagaraj, Shruthi;Baysal, Olga;Guerrouj, Latifa;Sharif, Bonita",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85083572364,10.1109/SANER48275.2020.9054854,JavaScript API Deprecation in the Wild: A First Assessment,"Building an application using third-party libraries is a common practice in software development. As any other software system, code libraries and their APIs evolve over time. In order to help version migration and ensure backward compatibility, a recommended practice during development is to deprecate API. Although studies have been conducted to investigate deprecation in some programming languages, such as Java and C#, there are no detailed studies on API deprecation in the JavaScript ecosystem. This paper provides an initial assessment of API deprecation in JavaScript by analyzing 50 popular software projects. Initial results suggest that the use of deprecation mechanisms in JavaScript packages is low. However, we find five different ways that developers use to deprecate API in the studied projects. Among these solutions, deprecation utility (i.e., any sort of function specially written to aid deprecation) and code comments are the most common practices in JavaScript. Finally, we find that the rate of helpful message is high: 67% of the deprecations have replacement messages to support developers when migrating APIs.",API deprecation | JavaScript | Software Library,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",2020-02-01,Conference Paper,"Nascimento, Romulo;Brito, Aline;Hora, Andre;Figueiredo, Eduardo",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85083579151,10.1109/SANER48275.2020.9054838,Leveraging Machine Learning for Software Redocumentation,"Source code comments contain key information about the underlying software system. Many redocumentation approaches, however, cannot exploit this valuable source of information. This is mainly due to the fact that not all comments have the same goals and target audience and can therefore only be used selectively for redocumentation. Performing a required classification manually, e.g. in the form of heuristic rules, is usually time-consuming and error-prone and strongly dependent on programming languages and guidelines of concrete software systems. By leveraging machine learning, it should be possible to classify comments and thus transfer valuable information from the source code into documentation with less effort but the same quality. We applied different machine learning techniques to a COBOL legacy system and compared the results with industry-strength heuristic classification. As a result, we found that machine learning outperforms the heuristics in number of errors and less effort.",CNNs | comment classification pipeline | heuristic rules | legacy system | machine learning | NLP | software redocumentation,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",2020-02-01,Conference Paper,"Geist, Verena;Moser, Michael;Pichler, Josef;Beyer, Stefanie;Pinzger, Martin",Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85083582705,10.1109/SANER48275.2020.9054789,Automatically Extracting Subroutine Summary Descriptions from Unstructured Comments,"Summary descriptions of subroutines are short (usually one-sentence) natural language explanations of a subroutine's behavior and purpose in a program. These summaries are ubiquitous in documentation, and many tools such as JavaDocs and Doxygen generate documentation built around them. And yet, extracting summaries from unstructured source code repositories remains a difficult research problem - it is very difficult to generate clean structured documentation unless the summaries are annotated by programmers. This becomes a problem in large repositories of legacy code, since it is cost prohibitive to retroactively annotate summaries in dozens or hundreds of old programs. Likewise, it is a problem for creators of automatic documentation generation algorithms, since these algorithms usually must learn from large annotated datasets, which do not exist for many programming languages. In this paper, we present a semi-automated approach via crowdsourcing and a fully-automated approach for annotating summaries from unstructured code comments. We present experiments validating the approaches, and provide recommendations and cost estimates for automatically annotating large repositories.",code comment extraction | crowdsourcing | mining software repositories | summarization,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",2020-02-01,Conference Paper,"Eberhart, Zachary;Leclair, Alexander;McMillan, Collin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85093959529,10.1109/VL/HCC50065.2020.9127274,Exploring Programmers' API Learning Processes: Collecting Web Resources as External Memory,"Modern programming frequently requires the use of APIs (Application Programming Interfaces). Yet many programmers struggle when trying to learn APIs. We ran an exploratory study in which we observed participants performing an API learning task. We analyze their processes using a proposed model of API learning, grounded in Cognitive Load Theory, Information Foraging Theory, and External Memory research. The results provide support for the model of API Learning and add new insights into the form and usage of external memory while learning APIs. Programmers quickly curated a set of API resources through Information Foraging which served as external memory and then primarily referred to these resources to meet information needs while coding.",APIs | cognitive load | external memory | information foraging | learning | programming,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2020-08-01,Conference Paper,"Gao, Gao;Voichick, Finn;Ichinco, Michelle;Kelleher, Caitlin",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85093963702,10.1109/VL/HCC50065.2020.9127264,Supporting Code Comprehension via Annotations: Right Information at the Right Time and Place,"Code comprehension, especially understanding relationships across project elements (code, documentation, etc.), is non-trivial when information is spread across different interfaces and tools. Bringing the right amount of information, to the place where it is relevant and when it is needed can help reduce the costs of seeking information and creating mental models of the code relationships. While non-traditional IDEs have tried to mitigate these costs by allowing users to spatially place relevant information together, thus far, no study has examined the effects of these non-traditional interactions on code comprehension. Here, we present an empirical study to investigate how the right information at the right time and right place allows users-especially newcomers-to reduce the costs of code comprehension. We use a non-traditional IDE, called Synectic, and implement link-able annotations which provide affordances for the accuracy, time, and space dimensions. We conducted a between-subjects user study of 22 newcomers performing code comprehension tasks using either Synectic or a traditional IDE, Eclipse. We found that having the right information at the right time and place leads to increased accuracy and reduced cognitive load during code comprehension tasks, without sacrificing the usability of developer tools.",annotation | Code comprehension | information foraging | Integrated development environment (IDE) | user study,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC",2020-08-01,Conference Paper,"Adeli, Marjan;Nelson, Nicholas;Chattopadhyay, Souti;Coffey, Hayden;Henley, Austin;Sarma, Anita",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85093833659,10.1109/IJCNN48605.2020.9207651,A Topic Modeling Approach to Evaluate the Comments Consistency to Source Code,"A significant amount of source code in software systems is made up of comments, parts of the code that are ignored by the compiler. Comments in the code are a primary source for system documentation. These are crucial for the work of software maintainers, as a basis for code traceability, for maintenance activities, but also for the use of the code itself as a library or framework in other projects. Although many software developers consider comments important, existing approaches to software quality analysis mainly disregard code comments and focus only on source code. This paper presents an approach, based on topic modeling, for analyzing the comments consistency to the source code. A model was provided to analyze the quality of comments in terms of consistency since comments should be consistent with the source code they refer to. The results show a similarity in the trend of topic distribution and it emerges that almost all classes are associated with no more than 3 topics.",comment | Natural Language Toolkit | topic modeling,Proceedings of the International Joint Conference on Neural Networks,2020-07-01,Conference Paper,"Iammarino, Martina;Aversano, Lerina;Bernardi, Mario Luca;Cimitile, Marta",Include,
10.1016/j.jss.2022.111515,2-s2.0-85092023296,10.22360/SpringSim.2020.HSAA.007,How do Modelers Code Artificial Societies? Investigating Practices and Quality of Netlogo Codes from Large Repositories,"Many guidelines have been developed for simulations in general or for agent-based models which support artificial societies. When applying such guidelines to examine existing practices, assessment studies are limited by the artifacts released by modelers. Although code is the final product defining an artificial society, 90% of the code produced is not released hence previous assessments necessarily focused on higher-level items such as conceptual design or validation. We address this gap by collecting 338 artificial societies from two hosting platforms, CoMSES/OpenABM and GitHub. An innovation of our approach is the use of software engineering techniques to automatically examine the models with respect to items such as commenting the code, using libraries, or dividing code into functions. We found that developers of artificial societies code the decision-making of their agents from scratch in every model, despite the existence of several libraries that could be used as building blocks.",Agent-Based Model | GitHub | Model Quality | NetLogo | Software Repository Mining,"Proceedings of the 2020 Spring Simulation Conference, SpringSim 2020",2020-05-01,Conference Paper,"Vendome, Christopher;Rao, Dhananjai M.;Giabbanelli, Philippe J.",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,,10.18653/v1/2020.acl-main.168,,,,,,,,Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85116272432,10.18653/v1/2020.acl-main.449,A transformer-based approach for source code summarization,"Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.",,Proceedings of the Annual Meeting of the Association for Computational Linguistics,2020-01-01,Conference Paper,"Ahmad, Wasi Uddin;Chakraborty, Saikat;Ray, Baishakhi;Chang, Kai Wei",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85079762032,10.1007/978-981-15-2930-6_3,Comment-Mine—A Semantic Search Approach to Program Comprehension from Code Comments,"Annotating programs with natural language comments is a common programming practice to increase the readability of code. While researchers have attempted specific tasks like comment quality analysis or classification, there is absence of an integrated knowledge representation based on comments to aid program comprehension. We propose Comment-Mine a semantic search architecture, which extracts knowledge related to design, implementation and evolution of a software in the form of a knowledge graph. This supports program comprehension and various comment analysis tasks. We manually annotate concepts for 5600 comments extracted from 672 C/C++ files/projects crawled from code repositories like GitHub. Comment-Mine extracts 38,992 concepts, out of which 79.8% is correct and validated using manual annotation.",Intelligent search | Natural language processing | Program comprehension | Semantic graph | Static instrumentation,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Majumdar, Srijoni;Papdeja, Shakti;Das, Partha Pratim;Ghosh, Soumya Kanti",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85082996934,10.1007/978-3-030-44289-7_49,What is the Message About? Automatic Multi-label Classification of Open Source Repository Messages into Content Types,"Users of Open Source Software (OSS) projects discuss a diverse range of topics online. The content of a post often corresponds to one or more context-sensitive content types, e.g. a suggestion for a solution, a request for further clarification or indication that a proposed solution did not work. The detection of content types can provide several benefits for software developers. For instance, content types can be used as indicators that summarise the content of the messages. These indicators can be exploited as part of a developer-centric knowledge mining platform allowing developers and project managers to create action alerts concerning new bugs found outside of a bug tracker or they can be combined with other metrics to assess the quality of an OSS project. We present a multi-label classifier, able to classify messages exchanged on communication means about OSS, and detailed evaluation results. We experimented with two state-of-the-art multi-label classification approaches HOMER (Hierarchy Of Multilabel classifiER) and RAkEL (RAndom k-labELsets) as these met the technical requirements of the CROSSMINER project. A manually-annotated threaded corpus of posts form newsgroups discussions, bug tracking systems and forums related to Eclipse projects was also used. The results are promising and indicate the potential to attract novel and deeper research for this task.",Content classification | Information retrieval | Machine learning | Multi-label classification | Natural language | Open Source Software | Text mining,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Campbell, Daniel;Cabrera-Diego, Luis Adrián;Korkontzelos, Yannis",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85096184938,10.1145/3372297.3423360,RTFM! Automatic Assumption Discovery and Verification Derivation from Library Document for API Misuse Detection,"To use library APIs, a developer is supposed to follow guidance and respect some constraints, which we call integration assumptions (IAs). Violations of these assumptions can have serious consequences, introducing security-critical flaws such as use-after-free, NULL-dereference, and authentication errors. Analyzing a program for compliance with IAs involves significant effort and needs to be automated. A promising direction is to automatically recover IAs from a library document using Natural Language Processing (NLP) and then verify their consistency with the ways APIs are used in a program through code analysis. However, a practical solution along this line needs to overcome several key challenges, particularly the discovery of IAs from loosely formatted documents and interpretation of their informal descriptions to identify complicated constraints (e.g., data-/control-flow relations between different APIs). In this paper, we present a new technique for automated assumption discovery and verification derivation from library documents. Our approach, called Advance, utilizes a suite of innovations to address those challenges. More specifically, we leverage the observation that IAs tend to express a strong sentiment in emphasizing the importance of a constraint, particularly those security-critical, and utilize a new sentiment analysis model to accurately recover them from loosely formatted documents. These IAs are further processed to identify hidden references to APIs and parameters, through an embedding model, to identify the information-flow relations expected to be followed. Then our approach runs frequent subtree mining to discover the grammatical units in IA sentences that tend to indicate some categories of constraints that could have security implications. These components are mapped to verification code snippets organized in line with the IA sentence's grammatical structure, and can be assembled into verification code executed through CodeQL to discover misuses inside a program. We implemented this design and evaluated it on 5 popular libraries (OpenSSL, SQLite, libpcap, libdbus and libxml2) and 39 real-world applications. Our analysis discovered 193 API misuses, including 139 flaws never reported before.",API misuse | documentation analysis | integration assumption | verification code generation,Proceedings of the ACM Conference on Computer and Communications Security,2020-10-30,Conference Paper,"Lv, Tao;Li, Ruishi;Yang, Yi;Chen, Kai;Liao, Xiaojing;Wang, Xiao Feng;Hu, Peiwei;Xing, Luyi",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85090201033,10.1145/3334480.3383008,API as curriculum: Designing high-level api affordances as instructional scaffolds,"APIs have been recognized in the CHI community and beyond as designed objects worthy of usability analysis. Some work in this vein has investigated the learnability of APIs in particular. Drawing on activity theory, we argue that APIs can also potentially have broader learning consequences for their users. By mediating interactions with code, APIs can shape their users' understanding of computing problems. We thus suggest that, by envisioning high-level API affordances as scaffolds, API designers can not only enhance users' productivity, but they can also help drive adoption of software components attempting to radically innovate on their past inheritance. We propose scaffold design recommendations that can augment existing API usability frameworks.",API usability | Application programming interface | Computing education | Data-oriented design,Conference on Human Factors in Computing Systems - Proceedings,2020-04-25,Conference Paper,"Mechtley, Adam",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85090209842,10.1145/3334480.3383051,Experiences with improving the transparency of AI models and services,"AI models and services are used in a growing number of high-stakes areas, resulting in a need for increased transparency. Consistent with this, several proposals for higher quality and more consistent documentation of AI data, models, and systems have emerged. Little is known, however, about the needs of those who would produce or consume these new forms of documentation. Through semi-structured developer interviews, and two document-creation exercises, we have assembled a clearer picture of these needs and the various challenges faced in creating accurate and useful AI documentation. Based on the observations from this work, supplemented by feedback received during multiple design explorations and stakeholder conversations, we make recommendations for easing the collection and flexible presentation of AI facts to promote transparency.",AI governance | AI transparency | Documentation | Factsheets,Conference on Human Factors in Computing Systems - Proceedings,2020-04-25,Conference Paper,"Hind, Michael;Houde, Stephanie;Martino, Jacquelyn;Mojsilovic, Aleksandra;Piorkowski, David;Richards, John;Varshney, Kush R.",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85090246000,10.1145/3334480.3383069,Learning rust: How experienced programmers leverage resources to learn a new programming language,"Experienced programmers are capable of learning new programming languages independently using various available resources, but we lack a comprehensive understanding of which resources they find most valuable in doing so. In this paper, we study how experienced programmers learn Rust, a systems programming language with extensive documentation and example code, an active online community, and descriptive compiler errors. We develop a task that requires learning Rust syntax and comprehending the Rust-specific approach to mutability and ownership. Our results show that users spend 43% of online time viewing example code and that programmers appreciate in-line compiler errors, choosing to refresh, on average, every 30.6 seconds after first discovering this feature. The average time between these refreshes predicted total task time, but individual resource choices did not. Based on our findings we offer design implications for language and IDE developers.",Computer science education | Learning resources | Programming languages | Rust,Conference on Human Factors in Computing Systems - Proceedings,2020-04-25,Conference Paper,"Abtahi, Parastoo;Dietz, Griffin",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85092090239,10.1007/978-3-030-59410-7_25,From Code to Natural Language: Type-Aware Sketch-Based Seq2Seq Learning,"Code comment generation aims to translate existing source code into natural language explanations. It provides an easy-to-understand description for developers who are unfamiliar with the functionality of source code. Existing approaches to code comment generation focus on summarizing multiple lines of code with a short text, but often cannot effectively explain a single line of code. In this paper, we propose an asynchronous learning model, which learns the code semantics and generates a fine-grained natural language explanation for each line of code. Different from a coarse-grained code comment generation, this fine-grained explanation can help developers better understand the functionality line-by-line. The proposed model adopts a type-aware sketch-based sequence-to-sequence learning method to generate natural language explanations for source code. This method incorporates the type of source code and the mask mechanism with the Long Short Term Memory (LSTM) network via encoding and decoding phases. We empirically compare the proposed model with state-of-the-art approaches on real data sets of source code and description in Python. Experimental results demonstrate that our model can outperform existing approaches on commonly used metrics for neural machine translation.",Attention mechanism | Code comment generation | Sketch,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Deng, Yuhang;Huang, Hao;Chen, Xu;Liu, Zuopeng;Wu, Sai;Xuan, Jifeng;Li, Zongpeng",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85092117204,10.1007/978-3-030-59419-0_6,Code2Text: Dual Attention Syntax Annotation Networks for Structure-Aware Code Translation,"Translating source code into natural language text helps people understand the computer program better and faster. Previous code translation methods mainly exploit human specified syntax rules. Since handcrafted syntax rules are expensive to obtain and not always available, a PL-independent automatic code translation method is much more desired. However, existing sequence translation methods generally regard source text as a plain sequence, which is not competent to capture the rich hierarchical characteristics inherently reside in the code. In this work, we exploit the abstract syntax tree (AST) that summarizes the hierarchical information of a code snippet to build a structure-aware code translation method. We propose a syntax annotation network called Code2Text to incorporate both source code and its AST into the translation. Our Code2Text features the dual encoders for the sequential input (code) and the structural input (AST) respectively. We also propose a novel dual-attention mechanism to guide the decoding process by accurately aligning the output words with both the tokens in the source code and the nodes in the AST. Experiments on a public collection of Python code demonstrate that Code2Text achieves better performance compared to several state-of-the-art methods, and the generation of Code2Text is accurate and human-readable.",Abstract syntax tree | Data mining | Natural language generation | Tree-LSTM,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Xiong, Yun;Xu, Shaofeng;Rong, Keyao;Liu, Xinyue;Kong, Xiangnan;Li, Shanshan;Yu, Philip;Zhu, Yangyong",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85123040571,10.1145/3424771.3424786,A Review of Pattern Languages for Software Documentation,"Software documentation is an important part of the captured knowledge of a software project and documentation patterns have often been used as a systematic way to describe good practices on software documentation. Still, many software teams are challenged by what to document, how to keep the documentation consistent and how to make their consumers aware of the relevant documents. A literature review was done over 14 publications and identified 16 quality attributes and 114 patterns about software documentation. This knowledge was analysed and classified and led to the proposal of new categories and relationships between the existing patterns. These are depicted as a new pattern map that provides a new perspective of documentation patterns and can be used to guide teams in adopting software documentation practices.",documentation engineering | documentation issues | documentation patterns | quality attributes | software documentation,ACM International Conference Proceeding Series,2020-07-01,Conference Paper,"Santos, João;Correia, Filipe Figueiredo",Exclude,A survey paper
10.1016/j.jss.2022.111515,2-s2.0-85103469918,10.3389/frai.2020.00013,Designing and Evaluating the Usability of a Machine Learning API for Rapid Prototyping Music Technology,"To better support creative software developers and music technologists' needs, and to empower them as machine learning users and innovators, the usability of and developer experience with machine learning tools must be considered and better understood. We review background research on the design and evaluation of application programming interfaces (APIs), with a focus on the domain of machine learning for music technology software development. We present the design rationale for the RAPID-MIX API, an easy-to-use API for rapid prototyping with interactive machine learning, and a usability evaluation study with software developers of music technology. A cognitive dimensions questionnaire was designed and delivered to a group of 12 participants who used the RAPID-MIX API in their software projects, including people who developed systems for personal use and professionals developing software products for music and creative technology companies. The results from questionnaire indicate that participants found the RAPID-MIX API a machine learning API which is easy to learn and use, fun, and good for rapid prototyping with interactive machine learning. Based on these findings, we present an analysis and characterization of the RAPID-MIX API based on the cognitive dimensions framework, and discuss its design trade-offs and usability issues. We use these insights and our design experience to provide design recommendations for ML APIs for rapid prototyping of music technology. We conclude with a summary of the main insights, a discussion of the merits and challenges of the application of the CDs framework to the evaluation of machine learning APIs, and directions to future work which our research deems valuable.",application programming interfaces | cognitive dimensions | interactive machine learning | music technology | user-centered design,Frontiers in Artificial Intelligence,2020-04-03,Article,"Bernardo, Francisco;Zbyszyński, Michael;Grierson, Mick;Fiebrink, Rebecca",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85080882203,10.1007/978-3-030-40690-5_30,Users preferences regarding types of help: Different contexts comparison,"Despite the significant advance in usability and intuitiveness of software interfaces, users still use help systems to seek solutions concerning doubts, whether they are beginners or advanced in the use of the software. Therefore, entirely intuitive, self-communicating interfaces that, regardless of task and user, do not require help systems, still do not exist. This study, as an exploratory type, aimed to verify if users refer to types of help significantly at different levels of intensity and preference when compared an initial user contact with software to a specific moment of doubt concerning any tool or task after being familiar. Thus, a user preference study was conducted by applying a questionnaire to 104 students of Software Engineering undergraduate course at the University of Brasília, Brazil. After that, the results from the two contexts were compared. Analysis of the data suggested that users refer more to types of help at an initial contact with software, and usually, they do not search directly for help documentation, referring more to search engines.",Help documentation | Help systems | User preferences,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Palmeira, Eduardo G.Q.;Roche-Lima, Abiel;de Sales, André B.",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85087033726,10.1007/978-3-030-50578-3_28,Applying natural language processing techniques to generate open data web apis documentation,"Information access globalisation has resulted in the continuous growing of online available data on the Web, especially open data portals. However, in current open data portals, data is difficult to understand and access. One of the reasons of such difficulty is the lack of suitable mechanisms to extract and learn valuable information from existing open data, such as Web Application Programming Interfaces (APIs) with proper documentation. Actually, in most cases, open data Web APIs documentation is very rudimentary, hard to follow, and sometimes incomplete or even inaccurate. To solve these data management problems, this paper proposes an approach to automatically generate Web API’s documentation which is both machine and user readable. Our approach consists of applying natural language processing techniques to create OpenAPI documentations. This manner, the access to data is facilitated because of the improvement on the comprehension of the APIs, thus promoting the reusability of data. The feasibility of our approach is presented through a case study that shows and compares the benefits of using our OpenAPI documentation process within an open data web API.",Natural Language Generation | Natural Language Processing | Open data Web API,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"González-Mora, César;Barros, Cristina;Garrigós, Irene;Zubcoff, Jose;Lloret, Elena;Mazón, Jose Norberto",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85086715634,10.1109/TCSS.2020.2999574,Specification Patterns of Service-Based Applications Using Blockchain Technology,"With the fast development of information technologies, traditional value-added chain business models are shifting to service-based applications (SBAs) to cope with ever-changing user behaviors and the modern social system. An SBA allows an organization to utilize external and distributed resources to achieve its business goals, especially when the Internet of Thing (IoT) will soon become mainstream. However, the development of SBAs is at its early stage with a number of unsolved issues, such as the availability of effective methods for services selection and composition, semantic representation of specifications, and security assurances. This article aims to address two main issues in SBAs: 1) the standardization of formulation and 2) the security assurance. A systematic method is proposed to formulate the specifications of services, the prevalent blockchain technology (BCT) is adopted to enable SBAs, and the proposed specification patterns and BCT are integrated as a BCT-based quality-of-service (QoS) framework to support service selections and workflow compositions.",Blockchain | Internet of Thing (IoT) | quality of service (QoS) | security assurance | service composition | service selection | service-based application (SBA) | smart contract | specification patterns,IEEE Transactions on Computational Social Systems,2020-08-01,Article,"Viriyasitavat, Wattana;Xu, Li Da;Bi, Zhuming",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85081662478,10.1109/TR.2019.2931725,Learning Code Context Information to Predict Comment Locations,"Code commenting is a common programming practice of practical importance to help developers review and comprehend source code. In our developer survey, commenting has become an important, yet often-neglected activity when programming. Moreover, there is a lack of formal and automatic way in current practice to remind developers where to comment in the source code. To provide informative guidance on commenting during development, we propose a novel method CommentSuggester to recommend developers regarding appropriate commenting locations in the source code. Because commenting is closely related to the context information of source code, we identify this important factor to determine comment positions and extract it as structural context features, syntactic context features, and semantic context features. Subsequently, machine learning techniques are applied to identify possible commenting locations in the source code. We evaluated CommentSuggester using large datasets from dozens of open-source software systems in GitHub. The encouraging experimental results and user study demonstrated the feasibility and effectiveness of our commenting suggestion method.",Code context information | code features extraction | comment location | comment quality | commenting decision,IEEE Transactions on Reliability,2020-03-01,Article,"Huang, Yuan;Hu, Xinyu;Jia, Nan;Chen, Xiangping;Xiong, Yingfei;Zheng, Zibin",Exclude,
10.1016/j.jss.2022.111515,2-s2.0-85097255326,10.1007/s11390-020-0496-0,Learning Human-Written Commit Messages to Document Code Changes,"Commit messages are important complementary information used in understanding code changes. To address message scarcity, some work is proposed for automatically generating commit messages. However, most of these approaches focus on generating summary of the changed software entities at the superficial level, without considering the intent behind the code changes (e.g., the existing approaches cannot generate such message: “fixing null pointer exception”). Considering developers often describe the intent behind the code change when writing the messages, we propose ChangeDoc, an approach to reuse existing messages in version control systems for automatical commit message generation. Our approach includes syntax, semantic, pre-syntax, and pre-semantic similarities. For a given commit without messages, it is able to discover its most similar past commit from a large commit repository, and recommend its message as the message of the given commit. Our repository contains half a million commits that were collected from SourceForge. We evaluate our approach on the commits from 10 projects. The results show that 21.5% of the recommended messages by ChangeDoc can be directly used without modification, and 62.8% require minor modifications. In order to evaluate the quality of the commit messages recommended by ChangeDoc, we performed two empirical studies involving a total of 40 participants (10 professional developers and 30 students). The results indicate that the recommended messages are very good approximations of the ones written by developers and often include important intent information that is not included in the messages generated by other tools.",code change comprehension | code semantic similarity | code syntax similarity | commit message recommendation,Journal of Computer Science and Technology,2020-11-01,Article,"Huang, Yuan;Jia, Nan;Zhou, Hao Jie;Chen, Xiang Ping;Zheng, Zi Bin;Tang, Ming Dong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.21105/joss.01960,,,,,,,,Exclude,Not from technical track
10.1016/j.jss.2022.111515,2-s2.0-85083023663,10.1145/3341105.3373972,Comparing identifiers and comments in engineered and non-engineered code: A large-scale empirical study,"Identifiers and comments are important sources of information to understand software. Approximately 70% of the code of a system consists of identifiers [1]. Identifier length and quality, as well as naming conventions overall, influence the difficulty and time required to complete maintenance tasks like code reading or debugging [2 - 4]. After the code itself, comments are the main source of documentation for software maintainers [5], and experiments show that commented code is easier to understand when compared to uncommented code [6]. Therefore these elements are of paramount importance for software development in general.",,Proceedings of the ACM Symposium on Applied Computing,2020-03-30,Conference Paper,"Lemos, Otávio Augusto Lazzarini;Suzuki, Marcelo;De Paula, Adriano Carvalho;Goes, Claire Le",Include,
10.1016/j.jss.2022.111515,2-s2.0-85083040700,10.1145/3341105.3374009,Is there a correlation between code comments and issues?: An exploratory study,"Comments in a software code base are one of the key artifacts that help developers in understanding the code with respect to development and maintenance. Comments provide us with the information that is used as a software metric to assess the code quality and which further can be applied to demonstrate its impact on the issues in the code. In this paper, we set out to understand the correlation between code comments and issues in Github. We conduct an empirical study on 625 repositories hosted on GitHub with Python as their primary language. We manually classify comments from a randomly selected sample of python repositories and then train and evaluate classifiers to automatically label comments as Relevant or Auxiliary. We extract the metadata of issues in each repository present in our dataset and perform various experiments to understand the correlation between code comments and issues. From our dataset of python repositories, we then plot a graph between the average time taken to resolve an issue and percentage of relevant comments in a repository to find if there is any relation or a pattern by which the latter affects the former. Our statistical approach of finding out the correlation between code comments and issues gives us the correlation factor by which code comments are related to issues. We conclude from our study that comments are indeed important and play an important role in solving issues of the project. We also found that increasing the percentage of relevant comments along with the source code can help in the reduction of the average number of days before an issue is resolved.",Code comments | Correlation | Empirical study | Issues | Machine learning | Python repositories,Proceedings of the ACM Symposium on Applied Computing,2020-03-30,Conference Paper,"Misra, Vishal;Reddy, Jakku Sai Krupa;Chimalakonda, Sridhar",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85099361933,10.1145/3422392.3422428,A Metadata Handling API for Framework Development: A Comparative Study,"Frameworks play an essential role in software development, providing not only code reuse, but also design reuse. Several Java frameworks and APIs such as Spring, JPA, and CDI rely on the use of metadata, mainly defined by code annotations. These frameworks usually use the Java Reflection API to consume code annotations, which only returns the annotations in a given code element. This strategy, however, is far from the needs of a real framework. The goal of this paper is to propose a novel API, named Esfinge Metadata, to assist in the development of frameworks based on metadata and applications based on custom annotations. Being based on annotations itself, this new API uses them to map metadata to class members. We carried out an experiment to evaluate our API and its impact on aspects such as code structure, complexity, and coupling, while also performing a comparison with the direct use of the Java Reflection API. The participants implemented a metadata-based framework based on realistic requirements in a sequence of 10 tasks that took an average of nine hours. As a result, participants that used our API maintained a more stable code evolution, regarding complexity and coupling as opposed to participants using the Java Reflection API, where the code metrics evolution and structure vary greatly.",Annotations | Code Metrics | Exploratory Experiment | Framework | Java | Meta-Framework | Metadata,ACM International Conference Proceeding Series,2020-10-21,Conference Paper,"Guerra, Eduardo;Lima, Phyllipe;Choma, Joelma;Nardes, Marco;Silva, Tiago;Lanza, Michele;Meirelles, Paulo",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85091597191,10.1007/978-3-030-59592-0_7,Automated Web Service Specification Generation Through a Transformation-Based Learning,"Web Application Programming Interface (API) allows third-party and subscribed users to access data and functions of a software application through the network or the Internet. Web APIs expose data and functions to the public users, authorized users or enterprise users. Web API providers publish API documentations to help users to understand how to interact with web-based API services, and how to use the APIs in their integration systems. The exponential raise of the number of public web service APIs may cause a challenge for software engineers to choose an efficient API. The challenge may become more complicated when web APIs updated regularly by API providers. In this paper, we introduce a novel transformation-based approach which crawls the web to collect web API documentations (unstructured documents). It generates a web API Language model from API documentations, employs different machine learning algorithms to extract information and produces a structured web API specification that compliant to Open API Specification (OAS) format. The proposed approach improves information extraction patterns and learns the variety of structured and terminologies. In our experiment, we collect a sheer number of web API documentations. Our evaluation shows that the proposed approach find RESTful API documentations with 75% accuracy, constructs API endpoints with 84%, constructs endpoint attributes with 95%, and assigns endpoints to attributes with an accuracy 98%. The proposed approach were able to produces more than 2,311 OAS web API Specifications.",Machine learning | Natural language processing | REST API | Web API service,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Bahrami, Mehdi;Chen, Wei Peng",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85084552078,10.1007/s11432-019-2720-1,Learning a graph-based classifier for fault localization,"Because software emerged, locating software faults has been intensively researched, culminating in various approaches and tools that have been applied in real development. Despite the success of these developments, improved tools are still demanded by programmers. Meanwhile, some programmers are reluctant to use any tools when locating faults in their development. The state-of-the-art situation can be naturally improved by learning how programmers locate faults. The rapid development of open-source software has accumulated many bug fixes. A bug fix is a specific type of comments containing a set of buggy files and their corresponding fixed files, which reveal how programmers repair bugs. Feasibly, an automatic model can learn fault locations from bug fixes, but prior attempts to achieve this vision have been prevented by various technical challenges. For example, most bug fixes are not compilable after checking out, which hinders analyzing bug fixes by most advanced static/dynamic tools. This paper proposes an approach called ClaFa that trains a graph-based fault classifier from bug fixes. ClaFa is built on a recent partial-code tool called Grapa, which enables the analysis of partial programs by the complete code tool called WALA. Once Grapa has built a program dependency graph from a bug fix, ClaFa compares the graph from the buggy code with the graph from the fixed code, locates the buggy nodes, and extracts the various graph features of the buggy and clean nodes. Based on the extraction result, ClaFa trains a classifier that combines Adaboost and decision tree learning. The trained ClaFa can predict whether a node of a program dependency graph is buggy or clean. We evaluate ClaFa on thousands of buggy files collected from four open-source projects: Aries, Mahout, Derby, and Cassandra. The f-scores of ClaFa achieves are approximately 80% on all projects.",bug fix analysis | fault classifier | partial code analysis,Science China Information Sciences,2020-06-01,Article,"Zhong, Hao;Mei, Hong",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85088835880,10.1007/s11432-019-2831-6,A quantitative benefit evaluation of code search platform for enterprises,,,Science China Information Sciences,2020-09-01,Letter,"Shi, Zhan;Tang, Jie;Yu, Hao;Xing, Yongxu;Liu, Zhiwei;Bai, Wei;Li, Tao",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85090509513,10.18293/SEKE2020-062,An ensemble approach to detect code comment inconsistencies using topic modeling,"In modern era, the size of software is increasing, as a result a large number of software developers are assigned into software projects. To have a better understanding about source codes these developers are highly dependent on code comments. However, comments and source codes are often inconsistent in a software project because keeping comments up-to-date is often neglected. Since these comments are written in natural language and consist of context related topics from source codes, manual inspection is needed to ensure the quality of the comment associated with the corresponding code. Existing approaches consider entire texts as feature, which fail to capture dominant topics to build the bridge between comments and its corresponding code. In this paper, an effective approach has been proposed to automatically extract dominant topics as well as to identify the consistency between a code snippet and its corresponding comment. This approach is evaluated with a benchmark dataset containing 2.8K Java code-comment pairs, which showed that proposed approach has achieved better performance with respect to the several evaluation metrics than the existing state-of-the-art Support Vector Machine on vector space model.",Code Comment | Software Artifact Analysis | Source Code | Topic Modeling,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2020-01-01,Conference Paper,"Rabbi, Fazle;Nazmul Haque, Md;Eusha Kadir, Md;Saeed Siddik, Md;Kabir, Ahmedul",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85075714098,10.1002/spe.2772,Does your code need comment?,"Code comments convey information about the programmers' intention in a more explicit but less rigorous manner than source code. This information can assist programmers in various tasks, such as code comprehension, reuse, and maintenance. To better understand the properties of the comments existing in the source code, we analyzed more than 450 000 comments across 136 popular open-source software systems coming different domains. We found that the methods involving header comments and internal comments were shown low percentages in software systems, ie, 4.4% and 10.27%, respectively. As an application of our findings, we propose an automatic approach to determine whether a method needs a header comment, known as commenting necessity identification. Specifically, we identify the important factors for determining the commenting necessity of a method and extract them as structural features, syntactic features, and textual features. Then, by applying machine learning techniques and noise-handling techniques, we achieve a precision of 88.5% on eight open-source software from GitHub. The encouraging experimental results demonstrate the feasibility and effectiveness of our approach.",code comment analysis | comment density analysis | commenting necessity | header comment,Software - Practice and Experience,2020-03-01,Conference Paper,"Huang, Yuan;Jia, Nan;Shu, Junhuai;Hu, Xinyu;Chen, Xiangping;Zhou, Qiang",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85091366273,10.1002/spe.2893,Effective approaches to combining lexical and syntactical information for code summarization,"Natural language summaries of source codes are important during software development and maintenance. Recently, deep learning based models have achieved good performance on the task of automatic code summarization, which encode token sequence or abstract syntax tree (AST) of code with neural networks. However, there has been little work on the efficient combination of lexical and syntactical information of code for better summarization quality. In this paper, we propose two general and effective approaches to leveraging both types of information: a convolutional neural network that aims to better extract vector representation of AST node for downstream models; and a Switch Network that learns an adaptive weight vector to combine different code representations for summary generation. We integrate these approaches into a comprehensive code summarization model, which includes a sequential encoder for token sequence of code and a tree based encoder for its AST. We evaluate our model on a large Java dataset. The experimental results show that our model outperforms several state-of-the-art models on various metrics, and the proposed approaches contribute a lot to the improvements.",code summarization | deep learning | program comprehension,Software - Practice and Experience,2020-12-01,Article,"Zhou, Ziyi;Yu, Huiqun;Fan, Guisheng",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85087511768,10.1007/s11219-020-09520-3,Predicting technical debt from commit contents: reproduction and extension with automated feature selection,"Self-admitted technical debt refers to sub-optimal development solutions that are expressed in written code comments or commits. We reproduce and improve on a prior work by Yan et al. (2018) on detecting commits that introduce self-admitted technical debt. We use multiple natural language processing methods: Bag-of-Words, topic modeling, and word embedding vectors. We study 5 open-source projects. Our NLP approach uses logistic Lasso regression from Glmnet to automatically select best predictor words. A manually labeled dataset from prior work that identified self-admitted technical debt from code level commits serves as ground truth. Our approach achieves + 0.15 better area under the ROC curve performance than a prior work, when comparing only commit message features, and + 0.03 better result overall when replacing manually selected features with automatically selected words. In both cases, the improvement was statistically significant (p < 0.0001). Our work has four main contributions, which are comparing different NLP techniques for SATD detection, improved results over previous work, showing how to generate generalizable predictor words when using multiple repositories, and producing a list of words correlating with SATD. As a concrete result, we release a list of the predictor words that correlate positively with SATD, as well as our used datasets and scripts to enable replication studies and to aid in the creation of future classifiers.",Data mining | Latent Dirichlet allocation | Logistic regression | Natural language processing | Topic modeling | Word embeddings,Software Quality Journal,2020-12-01,Article,"Rantala, Leevi;Mäntylä, Mika",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85091706173,10.1145/3366424.3384363,Open Geodata Reuse: Towards Natural Language Interfaces to Web APIs,"Open Government datasets have been flooding the Web recently, and Application Programming Interfaces (APIs) are key to the development of services on top of these datasets. An issue of current APIs worldwide is that their learnability is limited. This work has explored the potential of querying APIs using natural language terms to mitigate that issue. A user study with 20 participants has demonstrated that a natural-language-based, along with an order-agnostic approach to API design can produce easily learnable APIs for both novice and experienced API users. These insights can pave the way for a paradigm change on Web-API design for open geodata retrieval and beyond.",API Design Conventions | API Learnability | API Usability | Geographic Information Retrieval | Open Government Data | Smart Cities,"The Web Conference 2020 - Companion of the World Wide Web Conference, WWW 2020",2020-04-20,Conference Paper,"Degbelo, Auriol;Sherpa, Ang",Exclude,out of comments scope
10.1016/j.jss.2022.111515,2-s2.0-85086574266,10.1145/3366423.3380295,Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning,"Code summarization generates brief natural language description given a source code snippet, while code retrieval fetches relevant source code given a natural language query. Since both tasks aim to model the association between natural language and programming language, recent studies have combined these two tasks to improve their performance. However, researchers have yet been able to effectively leverage the intrinsic connection between the two tasks as they train these tasks in a separate or pipeline manner, which means their performance can not be well balanced. In this paper, we propose a novel end-to-end model for the two tasks by introducing an additional code generation task. More specifically, we explicitly exploit the probabilistic correlation between code summarization and code generation with dual learning, and utilize the two encoders for code summarization and code generation to train the code retrieval task via multi-task learning. We have carried out extensive experiments on an existing dataset of SQL and Python, and results show that our model can significantly improve the results of the code retrieval task over the-state-of-art models, as well as achieve competitive performance in terms of BLEU score for the code summarization task.",code generation | code retrieval | code summarization | dual learning,"The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020",2020-04-20,Conference Paper,"Ye, Wei;Xie, Rui;Zhang, Jinglei;Hu, Tianxiang;Wang, Xiaoyin;Zhang, Shikun",Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85085935787,10.1145/3380625.3380649,A survey of automatic generation of code comments,"Code comments are a valuable form of documentation attached to code that is the most intuitive and efficient way for programmers to understand software code. Good code comments can help programmers quickly understand the role of source code and facilitate understanding of programs and software maintenance tasks. However, in practice, most programmers only pay attention to the code and ignore the comments and documents, which makes the program's readability and maintainability greatly reduced. Based on the meaning of code comments, this paper discusses the current progress in the field of code comments research, adopts the comparative analysis method, focuses on the classification research of the methods and tools for automatic generation of code comments, expounds its advantages and disadvantages, and reveals the issues that need further study.",Automatic Generation | Code Comments | Comments method | Comments tool,ACM International Conference Proceeding Series,2020-01-17,Conference Paper,"Zhao, Fengrong;Zhao, Junqi;Bai, Yang",Exclude,A survey paper
10.1016/j.jss.2022.111515,,10.7936/97H3-M963,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.1109/tse.2020.2986415,,,,,,,,Exclude,out of comments scope
10.1016/j.jss.2022.111515,,10.14210/cotb.v11n1.p277-284,,,,,,,,Exclude,Does not assess comments
10.1016/j.jss.2022.111515,2-s2.0-85120323205,10.4108/eai.12-10-2019.2296547,Code Comment Assessment Development for Basic Programming Subject using Online Judge,"The computer program source code must have good documentation on it to be more understandable by others. One of the documentation is code comments. Therefore, university students need to learn and able to write comments. Some university students are familiar with an online judge as a tool for learning programming. An online judge usually does not check how the source code written instead of the output only. This research is aimed at implementing the code comment assessment module for online judge in the order it can check the comments in the source code which is uploaded by students. The comments on this module are categorized into two parts. There are header and process comments. The comments dataset have taken from the online judge which is used before as a learning tool for basic programming subject.",Basic programming subject | Code comment | Comment assessment | Online judge | Source code,"Proceedings of the 7th Mathematics, Science, and Computer Science Education International Seminar, MSCEIS 2019",2020-01-01,Conference Paper,"Sukamto, Rosa Ariani;Megasari, Rani;Piantari, Erna;Rischa, M. Nabillah Fihira",Exclude,Not a full paper
10.1016/j.jss.2022.111515,2-s2.0-85081325147,10.1007/978-981-15-0751-9_72,Revisiting Requirements Documentation Techniques and Challenges,"Requirement documentation plays a substantial role in requirement engineering (RE) process. It is a procedure of forming the document, which is used to communicate functions, events and operations to many stakeholders in software development procedure. The documents similarly perform as a proof of all the processes and actions included in software development. The precise level of attention to clearly share requirements and the vital concepts makes software projects successful. The comprehensive and approved requirements are specified and documented to aid as the root for advanced system development activities. In spite of various trends setting research outcomes, many practitioners and researchers have observed challenges and issues with requirement documentation phase in previous literature. However, very slight devotion has been given to explore the variety of challenges of documentation phase in various documentation methods. The study unveils the need for automation and standardization of requirement documentation process and techniques. Additionally, this paper attempts to analyze the substantial challenges of requirement documentation.",Issues and challenges in requirements documentation | Requirements documentation (RD) | Requirements documentation techniques | Requirements engineering (RE) | Software engineering,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Sharma, Shreta;Pandey, Santosh K.",Exclude,out of comments scope
10.1016/j.jss.2022.111515,,,,,,,,,,Exclude,Not a peer reviewed paper or is a pre-print
